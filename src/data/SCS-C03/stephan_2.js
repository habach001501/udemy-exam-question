export const stephan_2 = {
  count: 65,
  next: null,
  previous: null,
  results: [
    {
      _class: "assessment",
      id: 76165412,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company's security policy mandates enforcing VPC Flow Logs for all the VPCs defined on AWS. A Security Engineer has been tasked to automate this compliance check and subsequently inform the governance teams if any VPC is found to be non-compliant.</p>\n\n<p>Which steps will you combine for automating the process to meet the compliance guidelines? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create a custom Config rule that uses this Lambda function as its source</strong></p>\n\n<p><strong>Create a Lambda function that polls Config to detect non-compliant resources daily and send notifications via Amazon SNS</strong></p>\n\n<p>To create a custom rule that audits AWS resources for security compliance by enabling VPC Flow Logs for an Amazon Virtual Private Cloud (VPC), the following steps have to be followed:</p>\n\n<ol>\n<li>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant.</li>\n<li>Create a custom Config rule that uses the Lambda function created in Step 1 as the source.</li>\n<li>Create a Lambda function that polls Config to detect non-compliant resources daily and send notifications via Amazon SNS.</li>\n</ol>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q19-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/">https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create an Amazon CloudWatch Event rule that triggers when the state of the earlier declared Lambda function changes to non-compliant</strong> - AWS Lambda has predefined states - such as active, inactive, pending, failed - that can be used while implementing business logic. Lambda does not propagate a <code>non-compliant</code> state. To understand the full list of states that AWS Lambda supports, please check the link in the references.</p>\n\n<p><strong>Publish VPC Flow Logs to Amazon S3 bucket and query the data with AWS Athena for determining the non-compliant resources</strong> - This option has been added as a distractor. Just querying the VPC Flow Logs using Athena is not sufficient as this option does not provide any information about the total numbers of VPCs set up in the given AWS account and how many of those VPCs have Flow Logs enabled. So it cannot detect non-compliant status for the given requirement.</p>\n\n<p><strong>Create a Lambda function that checks the AWS Athena query status on a daily basis for detecting any non-compliant resources daily and sending notifications via Amazon SNS</strong> - This option has been added as a distractor. This option relies on Athena queries for detecting any non-compliant resources. As mentioned earlier, any Athena query running only on the VPC Flow Logs data would have insufficient information to detect non-compliant status for the given requirement.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/">https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/compute/coming-soon-expansion-of-aws-lambda-states-to-all-functions/">https://aws.amazon.com/blogs/compute/coming-soon-expansion-of-aws-lambda-states-to-all-functions/</a></p>\n',
        answers: [
          "<p>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create a custom Config rule that uses this Lambda function as its source</p>",
          "<p>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create an Amazon CloudWatch Event rule that triggers when the state of the earlier declared Lambda function changes to non-compliant</p>",
          "<p>Create a Lambda function that polls Config to detect non-compliant resources daily and send notifications via Amazon SNS</p>",
          "<p>Publish VPC Flow Logs to Amazon S3 bucket and query the data with AWS Athena for determining the non-compliant resources</p>",
          "<p>Create a Lambda function that checks the AWS Athena query status on a daily basis for detecting any non-compliant resources daily and sending notifications via Amazon SNS</p>",
        ],
      },
      correct_response: ["a", "c"],
      section: "Management and Security Governance",
      question_plain:
        "A company's security policy mandates enforcing VPC Flow Logs for all the VPCs defined on AWS. A Security Engineer has been tasked to automate this compliance check and subsequently inform the governance teams if any VPC is found to be non-compliant.\n\nWhich steps will you combine for automating the process to meet the compliance guidelines? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165386,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The IT Security team at a financial services firm has informed that a user's AWS access key has been found on the internet. As a security engineer, you must ensure that the access key is immediately disabled and the user's activities must be assessed for a potential breach.</p>\n\n<p>Which steps must be taken to meet the above needs?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Delete or rotate the user’s key. Review the AWS CloudTrail logs in all AWS regions and delete any unauthorized resources created or updated</strong></p>\n\n<p>Deleting or rotating the user’s access key ensures that it is not further used for any unauthorized activities. AWS CloudTrail logs will log the user access key usage which will help in tracking the AWS resources for which the key has been used. This information is valuable in narrowing down on any unauthorized resources created by using the access key or any existing resources modified by using the access key.</p>\n\n<p>Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK). As a best practice, use temporary security credentials (IAM roles) instead of access keys and disable any AWS account root user access keys.</p>\n\n<p>If you still need to use long-term access keys, you can create, modify, view, or rotate your access keys (access key IDs and secret access keys). You can have a maximum of two access keys. This allows you to rotate the active keys according to best practices.</p>\n\n<p>Rotating IAM user access keys from the console:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q6-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_RotateAccessKey">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_RotateAccessKey</a></p>\n\n<p>As AWS CloudTrail logs API activity for supported services, it provides an audit trail of your AWS account that you can use to track the history of an adversary. The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights, and does not require any tools to be installed. Refer to the complete list of steps below:</p>\n\n<p>Steps to investigate AWS CloudTrail:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q6-i2.jpg">\nvia - <a href="https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/">https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Delete the IAM user and all the resources created by the user. Create fresh user credentials and relaunch the resources from this user</strong> - Deleting the IAM user will also remove all the bonafide resources that are created by the user account (resources, policies, tags, S3 buckets, etc). Hence, this option is not a workable solution.</p>\n\n<p><strong>Call on the user to remove the access credentials from the internet. Rotate the user\'s key and re-deploy all the resources with the new credentials</strong> - This option does not mention the steps needed to access the historic usage of the access key, which is crucial to identify any potential breach.</p>\n\n<p><strong>Call on the user to remove the access credentials from the internet. Report abuse to AWS Trust &amp; Safety team</strong> - This option does not address any of the identified issues mentioned in the use case, such as, disabling the current key and assessing any potential breach.</p>\n\n<p>References:</p>\n\n<p><a href="https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/">https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/security-logging-and-monitoring.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/security-logging-and-monitoring.html</a></p>\n',
        answers: [
          "<p>Delete or rotate the user’s key. Review the AWS CloudTrail logs in all AWS regions and delete any unauthorized resources created or updated</p>",
          "<p>Delete the IAM user and all the resources created by the user. Create fresh user credentials and relaunch the resources from this user</p>",
          "<p>Call on the user to remove the access credentials from the internet. Rotate the user's key and re-deploy all the resources with the new credentials</p>",
          "<p>Call on the user to remove the access credentials from the internet. Report abuse to AWS Trust &amp; Safety team</p>",
        ],
      },
      correct_response: ["a"],
      section: "Identity and Access Management",
      question_plain:
        "The IT Security team at a financial services firm has informed that a user's AWS access key has been found on the internet. As a security engineer, you must ensure that the access key is immediately disabled and the user's activities must be assessed for a potential breach.\n\nWhich steps must be taken to meet the above needs?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165388,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The latest guidelines issued by the security team at a company mandate an application to block HTTP requests that don't have a User-Agent header or have a specific User-Agent in the request.</p>\n\n<p>How will you block these requests using AWS WAF?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Block requests that contain a specific User-Agent in the request using custom Rules. Block requests that don’t contain a User-Agent header using either AWS Managed Rules or custom rules</strong></p>\n\n<p>By default, AWS WAF filters don\'t check whether the HTTP request parameters are present or not. To check whether the HTTP request parameters are present or not, do the following:</p>\n\n<p>Block requests that don\'t contain a User-Agent header using AWS Managed Rules.</p>\n\n<p>-or-</p>\n\n<p>Block requests that don\'t contain a User-Agent header or block traffic if the requests contain a specific User-Agent using custom rules.</p>\n\n<p>The following rules inspect requests missing the HTTP User-Agent header and User-Agent strings that don\'t seem to be from a web browser:</p>\n\n<p>NoUserAgent_HEADER\nThis rule is from the Core rule set (CRS) managed rule group. This rule inspects for requests that are missing the HTTP User-Agent header.</p>\n\n<p>SignalNonBrowserUserAgent\nThis rule is from the AWS WAF Bot Control rule group. This rule inspects for User-Agent strings that don\'t seem to be from a web browser including requests with no User-Agent.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q7-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/">https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q7-i2.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/">https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Block requests that contain a specific User-Agent in the request using AWS Managed Rules. Block requests that don’t contain a User-Agent header using security group rules</strong></p>\n\n<p><strong>Block requests that contain a specific User-Agent in the request using custom rules. Block requests that don’t contain a User-Agent header using security group rules</strong></p>\n\n<p>These two options have been added as distractors. Security group rules cannot be associated with a WAF. You should also note that security group rules can only allow requests.</p>\n\n<p><strong>Block requests that contain a specific User-Agent in the request using AWS Managed Rules. Block requests that don’t contain a User-Agent header using either AWS Managed Rules or custom rules</strong> - AWS Managed Rules can only be used to block requests that don’t contain a User-Agent header. You cannot use the AWS Managed Rules to block requests that contain a specific User-Agent.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-regex-pattern-set-match.html">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-regex-pattern-set-match.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/">https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/</a></p>\n',
        answers: [
          "<p>Block requests that contain a specific User-Agent in the request using AWS Managed Rules. Block requests that don’t contain a User-Agent header using security group rules</p>",
          "<p>Block requests that contain a specific User-Agent in the request using AWS Managed Rules. Block requests that don’t contain a User-Agent header using either AWS Managed Rules or custom rules</p>",
          "<p>Block requests that contain a specific User-Agent in the request using custom rules. Block requests that don’t contain a User-Agent header using security group rules</p>",
          "<p>Block requests that contain a specific User-Agent in the request using custom Rules. Block requests that don’t contain a User-Agent header using either AWS Managed Rules or custom rules</p>",
        ],
      },
      correct_response: ["d"],
      section: "Security Logging and Monitoring",
      question_plain:
        "The latest guidelines issued by the security team at a company mandate an application to block HTTP requests that don't have a User-Agent header or have a specific User-Agent in the request.\n\nHow will you block these requests using AWS WAF?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165390,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A healthcare company only operates in the <code>us-east-1</code> region and stores encrypted data in S3 using SSE-KMS. Since the company wants to improve the backup and recovery architecture, it wants the encrypted data in S3 to be replicated into the <code>us-west-1</code> AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions.</p>\n\n<p>Which of the following represents the best solution to address these requirements?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Set up a new S3 bucket in the <code>us-east-1</code> region with replication enabled from this new bucket into another bucket in <code>us-west-1</code> region. Enable SSE-KMS encryption on the new bucket in <code>us-east-1</code> region by using an AWS KMS multi-region key. Copy the existing data from the current S3 bucket in <code>us-east-1</code> region into this new S3 bucket in <code>us-east-1</code> region</strong></p>\n\n<p>AWS KMS supports multi-region keys, which are AWS KMS keys in different AWS regions that can be used interchangeably – as though you had the same key in multiple regions. Each set of related multi-region keys has the same key material and key ID, so you can encrypt data in one AWS region and decrypt it in a different AWS region without re-encrypting or making a cross-region call to AWS KMS.</p>\n\n<p>You can use multi-region AWS KMS keys in Amazon S3. However, Amazon S3 currently treats multi-region keys as though they were single-region keys, and does not use the multi-region features of the key.</p>\n\n<p>Multi-region AWS KMS keys:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q8-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html">https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html</a></p>\n\n<p>For the given use case, you must create a new bucket in the <code>us-east-1</code> region with replication enabled from this new bucket into another bucket in <code>us-west-1</code> region. This would ensure that the data is available in another region for backup and recovery purposes. You should also enable SSE-KMS encryption on the new bucket in <code>us-east-1</code> region by using an AWS KMS multi-region key so that the data can be encrypted and decrypted using the same key in both AWS regions. Since the existing data in the current bucket was encrypted using the AWS KMS key restricted to the <code>us-east-1</code> region, so data must be copied to the new bucket in <code>us-east-1</code> region for replication as well as multi-region KMS key-based encryption to kick-in.</p>\n\n<p>To require server-side encryption of all objects in a particular Amazon S3 bucket, you can use a policy. For example, the following bucket policy denies the upload object (s3:PutObject) permission to everyone if the request does not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS.</p>\n\n<pre><code>{\n   "Version":"2012-10-17",\n   "Id":"PutObjectPolicy",\n   "Statement":[{\n         "Sid":"DenyUnEncryptedObjectUploads",\n         "Effect":"Deny",\n         "Principal":"*",\n         "Action":"s3:PutObject",\n         "Resource":"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*",\n         "Condition":{\n            "StringNotEquals":{\n               "s3:x-amz-server-side-encryption":"aws:kms"\n            }\n         }\n      }\n   ]\n}\n</code></pre>\n\n<p>The following example IAM policies show statements for using AWS KMS server-side encryption with replication.</p>\n\n<p>In this example, the encryption context is the object ARN. If you use SSE-KMS with an S3 Bucket Key enabled, you must use the bucket ARN as the encryption context.</p>\n\n<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [{\n            "Action": ["kms:Decrypt"],\n            "Effect": "Allow",\n            "Resource": "List of AWS KMS key ARNs used to encrypt source objects.",\n            "Condition": {\n                "StringLike": {\n                    "kms:ViaService": "s3.source-bucket-region.amazonaws.com",\n                    "kms:EncryptionContext:aws:s3:arn": "arn:aws:s3:::source-bucket-name/key-prefix1/*"\n                }\n            }\n        },\n\n        {\n            "Action": ["kms:Encrypt"],\n            "Effect": "Allow",\n            "Resource": "AWS KMS key ARNs (for the AWS Region of the destination bucket 1). Used to encrypt object replicas created in destination bucket 1.",\n            "Condition": {\n                "StringLike": {\n                    "kms:ViaService": "s3.destination-bucket-1-region.amazonaws.com",\n                    "kms:EncryptionContext:aws:s3:arn": "arn:aws:s3:::destination-bucket-name-1/key-prefix1/*"\n                }\n            }\n        },\n        {\n            "Action": ["kms:Encrypt"],\n            "Effect": "Allow",\n            "Resource": "AWS KMS key ARNs (for the AWS Region of destination bucket 2). Used to encrypt object replicas created in destination bucket 2.",\n            "Condition": {\n                "StringLike": {\n                    "kms:ViaService": "s3.destination-bucket-2-region.amazonaws.com",\n                    "kms:EncryptionContext:aws:s3:arn": "arn:aws:s3:::destination-bucket-2-name/key-prefix1*"\n                }\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the AWS KMS single region key used for the current S3 bucket into an AWS KMS multi-region key. Enable S3 batch replication for the existing data in the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region</strong> - S3 batch replication can certainly be used to replicate the existing data in the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region.</p>\n\n<p>However, you cannot convert an existing single-Region key to a multi-Region key. This design ensures that all data protected with existing single-Region keys maintain the same data residency and data sovereignty properties. So this option is incorrect.</p>\n\n<p><strong>Enable replication for the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region. Share the existing AWS KMS key from <code>us-east-1</code> region to <code>us-west-1</code> region</strong> - You cannot share an AWS KMS key to another region, so this option is incorrect.</p>\n\n<p><strong>Set up a CloudWatch scheduled rule to invoke a Lambda function to copy the daily data from the source bucket in <code>us-east-1</code> region to the destination bucket in <code>us-west-1</code> region. Provide AWS KMS key access to the Lambda function for encryption and decryption operations on the data in the source and destination S3 buckets</strong> - This option is a distractor as the daily frequency of data replication would result in significant data loss in case of a disaster. In addition, this option involves significant development effort to create the functionality to reliably replicate the data from source to destination buckets. So this option is not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html">https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html</a></p>\n',
        answers: [
          "<p>Set up a CloudWatch scheduled rule to invoke a Lambda function to copy the daily data from the source bucket in <code>us-east-1</code> region to the destination bucket in <code>us-west-1</code> region. Provide AWS KMS key access to the Lambda function for encryption and decryption operations on the data in the source and destination S3 buckets</p>",
          "<p>Set up a new S3 bucket in the <code>us-east-1</code> region with replication enabled from this new bucket into another bucket in <code>us-west-1</code> region. Enable SSE-KMS encryption on the new bucket in <code>us-east-1</code> region by using an AWS KMS multi-region key. Copy the existing data from the current S3 bucket in <code>us-east-1</code> region into this new S3 bucket in <code>us-east-1</code> region</p>",
          "<p>Change the AWS KMS single region key used for the current S3 bucket into an AWS KMS multi-region key. Enable S3 batch replication for the existing data in the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region</p>",
          "<p>Enable replication for the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region. Share the existing AWS KMS key from <code>us-east-1</code> region to <code>us-west-1</code> region</p>",
        ],
      },
      correct_response: ["b"],
      section: "Data Protection",
      question_plain:
        "A healthcare company only operates in the us-east-1 region and stores encrypted data in S3 using SSE-KMS. Since the company wants to improve the backup and recovery architecture, it wants the encrypted data in S3 to be replicated into the us-west-1 AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions.\n\nWhich of the following represents the best solution to address these requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165392,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A security team configured an Amazon CloudWatch alarm to notify one of the team members when a metric breaches a defined threshold for multiple periods in a row. But, the CloudWatch alarm is notifying the team after just one breach of the threshold.</p>\n\n<p>What is the issue and how will you fix the CloudWatch alarm to behave as expected?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>CloudWatch alarm might be configured to treat a missing data point the same way as a breaching data point. Configure the alarm to evaluate missing data points as NOT BREACHING</strong></p>\n\n<p>Your CloudWatch alarm might be configured to treat a missing data point the same way as a breaching data point.</p>\n\n<p>You might have a series of non-breaching data points, followed by a single breaching data point, and then missing data points. Your alarm counts any missing data points following a breaching data point as additional breaches. This causes your alarm to notify you after only one data point breaches the defined threshold.</p>\n\n<p>When all data points are missing except one breaching data point, your alarm goes into an ALARM state. This happens when the oldest available breaching data point in the alarm\'s evaluation range is at least as old as the value of Datapoints to Alarm, and all other more recent data points are breaching or missing. This causes an ALARM state even if the total number of breaching data points is lower than the Datapoints to Alarm setting, and even when missing data is being treated as missing.</p>\n\n<p>Edit your alarm to do one or more of the following:\n1. If you don\'t want to be notified after a single breach when data points may be intermittent, change how missing data points are evaluated to NOT BREACHING.</p>\n\n<ol>\n<li><p>Use a longer period for a data point.</p></li>\n<li><p>Increase the number of evaluation periods before the alarm is triggered.</p></li>\n</ol>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q9-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#alarms-and-missing-data">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#alarms-and-missing-data</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudWatch alarm might not be configured to treat a missing data point the same way as a breaching data point. Configure the alarm to evaluate missing data points as BREACHING</strong> - This statement is incorrect, as it contradicts the explanation provided above.</p>\n\n<p><strong>If the CloudWatch alarm is unable to access the metric to be monitored, the alarm is raised as a default behavior</strong> - This option is invalid and given only as a distractor.</p>\n\n<p><strong>The metric must be reporting data only intermittently by design. For such metrics, AWS Lambda function is used to send continuous data as per business logic</strong> - Sometimes, not every expected data point for a metric gets reported to CloudWatch. For example, this can happen when a connection is lost, a server goes down, or when a metric reports data only intermittently by design. AWS Lambda function cannot be used for generating data points. Amazon CloudWatch can be configured to react a certain way when data is reported only intermittently.</p>\n\n<p>For each alarm, you can specify CloudWatch to treat missing data points as any of the following:</p>\n\n<ol>\n<li><p>notBreaching – Missing data points are treated as "good" and within the threshold</p></li>\n<li><p>breaching – Missing data points are treated as "bad" and breaching the threshold</p></li>\n<li><p>ignore – The current alarm state is maintained</p></li>\n<li><p>missing – If all data points in the alarm evaluation range are missing, the alarm transitions to INSUFFICIENT_DATA.</p></li>\n</ol>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#alarms-and-missing-data">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#alarms-and-missing-data</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-alarm-single-data-point/">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-alarm-single-data-point/</a></p>\n',
        answers: [
          "<p>If the CloudWatch alarm is unable to access the metric to be monitored, the alarm is raised as a default behavior</p>",
          "<p>The metric must be reporting data only intermittently by design. For such metrics, the AWS Lambda function is used to send continuous data as per business logic</p>",
          "<p>CloudWatch alarm might not be configured to treat a missing data point the same way as a breaching data point. Configure the alarm to evaluate missing data points as BREACHING</p>",
          "<p>CloudWatch alarm might be configured to treat a missing data point the same way as a breaching data point. Configure the alarm to evaluate missing data points as NOT BREACHING</p>",
        ],
      },
      correct_response: ["d"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A security team configured an Amazon CloudWatch alarm to notify one of the team members when a metric breaches a defined threshold for multiple periods in a row. But, the CloudWatch alarm is notifying the team after just one breach of the threshold.\n\nWhat is the issue and how will you fix the CloudWatch alarm to behave as expected?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165394,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>After a recent DDoS assault, the IT security team of a media company has asked the Security Engineer to revamp the security of the application to prevent future attacks. The website is hosted on an Amazon EC2 instance and data is maintained on Amazon RDS. A large part of the application data is static and this data is in the form of images.</p>\n\n<p>Which of the following steps can be combined to constitute the revamped security model? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Use Amazon Route 53 to distribute traffic</strong></p>\n\n<p><strong>Move the static content to Amazon S3, and front this with an Amazon CloudFront distribution. Configure another layer of protection by adding AWS Web Application Firewall (AWS WAF) to the CloudFront distribution</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting.</p>\n\n<p>AWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync – services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end users. Blocked requests are stopped before they reach your web servers.</p>\n\n<p>Route 53 DNS requests and subsequent application traffic routed through CloudFront are inspected inline. Always-on monitoring, anomaly detection, and mitigation against common infrastructure DDoS attacks such as SYN/ACK floods, UDP floods, and reflection attacks are built into both Route 53 and CloudFront.</p>\n\n<p>Route 53 is also designed to withstand DNS query floods, which are real DNS requests that can continue for hours and attempt to exhaust DNS server resources. Route 53 uses shuffle sharding and anycast striping to spread DNS traffic across edge locations and help protect the availability of the service.</p>\n\n<p>When used with Amazon CloudFront distribution, AWS Shield adds security against DDoS attacks.</p>\n\n<p>AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection.</p>\n\n<p>All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the Amazon EC2 instance with an Auto Scaling Group (ASG) to scale in case of a DDoS assault. Front the ASG with AWS Web Application Firewall (AWS WAF) for another layer of security</strong> - AWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync – services that AWS customers commonly use to deliver content for their websites and applications. WAF cannot be directly configured in front of an ASG, so this option is incorrect.</p>\n\n<p><strong>Use Global Accelerator to distribute traffic</strong> - Global Accelerator is effective in traffic distribution across AWS Regions. However, the given use case needs services that can help mitigate DDoS attacks.</p>\n\n<p><strong>Configure Amazon Inspector with AWS Security Hub to mitigate DDoS attacks by continual scanning that delivers near real-time vulnerability findings</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. It cannot be used to mitigate DDoS attacks.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/shield/">https://aws.amazon.com/shield/</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos-attacks-by-using-amazon-cloudfront-and-amazon-route-53/">https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos-attacks-by-using-amazon-cloudfront-and-amazon-route-53/</a></p>\n\n<p><a href="https://aws.amazon.com/waf/faqs/b">https://aws.amazon.com/waf/faqs/</a></p>\n',
        answers: [
          "<p>Use Amazon Route 53 to distribute traffic</p>",
          "<p>Configure the Amazon EC2 instance with an Auto Scaling Group (ASG) to scale in case of a DDoS assault. Front the ASG with AWS Web Application Firewall (AWS WAF) for another layer of security</p>",
          "<p>Use Global Accelerator to distribute traffic</p>",
          "<p>Move the static content to Amazon S3, and front this with an Amazon CloudFront distribution. Configure another layer of protection by adding AWS Web Application Firewall (AWS WAF) to the CloudFront distribution</p>",
          "<p>Configure Amazon Inspector with AWS Security Hub to mitigate DDoS attacks by continual scanning that delivers near real-time vulnerability findings</p>",
        ],
      },
      correct_response: ["a", "d"],
      section: "Infrastructure Security",
      question_plain:
        "After a recent DDoS assault, the IT security team of a media company has asked the Security Engineer to revamp the security of the application to prevent future attacks. The website is hosted on an Amazon EC2 instance and data is maintained on Amazon RDS. A large part of the application data is static and this data is in the form of images.\n\nWhich of the following steps can be combined to constitute the revamped security model? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165396,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company uses an Amazon S3 bucket to store its business-critical data. Recently, all the members of the development team, that access the given S3 bucket, have been given MFA devices. A security engineer must configure permissions such that access to the given S3 bucket is allowed only after MFA authentication.</p>\n\n<p>How will you implement this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create an IAM group having the development team users. Add a customer-managed policy with Deny Effect to the group for all s3:*actions with the condition defined as <code>"Condition" : { "BoolIfExists" : { "aws:MultiFactorAuthPresent" : "false" } }</code></strong></p>\n\n<p>AWS recommends the use of <code>BoolIfExists</code> operator to check whether a request is authenticated using MFA. The aws:MultiFactorAuthPresent key is not present when an API or CLI command is called with long-term credentials, such as user access key pairs. Therefore AWS recommends that when you check for this key that you use the <code>IfExists</code> versions of the condition operators, like so:</p>\n\n<pre><code>    "Effect" : "Deny",\n    "Condition" : { "BoolIfExists" : { "aws:MultiFactorAuthPresent" : "false" } }\n</code></pre>\n\n<p>This combination of Deny, BoolIfExists, and false denies requests that are not authenticated using MFA. Specifically, it denies requests from temporary credentials that do not include MFA. It also denies requests that are made using long-term credentials, such as AWS CLI or AWS API operations made using access keys. The *IfExists operator checks for the presence of the aws:MultiFactorAuthPresent key and whether or not it could be present, as indicated by its existence. Use this when you want to deny any request that is not authenticated using MFA. This is more secure but can break any code or scripts that use access keys to access the AWS CLI or AWS API.</p>\n\n<p>Recommended Combination to mandate MFA through the policy:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q11-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q11-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM group having the development team users. Add a customer-managed policy with Deny Effect to the group for all s3:*actions with the condition defined as <code>"Condition" : { "Bool" : { "aws:MultiFactorAuthPresent" : "false" } }</code></strong> - This combination of the Deny effect, Bool element, and false value denies requests that can be authenticated using MFA, but were not. This applies only to temporary credentials that support using MFA. This statement does not deny access to requests that are made using long-term credentials, or to requests that are authenticated using MFA. Use this example with caution because its logic is complicated and it does not test whether MFA-authentication was actually used. Therefore, this option is incorrect for the given use case.</p>\n\n<p><strong>Create an IAM group having the development team users. Add a customer managed policy with Allow Effect to the group for all s3:*actions with the condition defined as <code>"Condition": {"BoolIfExists": {"aws:MultiFactorAuthPresent": "true"}}</code></strong> - This condition matches either if the key exists and is present or if the key does not exist. This combination of Allow, BoolIfExists, and true allows requests that are authenticated using MFA, or requests that cannot be authenticated using MFA. This means that AWS CLI, AWS API, and AWS SDK operations are allowed when the requester uses their long-term access keys. This combination does not allow requests from temporary credentials that could, but do not include MFA. Therefore, this option is incorrect for the given use case.</p>\n\n<p><strong>Create an IAM group having the development team users. Add a customer-managed policy with Allow Effect to the group for all s3:*actions with the condition defined as <code>"Condition" : { "BoolIfExists" : { "aws:MultiFactorAuthPresent" : "false" } }</code></strong> - This allows any request that is not authenticated using MFA. This is the opposite of what the use case demands.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent</a></p>\n',
        answers: [
          '<p>Create an IAM group having the development team users. Add a customer-managed policy with Deny Effect to the group for all s3:*actions with the condition defined as <code>"Condition" : { "Bool" : { "aws:MultiFactorAuthPresent" : "false" } }</code></p>',
          '<p>Create an IAM group having the development team users. Add a customer-managed policy with Allow Effect to the group for all s3:*actions with the condition defined as <code>"Condition" : { "BoolIfExists" : { "aws:MultiFactorAuthPresent" : "false" } }</code></p>',
          '<p>Create an IAM group having the development team users. Add a customer-managed policy with Deny Effect to the group for all s3:*actions with the condition defined as <code>"Condition" : { "BoolIfExists" : { "aws:MultiFactorAuthPresent" : "false" } }</code></p>',
          '<p>Create an IAM group having the development team users. Add a customer managed policy with Allow Effect to the group for all s3:*actions with the condition defined as <code>"Condition": {"BoolIfExists": {"aws:MultiFactorAuthPresent": "true"}}</code></p>',
        ],
      },
      correct_response: ["c"],
      section: "Data Protection",
      question_plain:
        "A company uses an Amazon S3 bucket to store its business-critical data. Recently, all the members of the development team, that access the given S3 bucket, have been given MFA devices. A security engineer must configure permissions such that access to the given S3 bucket is allowed only after MFA authentication.\n\nHow will you implement this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165398,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A web application is hosted on Amazon EC2 instances that are fronted by Application Load Balancer (ALB) configured with an Auto Scaling group (ASG). Enhanced security is provided to the ALB by AWS WAF web ACLs. As per the company's security policy, AWS CloudTrail is activated and logs are configured to be stored on Amazon S3 and CloudWatch Logs.</p>\n\n<p>A discount sales offer was run on the application for a week. The support team has noticed that a few of the instances have rebooted taking down the log files and all temporary data with them. Initial analysis has confirmed that the incident took place during off-peak hours. Even though the incident did not cause any sales or revenue loss, the CTO has asked the security team to fix the security error that has allowed the incident to go unnoticed and eventually untraceable.</p>\n\n<p>As Security Engineer, which series of steps will you implement to permanently record all traffic coming into the application?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Configure the WAF web ACL to deliver logs to Amazon Kinesis Data Firehose, which should be configured to eventually store the logs in an Amazon S3 bucket. Use Athena to query the logs for errors and tracking</strong></p>\n\n<p>The logging destinations that you can choose from for your AWS WAF logs are:</p>\n\n<ol>\n<li>Amazon CloudWatch Logs</li>\n<li>Amazon Simple Storage Service</li>\n<li>Amazon Kinesis Data Firehose</li>\n</ol>\n\n<p>To send logs to Amazon Kinesis Data Firehose, you send logs from your web ACL to an Amazon Kinesis Data Firehose with a configured storage destination. After you enable logging, AWS WAF delivers logs to your storage destination through the HTTPS endpoint of Kinesis Data Firehose.</p>\n\n<p>One AWS WAF log is equivalent to one Kinesis Data Firehose record. If you typically receive 10,000 requests per second and you enable full logs, you should have 10,000 records per second setting in Kinesis Data Firehose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the WAF web ACL to deliver logs to Amazon CloudTrail and create a trail that applies to all Regions. This delivers log files from all Regions to an S3 bucket. Use Athena to query the logs for errors and tracking</strong> - As discussed above, the logging destinations that you can choose from for your AWS WAF logs are Amazon CloudWatch Logs, Amazon Simple Storage Service, and Amazon Kinesis Data Firehose. Amazon CloudTrail is not a valid destination for WAF ACL logs.</p>\n\n<p><strong>To capture information about the IP traffic going to and from network interfaces, configure VPC Flow Logs to be directly streamed to Kinesis Data Streams and create alarms for automatic monitoring</strong> - VPC Flow You should also note that VPC Flow Logs cannot capture all traffic coming into the application as these can only capture information about the IP traffic going to and from network interfaces in your VPC. In addition, VPC Flow Logs can be directly published only to the following destinations: Amazon CloudWatch Logs, Amazon S3, or Amazon Kinesis Data Firehose. So this option is incorrect.</p>\n\n<p><strong>Configure Elastic Load Balancing to write access logs to Amazon Kinesis Data Firehose. The logs can be further directed from Firehose into an Amazon S3 bucket for further analysis and reporting</strong> - Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Elastic Load Balancing access logs are stored in Amazon S3 buckets and it is not possible to directly write the logs to Kinesis Data Firehose.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/about-aws/whats-new/2021/12/awf-waf-cloudwatch-log-s3-bucket/">https://aws.amazon.com/about-aws/whats-new/2021/12/awf-waf-cloudwatch-log-s3-bucket/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html">https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n',
        answers: [
          "<p>Configure the WAF web ACL to deliver logs to Amazon CloudTrail and create a trail that applies to all Regions. This delivers log files from all Regions to an S3 bucket. Use Athena to query the logs for errors and tracking</p>",
          "<p>Configure the WAF web ACL to deliver logs to Amazon Kinesis Data Firehose, which should be configured to eventually store the logs in an Amazon S3 bucket. Use Athena to query the logs for errors and tracking</p>",
          "<p>To capture information about the IP traffic going to and from network interfaces, configure VPC Flow Logs to be directly streamed to Kinesis Data Streams and create alarms for automatic monitoring</p>",
          "<p>Configure Elastic Load Balancing to write access logs to Amazon Kinesis Data Firehose. The logs can be further directed from Firehose into an Amazon S3 bucket for further analysis and reporting</p>",
        ],
      },
      correct_response: ["b"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A web application is hosted on Amazon EC2 instances that are fronted by Application Load Balancer (ALB) configured with an Auto Scaling group (ASG). Enhanced security is provided to the ALB by AWS WAF web ACLs. As per the company's security policy, AWS CloudTrail is activated and logs are configured to be stored on Amazon S3 and CloudWatch Logs.\n\nA discount sales offer was run on the application for a week. The support team has noticed that a few of the instances have rebooted taking down the log files and all temporary data with them. Initial analysis has confirmed that the incident took place during off-peak hours. Even though the incident did not cause any sales or revenue loss, the CTO has asked the security team to fix the security error that has allowed the incident to go unnoticed and eventually untraceable.\n\nAs Security Engineer, which series of steps will you implement to permanently record all traffic coming into the application?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165400,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A standard three-tier application is hosted on Amazon EC2 instances that are fronted by an Application Load Balancer. The application maintenance team has reported several small-scale malicious attacks on the application. The project manager has decided to ramp up the security of the application.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend as part of the best practices to scan and mitigate the known vulnerabilities?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Configure the application security groups to ensure that only the necessary ports are open. Use Amazon Inspector to periodically scan the EC2 instances for vulnerabilities</strong></p>\n\n<p>When you have Amazon EC2 instances behind an Application Load Balancer, the instances themselves might not need to be publicly accessible. Instead, you could provide users with access to the Application Load Balancer on certain TCP ports and allow only the Application Load Balancer to communicate with the instances. All internet traffic to a security group is implicitly denied unless you create an allow rule to permit the traffic.</p>\n\n<p>For example, if you have a web application that uses Elastic Load Balancing and multiple Amazon EC2 instances, you might decide to create one security group for the Elastic Load Balancing (Elastic Load Balancing security group) and one for the instances (web application server security group). You can then create an allow rule to permit internet traffic to the ELB security group, and another rule to permit traffic from the ELB security group to the web application server security group. This ensures that internet traffic can’t directly communicate with your Amazon EC2 instances, which makes it more difficult for an attacker to learn about and impact your application.</p>\n\n<p>To scan for known vulnerabilities, use Amazon Inspector. Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure.</p>\n\n<p>You can enable Amazon Inspector for your entire organization or an individual account with a few clicks in the AWS Management Console. Once enabled, Amazon Inspector automatically discovers running Amazon EC2 instances and Amazon ECR repositories and immediately starts continually scanning workloads for software vulnerabilities and unintended network exposure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the application security groups to ensure that only the necessary ports are open. Use Amazon Systems Manager to periodically scan the EC2 instances for vulnerabilities</strong> -  The Amazon Inspector uses the Systems Manager (SSM) agent to collect the software application inventory of the Amazon EC2 instances. Then, Inspector scans this data and identifies software vulnerabilities, a crucial step in vulnerability management. The Systems Manager itself cannot scan the EC2 instances for vulnerabilities.</p>\n\n<p><strong>Use AWS Key Management Service (KMS) to encrypt the traffic between the client and application servers. Configure the application security groups to ensure that only the necessary ports are open</strong> - This option is a distractor as KMS cannot be used to encrypt the traffic between the client and application servers. AWS Key Management Service (AWS KMS) lets you create, manage, and control cryptographic keys across your applications. For the given scenario, you can use HTTPS to enable website encryption by running HTTP over the Transport Layer Security (TLS) protocol.</p>\n\n<p><strong>Install AWS Certificate Manager (ACM) SSL/TLS certificate on the EC2 instances to secure traffic moving to and from the application servers</strong> - ACM certificates cannot be installed on Amazon EC2 instances. An exception to this is a public ACM certificate that can be installed on Amazon EC2 instances that are connected to a Nitro Enclave. ACM certificates are supported by the following services: Elastic Load Balancing, Amazon CloudFront, Amazon Cognito, AWS Elastic Beanstalk, AWS App Runner, Amazon API Gateway, AWS CloudFormation, AWS Amplify, Amazon OpenSearch Service, and AWS Nitro Enclaves.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/inspector/">https://aws.amazon.com/inspector/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/attack-surface-reduction.html">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/attack-surface-reduction.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html">https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/automate-vulnerability-management-and-remediation-in-aws-using-amazon-inspector-and-aws-systems-manager-part-1/">https://aws.amazon.com/blogs/mt/automate-vulnerability-management-and-remediation-in-aws-using-amazon-inspector-and-aws-systems-manager-part-1/</a></p>\n',
        answers: [
          "<p>Configure the application security groups to ensure that only the necessary ports are open. Use Amazon Systems Manager to periodically scan the EC2 instances for vulnerabilities</p>",
          "<p>Configure the application security groups to ensure that only the necessary ports are open. Use Amazon Inspector to periodically scan the EC2 instances for vulnerabilities</p>",
          "<p>Use AWS Key Management Services to encrypt all the traffic between the client and application servers. Configure the application security groups to ensure that only the necessary ports are open</p>",
          "<p>Install AWS Certificate Manager (ACM) SSL/TLS certificate on the EC2 instances to secure traffic moving to and from the application servers</p>",
        ],
      },
      correct_response: ["b"],
      section: "Infrastructure Security",
      question_plain:
        "A standard three-tier application is hosted on Amazon EC2 instances that are fronted by an Application Load Balancer. The application maintenance team has reported several small-scale malicious attacks on the application. The project manager has decided to ramp up the security of the application.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend as part of the best practices to scan and mitigate the known vulnerabilities?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165402,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A Security Engineer has followed the best practices to set up a trusted IP address list for Amazon GuardDuty. However, GuardDuty is generating alert findings for the configured trusted IP addresses.</p>\n\n<p>Which of the following checks will you perform to ensure GuardDuty works as expected? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Ensure that IP addresses added in the trusted IP list are publicly routable IPv4 addresses</strong></p>\n\n<p>Trusted IP lists and threat lists apply only to traffic destined for publicly routable IP addresses. The effects of a list apply to all VPC Flow Log and CloudTrail findings, but do not apply to DNS findings.</p>\n\n<p><strong>Ensure that the trusted IP lists are uploaded in the same AWS Region as your GuardDuty findings</strong></p>\n\n<p>Trusted IP lists and threat lists are account and Region-specific. At any given time, you can have only one uploaded trusted IP list per AWS account per Region. Whereas, you can have up to six uploaded threat lists per AWS account per Region.</p>\n\n<p>AWS suggested best practices to verify the trusted IP list settings:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q14-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/">https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ensure that multiple trusted IP lists per AWS account per Region have been configured</strong> - This statement is incorrect. At any given time, you can have only one uploaded trusted IP list per AWS account per Region.</p>\n\n<p><strong>Ensure that in multi-account environments, GuardDuty generates findings for member accounts based on activity that involves IP addresses from the administrator\'s trusted IP lists</strong> - This statement is incorrect. In multi-account environments, GuardDuty generates findings for member accounts based on activity that involves known malicious IP addresses from the administrator\'s threat lists. It does not generate findings based on activity that involves IP addresses from the administrator\'s trusted IP lists.</p>\n\n<p><strong>Ensure that the same IP is not enlisted on both a trusted IP list as well as a threat list, as it will be processed by the threat list on priority, thereby resulting in a finding</strong> - If you include the same IP on both a trusted IP list and threat list it will be processed by the trusted IP list first, and will not generate a finding.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/">https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/</a></p>\n',
        answers: [
          "<p>Ensure that multiple trusted IP lists per AWS account per Region have been configured</p>",
          "<p>Ensure that in multi-account environments, GuardDuty generates findings for member accounts based on activity that involves IP addresses from the administrator's trusted IP lists</p>",
          "<p>Ensure that IP addresses added in the trusted IP list are publicly routable IPv4 addresses</p>",
          "<p>Ensure that the trusted IP lists are uploaded in the same AWS Region as your GuardDuty findings</p>",
          "<p>Ensure that the same IP is not enlisted on both a trusted IP list as well as a threat list, as it will be processed by the threat list on priority, thereby resulting in a finding</p>",
        ],
      },
      correct_response: ["c", "d"],
      section: "Infrastructure Security",
      question_plain:
        "A Security Engineer has followed the best practices to set up a trusted IP address list for Amazon GuardDuty. However, GuardDuty is generating alert findings for the configured trusted IP addresses.\n\nWhich of the following checks will you perform to ensure GuardDuty works as expected? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165404,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>As a Security Engineer, you have been tasked with the job of automating the detection and remediation of threats against your AWS environments using Amazon GuardDuty findings.</p>\n\n<p>Which steps will you follow to implement this solution most efficiently? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Configure CloudWatch Event to filter GuardDuty findings when a malicious activity is suspected. Configure the CloudWatch Event to invoke a Lambda function to parse the GuardDuty finding and store it in the Amazon DynamoDB table, if required</strong></p>\n\n<p><strong>After checking the existing entries in the Amazon DynamoDB table, AWS Lambda function creates a Rule inside AWS WAF and in a VPC NAC, and a notification email is sent via Amazon Simple Notification Service (SNS)</strong></p>\n\n<p>Amazon GuardDuty can be configured to automatically update the AWS Web Application Firewall Web Access Control Lists (WebACLs) and VPC Network Access Control Lists (NACLs) in response to GuardDuty findings.</p>\n\n<p>How the solution works:</p>\n\n<ol>\n<li><p>A GuardDuty finding is raised with suspected malicious activity.</p></li>\n<li><p>A CloudWatch Event is configured to filter for GuardDuty Finding type.</p></li>\n<li><p>A Lambda function is invoked by the CloudWatch Event and parses the GuardDuty finding.</p></li>\n<li><p>State data for blocked hosts is stored in the Amazon DynamoDB table. The Lambda function checks the state table for existing host entry.</p></li>\n<li><p>The Lambda function creates a Rule inside AWS WAF and in a VPC NACL.</p></li>\n<li><p>A notification email is sent via Amazon Simple Notification Service (SNS).</p></li>\n</ol>\n\n<p>Diagrammatic representation of the proposed solution:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q15-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/">https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure GuardDuty to export its findings to an Amazon S3 bucket. Configure a Lambda function to be triggered every time an object is added to the Amazon S3 bucket</strong> - GuardDuty supports exporting active findings to CloudWatch Events and, optionally, to an Amazon S3 bucket. To configure settings for exporting Active findings to an Amazon S3 bucket you will need a KMS key that GuardDuty can use to encrypt findings and an S3 bucket with permissions that allows GuardDuty to upload objects. This implies that using S3 involves setting up more resources than required, as the solution can be built by leveraging the CloudWatch Events directly. So this option is not the best fit.</p>\n\n<p><strong>Configure GuardDuty to trigger an AWS Lambda function every time a finding is generated. Configure an Amazon DynamoDB table to store the data received by Lambda from GuarDuty integration</strong> - GuardDuty supports exporting active findings to CloudWatch Events and, optionally, to an Amazon S3 bucket. GuardDuty cannot directly invoke the Lambda function to process its findings.</p>\n\n<p><strong>Configure AWS Lambda function to create a Rule inside AWS WAF and in a VPC NAC for every GuardDuty finding and trigger an email notification via Amazon Simple Notification Service (SNS)</strong> - This step is correct if the state data of blocked hosts is stored in some database solution. In the absence of permanent storage, Lambda will continue to add duplicate entries for the blocked hosts in AWS WAF and VPC NACL.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/">https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_exportfindings.html">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_exportfindings.html</a></p>\n',
        answers: [
          "<p>Configure CloudWatch Event to filter GuardDuty findings when a malicious activity is suspected. Configure the CloudWatch Event to invoke a Lambda function to parse the GuardDuty finding and store it in the Amazon DynamoDB table, if required</p>",
          "<p>Configure GuardDuty to export its findings to an Amazon S3 bucket. Configure a Lambda function to be triggered every time an object is added to the Amazon S3 bucket</p>",
          "<p>After checking the existing entries in the Amazon DynamoDB table, AWS Lambda function creates a Rule inside AWS WAF and in a VPC NAC, and a notification email is sent via Amazon Simple Notification Service (SNS)</p>",
          "<p>Configure GuardDuty to trigger an AWS Lambda function every time a finding is generated. Configure an Amazon DynamoDB table to store the data received by Lambda from GuardDuty integration</p>",
          "<p>Configure AWS Lambda function to create a Rule inside AWS WAF and in a VPC NAC for every GuardDuty finding and trigger an email notification via Amazon Simple Notification Service (SNS)</p>",
        ],
      },
      correct_response: ["a", "c"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "As a Security Engineer, you have been tasked with the job of automating the detection and remediation of threats against your AWS environments using Amazon GuardDuty findings.\n\nWhich steps will you follow to implement this solution most efficiently? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165406,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A retail company recently faced a cyber attack and lost all its data stored in the EBS volumes for the EC2 instances. However, the EBS snapshots were not manipulated. The company could restore the data from the EBS snapshots. However, the incident highlighted the security gaps in the current security plan. An immediate need is to protect the EBS snapshots from any manipulation or deletion.</p>\n\n<p>As a Security Engineer, what measures will you take to protect these AWS KMS Customer Master Keys (CMKs) encrypted snapshots?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create a new AWS account with limited privileges. Allow the newly created account to access the AWS KMS CMK key used to encrypt the EBS snapshots. Copy the encrypted snapshots to the new account on a regular basis</strong></p>\n\n<p>Automating cross-account snapshot copies enables you to copy your Amazon EBS snapshots to specific Regions in an isolated account and encrypt those snapshots with an encryption key. This enables you to protect yourself against data loss in the event of your account being compromised. When you share an encrypted snapshot, you must also share the customer-managed key used to encrypt the snapshot.</p>\n\n<p>By giving the new account access to the AWS KMS CMK key, the account will have the necessary permissions to copy the EBS snapshots. The new account will safeguard the EBS snapshots in case the account created by them is compromised while also restricting access through limited privileges.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new AWS account with limited privileges. Configure the snapshot to use encryption with the default AWS-managed key while copying the encrypted snapshots to the new account. Since default encryption is used, you don\'t need to share access to the AWS KMS CMK key</strong> - You can only share snapshots that are unencrypted or that are encrypted using a customer-managed key. You can\'t share snapshots that are encrypted with the default EBS encryption KMS key. Also, you cannot change the encryption key of a snapshot once it is created.</p>\n\n<p><strong>Create a new Amazon S3 bucket. Use AWS Systems Manager to move EBS snapshots to the new S3 bucket. Use S3 lifecycle policies to move the snapshots to Amazon S3 Glacier and subsequently apply Glacier Vault policies to prevent deletion</strong> - AWS Systems Manager allows you to safely automate common and repetitive IT operations and management tasks. With Systems Manager Automation, you use predefined playbooks, or you can build, run, and share wiki-style automated playbooks to enable AWS resource management across multiple accounts and AWS Regions. It does not manage EBS snapshots or its backups.</p>\n\n<p><strong>Use S3 Lifecycle transitions to regularly copy EBS snapshots to Amazon S3 through automation</strong> - Amazon S3 supports lifecycle transitions between storage classes using an S3 Lifecycle configuration. This option has been added as a distractor. You cannot use S3 lifecycle transitions to copy EBS snapshots to Amazon S3.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/event-policy.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/event-policy.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html</a></p>\n',
        answers: [
          "<p>Create a new Amazon S3 bucket. Use AWS Systems Manager to move EBS snapshots to the new S3 bucket. Use S3 lifecycle policies to move the snapshots to Amazon S3 Glacier and subsequently apply Glacier Vault policies to prevent deletion</p>",
          "<p>Use S3 Lifecycle transitions to regularly copy EBS snapshots to Amazon S3 through automation</p>",
          "<p>Create a new AWS account with limited privileges. Configure the snapshot to use encryption with the default AWS-managed key while copying the encrypted snapshots to the new account. Since default encryption is used, you don't need to share access to the AWS KMS CMK key</p>",
          "<p>Create a new AWS account with limited privileges. Allow the newly created account to access the AWS KMS CMK key used to encrypt the EBS snapshots. Copy the encrypted snapshots to the new account on a regular basis</p>",
        ],
      },
      correct_response: ["d"],
      section: "Data Protection",
      question_plain:
        "A retail company recently faced a cyber attack and lost all its data stored in the EBS volumes for the EC2 instances. However, the EBS snapshots were not manipulated. The company could restore the data from the EBS snapshots. However, the incident highlighted the security gaps in the current security plan. An immediate need is to protect the EBS snapshots from any manipulation or deletion.\n\nAs a Security Engineer, what measures will you take to protect these AWS KMS Customer Master Keys (CMKs) encrypted snapshots?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165408,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A Security Engineer received a GuardDuty security alert pertaining to one of the Amazon EC2 instances that is attempting to communicate with the IP address of a remote host known to hold credentials and stolen data captured by malware. The Security Engineer immediately tried to isolate the instance by activating the isolation security group on the instance. However, within a few minutes, the engineer received a similar alert again.</p>\n\n<p>Which of the following represents the underlying reason for this behavior and what is the solution to remediate the issue?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>When you change a security group rule, its tracked connections are not immediately interrupted. The tracked connections need to be configured to change to untracked connections and then apply the isolation security group to isolate the compromised instance</strong></p>\n\n<p>When you change a security group rule, its tracked connections are not immediately interrupted. The security group continues to allow packets until existing connections time out. To ensure that traffic is immediately interrupted, the tracked connections need to be configured to change to untracked connections and then apply the isolation security group to isolate the instance completely.</p>\n\n<p>An untracked flow of traffic is immediately interrupted if the rule that enables the flow is removed or modified. For example, if you have an open (0.0.0.0/0) outbound rule, and you remove a rule that allows all (0.0.0.0/0) inbound SSH (TCP port 22) traffic to the instance (or modify it such that the connection would no longer be permitted), your existing SSH connections to the instance are immediately dropped. The connection was not previously being tracked, so the change will break the connection. On the other hand, if you have a narrower inbound rule that initially allows an SSH connection (meaning that the connection was tracked), but change that rule to no longer allow new connections from the address of the current SSH client, the existing SSH connection is not interrupted because it is tracked.</p>\n\n<p>There are multiple ways to change tracked connections to being untracked. You can implement the isolation with an existing security group using the following steps:</p>\n\n<ol>\n<li>Identify the security group of the instance</li>\n<li>Delete all existing rules</li>\n<li>Create a single rule of 0.0.0.0/0 (0-65535) for all traffic in both inbound and outbound rules. This converts all existing and new traffic to being untracked.</li>\n<li>Remove the 0.0.0.0/0 (0-65535) inbound and outbound rules to terminate all connections and isolate the instance.</li>\n</ol>\n\n<p>Security Group level containment:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q17-i1.jpg">\nvia - <a href="https://www.youtube.com/watch?v=pPCuCYrhIyI">https://www.youtube.com/watch?v=pPCuCYrhIyI</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When you associate multiple security groups with an instance, rules with deny access need to be mutually exclusive. Delete all the security groups and create only the isolation security group to isolate the compromised instance</strong> - Security group rules can only allow traffic; you can\'t create rules that deny access. Hence, this option is incorrect.</p>\n\n<p><strong>When the isolation security group is unable to isolate an instance, the immediate fix is to shut down the compromised instance to cut off further damage to your AWS resources</strong> - Shutting down an instance is the last resort when trying to isolate the compromised instance. The reason is when an instance is shut down all the cache data is lost which is crucial in understanding the security breach that has taken place on the instance and the extent to which AWS resources have been compromised.</p>\n\n<p><strong>If you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Hence, to isolate the instance, cut off Internet Gateway from the instance</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway enables resources (like EC2 instances) in your public subnets to connect to the internet if the resource has a public IPv4 address or an IPv6 address. Internet Gateway operates at the VPC level and not at the instance level. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-connection-tracking.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-connection-tracking.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/security/automate-amazon-ec2-instance-isolation-by-using-tags/">https://aws.amazon.com/blogs/security/automate-amazon-ec2-instance-isolation-by-using-tags/</a></p>\n',
        answers: [
          "<p>When you associate multiple security groups with an instance, rules with deny access need to be mutually exclusive. Delete all the security groups and create only the isolation security group to isolate the compromised instance</p>",
          "<p>When you change a security group rule, its tracked connections are not immediately interrupted. The tracked connections need to be configured to change to untracked connections and then apply the isolation security group to isolate the compromised instance</p>",
          "<p>When the isolation security group is unable to isolate an instance, the immediate fix is to shut down the compromised instance to cut off further damage to your AWS resources</p>",
          "<p>If you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Hence, to isolate the instance, cut off Internet Gateway from the instance</p>",
        ],
      },
      correct_response: ["b"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "A Security Engineer received a GuardDuty security alert pertaining to one of the Amazon EC2 instances that is attempting to communicate with the IP address of a remote host known to hold credentials and stolen data captured by malware. The Security Engineer immediately tried to isolate the instance by activating the isolation security group on the instance. However, within a few minutes, the engineer received a similar alert again.\n\nWhich of the following represents the underlying reason for this behavior and what is the solution to remediate the issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165410,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A web application is deployed on EC2 instances running under an Auto Scaling Group. The application needs to be accessible from an Application Load Balancer that provides HTTPS termination, and accesses a PostgreSQL database managed by RDS.</p>\n\n<p>As an AWS Certified Security Specialist, how would you configure the security groups? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 5432</strong></p>\n\n<p><strong>The security group of the EC2 instances should have an inbound rule from the security group of the ALB on port 80</strong></p>\n\n<p><strong>The security group of the ALB should have an inbound rule from anywhere on port 443</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.\nThe following are the characteristics of security group rules:\nBy default, security groups allow all outbound traffic.\nSecurity group rules are always permissive; you can\'t create rules that deny access.\nSecurity groups are stateful</p>\n\n<p>PostgreSQL port = 5432\nHTTP port = 80\nHTTPS port = 443</p>\n\n<p>The traffic follows this route :\nThe client sends an HTTPS request to ALB on port 443. This is handled by the rule - <strong>The security group of the ALB should have an inbound rule from anywhere on port 443.</strong>\nThe ALB then forwards the request to one of the EC2 instances. This is handled by the rule - <strong>The security group of the EC2 instances should have an inbound rule from the security group of the ALB on port 80.</strong>\nThe EC2 instance further accesses the PostgreSQL database managed by RDS on port 5432. This is handled by the rule - <strong>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 5432.</strong></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The security group of the ALB should have an inbound rule from anywhere on port 80</strong> - The client sends an HTTPS request to ALB on port 443 and not on port 80, so this is incorrect.</p>\n\n<p><strong>The security group of the EC2 instances should have an inbound rule from the security group of the RDS database on port 5432</strong> - The security group of the EC2 instances should have an inbound rule from the security group of the ALB and not from the security group of the RDS database, so this option is incorrect.</p>\n\n<p><strong>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 80</strong> - The EC2 instance further accesses the PostgreSQL database managed by RDS on port 5432 and not on port 80, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n',
        answers: [
          "<p>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 5432</p>",
          "<p>The security group of the EC2 instances should have an inbound rule from the security group of the ALB on port 80</p>",
          "<p>The security group of the ALB should have an inbound rule from anywhere on port 443</p>",
          "<p>The security group of the ALB should have an inbound rule from anywhere on port 80</p>",
          "<p>The security group of the EC2 instances should have an inbound rule from the security group of the RDS database on port 5432</p>",
          "<p>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 80</p>",
        ],
      },
      correct_response: ["a", "b", "c"],
      section: "Infrastructure Security",
      question_plain:
        "A web application is deployed on EC2 instances running under an Auto Scaling Group. The application needs to be accessible from an Application Load Balancer that provides HTTPS termination, and accesses a PostgreSQL database managed by RDS.\n\nAs an AWS Certified Security Specialist, how would you configure the security groups? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165332,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A project manager has connected with you for the resolution of an issue. Although an AWS Identity and Access Management (IAM) entity has admin permissions, it has received an access denied error.</p>\n\n<p>As an AWS Certified Security Specialist, how will you troubleshoot and resolve this issue? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>If the requests are routed through a VPC endpoint, then check for any restrictions coming from the associated VPC endpoint policy</strong></p>\n\n<p>A VPC endpoint policy is a resource-based policy that you can attach to a VPC endpoint. It can restrict access to IAM entities. If you route your requests through a VPC endpoint, then check for any restrictions coming from the associated VPC endpoint policy.</p>\n\n<p><strong>A session policy is in place and is causing an authorization issue</strong></p>\n\n<p>Session policies can be passed programmatically when you create a temporary session for your IAM role for a federated user. The permissions for a session are at the intersection of the identity-based policies assigned to the IAM entity that the session is created for and the session policy itself. Check if a session policy is passed for your IAM role session using the AWS CloudTrail logs for <code>AssumeRole/AssumeRoleWithSAML/AssumeRoleWithWebIdentity</code> API calls. To check for session policies passed for a federated user session, check CloudTrail logs for GetFederationToken API calls.</p>\n\n<p>Resolving authorization issues for IAM entities with admin permissions\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q34-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/">https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A resource-based policy defines the maximum permissions that an identity-based policy can grant to an entity. Check for any restrictive resource-based policies</strong> - A permission boundary defines the maximum permissions that an identity-based policy can grant to an entity, not a resource-based policy. If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the permissions boundary. So the definition of the resource-based policy is incorrect in this statement.</p>\n\n<p><strong>An Organization NACL can restrict access to member account IAM entities. Check for restrictions coming from an NACL using the management account of the Organization</strong> - An Organization has a Service Control Policy (SCP) for restricting access to a service. A Network Access Control List (NACL) is an optional layer of security for a VPC that acts as a firewall for controlling traffic in and out of one or more subnets.</p>\n\n<p><strong>If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the concerned resource-based policy. Check for any restrictive permissions boundary</strong> - A permissions boundary limits the actions your entity can perform. If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the permissions boundary. So the reference to the resource-based policy is incorrect in this statement.</p>\n\n<p>Evaluating effective permissions with boundaries:\n<img src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/EffectivePermissions-rbp-boundary-id.png">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/">https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n',
        answers: [
          "<p>A resource-based policy defines the maximum permissions that an identity-based policy can grant to an entity. Check for any restrictive resource-based policies</p>",
          "<p>An Organization NACL can restrict access to member account IAM entities. Check for restrictions coming from an NACL using the management account of the Organization</p>",
          "<p>If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the concerned resource-based policy. Check for any restrictive permissions boundary</p>",
          "<p>If the requests are routed through a VPC endpoint, then check for any restrictions coming from the associated VPC endpoint policy</p>",
          "<p>A session policy is in place and is causing an authorization issue</p>",
        ],
      },
      correct_response: ["d", "e"],
      section: "Identity and Access Management",
      question_plain:
        "A project manager has connected with you for the resolution of an issue. Although an AWS Identity and Access Management (IAM) entity has admin permissions, it has received an access denied error.\n\nAs an AWS Certified Security Specialist, how will you troubleshoot and resolve this issue? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165414,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The security team at a company needs to analyze the AWS Web Application Firewall (AWS WAF) logs quickly and it wants to build multiple dashboards using a serverless architecture. The logging process should be automated so that the log data for dashboards is available on a real-time or near-real-time basis.</p>\n\n<p>Which of the following represents the most optimal solution for this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Configure AWS Web Application Firewall (AWS WAF) to feed logs to Amazon S3 bucket via Amazon Kinesis Data Firehose. Set up an AWS Glue crawler job and an Amazon Athena table to query for required data and create visualizations using Amazon QuickSight dashboards</strong></p>\n\n<p>This solution will show you how to analyze AWS Web Application Firewall (AWS WAF) logs and quickly build multiple dashboards, without booting up any servers.</p>\n\n<p>With the new AWS WAF full logs feature, you can log all traffic inspected by AWS WAF into Amazon Simple Storage Service (Amazon S3) buckets by leveraging Amazon Kinesis Data Firehose. First, you need to enable AWS WAF logging for the given web ACL(s). You can then create an Amazon Kinesis Data Firehose delivery stream into which the AWS WAF full logs are delivered. Then you need to set up an AWS Glue crawler job and an Amazon Athena table. Finally, you’ll set up Amazon QuickSight dashboards to read the logs data by querying the Athena tables and thereby help you visualize the web traffic traversing through the AWS WAF layer.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q20-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/enabling-serverless-security-analytics-using-aws-waf-full-logs/">https://aws.amazon.com/blogs/security/enabling-serverless-security-analytics-using-aws-waf-full-logs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Web Application Firewall (AWS WAF) to send logs to CloudWatch Logs. Use CloudWatch Logs Insights to interactively search and analyze your log data. Publish logs data to Amazon QuickSight dashboards through direct CloudWatch integration with QuickSight</strong> - It is possible to send web ACL traffic logs to a CloudWatch Logs log group. However, CloudWatch and Amazon QuickSight cannot directly connect and will need Amazon Athena as part of the solution. Hence, this option is incorrect.</p>\n\n<p><strong>Configure AWS Web Application Firewall (AWS WAF) to feed logs to Amazon S3 bucket via Amazon Kinesis Data Streams. Set up an AWS Glue crawler job and an Amazon Athena table to query for required data and create visualizations using Amazon QuickSight dashboards</strong> - AWS WAF logs only support the following logging destinations - Amazon CloudWatch Logs, Amazon Simple Storage Service, and Amazon Kinesis Data Firehose. So this option is incorrect.</p>\n\n<p><strong>Configure AWS Web Application Firewall (AWS WAF) to send your web ACL traffic logs to the Amazon S3 bucket. Use Amazon Redshift Spectrum to query data directly from files on Amazon S3. Using the existing integration of RedShift Spectrum with Amazon QuickSight, generate visualizations to be used in manager dashboards</strong> - To use Redshift Spectrum, you need an Amazon Redshift cluster and a SQL client that\'s connected to your cluster, so that you can run SQL commands. Since the required solution should be serverless, this option is not the right fit.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html">https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html">https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html">https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html</a></p>\n',
        answers: [
          "<p>Configure AWS Web Application Firewall (AWS WAF) to send logs to CloudWatch Logs. Use CloudWatch Logs Insights to interactively search and analyze your log data. Publish logs data to Amazon QuickSight dashboards through direct CloudWatch integration with QuickSight</p>",
          "<p>Configure AWS Web Application Firewall (AWS WAF) to feed logs to Amazon S3 bucket via Amazon Kinesis Data Streams. Set up an AWS Glue crawler job and an Amazon Athena table to query for required data and create visualizations using Amazon QuickSight dashboards</p>",
          "<p>Configure AWS Web Application Firewall (AWS WAF) to feed logs to Amazon S3 bucket via Amazon Kinesis Data Firehose. Set up an AWS Glue crawler job and an Amazon Athena table to query for required data and create visualizations using Amazon QuickSight dashboards</p>",
          "<p>Configure AWS Web Application Firewall (AWS WAF) to send your web ACL traffic logs to Amazon S3 bucket. Use Amazon Redshift Spectrum to query data directly from files on Amazon S3. Using the existing integration of RedShift Spectrum with Amazon QuickSight, generate visualizations to be used in manager dashboards</p>",
        ],
      },
      correct_response: ["c"],
      section: "Security Logging and Monitoring",
      question_plain:
        "The security team at a company needs to analyze the AWS Web Application Firewall (AWS WAF) logs quickly and it wants to build multiple dashboards using a serverless architecture. The logging process should be automated so that the log data for dashboards is available on a real-time or near-real-time basis.\n\nWhich of the following represents the most optimal solution for this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165416,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A financial services company is revamping its technology solutions on AWS to meet the company's new security guidelines that mandate the use of the company's own imported key material to create Customer Master keys (CMKs) to be used with AWS services. All encryption keys must also be rotated annually.</p>\n\n<p>How will you implement this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create a new CMK and import the new key material into it. Point the key alias of the older CMK to the new CMK created</strong></p>\n\n<p>When you import key material into a KMS key, the KMS key is permanently associated with that key material. You can reimport the same key material, but you cannot import a different key material into that KMS key. Also, you cannot enable automatic key rotation for a KMS key with imported key material. However, you can manually rotate a KMS key with imported key material.</p>\n\n<p>How to enable and disable automatic key rotation:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q21-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually</a></p>\n\n<p>Because the new KMS key is a different resource from the current KMS key, it has a different key ID and ARN. When you change KMS keys, you need to update references to the KMS key ID or ARN in your applications. Aliases, which associate a friendly name with a KMS key, make this process easier. Use an alias to refer to a KMS key in your applications. Then, when you want to change the KMS key that the application uses, change the target KMS key of the alias.</p>\n\n<p>Rotating keys manually:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q21-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually</a></p>\n\n<p>A key alias allows you to abstract key users away from the underlying Region-specific key ID and key ARN. Authorized individuals can create a key alias that allows their applications to use a specific CMK independent of the Region or rotation schedule. Thus, multi-Region applications can use the same key alias to refer to KMS keys in multiple Regions without worrying about the key ID or the key ARN. You can also trigger the manual rotation of a CMK by pointing a given key alias to a different CMK. Similar to how Domain Name Services (DNS) allows the abstraction of IP addresses, a key alias does the same for the key ID. When you are creating a key alias, we recommend that you determine a naming scheme that can be applied across your accounts such as alias/&lt;Environment&gt;-&lt;Function&gt;-&lt;Service Team&gt;.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable automatic key rotation for the KMS key with imported key material. Use this method to rotate the keys annually</strong> - You cannot automatically rotate asymmetric KMS keys, HMAC KMS keys, KMS keys with imported key material, or KMS keys in custom key stores. However, you can rotate them manually. So, this option is incorrect.</p>\n\n<p><strong>Associate the existing CMK with the new key material and run the List operation to update the association</strong> - When you import key material into a KMS key, the KMS key is permanently associated with that key material. You can reimport the same key material, but you cannot import a different key material into an existing KMS key.</p>\n\n<p><strong>Delete the old KMS key first and create a new key with the same name immediately. Import new key material into this newly created KMS key</strong> - Because it is destructive and potentially dangerous to delete a KMS key, AWS KMS requires you to set a waiting period of 7 – 30 days. The default waiting period is 30 days. Hence, it is not possible to delete a KMS key and immediately create a new one with the same name.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/key-aliases.html">https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/key-aliases.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually</a></p>\n',
        answers: [
          "<p>Enable automatic key rotation for the KMS key with imported key material. Use this method to rotate the keys annually</p>",
          "<p>Associate the existing CMK with the new key material and run the List operation to update the association</p>",
          "<p>Create a new CMK and import the new key material into it. Point the key alias of the older CMK to the new CMK created</p>",
          "<p>Delete the old KMS key first and create a new key with the same name immediately. Import new key material into this newly created KMS key</p>",
        ],
      },
      correct_response: ["c"],
      section: "Data Protection",
      question_plain:
        "A financial services company is revamping its technology solutions on AWS to meet the company's new security guidelines that mandate the use of the company's own imported key material to create Customer Master keys (CMKs) to be used with AWS services. All encryption keys must also be rotated annually.\n\nHow will you implement this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165418,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company manages separate AWS accounts for each of its business units. An enhanced monitoring solution has been proposed by the security team that mandates tracking all the API calls using CloudTrail for all the AWS accounts. The centralized monitoring logs will be available in a new AWS account created for security and audit purposes. Logs of one business unit should be distinguishable from others via its own top-level prefix. Also, any updates to the log files should be traceable.</p>\n\n<p>As a Security Engineer, which of the following options will you combine to implement this requirement? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Create a new Amazon S3 bucket in the centralized account to store all the CloudTrail log files. Enable log file validation on all Trails in AWS accounts of all business units. Use unique log file prefixes for trails in each AWS account</strong></p>\n\n<p><strong>Apply a bucket policy to the new centralized S3 bucket that permits the CloudTrail service to use the "s3 PutObject" action and the "s3 GetBucketACL" action, and specify the appropriate resource ARNs for the CloudTrail trails</strong> -</p>\n\n<p>A new S3 bucket should be created in the centralized account as per the given requirements. Log file validation on CloudTrail logs is necessary to determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it. This feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection.</p>\n\n<p>A complete log file object name with Top-level prefix:\n<code>bucket_name/prefix_name/AWSLogs/Account ID/CloudTrail/region/YYYY/MM/DD/file_name.json.gz</code></p>\n\n<p>It helps in identifying the logs of a business unit at the top level without having to filter for it at the log file level.</p>\n\n<p>For a bucket to receive log files from multiple accounts, its bucket policy must grant CloudTrail permission to write log files from all the accounts you specify. This means that you must modify the bucket policy on your destination bucket to grant CloudTrail permission to write log files from each specified account.</p>\n\n<p>S3 bucket policy so that files can be received from multiple accounts:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q22-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-set-bucket-policy-for-multiple-accounts.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-set-bucket-policy-for-multiple-accounts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new Amazon S3 bucket in the centralized account to store all the CloudTrail log files. Enable log file validation on all Trails in all AWS accounts including the centralized account. Use unique log file prefixes for trails in each AWS account</strong> - Enabling Log file validation in the centralized account is not required for the given use case.</p>\n\n<p><strong>Create a new Amazon S3 bucket in each of the AWS accounts. Use S3 bucket replication to copy the CloudTrail logs to the S3 bucket in the centralized account. Enable log file validation on all Trails in all of the AWS accounts used. Use unique log file prefixes for trails in each AWS account</strong> - AWS provides a straightforward solution for logging Trails from different AWS accounts. The S3 bucket that receives the log files from multiple accounts should have a bucket policy that must grant CloudTrail permission to write log files from all the accounts you specify. There is no reason to use S3 bucket replication and complicate the process.</p>\n\n<p><strong>Apply a bucket policy to all S3 buckets to permit the CloudTrail service to use the "s3 PutObject" action, "s3 GetObjectAcl" action, and the "s3 GetObject" action. Specify the appropriate resource ARNs for the CloudTrail trails</strong> - As discussed above, the S3 bucket policy should permit the CloudTrail service to use the "s3 PutObject" action and the "s3 GetBucketACL" action only.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html</a></p>\n',
        answers: [
          "<p>Create a new Amazon S3 bucket in the centralized account to store all the CloudTrail log files. Enable log file validation on all Trails in all AWS accounts including the centralized account. Use unique log file prefixes for trails in each AWS account</p>",
          "<p>Create a new Amazon S3 bucket in the centralized account to store all the CloudTrail log files. Enable log file validation on all Trails in AWS accounts of all business units. Use unique log file prefixes for trails in each AWS account</p>",
          "<p>Apply a bucket policy to the new centralized S3 bucket that permits the CloudTrail service to use the <code>s3 PutObject</code> action and the <code>s3 GetBucketACL</code> action, and specify the appropriate resource ARNs for the CloudTrail trails</p>",
          "<p>Create a new Amazon S3 bucket in each of the AWS accounts. Use S3 bucket replication to copy the CloudTrail logs to the S3 bucket in the centralized account. Enable log file validation on all Trails in all of the AWS accounts used. Use unique log file prefixes for trails in each AWS account</p>",
          "<p>Apply a bucket policy to all S3 buckets to permit the CloudTrail service to use the <code>s3 PutObject</code> action, <code>s3 GetObjectAcl</code> action, and the <code>s3 GetObject</code> action. Specify the appropriate resource ARNs for the CloudTrail trails</p>",
        ],
      },
      correct_response: ["b", "c"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A company manages separate AWS accounts for each of its business units. An enhanced monitoring solution has been proposed by the security team that mandates tracking all the API calls using CloudTrail for all the AWS accounts. The centralized monitoring logs will be available in a new AWS account created for security and audit purposes. Logs of one business unit should be distinguishable from others via its own top-level prefix. Also, any updates to the log files should be traceable.\n\nAs a Security Engineer, which of the following options will you combine to implement this requirement? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165420,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>During an internal IT Audit, the security team realized that AWS CloudTrail was disabled for a few AWS Regions leading to security and audit lapses. Now, the management wants to tighten the security measures across the company. As an AWS Certified Security Specialist, you have been tasked to build a solution for automatic re-enabling of AWS CloudTrail in any AWS Region if it happens to be turned off.</p>\n\n<p>What is the most optimal way of addressing this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use AWS Config with a managed rule <code>cloudtrail-enabled</code> to trigger a remediation action to fix the non-compliant status using AWS Systems Manager Automation documents</strong></p>\n\n<p>AWS Config provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. You can customize the behavior of a managed rule to suit your needs.</p>\n\n<p>To ensure that CloudTrail remains enabled in your account, AWS Config provides the \'cloudtrail-enabled\' managed rule. If CloudTrail is turned off, the \'cloudtrail-enabled\' rule automatically re-enables it by using automatic remediation.</p>\n\n<p>However, you must make sure that you follow security best practices for CloudTrail if you use automatic remediation. These best practices include enabling CloudTrail in all AWS Regions, logging read and write workloads, enabling insights, and encrypting log files with server-side encryption using AWS Key Management Service (AWS KMS) managed keys (SSE-KMS).</p>\n\n<p>This pattern helps you follow these security best practices by providing a custom remediation action to automatically re-enable CloudTrail in your account.</p>\n\n<p>Automatically re-enable AWS CloudTrail by using a custom remediation rule in AWS Config:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q23-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-re-enable-aws-cloudtrail-by-using-a-custom-remediation-rule-in-aws-config.html">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-re-enable-aws-cloudtrail-by-using-a-custom-remediation-rule-in-aws-config.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Security Hub with a managed rule <code>cloudtrail-enabled</code> to trigger a remediation action to fix the non-compliant status using AWS Systems Manager Automation documents</strong> - AWS Security Hub provides you with a comprehensive view of your security state in AWS and helps you check your environment against security industry standards and best practices. Security Hub collects security data from across AWS accounts, services, and supported third-party partner products and helps you analyze your security trends and identify the highest priority security issues. AWS Security Hub does not have any built-in managed rules to handle remediation for CloudTrail non-compliant status. In fact, AWS Config itself uses Amazon EventBridge to send AWS Config rule evaluations to Security Hub. So, this option is not the right fit.</p>\n\n<p><strong>Create an event in Amazon EventBridge with Cloudtrail as the event source and a StopLogging event name to trigger an AWS Lambda function to call the StartLogging API</strong> - You would need to develop and maintain the code in the Lambda function to handle this requirement. Instead, it is optimal to use the off-the-shelf capability offered by AWS Config to address the given use case.</p>\n\n<p><strong>Use AWS Trusted Advisor security check on AWS CloudTrail Logging to trigger a Lambda function in case logging is disabled. The Lambda function implements the functionality to enable CloudTrail logging if it is disabled</strong> - AWS Trusted Advisor cannot be used to trigger an AWS Lambda function for non-compliant security checks. Hence, this option is invalid.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><a href="https://aws.amazon.com/cloudtrail/faqs/">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n',
        answers: [
          "<p>Use AWS Security Hub with a managed rule <code>cloudtrail-enabled</code> to trigger a remediation action to fix the non-compliant status using AWS Systems Manager Automation documents</p>",
          "<p>Use AWS Config with a managed rule <code>cloudtrail-enabled</code> to trigger a remediation action to fix the non-compliant status using AWS Systems Manager Automation documents</p>",
          "<p>Create an Amazon CloudWatch alarm with a cloudtrail.amazonaws.com event source and a StartLogging event name to trigger an AWS Lambda function to call the StartLogging API</p>",
          "<p>Use AWS Trusted Advisor security check on AWS CloudTrail Logging to trigger a Lambda function in case logging is disabled. The Lambda function implements the functionality to enable CloudTrail logging if it is disabled</p>",
        ],
      },
      correct_response: ["b"],
      section: "Security Logging and Monitoring",
      question_plain:
        "During an internal IT Audit, the security team realized that AWS CloudTrail was disabled for a few AWS Regions leading to security and audit lapses. Now, the management wants to tighten the security measures across the company. As an AWS Certified Security Specialist, you have been tasked to build a solution for automatic re-enabling of AWS CloudTrail in any AWS Region if it happens to be turned off.\n\nWhat is the most optimal way of addressing this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165422,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The security team at a company has recently decided that CloudTrail logs of each department will be prefixed with the department code. Currently, CloudTrail logs are created with similar names across the company with no immediate way of identifying the departments sending those logs. When the security team tried to add the prefix to the log files in the CloudTrail console, the following error popped up: 'There is a problem with the bucket policy'.</p>\n\n<p>How will you fix this issue?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use the Amazon S3 console to update the prefix in the current bucket policy, and then use the CloudTrail console to specify the same prefix for the bucket in the trail</strong></p>\n\n<p>If you try to add, modify, or remove a log file prefix for an S3 bucket that receives logs from a trail, you might see the error: "There is a problem with the bucket policy." A bucket policy with an incorrect prefix can prevent your trail from delivering logs to the bucket. To resolve this issue, use the Amazon S3 console to update the prefix in the bucket policy, and then use the CloudTrail console to specify the same prefix for the bucket in the trail.</p>\n\n<p>Changing a prefix for an existing bucket:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q24-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the service-specific context keys used in the Condition element of the Amazon S3 bucket policy statements</strong> - CloudTrail does not have service-specific context keys that can be used in the Condition element of policy statements.</p>\n\n<p>Policy best practices for CloudTrail:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q24-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/security_iam_id-based-policy-examples.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/security_iam_id-based-policy-examples.html</a></p>\n\n<p><strong>Update the permissions of all users in the security team to <code>AWSCloudTrail_FullAccess</code> policy to impart all the necessary permissions to the users on CloudTrail logs</strong> - The <code>AWSCloudTrail_FullAccess</code> policy is not intended to be shared broadly across your AWS account. Users with this role can disable or reconfigure the most sensitive and important auditing functions in their AWS accounts. For this reason, this policy should be applied only to account administrators, and the use of this policy should be closely controlled and monitored.</p>\n\n<p><strong>Manually edit your Amazon S3 bucket policy to add an <code>aws:SourceArn</code> condition key to the policy statement attached for CloudTrail</strong> - This change does not affect the given error.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/security_iam_id-based-policy-examples.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/security_iam_id-based-policy-examples.html</a></p>\n',
        answers: [
          "<p>Update the service-specific context keys used in the Condition element of the Amazon S3 bucket policy statements</p>",
          "<p>Update the permissions of all users in the security team to <code>AWSCloudTrail_FullAccess</code> policy to impart all the necessary permissions to the users on CloudTrail logs</p>",
          "<p>Manually edit your Amazon S3 bucket policy to add an <code>aws:SourceArn</code> condition key to the policy statement attached for CloudTrail</p>",
          "<p>Use the Amazon S3 console to update the prefix in the current bucket policy, and then use the CloudTrail console to specify the same prefix for the bucket in the trail</p>",
        ],
      },
      correct_response: ["d"],
      section: "Identity and Access Management",
      question_plain:
        "The security team at a company has recently decided that CloudTrail logs of each department will be prefixed with the department code. Currently, CloudTrail logs are created with similar names across the company with no immediate way of identifying the departments sending those logs. When the security team tried to add the prefix to the log files in the CloudTrail console, the following error popped up: 'There is a problem with the bucket policy'.\n\nHow will you fix this issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165424,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A junior developer has been asked to configure access to an Amazon EC2 instance hosting a web application. The developer has configured a new security group to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. A custom Network Access Control List (NACL) connected with the instance's subnet is configured to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules.</p>\n\n<p>As a Security Engineer, which of the following solutions would you suggest if the EC2 instance needs to accept and respond to requests from the internet?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</strong> - Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic. By default, each custom Network ACL denies all inbound and outbound traffic until you add rules.</p>\n\n<p>To enable the connection to a service running on an instance, the associated network ACL must allow both:\n1. Inbound traffic on the port that the service is listening on\n2. Outbound traffic to ephemeral ports</p>\n\n<p>When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client\'s source port.</p>\n\n<p>The designated ephemeral port becomes the destination port for return traffic from the service. Outbound traffic to the ephemeral port must be allowed in the network ACL.</p>\n\n<p>Network ACL basics:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q25-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The configuration is complete on the EC2 instance for accepting and responding to requests</strong> - As explained above, this is an incorrect statement.</p>\n\n<p><strong>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</strong> - Security groups are stateful. Therefore you don\'t need a rule that allows responses to inbound traffic.</p>\n\n<p><em>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</em>* - Security Groups are stateful. Hence, return traffic is automatically allowed, so there is no need to configure an outbound rule on the security group.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/">https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports</a></p>\n',
        answers: [
          "<p>The configuration is complete on the EC2 instance for accepting and responding to requests</p>",
          "<p>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</p>",
          "<p>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</p>",
          "<p>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</p>",
        ],
      },
      correct_response: ["b"],
      section: "Infrastructure Security",
      question_plain:
        "A junior developer has been asked to configure access to an Amazon EC2 instance hosting a web application. The developer has configured a new security group to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. A custom Network Access Control List (NACL) connected with the instance's subnet is configured to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules.\n\nAs a Security Engineer, which of the following solutions would you suggest if the EC2 instance needs to accept and respond to requests from the internet?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165426,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A financial services company wants to share sensitive accounting data that is stored in an Amazon RDS DB instance with an external auditor. The auditor has another AWS account and must own a copy of the database.</p>\n\n<p>Which of the following would you recommend to securely share the database with the auditor?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key</strong></p>\n\n<p>You can share the AWS Key Management Service (AWS KMS) customer master key (CMK) that was used to encrypt the snapshot with any accounts that you want to be able to access the snapshot. You can share AWS KMS CMKs with another AWS account by adding the other account to the AWS KMS key policy.</p>\n\n<p>Making an encrypted snapshot of the database will give the auditor a copy of the database, as required for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket</strong> - RDS stores the DB snapshots in the Amazon S3 bucket belonging to the same AWS region where the RDS instance is located. RDS stores these on your behalf and you do not have direct access to these snapshots in S3, so it\'s not possible to grant access to the snapshot objects in S3.</p>\n\n<p><strong>Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket</strong> - This solution is feasible though not optimal. It requires a lot of unnecessary work and is difficult to audit when such bulk data is exported into text files.</p>\n\n<p><strong>Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access</strong> - Read Replicas make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Creating Read Replicas for audit purposes is overkill. Also, the question mentions that the auditor needs to own a copy of the database, which is not possible with replicas.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html</a></p>\n',
        answers: [
          "<p>Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key</p>",
          "<p>Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket</p>",
          "<p>Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket</p>",
          "<p>Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access</p>",
        ],
      },
      correct_response: ["a"],
      section: "Data Protection",
      question_plain:
        "A financial services company wants to share sensitive accounting data that is stored in an Amazon RDS DB instance with an external auditor. The auditor has another AWS account and must own a copy of the database.\n\nWhich of the following would you recommend to securely share the database with the auditor?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165428,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A healthcare company has recently completed a security review that has highlighted several gaps in the security standards mandated by the company while using the AWS Key Management Service (AWS KMS) keys. As an initial step to address the gap, the security team has decided that access to AWS KMS keys should be restricted to only the principals belonging to their AWS Organizations.</p>\n\n<p>How will you implement this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>The <code>aws:PrincipalOrgID</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. You need to specify the Organization ID in the <code>Condition</code> element</strong></p>\n\n<p>The aws:PrincipalOrgID global condition key can be used with the Principal element in a resource-based policy with AWS KMS. Instead of listing all the AWS account IDs in an Organization, you can specify the Organization ID in the Condition element.</p>\n\n<p>Create an AWS KMS key policy to allow all accounts in an AWS Organization to perform AWS KMS actions using the AWS global condition context key <code>aws:PrincipalOrgID</code>. It is a best practice to grant the least privilege permissions with AWS Identity and Access Management (IAM) policies. Specify your AWS Organization ID in the condition element of the statement to make sure that only the principals from the accounts in your Organization can access the AWS KMS key.</p>\n\n<p><code>aws:PrincipalOrgID</code> - Use this key to compare the identifier of the organization in AWS Organizations to which the requesting principal belongs with the identifier specified in the policy. This global key provides an alternative to listing all the account IDs for all AWS accounts in an organization. You can use this condition key to simplify specifying the Principal element in a resource-based policy. You can specify the organization ID in the condition element. When you add and remove accounts, policies that include the aws:PrincipalOrgID key automatically include the correct accounts and don\'t require manual updating.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The <code>aws:PrincipalOrgID</code> global condition context key can be used to restrict access to an AWS service principal</strong> - AWS Organizations help you centrally manage and govern your environment as you grow and scale your AWS resources.  The <code>aws:PrincipalOrgID</code> global condition context key can\'t be used to restrict access to an AWS service principal. AWS services that invoke an API call are made from an internal AWS account that is not part of the AWS Organizations.</p>\n\n<p><strong>The <code>aws:PrincipalOrgID</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. List all the AWS account IDs in the <code>Condition</code> element</strong> - It is possible to configure the requirement with all the account IDs. But, such a solution is not an elegant solution. Instead of listing all the AWS account IDs in an Organization, you can specify the Organization ID in the Condition element.</p>\n\n<p><strong>The <code>aws:PrincipalIsAWSService</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. List all the AWS account IDs in the <code>Condition</code> element</strong> - You can use this key to check whether the call to your resource is being made directly by an AWS service principal. For example, AWS CloudTrail uses the service principal cloudtrail.amazonaws.com to write logs to your Amazon S3 bucket. Also, instead of listing all the AWS account IDs in an Organization, you can specify the Organization ID in the Condition element.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-principalisawsservice">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-principalisawsservice</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/kms-key-organization-account/">https://aws.amazon.com/premiumsupport/knowledge-center/kms-key-organization-account/</a></p>\n',
        answers: [
          "<p>The <code>aws:PrincipalOrgID</code> global condition context key can be used to restrict access to an AWS service principal</p>",
          "<p>The <code>aws:PrincipalOrgID</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. List all the AWS account IDs in the <code>Condition</code> element</p>",
          "<p>The <code>aws:PrincipalIsAWSService</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. List all the AWS account IDs in the <code>Condition</code> element</p>",
          "<p>The <code>aws:PrincipalOrgID</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. You need to specify the Organization ID in the <code>Condition</code> element</p>",
        ],
      },
      correct_response: ["d"],
      section: "Data Protection",
      question_plain:
        "A healthcare company has recently completed a security review that has highlighted several gaps in the security standards mandated by the company while using the AWS Key Management Service (AWS KMS) keys. As an initial step to address the gap, the security team has decided that access to AWS KMS keys should be restricted to only the principals belonging to their AWS Organizations.\n\nHow will you implement this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165430,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An application hosted on an Amazon EC2 instance writes its request logs, availability logs, and threat logs to a text file. This file is read by a custom program to track and process any security issues inferred from the logs. An increase in log data has resulted in the malfunctioning of the custom program. The company is looking at a scalable solution to collect and analyze log files.</p>\n\n<p>Which design will ensure that the aforementioned criteria are met with the LEAST amount of effort?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Install and configure the unified CloudWatch agent on the application\'s EC2 instance. Create a CloudWatch metric filter to monitor the application logs. Configure CloudWatch alerts based on these metrics</strong></p>\n\n<p>The unified CloudWatch agent enables you to do the following:</p>\n\n<ol>\n<li><p>Collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in the Metrics collected by the CloudWatch agent.</p></li>\n<li><p>Collect system-level metrics from on-premises servers.</p></li>\n<li><p>Retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collected</code> protocols.</p></li>\n<li><p>Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p></li>\n</ol>\n\n<p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs, just like logs collected by the older CloudWatch Logs agent.</p>\n\n<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a scheduled process to copy the application log files to AWS CloudTrail. Configure a Lambda function that processes CloudTrail logs and sends an SNS notification whenever a log file is created</strong> - This option is invalid since the logs for the given use case cannot be imported or written into CloudTrail.</p>\n\n<p><strong>Configure Amazon Inspector to collect all log files from the EC2 instance. Use Amazon EventBridge integration with Amazon Inspector to trigger Lambda function that can read the logs and raise notifications for events on security</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. Amazon Inspector is a very useful tool, but it cannot process log files as described in the given use case.</p>\n\n<p><strong>Create a cron job on the Amazon EC2 instance to copy the logs into the Amazon S3 bucket. Use S3 events to trigger a Lambda function that refreshes Amazon CloudWatch metrics with the log data. Set up CloudWatch alerts based on the metrics</strong> - Although this solution is certainly feasible, however, it is not an optimal solution when compared to directly using the unified CloudWatch agent.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n',
        answers: [
          "<p>Create a scheduled process to copy the application log files to AWS CloudTrail. Configure a Lambda function that processes CloudTrail logs and sends an SNS notification whenever a log file is created</p>",
          "<p>Configure Amazon Inspector to collect all log files from the EC2 instance. Use Amazon EventBridge integration with Amazon Inspector to trigger Lambda function that can read the logs and raise notifications for events on security</p>",
          "<p>Install and configure the unified CloudWatch agent on the application's EC2 instance. Create a CloudWatch metric filter to monitor the application logs. Configure CloudWatch alerts based on these metrics</p>",
          "<p>Create a cron job on the Amazon EC2 instance to copy the logs into the Amazon S3 bucket. Use S3 events to trigger a Lambda function that refreshes Amazon CloudWatch metrics with the log data. Set up CloudWatch alerts based on the metrics</p>",
        ],
      },
      correct_response: ["c"],
      section: "Security Logging and Monitoring",
      question_plain:
        "An application hosted on an Amazon EC2 instance writes its request logs, availability logs, and threat logs to a text file. This file is read by a custom program to track and process any security issues inferred from the logs. An increase in log data has resulted in the malfunctioning of the custom program. The company is looking at a scalable solution to collect and analyze log files.\n\nWhich design will ensure that the aforementioned criteria are met with the LEAST amount of effort?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165432,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>While consolidating logs for the weekly reporting, a development team at a retail company realized that an unusually large number of illegal AWS API queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs.</p>\n\n<p>Which of the following represents the best solution for the given scenario?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create an Amazon CloudWatch metric filter that processes CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric\'s rate to send an SNS notification to the required team</strong></p>\n\n<p>AWS CloudTrail log data can be ingested into Amazon CloudWatch to monitor and identify your AWS account activity against security threats, and create a governance framework for security best practices. You can analyze log trail event data in CloudWatch using features such as Logs Insight, Contributor Insights, Metric filters, and CloudWatch Alarms.</p>\n\n<p>CloudTrail integrates with the CloudWatch service to publish the API calls being made to resources or services in the AWS account. The published event has invaluable information that can be used for compliance, auditing, and governance of your AWS accounts. Below we introduce several features available in CloudWatch to monitor API activity, analyze the logs at scale, and take action when malicious activity is discovered, without provisioning your infrastructure.</p>\n\n<p>For the Cloudtrail logs available in CloudWatch Logs, you can begin searching and filtering the log data by creating one or more metric filters. Use these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set a CloudWatch Alarm on.</p>\n\n<p>Using CloudWatch Metric Filters to analyze log data from CloudTrail:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q29-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/mt/analyzing-cloudtrail-in-cloudwatch/">https://aws.amazon.com/blogs/mt/analyzing-cloudtrail-in-cloudwatch/</a></p>\n\n<p>Note: AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with <code>write</code> API calls by continuously analyzing CloudTrail management events.</p>\n\n<p>Insights events are logged when CloudTrail detects unusual <code>write</code> management API activity in your account. If you have CloudTrail Insights enabled and CloudTrail detects unusual activity, Insights events are delivered to the destination S3 bucket for your trail. You can also see the type of insight and the incident time when you view Insights events on the CloudTrail console. Unlike other types of events captured in a CloudTrail trail, Insights events are logged only when CloudTrail detects changes in your account\'s API usage that differ significantly from the account\'s typical usage patterns.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Kinesis stream-level metrics in the CloudWatch to trigger an AWS Lambda function that will trigger an error workflow</strong> -  AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and CloudWatch logs are the only destinations possible.</p>\n\n<p><strong>Run Amazon Athena SQL queries against CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards</strong> - Generating reports and visualizations help in understanding and analyzing patterns but is not useful as a near-real-time automatic solution for the given problem.</p>\n\n<p><strong>Trusted Advisor publishes metrics about check results to CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify you when the service quota is reached or exceeded</strong> - When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your check results to CloudWatch. You can view the metrics in CloudWatch. You can also create alarms to detect status changes to Trusted Advisor checks and status changes for resources, and service quota usage (formerly referred to as limits). The alarm will then notify you when you reach or exceed a service quota for your AWS account. However, the alarm is triggered only when the service limit is reached. We need a solution that raises an alarm when the number of API calls randomly increases or an abnormal pattern is detected. Hence, this option is not the right fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html</a></p>\n',
        answers: [
          "<p>Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Kinesis stream-level metrics in the CloudWatch to trigger an AWS Lambda function that will trigger an error workflow</p>",
          "<p>Run Amazon Athena SQL queries against CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards</p>",
          "<p>Trusted Advisor publishes metrics about check results to CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded</p>",
          "<p>Create an Amazon CloudWatch metric filter that processes CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an SNS notification to the required team</p>",
        ],
      },
      correct_response: ["d"],
      section: "Security Logging and Monitoring",
      question_plain:
        "While consolidating logs for the weekly reporting, a development team at a retail company realized that an unusually large number of illegal AWS API queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs.\n\nWhich of the following represents the best solution for the given scenario?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165434,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>As part of the organization-wide security best practices, a company has mandated that all software installed on the EC2 instances should be upgraded to its most recent authorized version every 30 days. For this requirement, the Security Administrator has to provide a weekly report that lists all the instances that do not have the latest software updates deployed.</p>\n\n<p>What is the most optimal way to implement this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use Patch Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule, install available software updates on a schedule, and scan targets on demand</strong></p>\n\n<p>Patch Manager, a capability of AWS Systems Manager, automates the process of patching managed nodes with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. You can use Patch Manager to install Service Packs on Windows nodes and perform minor version upgrades on Linux nodes.</p>\n\n<p>Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, in addition to a list of approved and rejected patches. You can install patches regularly by scheduling patching to run as a Systems Manager maintenance window task. You can also install patches individually or to a large group of managed nodes by using tags.</p>\n\n<p>Patch Manager provides options to scan your managed nodes and report compliance on a schedule, install available patches on a schedule, and patch or scan targets on demand whenever you need to. You can also generate patch compliance reports that are sent to an Amazon Simple Storage Service (Amazon S3) bucket of your choice. You can generate one-time reports, or generate reports on a regular schedule. For a single managed node, reports include details of all patches for the node. For a report on all managed nodes, only a summary of how many patches are missing is provided.</p>\n\n<p>Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon EventBridge to provide a secure patching experience that includes event notifications and the ability to audit usage.</p>\n\n<p>How AWS Systems Manager works:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q30-i1.jpg">\nvia - <a href="https://aws.amazon.com/systems-manager/">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Inspector to determine the systems that do not have the latest patches applied after a time of 30 days and configure Inspector to redeploy these instances with the latest AMI version</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. Amazon Inspector removes the operational overhead associated with deploying and configuring a vulnerability management solution by allowing you to deploy Amazon Inspector across all accounts with a single click. Amazon Inspector, however, does not cater to patch management requirements.</p>\n\n<p><strong>Use Version Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule, install available software versions on a schedule, and scan targets on demand</strong> - There is no such feature as Version Manager within AWS Systems Manager. This is a made-up option, meant to serve as a distractor.</p>\n\n<p><strong>Use Change Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule and install available software updates on a schedule through automated runbooks</strong> - Change Manager, a capability of AWS Systems Manager, is an enterprise change management framework for requesting, approving, implementing, and reporting on operational changes to your application configuration and infrastructure. From a single delegated administrator account, if you use AWS Organizations, you can manage changes across multiple AWS accounts and AWS Regions. Change Manager is not meant for managing software updates via patching.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href="https://aws.amazon.com/about-aws/whats-new/2017/11/aws-announces-aws-systems-manager/">https://aws.amazon.com/about-aws/whats-new/2017/11/aws-announces-aws-systems-manager/</a></p>\n',
        answers: [
          "<p>Use Amazon Inspector to determine the systems that do not have the latest patches applied after a time of 30 days and configure Inspector to redeploy these instances with the latest AMI version</p>",
          "<p>Use Version Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule, install available software versions on a schedule, and scan targets on demand</p>",
          "<p>Use Patch Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule, install available software updates on a schedule, and scan targets on demand</p>",
          "<p>Use Change Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule and install available software updates on a schedule through automated runbooks</p>",
        ],
      },
      correct_response: ["c"],
      section: "Management and Security Governance",
      question_plain:
        "As part of the organization-wide security best practices, a company has mandated that all software installed on the EC2 instances should be upgraded to its most recent authorized version every 30 days. For this requirement, the Security Administrator has to provide a weekly report that lists all the instances that do not have the latest software updates deployed.\n\nWhat is the most optimal way to implement this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165436,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company maintains separate AWS accounts for its various lines of business. All the accounts are configured with Amazon GuardDuty to detect threats and malicious activities. A partner security firm generates a common threat list quarterly and shares it with all the business lines.</p>\n\n<p>As a Security Engineer, how will you configure the threat list across all AWS accounts with minimum effort? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Upload the threat list to an Amazon S3 bucket and share the access with the administrator account</strong></p>\n\n<p><strong>Specify an administrator account in GuardDuty and then use the administrator account to invite other AWS accounts to become member accounts. Add the threat list to the administrator account by referencing the S3 object that contains the threat list</strong></p>\n\n<p>If the accounts you want to associate with are not part of your AWS Organizations organization, you can specify an administrator account in GuardDuty and then use the administrator account to invite other AWS accounts to become member accounts. When the invited account accepts the invitation, that account becomes a GuardDuty member account associated with the administrator account.</p>\n\n<p>When you use GuardDuty in a multiple-account environment, the administrator account can manage certain aspects of GuardDuty on behalf of the member accounts. The primary functions the administrator account can perform are the following:</p>\n\n<ol>\n<li><p>Add and remove associated member accounts. The process by which this is done differs based on whether the accounts are associated through organizations or by invitation.</p></li>\n<li><p>Manage the status of GuardDuty within associated member accounts, including enabling and suspending GuardDuty.</p></li>\n<li><p>Customize findings within the GuardDuty network through the creation and management of suppression rules, trusted IP lists, and threat lists. Member accounts lose access to these features in a multiple-account environment.</p></li>\n</ol>\n\n<p>Relationship between GuardDuty administrator and member accounts:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q31-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html</a></p>\n\n<p>For the given use case, you can upload the threat list to an Amazon S3 bucket and share the access with the administrator account for referencing the S3 object that contains the threat list, which is finally propagated through the GuardDuty network across all accounts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload the threat list to an Amazon S3 bucket and share the access with the organization\'s delegated administrator for GuardDuty</strong> - This statement is correct only if AWS Organizations is used by the company. Since it is not specified for the given use case, so this option is not the right fit.</p>\n\n<p><strong>Configure all AWS accounts to be part of AWS Organizations and add the threat list to all members of the organization using AWS Resource Access Manager (RAM)</strong> - AWS Organizations is not mentioned in the given use case, so this option is not the right fit. Also, the organization\'s delegated administrator for GuardDuty will upload a threat list to all member accounts, it is not done through AWS Resource Access Manager.</p>\n\n<p><strong>Upload the threat list to an Amazon S3 bucket and trigger an Amazon EventBridge event every time a new threat list is added to the bucket. Define Amazon GuardDuty as a target to EventBridge to automatically configure the threat list to the administrator account in GuardDuty</strong> - Amazon GuardDuty is not a supported target for Amazon EventBridge and hence this option is invalid.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html</a></p>\n',
        answers: [
          "<p>Upload the threat list to an Amazon S3 bucket and share the access with the organization's delegated administrator for GuardDuty</p>",
          "<p>Configure all AWS accounts to be part of AWS Organizations and add the threat list to all members of the organization using AWS Resource Access Manager (RAM)</p>",
          "<p>Upload the threat list to an Amazon S3 bucket and trigger an Amazon EventBridge event every time a new threat list is added to the bucket. Define Amazon GuardDuty as a target to EventBridge to automatically configure the threat list to the administrator account in GuardDuty</p>",
          "<p>Upload the threat list to an Amazon S3 bucket and share the access with the administrator account</p>",
          "<p>Specify an administrator account in GuardDuty and then use the administrator account to invite other AWS accounts to become member accounts. Add the threat list to the administrator account by referencing the S3 object that contains the threat list</p>",
        ],
      },
      correct_response: ["d", "e"],
      section: "Infrastructure Security",
      question_plain:
        "A company maintains separate AWS accounts for its various lines of business. All the accounts are configured with Amazon GuardDuty to detect threats and malicious activities. A partner security firm generates a common threat list quarterly and shares it with all the business lines.\n\nAs a Security Engineer, how will you configure the threat list across all AWS accounts with minimum effort? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165438,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A security engineer has configured trusted IP lists and threat lists on Amazon GuardDuty to monitor the security of the AWS environment. Consider the following scenarios:</p>\n\n<p>a) While configuring the lists the engineer mistakenly added the same IP to both lists. What is the outcome of this configuration?</p>\n\n<p>b) To grant the identities full access (such as renaming, deactivating, uploading, activating, deleting) for working with trusted IP lists and threat lists, which managed policy needs to be added? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>The IP will be processed by the trusted IP list first, and will not generate a finding</strong></p>\n\n<p>Trusted IP lists consist of IP addresses that you have trusted for secure communication with your AWS infrastructure and applications. GuardDuty does not generate VPC flow logs or CloudTrail findings for IP addresses on trusted IP lists.</p>\n\n<p>Threat lists consist of known malicious IP addresses. This list can be supplied by third-party threat intelligence or created specifically for your organization. In addition to generating findings because of potentially suspicious activity, GuardDuty also generates findings based on these threat lists.</p>\n\n<p>If you include the same IP on both a trusted IP list and a threat list it will be processed by the trusted IP list first, and will not generate a finding.</p>\n\n<p><strong>Attach <code>AmazonGuardDutyFullAccess</code> managed policy to provide full access privileges to an identity to work with trusted IP lists and threat lists. You also need to add the following privileges</strong></p>\n\n<pre><code>{\n    "Effect": "Allow",\n    "Action": [\n        "iam:PutRolePolicy",\n        "iam:DeleteRolePolicy"\n    ],\n    "Resource": "arn:aws:iam::123456789123:role/aws-service-role/guardduty.amazonaws.com/AWSServiceRoleForAmazonGuardDuty"\n}\n</code></pre>\n\n<p>Various IAM identities require special permissions to work with trusted IP lists and threat lists in GuardDuty. Identity with the attached <code>AmazonGuardDutyFullAccess</code> managed policy can only rename and deactivate uploaded trusted IP lists and threat lists.</p>\n\n<p>Permissions required to upload trusted IP lists and threat lists:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q32-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The IP will be processed by the threat IP list first, and will generate findings</strong> - As explained above, this statement is incorrect.</p>\n\n<p><strong>Attach <code>AmazonGuardDutyFullAccess</code> managed policy to provide full access privileges to an identity to work with trusted IP lists and threat lists</strong> - An identity with the attached <code>AmazonGuardDutyFullAccess</code> managed policy can only rename and deactivate uploaded trusted IP lists and threat lists.</p>\n\n<p><strong>Attach <code>AWSServiceRoleForAmazonGuardDuty</code> policy to your IAM entities to provide full access privileges to an identify to work with trusted IP lists and threat lists</strong> - This statement is incorrect. You can\'t attach AWSServiceRoleForAmazonGuardDuty to your IAM entities. This AWS-managed policy is attached to a service-linked role that allows GuardDuty to perform actions on your behalf.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonGuardDutyFullAccess">https://docs.aws.amazon.com/guardduty/latest/ug/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonGuardDutyFullAccess</a></p>\n',
        answers: [
          "<p>The IP will be processed by the trusted IP list first, and will not generate a finding</p>",
          "<p>The IP will be processed by the threat IP list first, and will generate findings</p>",
          '<p>Attach <code>AmazonGuardDutyFullAccess</code> managed policy to provide full access privileges to an identity to work with trusted IP lists and threat lists. You also need to add the following privileges\n<code>{\n    "Effect": "Allow",\n    "Action": [\n        "iam:PutRolePolicy",\n        "iam:DeleteRolePolicy"\n    ],\n    "Resource": "arn:aws:iam::123456789123:role/aws-service-role/guardduty.amazonaws.com/AWSServiceRoleForAmazonGuardDuty"\n}\n</code></p>',
          "<p>Attach <code>AmazonGuardDutyFullAccess</code> managed policy to provide full access privileges to an identity to work with trusted IP lists and threat lists</p>",
          "<p>Attach <code>AWSServiceRoleForAmazonGuardDuty</code> policy to your IAM entities to provide full access privileges to an identity to work with trusted IP lists and threat lists</p>",
        ],
      },
      correct_response: ["a", "c"],
      section: "Infrastructure Security",
      question_plain:
        "A security engineer has configured trusted IP lists and threat lists on Amazon GuardDuty to monitor the security of the AWS environment. Consider the following scenarios:\n\na) While configuring the lists the engineer mistakenly added the same IP to both lists. What is the outcome of this configuration?\n\nb) To grant the identities full access (such as renaming, deactivating, uploading, activating, deleting) for working with trusted IP lists and threat lists, which managed policy needs to be added? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165382,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A media company uses Amazon S3 to store the images uploaded by the users. These images are kept encrypted in S3 by using AWS-KMS and the company manages its own Customer Master Key (CMK) for encryption. A member of the security team accidentally deleted the CMK a day ago, thereby rendering the user's photo data unrecoverable. As an AWS Certified Security Specialist, you have been tasked by the company to provide a solution for this issue.</p>\n\n<p>Which of the following steps would you recommend to solve this issue?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>As the CMK was deleted a day ago, it must be in the \'pending deletion\' status and hence you can just cancel the CMK deletion and recover the key</strong></p>\n\n<p>AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-3.</p>\n\n<p>Deleting a customer master key (CMK) in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. Therefore, AWS KMS enforces a waiting period. To delete a CMK in AWS KMS you schedule key deletion. You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. The default waiting period is 30 days. During the waiting period, the CMK status and key state is Pending deletion. To recover the CMK, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the CMK.</p>\n\n<p>How Deleting Customer Master Keys Works:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q4-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Contact AWS support to retrieve the CMK from their backup</strong></p>\n\n<p><strong>The CMK can be recovered by the AWS root account user</strong></p>\n\n<p>The AWS root account user cannot recover CMK and the AWS support does not have access to CMK via any backups. Both these options have been added as distractors.</p>\n\n<p><strong>The company should issue a notification on its web application informing the users about the loss of their data</strong> - This option is not required as the data can be recovered via the cancel key deletion feature.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys-scheduling-key-deletion.html">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys-scheduling-key-deletion.html</a></p>\n',
        answers: [
          "<p>Contact AWS support to retrieve the CMK from their backup</p>",
          "<p>The company should issue a notification on its web application informing the users about the loss of their data</p>",
          "<p>As the CMK was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the CMK deletion and recover the key</p>",
          "<p>The CMK can be recovered by the AWS root account user</p>",
        ],
      },
      correct_response: ["c"],
      section: "Data Protection",
      question_plain:
        "A media company uses Amazon S3 to store the images uploaded by the users. These images are kept encrypted in S3 by using AWS-KMS and the company manages its own Customer Master Key (CMK) for encryption. A member of the security team accidentally deleted the CMK a day ago, thereby rendering the user's photo data unrecoverable. As an AWS Certified Security Specialist, you have been tasked by the company to provide a solution for this issue.\n\nWhich of the following steps would you recommend to solve this issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165330,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A security engineer has configured a unified CloudWatch agent to push Amazon EC2 logs to Amazon CloudWatch Logs. However, the security team can't see any logs in the CloudWatch Logs console.</p>\n\n<p>Why isn't the unified CloudWatch agent pushing log events? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Creating an Amazon Machine Image (AMI) after the CloudWatch agent is installed can lead to errors in the CloudWatch agent</strong></p>\n\n<p>It\'s a best practice to install the CloudWatch agent at launch using AWS CloudFormation, AWS Systems Manager Agent (SSM Agent), user data scripts, or the AWS CLI. It is also a best practice to create an AMI before installing the CloudWatch agent. AMIs typically capture unique information from the original instance. Metadata becomes out of sync, and this state can lead to the CloudWatch agent not working as intended. Out-of-sync metadata is the reason that many Windows instances require Sysprep when working with AMI.</p>\n\n<p><strong>IAM user or IAM role policy should include the following IAM permissions:</strong></p>\n\n<pre><code>"logs:CreateLogGroup",\n"logs:CreateLogStream",\n"logs:PutLogEvents",\n"logs:DescribeLogStreams"\n</code></pre>\n\n<p>The CloudWatch agent uses credentials from either the IAM user or IAM role policy to push log events to the CloudWatch service. Before a log event can be published, you must create a log group and log stream. If there\'s no log group or log stream, the CloudWatch agent creates them.</p>\n\n<p>You must also confirm that your policy includes the following IAM permissions:</p>\n\n<pre><code>"logs:CreateLogGroup",\n"logs:CreateLogStream",\n"logs:PutLogEvents",\n"logs:DescribeLogStreams"\n</code></pre>\n\n<p>Add any missing IAM permissions to the user policy or the role policy.</p>\n\n<p>Complete list of scenarios to be checked:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q33-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-logs-with-unified-agent/">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-logs-with-unified-agent/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Installing the CloudWatch agent after creating the Amazon Machine Image (AMI) can lead to errors in CloudWatch agent</strong> - As explained above, this option is incorrect.</p>\n\n<p><strong>If the CloudWatch Logs endpoint is configured to be a public endpoint using an internet gateway the connectivity fails. VPC endpoints have to be used to keep the log files in the AWS network</strong> - This statement is incorrect. Publicly routable endpoints accessed through an internet gateway or a network address translation (NAT) gateway is an acceptable configuration for a unified CloudWatch agent.</p>\n\n<p><strong>CloudWatch agent runs into errors if <code>run_as_user</code> parameter is any user other than the root user</strong> - CloudWatch agent can be run as a non-root user too. If you\'re using the run_as_user parameter, the user should have permission to the log location path. Without the necessary permissions, the CloudWatch agent can\'t write logs to the location. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-logs-with-unified-agent/">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-logs-with-unified-agent/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/general/latest/gr/rande.html#cwl_region">https://docs.aws.amazon.com/general/latest/gr/rande.html#cwl_region</a></p>\n',
        answers: [
          "<p>If the CloudWatch Logs endpoint is configured to be a public endpoint using an internet gateway the connectivity fails. VPC endpoints have to be used to keep the log files in AWS network</p>",
          "<p>Installing the CloudWatch agent after creating the Amazon Machine Image (AMI) can lead to errors in the CloudWatch agent</p>",
          "<p>Creating an Amazon Machine Image (AMI) after the CloudWatch agent is installed can lead to errors in the CloudWatch agent</p>",
          '<p>IAM user or IAM role policy should include the following IAM permissions:</p>\n\n<pre><code>"logs:CreateLogGroup",\n"logs:CreateLogStream",\n"logs:PutLogEvents",\n"logs:DescribeLogStreams"\n</code></pre>',
          "<p>CloudWatch agent runs into errors if <code>run_as_user</code> parameter is any user other than the root user</p>",
        ],
      },
      correct_response: ["c", "d"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A security engineer has configured a unified CloudWatch agent to push Amazon EC2 logs to Amazon CloudWatch Logs. However, the security team can't see any logs in the CloudWatch Logs console.\n\nWhy isn't the unified CloudWatch agent pushing log events? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165334,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has migrated most of its business to AWS Cloud using Amazon EC2 instances for Windows to host its applications. The domain services used by these applications are built on Active Directory servers which have been retained as on-premises servers. The company has issued guidelines to enable GuardDuty for all its applications.</p>\n\n<p>While analyzing GuardDuty reports, the security team realized that DNS logs are not being tracked/reported by GuardDuty. How will you fix this issue?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>If you use a custom DNS resolver, then GuardDuty cannot access and process data from this data source</strong></p>\n\n<p>If you use AWS DNS resolvers for your Amazon EC2 instances (the default setting), then GuardDuty can access and process your request and response DNS logs through the internal AWS DNS resolvers. If you use another DNS resolver, such as OpenDNS or GoogleDNS, or if you set up your own DNS resolvers, then GuardDuty cannot access and process data from this data source.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>GuardDuty reports only on VPC Flow Logs, CloudTrail global events, and Kubernetes audit logs. GuardDuty does not analyze DNS logs</strong> - This statement is incorrect. GuardDuty can analyze DNS logs for AWS DNS resolvers.</p>\n\n<p><strong>GuardDuty analyzes your DNS logs from the stream of data provided through the Route 53 Resolver query logging feature. Check the path specified in this configuration</strong> - This statement is incorrect. When you enable GuardDuty, it immediately starts analyzing your DNS logs from an independent stream of data. This data stream is separate from the data provided through the Route 53 Resolver query logging feature. Configuration of this feature does not affect GuardDuty analysis.</p>\n\n<p><strong>Check the permissions attached on the IAM role used by GuardDuty for accessing DNS logs</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_data-sources.html">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_data-sources.html</a></p>\n',
        answers: [
          "<p>GuardDuty reports only on VPC Flow Logs, CloudTrail global events, and Kubernetes audit logs. GuardDuty does not analyze DNS logs</p>",
          "<p>GuardDuty analyzes your DNS logs from the stream of data provided through the Route 53 Resolver query logging feature. Check the path specified in this configuration</p>",
          "<p>If you use a custom DNS resolver, then GuardDuty cannot access and process data from this data source</p>",
          "<p>Check the permissions attached on the IAM role used by GuardDuty for accessing DNS logs</p>",
        ],
      },
      correct_response: ["c"],
      section: "Infrastructure Security",
      question_plain:
        "A company has migrated most of its business to AWS Cloud using Amazon EC2 instances for Windows to host its applications. The domain services used by these applications are built on Active Directory servers which have been retained as on-premises servers. The company has issued guidelines to enable GuardDuty for all its applications.\n\nWhile analyzing GuardDuty reports, the security team realized that DNS logs are not being tracked/reported by GuardDuty. How will you fix this issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165336,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          '<p>The security team at a company needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The team created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose. The team created the following IAM policy and attached it to an IAM role:</p>\n\n<pre><code>{\n  "Version": "2012-10-17",\n  "Id": "key-policy-1",\n  "Statement": [\n    {\n      "Sid": "GetPut",\n      "Effect": "Allow",\n      "Action": [\n        "s3:GetObject",\n        "s3:PutObject"\n      ],\n      "Resource": "arn:aws:s3:::ExampleBucket/*"\n    },\n    {\n      "Sid": "KMS",\n      "Effect": "Allow",\n      "Action": [\n        "kms:Decrypt",\n        "kms:Encrypt"\n      ],\n      "Resource": "arn:aws:kms:us-west-1:111122223333:key/keyid-12345"\n    }\n  ]\n}\n</code></pre>\n\n<p>The team was able to successfully get existing objects from the S3 bucket while testing. But any attempts to upload a new object resulted in an error. The error message stated that the action was forbidden.</p>\n\n<p>Which IAM policy action should be added to the IAM policy to resolve the error?</p>\n',
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>kms:GenerateDataKey</strong></p>\n\n<p>GenerateDataKey returns a unique symmetric data key for use outside of AWS KMS. This operation returns a plaintext copy of the data key and a copy that is encrypted under a symmetric encryption KMS key that you specify. The bytes in the plaintext key are random; they are not related to the caller or the KMS key. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q36-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>kms:GetPublicKey</strong> - This option returns the public key of an asymmetric KMS key. Unlike the private key of an asymmetric KMS key, which never leaves AWS KMS unencrypted, callers with kms:GetPublicKey permission can download the public key of an asymmetric KMS key. It cannot be used for a client-side encryption mechanism.</p>\n\n<p><strong>kms:GetKeyPolicy</strong> - This option gets a key policy attached to the specified KMS key. It cannot be used for a client-side encryption mechanism.</p>\n\n<p><strong>kms:GetDataKey</strong> - This is a made-up option that serves as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/APIReference/API_GetKeyPolicy.html">https://docs.aws.amazon.com/kms/latest/APIReference/API_GetKeyPolicy.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/APIReference/API_GetPublicKey.html">https://docs.aws.amazon.com/kms/latest/APIReference/API_GetPublicKey.html</a></p>\n',
        answers: [
          "<p>kms:GetPublicKey</p>",
          "<p>kms:GetKeyPolicy</p>",
          "<p>kms:GetDataKey</p>",
          "<p>kms:GenerateDataKey</p>",
        ],
      },
      correct_response: ["d"],
      section: "Data Protection",
      question_plain:
        'The security team at a company needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The team created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose. The team created the following IAM policy and attached it to an IAM role:\n\n{\n  "Version": "2012-10-17",\n  "Id": "key-policy-1",\n  "Statement": [\n    {\n      "Sid": "GetPut",\n      "Effect": "Allow",\n      "Action": [\n        "s3:GetObject",\n        "s3:PutObject"\n      ],\n      "Resource": "arn:aws:s3:::ExampleBucket/*"\n    },\n    {\n      "Sid": "KMS",\n      "Effect": "Allow",\n      "Action": [\n        "kms:Decrypt",\n        "kms:Encrypt"\n      ],\n      "Resource": "arn:aws:kms:us-west-1:111122223333:key/keyid-12345"\n    }\n  ]\n}\n\n\nThe team was able to successfully get existing objects from the S3 bucket while testing. But any attempts to upload a new object resulted in an error. The error message stated that the action was forbidden.\n\nWhich IAM policy action should be added to the IAM policy to resolve the error?',
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165338,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A security engineer has attached an AWS Identity and Access Management (IAM) role to an Amazon Elastic Compute Cloud (Amazon EC2) instance. Upon testing, the engineer realized that the Amazon EC2 instance makes API calls with an IAM user instead of the attached IAM role.</p>\n\n<p>What is the issue and how will you fix it?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Check if the IAM user credentials are stored in the .aws/credentials file. Because these credentials have higher precedence over role credentials, IAM user credentials will be used to make the API calls. Delete this credentials file</strong></p>\n\n<p>The AWS CLI uses credentials and configuration settings located in multiple places, such as the system or user environment variables, local AWS configuration files, or explicitly declared on the command line as a parameter. Certain locations take precedence over others. The AWS CLI credentials and configuration settings take precedence in a particular order.</p>\n\n<p>The credentials and config file are updated when you run the command aws configure. This file can contain the credential details for the default profile and any named profiles. If this file is present, it takes precedence over the IAM role defined in the instance.</p>\n\n<p>Configuration settings and precedence:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q37-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence">https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The IAM role attached does not have enough permissions to make the API calls. Hence, the default user credentials of the instance are being used for the API calls. Add the required permissions to the role</strong> - As explained above, this is an incorrect statement, given only as a distractor.</p>\n\n<p><strong>You cannot associate an IAM role with your Amazon Elastic Container Service (Amazon ECS) task definitions. While this association does not result in an error, the IAM role credentials are not used. Use service-based roles for container applications</strong> - You can associate an IAM role with each of your Amazon Elastic Container Service (Amazon ECS) task definitions. Temporary credentials for that role are then available to that task\'s containers. So this option is incorrect.</p>\n\n<p><strong>The EC2 instance needs to be refreshed after attaching the necessary IAM role. Refresh the instance and the API calls with be done using the newly attached IAM role</strong> - This option has been added as a distractor. EC2 instance refresh is a feature in EC2 Auto Scaling that enables automatic deployments of instances in Auto Scaling Groups (ASGs), in order to release new application versions or make infrastructure updates.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence">https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence</a></p>\n',
        answers: [
          "<p>The IAM role attached does not have enough permissions to make the API calls. Hence, the default user credentials of the instance are being used for the API calls. Add the required permissions to the role</p>",
          "<p>You cannot associate an IAM role with your Amazon Elastic Container Service (Amazon ECS) task definitions. While this association does not result in an error, the IAM role credentials are not used. Use service-based roles for container applications</p>",
          "<p>Check if the IAM user credentials are stored in the .aws/credentials file. Because these credentials have higher precedence over role credentials, IAM user credentials will be used to make the API calls. Delete the credentials file</p>",
          "<p>The EC2 instance needs to be refreshed after attaching the necessary IAM role. Refresh the instance and the API calls with be done using the newly attached IAM role</p>",
        ],
      },
      correct_response: ["c"],
      section: "Identity and Access Management",
      question_plain:
        "A security engineer has attached an AWS Identity and Access Management (IAM) role to an Amazon Elastic Compute Cloud (Amazon EC2) instance. Upon testing, the engineer realized that the Amazon EC2 instance makes API calls with an IAM user instead of the attached IAM role.\n\nWhat is the issue and how will you fix it?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165340,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A project manager has connected with you for a security requirement from the client. The client wants to ensure that the authenticated encryption with associated data encryption is used when calling AWS Key Management Service (AWS KMS) Encrypt, Decrypt, and ReEncrypt APIs.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend to address this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use encryption context that you can use to verify the authenticity of AWS KMS API calls, and the integrity of the ciphertext returned by the AWS Decrypt API</strong></p>\n\n<p>All AWS KMS cryptographic operations with symmetric encryption KMS keys accept an encryption context, an optional set of key–value pairs that can contain additional contextual information about the data. AWS KMS uses the encryption context as additional authenticated data (AAD) to support authenticated encryption.</p>\n\n<p>When you include an encryption context in an encryption request, it is cryptographically bound to the ciphertext such that the same encryption context is required to decrypt (or decrypt and re-encrypt) the data. If the encryption context provided in the decryption request is not an exact, case-sensitive match, the decrypt request fails. Only the order of the key-value pairs in the encryption context can vary.</p>\n\n<p>The encryption context is not secret and not encrypted. It appears in plaintext in AWS CloudTrail Logs so you can use it to identify and categorize your cryptographic operations. Your encryption context should not include sensitive information.</p>\n\n<p>An example explaining the usage of encryption context in KMS:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q38-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/how-to-protect-the-integrity-of-your-encrypted-data-by-using-aws-key-management-service-and-encryptioncontext/">https://aws.amazon.com/blogs/security/how-to-protect-the-integrity-of-your-encrypted-data-by-using-aws-key-management-service-and-encryptioncontext/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the envelope encryption strategy of AWS KMS to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</strong> - AWS KMS supports sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency. So, using envelope encryption for validating the KMS API request for the given use case does not make sense.</p>\n\n<p><strong>Use multi-factor authentication (MFA) to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</strong> - When you enable multi-factor authentication (MFA), users must sign in to the AWS access portal with their user name and password. This is the first factor, something they know. Users must also sign in with either a code or security key. This is the second factor, something they have or something they are. The second factor could be either an authentication code generated from their mobile device or alternatively by tapping on a security key connected to their computer. Taken together, these multiple factors provide increased security by preventing unauthorized access to your AWS resources unless a valid MFA challenge has been successfully completed.</p>\n\n<p>MFA does not apply to the given use case where we want the authenticated encryption with associated data encryption to be used with KMS API calls.</p>\n\n<p><strong>Use AWS CloudHSM to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</strong> - The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a tamper-resistant hardware device. HSMs are designed to securely store cryptographic key material and use the key material without exposing it outside the cryptographic boundary of the hardware.</p>\n\n<p>Using CloudHSM is not relevant to the given use case where we want the authenticated encryption with associated data encryption to be used with KMS API calls.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context</a></p>\n\n<p><a href="https://docs.aws.amazon.com/wellarchitected/latest/financial-services-industry-lens/use-envelope-encryption-with-customer-master-keys.html">https://docs.aws.amazon.com/wellarchitected/latest/financial-services-industry-lens/use-envelope-encryption-with-customer-master-keys.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-sdk.html">https://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-sdk.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/singlesignon/latest/userguide/enable-mfa.html">https://docs.aws.amazon.com/singlesignon/latest/userguide/enable-mfa.html</a></p>\n\n<p><a href="https://aws.amazon.com/cloudhsm/">https://aws.amazon.com/cloudhsm/</a></p>\n',
        answers: [
          "<p>Use encryption context that you can use to verify the authenticity of AWS KMS API calls and the integrity of the ciphertext returned by the AWS Decrypt API</p>",
          "<p>Use envelope encryption strategy of AWS KMS to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</p>",
          "<p>Use AWS CloudHSM to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</p>",
          "<p>Use multi-factor authentication (MFA) to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</p>",
        ],
      },
      correct_response: ["a"],
      section: "Management and Security Governance",
      question_plain:
        "A project manager has connected with you for a security requirement from the client. The client wants to ensure that the authenticated encryption with associated data encryption is used when calling AWS Key Management Service (AWS KMS) Encrypt, Decrypt, and ReEncrypt APIs.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend to address this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165342,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An Amazon EC2 instance connects to an Amazon S3 bucket using an IAM role with necessary permissions. While analyzing the logs, a security engineer raised the possibility of the instance being compromised. The instance hosts a critical application and cannot be immediately terminated.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following will you suggest as the fastest way to block further access to sensitive data from the compromised instance?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Revoke all active sessions for the IAM role. Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance profile</strong></p>\n\n<p>Revoke all active sessions for the IAM role - This step ensures that all users with current sessions created by assuming the role are denied access to all AWS actions and resources. This can result in users losing unsaved work but the compromised sessions trying to access the critical S3 data will also be dropped.</p>\n\n<p>Update the S3 bucket policy to deny access to the IAM role - This step will force S3 to deny access to the IAM role.</p>\n\n<p>Remove the IAM role from the EC2 instance profile - Finally, remove the IAM role from the compromised instance to stop further access to S3 bucket.</p>\n\n<p>Revoking active sessions and session permissions:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q39-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Remove the IAM role from the EC2 instance profile. Block all public access to the S3 bucket. To allow access to your S3 objects to trusted entities outside your account, create a pre-signed URL through S3</strong> - This option has been added as a distractor. This will not immediately stop the active sessions from accessing the S3 data from the instance.</p>\n\n<p><strong>Update the S3 bucket policy to deny access to the IAM role. Create an Amazon EBS snapshot of the instance and terminate the instance</strong> - This will not immediately stop the active sessions from accessing the S3 data from the instance. Also, it is mentioned in the question that the instance cannot be terminated. Hence, this option is incorrect.</p>\n\n<p><strong>Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance profile. Use S3 Access Points to create permission sets that restrict access to only those within your private network</strong> - This option as well does not consider the active sessions that need to be dropped immediately to curb further access to the critical S3 data.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_remediate.html#compromised-ec2">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_remediate.html#compromised-ec2</a></p>\n',
        answers: [
          "<p>Remove the IAM role from the EC2 instance profile. Block all public access to the S3 bucket. To allow access to your S3 objects to trusted entities outside your account, create a pre-signed URL through S3</p>",
          "<p>Revoke all active sessions for the IAM role. Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance profile</p>",
          "<p>Update the S3 bucket policy to deny access to the IAM role. Create an Amazon EBS snapshot of the instance and terminate the instance</p>",
          "<p>Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance profile. Use S3 Access Points to create permission sets that restrict access to only those within your private network</p>",
        ],
      },
      correct_response: ["b"],
      section: "Infrastructure Security",
      question_plain:
        "An Amazon EC2 instance connects to an Amazon S3 bucket using an IAM role with necessary permissions. While analyzing the logs, a security engineer raised the possibility of the instance being compromised. The instance hosts a critical application and cannot be immediately terminated.\n\nAs an AWS Certified Security Specialist, which of the following will you suggest as the fastest way to block further access to sensitive data from the compromised instance?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165344,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A pharmaceutical company is showcasing its new business lines and is promoting them to its partner organizations. These flagship applications are hosted on Amazon EC2 instances. The technology teams at the partner organizations are expected to access these instances for a first-hand understanding of these applications. The EC2 instances will be shared, and non-root SSH access is needed for the teams.</p>\n\n<p>As a Security Engineer, how will you block the EC2 instance metadata service for the given use case to avoid an assault on other AWS account resources?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Implement local firewall rules using iptables based restrictions on the instances</strong></p>\n\n<p>iptables is a command line interface used to set up and maintain tables for the Netfilter firewall for IPv4, included in the Linux kernel. The firewall matches packets with rules defined in these tables and then takes the specified action on a possible match.</p>\n\n<p>You can consider using local firewall rules to disable access from some or all processes to the instance metadata service. The following example uses Linux iptables and its owner module to prevent the Apache webserver (based on its default installation user ID of apache) from accessing 169.254.169.254. It uses a deny rule to reject all instance metadata requests (whether IMDSv1 or IMDSv2) from any process running as that user.</p>\n\n<p><code>sudo iptables --append OUTPUT --proto tcp --destination 169.254.169.254 --match owner --uid-owner apache --jump REJECT</code></p>\n\n<p>The following example prevents access to the instance metadata service by all processes, except for processes running in the user account trustworthy-user.</p>\n\n<p><code>sudo iptables --append OUTPUT --proto tcp --destination 169.254.169.254 --match owner ! --uid-owner trustworthy-user --jump REJECT</code></p>\n\n<p>Using iptables to limit access:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q40-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the instance metadata service on each instance so that users must use Instance Metadata Service Version 2 (IMDSv2). The session-oriented methods will not respond to usual request/response queries</strong> - You can access instance metadata from a running instance using one of the following methods:\n1. Instance Metadata Service Version 1 (IMDSv1) – a request/response method\n2. Instance Metadata Service Version 2 (IMDSv2) – a session-oriented method</p>\n\n<p>By default, you can use either IMDSv1 or IMDSv2, or both. The instance metadata service distinguishes between IMDSv1 and IMDSv2 requests based on whether, for any given request, either the PUT or GET headers, which are unique to IMDSv2, are present in that request.</p>\n\n<p>This solution will not help block access to instance metadata as needed.</p>\n\n<p><strong>Disable the instance metadata service on all the instances</strong> - An application on the instance retrieves the security credentials provided by the role from the instance metadata item <code>iam/security-credentials/role-name</code>. The application is granted the permissions for the actions and resources that you\'ve defined for the role through the security credentials associated with the role. Therefore, disabling metadata service can cause applications that use the roles to crash. Hence, this option is incorrect.</p>\n\n<p><strong>Install intrusion prevention software (IPS) on each instance to disable access to instance metadata</strong> - EC2 Instance IDS/IPS solutions offer key features to help protect your EC2 instances. This includes alerting administrators of malicious activity and policy violations, as well as identifying and taking action against attacks. This solution is not useful for blocking access to instance metadata.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/configuring-instance-metadata-service.html">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/configuring-instance-metadata-service.html</a></p>\n\n<p><a href="https://aws.amazon.com/mp/scenarios/security/ids/">https://aws.amazon.com/mp/scenarios/security/ids/</a></p>\n',
        answers: [
          "<p>Configure the instance metadata service on each instance so that users must use Instance Metadata Service Version 2 (IMDSv2). The session-oriented methods will not respond to usual request/response queries</p>",
          "<p>Implement local firewall rules using iptables based restrictions on the instances</p>",
          "<p>Disable the instance metadata service on all the instances</p>",
          "<p>Install intrusion prevention software (IPS) on each instance to disable access to instance metadata</p>",
        ],
      },
      correct_response: ["b"],
      section: "Infrastructure Security",
      question_plain:
        "A pharmaceutical company is showcasing its new business lines and is promoting them to its partner organizations. These flagship applications are hosted on Amazon EC2 instances. The technology teams at the partner organizations are expected to access these instances for a first-hand understanding of these applications. The EC2 instances will be shared, and non-root SSH access is needed for the teams.\n\nAs a Security Engineer, how will you block the EC2 instance metadata service for the given use case to avoid an assault on other AWS account resources?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165346,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>As a Security Engineer, you received a notification from AWS about suspicious activity in your account. What are the security checks/actions that you will need to perform before responding to the AWS Support Center? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Create new access keys and modify the application to use new ones. Deactivate the exposed account access keys immediately. Subsequently, delete the exposed keys only when you have verified the proper functioning of the application</strong></p>\n\n<ol>\n<li>Create a new AWS access key.</li>\n<li>Modify your application to use the new access key.</li>\n<li>Deactivate the original access key. Don\'t delete the original access key yet. Deactivate the original access key only.</li>\n<li>Verify that there aren\'t any issues with your application. If there are issues, reactivate the original access key temporarily to remediate the problem.</li>\n<li>If your application is fully functional after deactivating the original access key, then delete the original access key.</li>\n<li>Delete the AWS account root user access keys that you no longer need or didn\'t create.</li>\n</ol>\n\n<p><strong>If you must retain an EC2 instance for regulatory, compliance, or legal reasons, then create an Amazon EBS snapshot before terminating the instance</strong></p>\n\n<p>Delete any unrecognized or unauthorized resources. If you must keep any resources for investigation, consider backing up those resources. For example, if you must retain an EC2 instance for regulatory, compliance, or legal reasons, then create an Amazon EBS snapshot before terminating the instance.</p>\n\n<p>Steps to follow to delete any unrecognized or unauthorized resources:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q41-i1.jpg">\nvia - <a href="https://aws.amazon.com//premiumsupport/knowledge-center/potential-account-compromise/">https://aws.amazon.com//premiumsupport/knowledge-center/potential-account-compromise/</a></p>\n\n<p><strong>In the IAM console, under the Permissions tab, look for a policy named <code>AWSExposedCredentialPolicy_DO_NOT_REMOVE</code>. If the user has this policy attached, then rotate the access keys for the user</strong> - Rotate any potentially unauthorized IAM user credentials.</p>\n\n<p>Rotate any potentially unauthorized IAM user credentials:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q41-i2.jpg">\nvia - <a href="https://aws.amazon.com//premiumsupport/knowledge-center/potential-account-compromise/">https://aws.amazon.com//premiumsupport/knowledge-center/potential-account-compromise/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>To contain access for an IAM principal where an IAM access key has been compromised, the access key can be deactivated or deleted. It is important to note that an IAM principal can have up to five access keys at any given time</strong> - While it is correct that it is necessary to contain access for an IAM principal where an IAM access key has been compromised, an IAM principal can only have up to two access keys at any given time and not five as given in the statement.</p>\n\n<p><strong>If the exposed EC2 instance cannot be shut down, move it to Isolation VPC to contain the exposure of other resources while having the ability to keep the instance working</strong> - Isolation VPCs can be used to provide effective containment of resources while providing access to legitimate traffic. Isolation VPCs can be preconfigured in advance of a security event to permit valid IP addresses and ports, and targeted resources can immediately be moved into this isolation VPC during an active security event to contain the resource while allowing legitimate traffic to be sent and received by the targeted resource during subsequent phases of incident response.</p>\n\n<p>An important aspect of using an isolation VPC is that resources, such as EC2 instances, need to be shut down and relaunched in the new isolation VPC before use.</p>\n\n<p><strong>There is no need to revoke any temporary IAM security credentials</strong> - Temporary credentials are typically used by IAM roles and do not have to be rotated or explicitly revoked because they have a limited lifetime. In cases where a security event occurs involving a temporary security credential before the temporary security credential expiration, you might need to alter the effective permissions of the existing temporary security credentials. When you permit users to access the AWS Management Console with a long session duration time (such as 12 hours), their temporary credentials do not expire as quickly. If users inadvertently expose their credentials to an unauthorized third-party, that party has access for the duration of the session. However, you can immediately revoke all permissions to the role\'s credentials issued before a certain point in time if you need to. All temporary credentials for that role issued before the specified time become invalid. This forces all users to re-authenticate and request new credentials.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/technique-access-containment.html">https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/technique-access-containment.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/destination-containment.html">https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/destination-containment.html</a></p>\n',
        answers: [
          "<p>Create new access keys and modify the application to use new ones. Deactivate the exposed account access keys immediately. Subsequently, delete the exposed keys only when you have verified the proper functioning of the application</p>",
          "<p>To contain access for an IAM principal where an IAM access key has been compromised, the access key can be deactivated or deleted. It is important to note that an IAM principal can have up to five access keys at any given time</p>",
          "<p>If the exposed EC2 instance cannot be shut down, move it to Isolation VPC to contain the expose of other resources while having the ability to keep the instance working</p>",
          "<p>There is no need to revoke any temporary IAM security credentials</p>",
          "<p>If you must retain an EC2 instance for regulatory, compliance, or legal reasons, then create an Amazon EBS snapshot before terminating the instance</p>",
          "<p>In the IAM console, under the Permissions tab, look for a policy named <code>AWSExposedCredentialPolicy_DO_NOT_REMOVE</code>. If the user has this policy attached, then rotate the access keys for the user</p>",
        ],
      },
      correct_response: ["a", "e", "f"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "As a Security Engineer, you received a notification from AWS about suspicious activity in your account. What are the security checks/actions that you will need to perform before responding to the AWS Support Center? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165348,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A security engineer has been asked to enable AWS CloudTrail trail to log data events on an S3 bucket with an empty object prefix. The S3 bucket is owned by the Owner user. Another user Bob has a separate account that has been granted access to the S3 bucket. Bob also wants to log data events for all objects in the same S3 bucket, so Bob configures a trail and specifies the same S3 bucket with an empty object prefix.</p>\n\n<p>Consider the following events:</p>\n\n<ol>\n<li><p>Bob uploads an object to the S3 bucket with the <code>PutObject</code> API operation.</p></li>\n<li><p>Owner uploads an object to the S3 bucket.</p></li>\n</ol>\n\n<p>What will be the outcome of the two events defined above? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          "<p>Correct options:</p>\n\n<p><strong>When Bob uploaded the object, the upload event occurred in Bob's account and it matches the settings for Bob's trail. Bob's trail processes and logs the event. The Owner's trail settings also match the event, so the event is logged in Owner's trail too</strong></p>\n\n<p><strong>When the Owner uploaded the object, the upload event occurs in Owner's account and it matches the settings for Owner's trail. The trail processes and logs the event in Owner's account</strong></p>\n\n<p>When you configure your trail to log data events, you can also specify S3 objects that belong to other AWS accounts. When an event occurs on a specified object, CloudTrail evaluates whether the event matches any trails in each account. If the event matches the settings for a trail, the trail processes and logs the event for that account. Generally, both API callers and resource owners can receive events.</p>\n\n<p>If you own an S3 object and you specify it in your trail, your trail logs events that occur on the object in your account. Because you own the object, your trail also logs events when other accounts call the object.</p>\n\n<p>If you specify an S3 object in your trail, and another account owns the object, your trail only logs events that occur on that object in your account. Your trail doesn't log events that occur in other accounts.</p>\n\n<p>Logging data events for an Amazon S3 object for two AWS accounts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q42-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When Bob uploaded the object, the upload event occurred in Bob's account and it matches the settings for Bob's trail. Bob's trail processes and logs the event. The Owner's trail settings do not match the event, so the event is not logged in Owner's trail</strong></p>\n\n<p><strong>When the Owner uploaded the object, the upload event occurs in Owner's account and it matches the settings for Owner's trail. The trail processes and logs the event in Owner's account. The event also matches the trail settings of Bob, so Bob's trail too logs the event in Bob's account</strong></p>\n\n<p><strong>In both the upload events, the trail is logged only in the uploaded user's account ie Bob's upload event is only logged in Bob's account and Owner's upload event is logged only in Owner's account</strong></p>\n\n<p>These three options contradict the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a></p>\n",
        answers: [
          "<p>When Bob uploaded the object, the upload event occurred in Bob's account and it matches the settings for Bob's trail. Bob's trail processes and logs the event. The Owner's trail settings also match the event, so the event is logged in Owner's trail too</p>",
          "<p>When Bob uploaded the object, the upload event occurred in Bob's account and it matches the settings for Bob's trail. Bob's trail processes and logs the event. The Owner's trail settings do not match the event, so the event is not logged in Owner's trail</p>",
          "<p>When the Owner uploaded the object, the upload event occurs in Owner's account and it matches the settings for Owner's trail. The trail processes and logs the event in Owner's account. The event also matches trail settings of Bob, so Bob's trail logs the event in Bob's account</p>",
          "<p>When the Owner uploaded the object, the upload event occurs in Owner's account and it matches the settings for Owner's trail. The trail processes and logs the event in Owner's account</p>",
          "<p>In both the upload events, the trail is logged only in the uploaded user's account ie Bob's upload event is only logged in Bob's account and Owner's upload event is logged only in Owner's account</p>",
        ],
      },
      correct_response: ["a", "d"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A security engineer has been asked to enable AWS CloudTrail trail to log data events on an S3 bucket with an empty object prefix. The S3 bucket is owned by the Owner user. Another user Bob has a separate account that has been granted access to the S3 bucket. Bob also wants to log data events for all objects in the same S3 bucket, so Bob configures a trail and specifies the same S3 bucket with an empty object prefix.\n\nConsider the following events:\n\n\nBob uploads an object to the S3 bucket with the PutObject API operation.\nOwner uploads an object to the S3 bucket.\n\n\nWhat will be the outcome of the two events defined above? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165350,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A Security Engineer is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the engineer wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.</p>\n\n<p>Which of the following actions meets the given requirements?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          "<p>Correct option:</p>\n\n<p><strong>Set up a service control policy (SCP) that prohibits changes to CloudTrail, and attach it to the developer accounts</strong> - Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines.</p>\n\n<p>An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with <em>/</em> permissions to the user.</p>\n\n<p>SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new trail in CloudTrail from within the developer accounts with the organization trails option enabled</strong> - Configuring each developer account individually is not a viable solution to start with. In addition, any configuration changes can be undone by the user once they are logged into their individual accounts as root users.</p>\n\n<p><strong>Set up an IAM policy that prohibits changes to CloudTrail and attach it to the root user</strong> - The root user can modify this IAM policy itself, so this option is not correct.</p>\n\n<p><strong>Set up a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account</strong> - A service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. The linked service also defines how you create, modify, and delete a service-linked role.</p>\n\n<p>The linked service defines the permissions of its service-linked roles, and unless defined otherwise, only that service can assume the roles. The defined permissions include the trust policy and the permissions policy, and that permissions policy cannot be attached to any other entity such as the ARN in the master account.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n",
        answers: [
          "<p>Configure a new trail in CloudTrail from within the developer accounts with the organization trails option enabled</p>",
          "<p>Set up a service control policy (SCP) that prohibits changes to CloudTrail, and attach it to the developer accounts</p>",
          "<p>Set up an IAM policy that prohibits changes to CloudTrail and attach it to the root user</p>",
          "<p>Set up a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account</p>",
        ],
      },
      correct_response: ["b"],
      section: "Identity and Access Management",
      question_plain:
        "A Security Engineer is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the engineer wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.\n\nWhich of the following actions meets the given requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165352,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An organization has added virtual machine images, software, and a few databases to its AWS Service Catalog. These will be used by multiple development teams to build their business workloads. The organization does not want the end users to launch and manage products using their own IAM credentials.</p>\n\n<p>How will you address this requirement and implement it in the least possible time?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Add launch constraint(s) to each product in the service catalog portfolio</strong></p>\n\n<p>A launch constraint specifies the AWS Identity and Access Management (IAM) role that Service Catalog assumes when an end user launches, updates, or terminates a product. An IAM role is a collection of permissions that an IAM user or AWS service can assume temporarily to use AWS services.</p>\n\n<p>Launch constraints apply to products in the portfolio (product portfolio association). Launch constraints do not apply at the portfolio level or to a product across all portfolios. To associate a launch constraint with all products in a portfolio, you must apply the launch constraint to each product individually.</p>\n\n<p>Without a launch constraint, end users must launch and manage products using their own IAM credentials. To do so, they must have permissions for AWS CloudFormation, AWS services that the products use, and Service Catalog. By using a launch role, you can instead limit the end users\' permissions to the minimum they require for that product.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new portfolio and add the new products to it. Attach a launch constraint to this portfolio</strong> - Launch constraints are at the product level and not at the portfolio level. Hence, this option is incorrect.</p>\n\n<p><strong>Use the service actions feature of the service catalog to define rules and constraints for the products in the portfolio. A CloudFormation template can also be used for easier implementation</strong> - Service Catalog enables you to reduce administrative maintenance and end-user training while adhering to compliance and security measures. With service actions, as the administrator, you can enable end users to perform operational tasks, troubleshoot issues, run approved commands, or request permissions in Service Catalog. Service actions cannot be used for limiting user permissions on the service catalog products.</p>\n\n<p><strong>Add the newly added products under a single tag. Add tag constraints to control the end user behavior and permissions on the products</strong> - This is a made-up option. There are no tag constraints, only tag update constraints. With tag update constraints, Service Catalog administrators can allow or disallow end users to update tags on resources associated with a Service Catalog provisioned product.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-launch.html">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-launch.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/servicecatalog/latest/adminguide/using-service-actions.html">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/using-service-actions.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-resourceupdate.html">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-resourceupdate.html</a></p>\n',
        answers: [
          "<p>Create a new portfolio and add the new products to it. Attach a launch constraint to this portfolio</p>",
          "<p>Use the service actions feature of the service catalog to define rules and constraints for the products in the portfolio. A CloudFormation template can also be used for easier implementation</p>",
          "<p>Add the newly added products under a single tag. Add tag constraints to control the end user behavior and permissions on the products</p>",
          "<p>Add launch constraint(s) to each product in the service catalog portfolio</p>",
        ],
      },
      correct_response: ["d"],
      section: "Management and Security Governance",
      question_plain:
        "An organization has added virtual machine images, software, and a few databases to its AWS Service Catalog. These will be used by multiple development teams to build their business workloads. The organization does not want the end users to launch and manage products using their own IAM credentials.\n\nHow will you address this requirement and implement it in the least possible time?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165354,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A Security Engineer has configured an AWS Web Application Firewall (WAF) for all the Application Load Balancers (ALBs) after getting a possible threat alert from the company's IT security department.</p>\n\n<p>How can the Engineer validate if the AWS WAF rules are working?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Enable WAF comprehensive logs that are delivered through Amazon Kinesis Firehose to a destination of your choice</strong></p>\n\n<p>AWS WAF supports full logging of all web requests inspected by the service. Customers can store these logs in Amazon S3 for compliance and auditing needs as well as use them for debugging and additional forensics. The logs will help customers understand why certain rules are triggered and why certain web requests are blocked. Customers can also integrate the logs with their SIEM and log analysis tools.</p>\n\n<p>For each web request, AWS WAF logs now provide raw HTTP/S headers along with information on which AWS WAF rules are triggered. This is useful for troubleshooting custom WAF rules and Managed Rules for AWS WAF. These logs will be made available via Amazon Kinesis Data Firehose in JSON format.</p>\n\n<p>Enabling AWS WAF full logs is done in two steps. First, on the Amazon Kinesis console, create an instance of the Amazon Kinesis Data Firehose in the relevant account(s). As part of this configuration, customers can choose a destination for the data from Amazon S3, Amazon ElasticSearch, or Amazon RedShift. Customers can also leverage third-party tool(s) from Splunk or Sumo Logic to enable advanced SIEM solutions, giving them a platform for advanced monitoring. Second, on the AWS WAF console, enable the logs and select the Firehose instance. When configuring, customers also have the option of redacting fields from web requests that they do not want to be logged.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS WAF reports metrics once a minute to CloudTrail. You can use statistics in Amazon CloudTrail to gather insights about the WAF responses</strong> - This statement is incorrect. If CloudTrail is substituted with CloudWatch then this statement stands correct i.e. AWS WAF reports metrics once a minute to CloudWatch. You can use statistics in Amazon CloudWatch to gather insights about the WAF responses.</p>\n\n<p><strong>Use iPerf Testing tool to emulate DDoS attack on the resources and check for WAF responses through Amazon CloudWatch logs</strong> - iPerf is a tool for network performance measurement and tuning. It is a cross-platform tool that can produce standardized performance measurements for any network. This is not relevant to the given use case.</p>\n\n<p><strong>Request penetration testing for login request flooding or API request flooding, whichever is applicable for your configuration</strong> - This statement is incorrect. Firstly, WAF service falls under “Permitted Services” which can be used to carry out penetration testing without prior approval from AWS. Secondly, Request flooding (login request flooding, API request flooding) fall under prohibited activities and hence should not be carried out on AWS resources.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/about-aws/whats-new/2018/08/aws-waf-launches-new-comprehensive-logging-functionality/">https://aws.amazon.com/about-aws/whats-new/2018/08/aws-waf-launches-new-comprehensive-logging-functionality/</a></p>\n\n<p><a href="https://aws.amazon.com/security/penetration-testing/">https://aws.amazon.com/security/penetration-testing/</a></p>\n\n<p><a href="https://aws.amazon.com/shield/faqs/">https://aws.amazon.com/shield/faqs/</a></p>\n',
        answers: [
          "<p>AWS WAF reports metrics once a minute to CloudTrail. You can use statistics in Amazon CloudTrail to gather insights about the WAF responses</p>",
          "<p>Enable WAF comprehensive logs that are delivered through Amazon Kinesis Firehose to a destination of your choice</p>",
          "<p>Use iPerf Testing tool to emulate DDoS attack on the resources and check for WAF responses through Amazon CloudWatch logs</p>",
          "<p>Request penetration testing for login request flooding or API request flooding, whichever is applicable for your configuration</p>",
        ],
      },
      correct_response: ["b"],
      section: "Infrastructure Security",
      question_plain:
        "A Security Engineer has configured an AWS Web Application Firewall (WAF) for all the Application Load Balancers (ALBs) after getting a possible threat alert from the company's IT security department.\n\nHow can the Engineer validate if the AWS WAF rules are working?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165356,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A Systems Administrator is no longer able to access the Windows Amazon EC2 instance because the Windows administrator password is lost. As a Security Engineer, you have been tasked with the job of resetting the password of the instance.</p>\n\n<p>Which of the following steps would you suggest to reset the password using EC2Launch v2? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Verify that the EC2Launch v2 service is running. Detach the EBS root volume from the instance</strong></p>\n\n<p><strong>Launch a temporary instance and attach the volume to it as a secondary volume. Delete the <code>.run-once</code> file from the instance, located at <code>%ProgramData%/Amazon/EC2Launch/state/.run-once</code></strong></p>\n\n<p><strong>Reattach the volume to the original instance as the root volume and connect to the instance using its key pair to retrieve the administrator password. Connect to the instance using its current public DNS name</strong></p>\n\n<p>If you have lost your Windows administrator password and are using a supported Windows AMI that includes the EC2Launch v2 agent, you can use EC2Launch v2 to generate a new password.</p>\n\n<p>The steps to be followed are as follows:\n1. Verify that the EC2Launch v2 service is running</p>\n\n<ol>\n<li><p>Detach the root volume from the instance - You can\'t use EC2Launch v2 to reset an administrator password if the volume on which the password is stored is attached to an instance as the root volume. You must detach the volume from the original instance before you can attach it to a temporary instance as a secondary volume.</p></li>\n<li><p>Attach the volume to a temporary instance - Launch a temporary instance and attach the volume to it as a secondary volume. This is the instance you use to modify the configuration file. The temporary instance must be in the same Availability Zone as the original instance.</p></li>\n<li><p>Delete the <code>.run-once</code> file - After you have attached the volume to the temporary instance as a secondary volume, delete the <code>.run-once</code> file from the instance, located at <code>%ProgramData%/Amazon/EC2Launch/state/.run-once</code>. This directs EC2Launch v2 to run all tasks with a frequency of once, which includes setting the administrator password.</p></li>\n<li><p>Restart the original instance - After you have deleted the .run-once file, reattach the volume to the original instance as the root volume and connect to the instance using its key pair to retrieve the administrator password.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>To reset the administrator password, Download the EC2Rescue for Windows Server zip file, extract the contents, and run <code>EC2Rescue.exe</code></strong></p>\n\n<p><strong>Select Offline Instance Option -&gt; Diagnose and Rescue -&gt; Reset Administrator Password. Reattach the volume to the original instance, then restart the instance</strong></p>\n\n<p>These two steps are part of resetting the Windows administrator password using EC2Launch when EC2Launch v2 is not installed on the instance.</p>\n\n<p><strong>When you launch the temporary instance, to avoid disk signature collisions, you must select an AMI for the same version of Windows</strong> - To avoid disk signature collisions, you must select an AMI for a different version of Windows. For example, if the original instance runs Windows Server 2019, launch the temporary instance using the base AMI for Windows Server 2016. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ResettingAdminPassword_EC2Launchv2.html">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ResettingAdminPassword_EC2Launchv2.html</a></p>\n',
        answers: [
          "<p>To reset the administrator password, Download the EC2Rescue for Windows Server zip file, extract the contents, and run <code>EC2Rescue.exe</code></p>",
          "<p>Verify that the EC2Launch v2 service is running. Detach the EBS root volume from the instance</p>",
          "<p>Select Offline Instance Option -&gt; Diagnose and Rescue -&gt; Reset Administrator Password. Reattach the volume to the original instance, then restart the instance</p>",
          "<p>Reattach the volume to the original instance as the root volume and connect to the instance using its key pair to retrieve the administrator password. Connect to the instance using its current public DNS name</p>",
          "<p>Launch a temporary instance and attach the volume to it as a secondary volume. Delete the <code>.run-once</code> file from the instance, located at <code>%ProgramData%/Amazon/EC2Launch/state/.run-once</code></p>",
          "<p>When you launch the temporary instance, to avoid disk signature collisions, you must select an AMI for the same version of Windows</p>",
        ],
      },
      correct_response: ["b", "d", "e"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "A Systems Administrator is no longer able to access the Windows Amazon EC2 instance because the Windows administrator password is lost. As a Security Engineer, you have been tasked with the job of resetting the password of the instance.\n\nWhich of the following steps would you suggest to reset the password using EC2Launch v2? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165360,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An AWS organization manages its security and compliance units through two different AWS accounts. Both the accounts need AWS Config configuration and compliance data from multiple AWS accounts and Regions to get a centralized view of the resource inventory. Currently, the teams use shared access to the management account to fetch the required data.</p>\n\n<p>To enforce enhanced security measures, the company is looking at eliminating the need to share management account credentials with the team. As a Security Engineer, how will you implement this requirement with the least time and effort?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use the aggregator feature of AWS Config to provide access to AWS Config data to both accounts without the need to share the management account details</strong> - An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from multiple AWS accounts and Regions into a single account and Region to get a centralized view of your resource inventory and compliance.</p>\n\n<p>You can also use an aggregator to collect configuration and compliance data from an organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled. Previously, organization-wide data aggregation was available only from the organization management account, but AWS Config recently now supports organization-wide resource data aggregation in a delegated administrator account.</p>\n\n<p>A delegated administrator account is an account in an AWS organization that is granted additional administrative permissions for a specified AWS service. This means that in addition to the management account, you can also use a delegated admin account to aggregate data from all the member accounts in AWS Organizations without any additional authorization. With this capability, different teams in an organization (auditing, security, or compliance) can use separate accounts and aggregate organization-wide data in their respective administration accounts for centralized governance. This capability also eliminates the need for those teams to gain access to the management account to fetch the aggregated data.</p>\n\n<p>How AWS Config Aggregator works:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q48-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use OpsCenter capability of AWS Systems Manager with AWS Config to detect a resource that is out of compliance and automate remediation</strong> - OpsCenter is a Systems Manager capability that provides a central location where operations engineers, IT professionals, and others can view, investigate, and resolve operational issues related to their environment. You can use a central account to view operational issues in another account (e.g. impaired instances, degraded storage volumes, or non-compliant resources), view pertinent diagnostic information for each issue, and use pre-defined automation runbooks to remediate the issues.</p>\n\n<p>OpsCenter is designed to reduce mean time to resolution (MTTR) for impacted AWS and hybrid cloud resources. For the AWS resource, OpsCenter aggregates information from AWS Config, AWS CloudTrail logs, and Amazon CloudWatch Events, so you don\'t have to navigate across multiple console pages during your investigation.</p>\n\n<p><strong>Configure AWS Systems Manager Explorer with a customizable operations dashboard that displays information from AWS Config</strong> - AWS Systems Manager Explorer is a customizable operations dashboard that reports information about your AWS resources. Explorer displays an aggregated view of operations data (OpsData) for your AWS accounts and across AWS Regions. Explorer displays information from supporting AWS services like AWS Config, AWS Trusted Advisor, AWS Compute Optimizer, and AWS Support (support cases).</p>\n\n<p>For Multiple-account/multiple-Region support, Explorer aggregates all account data into a management account. This is exactly what the use case wants to avoid, hence this is not the right option.</p>\n\n<p><strong>Use the Configuration snapshots feature of AWS Config to create point-in-time capture of all your resources and their configurations. Save these snapshots in an Amazon S3 bucket and analyze the data using AWS Athena for trends and security aberrations</strong> - AWS Config provides you with a configuration snapshot, which is a point-in-time capture of all your resources and their configurations. Configuration snapshots are generated on demand by using the AWS CLI or API and delivered to the Amazon S3 bucket that you specify. But saving to S3 buckets and analyzing from buckets is not an optimal way of implementing that asked requirement when the aggregator feature of AWS Config is tailor-made for such requirements.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/">https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/</a></p>\n\n<p><a href="https://aws.amazon.com/about-aws/whats-new/2022/11/aws-systems-manager-opscenter-supports-managing-opsitems-across-accounts/">https://aws.amazon.com/about-aws/whats-new/2022/11/aws-systems-manager-opscenter-supports-managing-opsitems-across-accounts/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/Explorer.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/Explorer.html</a></p>\n',
        answers: [
          "<p>Use OpsCenter capability of AWS Systems Manager with AWS Config to detect a resource that is out of compliance and automate remediation</p>",
          "<p>Configure AWS Systems Manager Explorer with a customizable operations dashboard that displays information from AWS Config</p>",
          "<p>Use the aggregator feature of AWS Config to provide access to AWS Config data to both accounts without the need to share the management account details</p>",
          "<p>Use Configuration snapshots feature of AWS Config to create point-in-time capture of all your resources and their configurations. Save these snapshots in an Amazon S3 bucket and analyze the data using AWS Athena for trends and security aberrations</p>",
        ],
      },
      correct_response: ["c"],
      section: "Management and Security Governance",
      question_plain:
        "An AWS organization manages its security and compliance units through two different AWS accounts. Both the accounts need AWS Config configuration and compliance data from multiple AWS accounts and Regions to get a centralized view of the resource inventory. Currently, the teams use shared access to the management account to fetch the required data.\n\nTo enforce enhanced security measures, the company is looking at eliminating the need to share management account credentials with the team. As a Security Engineer, how will you implement this requirement with the least time and effort?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165384,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>The security team at a financial services company has received a notification that the resources in the company's AWS account might be compromised.</p>\n\n<p>What actions would you recommend to handle this issue? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Rotate and delete all root and AWS Identity and Access Management (IAM) access keys</strong></p>\n\n<p>If your application currently uses access keys, you need to replace the existing keys with new ones. To start, create a new access key. Then, modify your application to use the new access key. You can then deactivate the original access keys that your application no longer uses. Verify that there aren\'t any issues with your application. If everything works, then you can delete the original access keys.</p>\n\n<p><strong>Check your AWS account bill to know the charged resources</strong></p>\n\n<p>The Bills page of your AWS Management Console lists all charges for all resources on your account. Check your bill for the following:</p>\n\n<ol>\n<li>AWS services that you don\'t normally use</li>\n<li>Resources in AWS Regions that you don\'t normally use</li>\n<li>A significant change in the size of your bill</li>\n</ol>\n\n<p>You can use this information to help you to delete or terminate any resources you don\'t want to keep.</p>\n\n<p><strong>Use AWS Git projects to scan for evidence of unauthorized use</strong></p>\n\n<p>AWS offers Git projects that you can install to help you protect your account:</p>\n\n<ol>\n<li><p>Git Secrets can scan merges, commits, and commit messages for secret information (that is, access keys). If Git Secrets detects prohibited regular expressions, it can reject those commits from being posted to public repositories.</p></li>\n<li><p>Use AWS Step Functions and AWS Lambda to generate Amazon CloudWatch Events from AWS Health or by AWS Trusted Advisor. If there is evidence that your access keys are exposed, the projects can help you to automatically detect, log, and mitigate the event.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the health check report in AWS Systems Manager (formerly known as SSM) to find out the details about the compromised AWS resources</strong> - AWS Systems Manager (formerly known as SSM) is an AWS service that you can use to view and control your infrastructure on AWS. Systems Manager cannot be used to know if a resource is compromised.</p>\n\n<p><strong>Use AWS Trusted Advisor security check report to find out the details about the compromised AWS resources</strong> - AWS Trusted Advisor provides recommendations that help you follow AWS best practices. Trusted Advisor can help improve the security of your AWS environment by suggesting foundational security best practices curated by security experts. Trusted Advisor provides suggestions based on best practices and cannot be used to know if a resource is compromised.</p>\n\n<p><strong>Use Amazon Inspector to detect the compromised resources of your account</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. Amazon Inspector removes the operational overhead associated with deploying and configuring a vulnerability management solution by allowing you to deploy Amazon Inspector across all accounts with a single click. Inspector cannot be used to detect the compromised resources of your account.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/">https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/</a></p>\n',
        answers: [
          "<p>Use the health check report in AWS Systems Manager (formerly known as SSM) to find out the details about the compromised AWS resources</p>",
          "<p>Rotate and delete all root and AWS Identity and Access Management (IAM) access keys</p>",
          "<p>Use AWS Git projects to scan for evidence of unauthorized use</p>",
          "<p>Use AWS Trusted Advisor security check report to find out the details about the compromised AWS resources</p>",
          "<p>Check your AWS account bill to know the charged resources</p>",
          "<p>Use Amazon Inspector to detect the compromised resources of your account</p>",
        ],
      },
      correct_response: ["b", "c", "e"],
      section: "Infrastructure Security",
      question_plain:
        "The security team at a financial services company has received a notification that the resources in the company's AWS account might be compromised.\n\nWhat actions would you recommend to handle this issue? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165380,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>As a Security Specialist, you have been asked to create an AWS Identity and Access Management (IAM) policy that explicitly grants permissions to an IAM role for creating and managing Amazon Elastic Compute Cloud (Amazon EC2) instances in a specified VPC. The policy must limit permissions so that the IAM role can only create EC2 instances with specific tags and then manage those EC2 instances in a VPC by using those tags.</p>\n\n<p>Which of the following solutions will meet this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Use policy condition <code>ec2:ResourceTags</code> to limit control to instances</strong></p>\n\n<p>Amazon EC2 provides limited supported resource-level permissions, but there are several actions, resources, and conditions to consider. Certain Amazon EC2 API actions, such as launching an EC2 instance, can be controlled through the VPC ARN using tags to control the instances.</p>\n\n<p>For the given use case, you can apply a custom IAM policy to restrict the permissions of an IAM user, group, or role for creating EC2 instances in a specified VPC with tags. Use policy condition "ec2:ResourceTags" to limit control to instances. This policy grants permission to launch EC2 instances in a designated VPC with a unique tag. You can then manage those EC2 instances using restrictive tags.</p>\n\n<p>Check out the relevant snippet of the policy definition here:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q3-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-permission-ec2-tags-vpc/">https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-permission-ec2-tags-vpc/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Replace the TAG-KEY or TAG-VALUE parameters with the IAM policy variable ${aws:username}</strong> - If you\'re assigning this policy to only IAM users or groups, then you can replace the TAG-KEY or TAG-VALUE parameters with the IAM policy variable ${aws:username}. Since the IAM role is being used in this example, this configuration is incorrect.</p>\n\n<p><strong>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags using the policy condition <code>aws:sourceVPC</code> so that it also limits the instances within the specified VPC</strong> - This option has been added as a distractor. You can create an S3 bucket policy that restricts access for the VPC endpoint to a specific VPC by using the aws:SourceVpc condition. This is useful if you have multiple VPC endpoints configured in the same VPC, and you want to manage access to your Amazon S3 buckets for all of your endpoints.</p>\n\n<p><strong>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Use the policy condition <code>ec2:CreateTags</code> to limit control to instances</strong> - This option has been added as a distractor. You should note that <code>ec2:CreateTags</code> is an action and not a policy condition. In addition, the <code>ec2:CreateTags</code> action is used for creating tags for ec2 instances.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-permission-ec2-tags-vpc/">https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-permission-ec2-tags-vpc/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html</a></p>\n',
        answers: [
          "<p>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Replace the TAG-KEY or TAG-VALUE parameters with the IAM policy variable ${aws:username}</p>",
          "<p>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags using the policy condition <code>ec2:ResourceTags</code> to limit control to instances</p>",
          "<p>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags using the policy condition <code>aws:sourceVPC</code> so that it also limits the instances within the specified VPC</p>",
          "<p>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Use policy condition <code>ec2:CreateTags</code> to limit control to instances</p>",
        ],
      },
      correct_response: ["b"],
      section: "Identity and Access Management",
      question_plain:
        "As a Security Specialist, you have been asked to create an AWS Identity and Access Management (IAM) policy that explicitly grants permissions to an IAM role for creating and managing Amazon Elastic Compute Cloud (Amazon EC2) instances in a specified VPC. The policy must limit permissions so that the IAM role can only create EC2 instances with specific tags and then manage those EC2 instances in a VPC by using those tags.\n\nWhich of the following solutions will meet this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165378,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An AWS Firewall Manager policy scope has been defined for all resources of an AWS Organization. Due to a recent organization-wide resource optimization effort, a Security Engineer is reviewing the status of several out-of-scope resources that were earlier covered under the policy.</p>\n\n<p>Which of the following correctly outlines the default behavior of AWS Firewall Manager for the given context?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          "<p>Correct option:\n<strong>The associated AWS Config-managed rules are deleted. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted</strong></p>\n\n<p>The policy scope defines where the policy applies. You can either apply centrally controlled policies to all of your accounts and resources within your organization in AWS Organizations or to a subset of your accounts and resources.\nTo determine which resources should be removed from protection when a customer resource leaves the policy scope, Firewall Manager follows these guidelines:</p>\n\n<p>Default behavior:\n1. The associated AWS Config-managed rules are deleted. This behavior is independent of the check box.\n2. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted. This behavior is independent of the check box.\n3. Any protected resource that goes out of scope remains associated and protected. For example, an Application Load Balancer or API from API Gateway that's associated with a web ACL remains associated with the web ACL, and the protection remains in place.</p>\n\n<p>Firewall Manager behavior:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/policy-scope.html\">https://docs.aws.amazon.com/waf/latest/developerguide/policy-scope.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>An Application Load Balancer that's associated with a web ACL is deleted from the web ACL while the protection remains in place. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted</strong></p>\n\n<p><strong>Any protected resource that goes out of scope is automatically disassociated and removed from protection when it leaves the policy scope</strong></p>\n\n<p><strong>Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted. Similarly, an Amazon EC2 instance is automatically disassociated from the replicated security group when it leaves the policy scope</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/policy-scope.html\">https://docs.aws.amazon.com/waf/latest/developerguide/policy-scope.html</a></p>\n",
        answers: [
          "<p>The associated AWS Config managed rules are deleted. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted</p>",
          "<p>An Application Load Balancer that's associated with a web ACL is deleted from the web ACL while the protection remains in place. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted</p>",
          "<p>Any protected resource that goes out of scope is automatically disassociated and removed from protection when it leaves the policy scope</p>",
          "<p>Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted. Similarly, an Amazon EC2 instance is automatically disassociated from the replicated security group when it leaves the policy scope</p>",
        ],
      },
      correct_response: ["a"],
      section: "Infrastructure Security",
      question_plain:
        "An AWS Firewall Manager policy scope has been defined for all resources of an AWS Organization. Due to a recent organization-wide resource optimization effort, a Security Engineer is reviewing the status of several out-of-scope resources that were earlier covered under the policy.\n\nWhich of the following correctly outlines the default behavior of AWS Firewall Manager for the given context?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165376,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>As a security engineer for an IT company, you have received a notice from AWS that the resources for your company's AWS account were reported for abusive activity.</p>\n\n<p>What should be your course of action after receiving the notice?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Review the abuse notice and reply explaining how you will prevent the abusive activity from recurring in the future</strong> - The AWS Trust &amp; Safety Team sends abuse reports to the security contact on your account. If there is no security contact listed, the AWS Trust &amp; Safety Team contacts you using the email address listed on your account.</p>\n\n<p>If you receive an abuse notice from AWS, do the following:</p>\n\n<ol>\n<li><p>Review the abuse notice to see what content or activity was reported. Logs that implicate abuse are included along with the abuse report, as provided by the reporter.</p></li>\n<li><p>Reply directly to the abuse report and explain how you\'re preventing the abusive activity from recurring in the future.</p></li>\n</ol>\n\n<p>If you don\'t respond to an abuse notice within 24 hours, AWS might block your resources or suspend your AWS account. If more information is required, reply directly to the email from the AWS Trust &amp; Safety Team. The team can request additional information from the reporter.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The AWS Trust &amp; Safety Team provides technical support for issues related to abusive activity. Contact the team and resolve the issue with their assistance</strong></p>\n\n<p><strong>The technical support provided by AWS Trust &amp; Safety Team is available for only Enterprise and Business accounts. Upgrade your account and then contact the AWS Trust &amp; Safety Team for technical support. Otherwise, you need to contact the AWS Support team</strong></p>\n\n<p>These two options are incorrect since the AWS Trust &amp; Safety Team doesn\'t provide technical support.</p>\n\n<p>You should also note that the Developer, Business, Enterprise On-Ramp, or Enterprise Support plans provide one-on-one fast-response support from experienced technical support engineers. With these Support plans, you get pay-by-the-month pricing and unlimited support cases. If you have operational issues or technical questions, you can contact a team of support engineers and receive predictable response times and personalized support.</p>\n\n<p><strong>Make sure that your instances and all applications are properly secured as per the shared responsibility model</strong> - Security and Compliance is a shared responsibility between AWS and the customer. This shared responsibility model can help relieve the customer’s operational burden as AWS operates, manages and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates. The customer assumes responsibility and management of the guest operating system (including updates and security patches), other associated application software as well as the configuration of the AWS provided security group firewall. The shared responsibility model provides guidelines and best practices that can help keep all your AWS resources safe. However, it is not the correct solution for the given issue.</p>\n\n<p><img src="https://docs.aws.amazon.com/images/whitepapers/latest/aws-risk-and-compliance/images/image2.png">\nvia - <a href="https://aws.amazon.com/compliance/shared-responsibility-model/">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/aws-abuse-report/">https://aws.amazon.com/premiumsupport/knowledge-center/aws-abuse-report/</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/get-aws-technical-support/">https://aws.amazon.com/premiumsupport/knowledge-center/get-aws-technical-support/</a></p>\n\n<p><a href="https://aws.amazon.com/compliance/shared-responsibility-model/">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n',
        answers: [
          "<p>The AWS Trust &amp; Safety Team provides technical support for issues related to abusive activity. Contact the team and resolve the issue with their assistance</p>",
          "<p>The technical support provided by AWS Trust &amp; Safety Team is available for only Enterprise and Business accounts. Upgrade your account and then contact the AWS Trust &amp; Safety Team for technical support. Otherwise, you need to contact the AWS Support team</p>",
          "<p>Make sure that your instances and all applications are properly secured as per the shared responsibility model</p>",
          "<p>Review the abuse notice and reply explaining how you will prevent the abusive activity from recurring in the future</p>",
        ],
      },
      correct_response: ["d"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "As a security engineer for an IT company, you have received a notice from AWS that the resources for your company's AWS account were reported for abusive activity.\n\nWhat should be your course of action after receiving the notice?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165374,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A Security Engineer has created a web ACL using an AWS Firewall Manager AWS WAF policy. Still, the web ACL isn't correctly associated with its in-scope resources.</p>\n\n<p>What could be the underlying reason for this issue?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>If <code>auto remediate any non-compliant resources</code> isn\'t turned on, then the Firewall Manager created web ACL won\'t be associated with in-scope resources</strong></p>\n\n<p>The web ACL association behavior for the Firewall Manager AWS WAF policy depends on the following:</p>\n\n<ol>\n<li><p>How auto-remediation is configured</p></li>\n<li><p>If your in-scope resource already has a web ACL associated</p></li>\n</ol>\n\n<p>If <code>auto remediate any non-compliant resources</code> isn\'t turned on, then the Firewall Manager created web ACL won\'t be associated with in-scope resources.</p>\n\n<p>If only <code>auto remediate any non-compliant resources</code> is turned on, then the following happens:</p>\n\n<ol>\n<li><p>For non-compliant AWS accounts that are within the policy scope, Firewall Manager creates a web ACL whose name starts with FMManagedWebACLV2 . This web ACL contains the rule groups that are defined in the policy.</p></li>\n<li><p>Firewall Manager associates the web ACL with all non-compliant resources in the accounts. However, if an in-scope resource already has a web ACL associated with it, then it won\'t replace the existing web ACL with the Firewall Manager policy web ACL.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>If <code>auto remediate any non-compliant resources and Replace web ACLs that are currently associated with in-scope resources with the web ACLs created by this policy</code> are turned on for a Firewall Manager AWS WAF policy, and if an in-scope resource has a Web ACL created by Firewall Manager AWS WAF policy, then it gets replaced by Firewall Manager AWS WAF policy web ACL</strong></p>\n\n<p><strong>If only <code>auto remediate any non-compliant resources</code> is turned on, Firewall Manager associates the web ACL with all non-compliant resources in the accounts irrespective of whether it has a web ACL associated with it</strong></p>\n\n<p><strong>If <code>auto remediate any non-compliant resources and Replace web ACLs that are currently associated with in-scope resources with the web ACLs created by this policy</code> is turned on for a Firewall Manager AWS WAF policy and if an in-scope resource has a Web ACL created by AWS Shield Advanced policy, then it cannot be replaced by Firewall Manager AWS WAF policy web ACL</strong></p>\n\n<p>All three options are incorrect. The web ACL association behavior for the Firewall Manager AWS WAF policy for the above use cases is described below.</p>\n\n<p>The web ACL association behavior:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q55-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/waf-web-acl-association-behavior/">https://aws.amazon.com/premiumsupport/knowledge-center/waf-web-acl-association-behavior/</a></p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/waf-web-acl-association-behavior/">https://aws.amazon.com/premiumsupport/knowledge-center/waf-web-acl-association-behavior/</a></p>\n',
        answers: [
          "<p>If <code>auto remediate any non-compliant resources</code> isn't turned on, then the Firewall Manager created web ACL won't be associated with in-scope resources</p>",
          "<p>If <code>auto remediate any non-compliant resources and Replace web ACLs that are currently associated with in-scope resources with the web ACLs created by this policy</code> are turned on for a Firewall Manager AWS WAF policy, then if an in-scope resource has a Web ACL created by Firewall Manager AWS WAF policy, then it gets replaced by Firewall Manager AWS WAF policy web ACL</p>",
          "<p>If only <code>auto remediate any non-compliant resources</code> is turned on, Firewall Manager associates the web ACL with all non-compliant resources in the accounts irrespective of whether it has a web ACL associated with it</p>",
          "<p>If <code>auto remediate any non-compliant resources and Replace web ACLs that are currently associated with in-scope resources with the web ACLs created by this policy</code> is turned on for a Firewall Manager AWS WAF policy and if an in-scope resource has a Web ACL created by AWS Shield Advanced policy, then it cannot be replaced by Firewall Manager AWS WAF policy web ACL</p>",
        ],
      },
      correct_response: ["a"],
      section: "Data Protection",
      question_plain:
        "A Security Engineer has created a web ACL using an AWS Firewall Manager AWS WAF policy. Still, the web ACL isn't correctly associated with its in-scope resources.\n\nWhat could be the underlying reason for this issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165372,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>The development team at an e-commerce company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Security Specialist to advise on CloudFront capabilities on routing and security.</p>\n\n<p>Which of the following would you identify as correct regarding CloudFront? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>CloudFront can route to multiple origins based on the content type</strong></p>\n\n<p>You can configure a single CloudFront web distribution to serve different types of requests from multiple origins. For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a CloudFront web distribution.</p>\n\n<p><strong>Use an origin group with primary and secondary origins to configure CloudFront for high availability and failover</strong></p>\n\n<p>You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.</p>\n\n<p>To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group.</p>\n\n<p><img src="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/origingroups-overview.png">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p><strong>Use field-level encryption in CloudFront to protect sensitive data for specific content</strong></p>\n\n<p>Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the data—and have the credentials to decrypt it—can do so.</p>\n\n<p>To use field-level encryption, when you configure your CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request. (You can’t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt.)</p>\n\n<p><img src="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/fleoverview.png">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use KMS encryption in CloudFront to protect sensitive data for specific content</strong> - This option has been added as a distractor. You can use field level encryption in CloudFront to protect sensitive data for specific content.</p>\n\n<p><strong>Use geo-restriction to configure CloudFront for high-availability and failover</strong> - You can use geo-restriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you\'re distributing through a CloudFront distribution. Geo restriction is not used to configure CloudFront for high availability and failover.</p>\n\n<p><strong>CloudFront can route to multiple origins based on the price class</strong> - CloudFront edge locations are grouped into geographic regions, and AWS has grouped regions into price classes. The default price class includes all regions. Another price class includes most regions (the United States; Canada; Europe; Hong Kong, Philippines, South Korea, Taiwan, and Singapore; Japan; India; South Africa; and Middle East regions) but excludes the most expensive regions. A third price class includes only the least expensive regions (the United States, Canada, and Europe regions). CloudFront can only route to multiple origins based on content type and not based on the price class.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html</a></p>\n',
        answers: [
          "<p>Use KMS encryption in CloudFront to protect sensitive data for specific content</p>",
          "<p>Use geo-restriction to configure CloudFront for high-availability and failover</p>",
          "<p>CloudFront can route to multiple origins based on the price class</p>",
          "<p>CloudFront can route to multiple origins based on the content type</p>",
          "<p>Use an origin group with primary and secondary origins to configure CloudFront for high-availability and failover</p>",
          "<p>Use field-level encryption in CloudFront to protect sensitive data for specific content</p>",
        ],
      },
      correct_response: ["d", "e", "f"],
      section: "Infrastructure Security",
      question_plain:
        "The development team at an e-commerce company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Security Specialist to advise on CloudFront capabilities on routing and security.\n\nWhich of the following would you identify as correct regarding CloudFront? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165370,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A media company stores all of its business data on Amazon S3 buckets. Since a massive growth in the number of customers has resulted in complicated bucket policies, the company has now hired you as an AWS Certified Security Specialist for simplifying the company's S3 buckets configuration to facilitate access for the company's customers as well as other connected applications.</p>\n\n<p>What are the important configuration characteristics to consider while defining access points for the S3 buckets? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>You can only use access points to perform operations on objects. You can\'t use access points to perform Amazon S3 operations, such as modifying or deleting buckets</strong> - Access points support access only over HTTPS.</p>\n\n<p><strong>The cross-account access points don’t grant access to data until you are granted permissions from the bucket owner</strong> - The cross-account access points don’t grant access to data until you are granted permissions from the bucket owner. The bucket owner always retains ultimate control of the data and must update the bucket policy to authorize requests from the cross-account access point.</p>\n\n<p><strong>You can\'t configure Cross-Region Replication to operate through an access point</strong> - You can\'t use an access point as a destination for S3 Replication.</p>\n\n<p>Access points restrictions and limitations:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q53-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-restrictions-limitations.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-restrictions-limitations.html</a></p>\n\n<p>Access point alias limitations:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q53-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-alias.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-alias.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Access points support access over HTTP and HTTPS alone. It does not offer support over TCP and UDP protocols</strong> - This statement is incorrect. Access points support access only over HTTPS.</p>\n\n<p><strong>Aliases for S3 Access Points are interchangeable with S3 bucket names. Aliases can be used as a logging destination for AWS CloudTrail logs and S3 server access logs. However, an alias cannot be used in AWS Identity and Access Management (IAM) policies</strong> - Aliases for S3 Access Points are automatically generated and are interchangeable with S3 bucket names anywhere you use a bucket name for data access. Every time you create an access point for a bucket, S3 automatically generates a new Access Point Alias.</p>\n\n<p>Aliases cannot be used as a logging destination for S3 server access logs. Aliases cannot be used as a logging destination for AWS CloudTrail logs.</p>\n\n<p><strong>After you create an access point, you can\'t change its virtual private cloud (VPC) configuration from the console anymore. An AWS CLI has to be used for modifying the configuration</strong> - After you create an access point, you can\'t change its virtual private cloud (VPC) configuration.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/s3/features/access-points/">https://aws.amazon.com/s3/features/access-points/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-alias.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-alias.html</a></p>\n',
        answers: [
          "<p>Access points support access over HTTP and HTTPS alone. It does not offer support over TCP and UDP protocols</p>",
          "<p>You can only use access points to perform operations on objects. You can't use access points to perform Amazon S3 operations, such as modifying or deleting buckets</p>",
          "<p>The cross-account access points don’t grant access to data until you are granted permissions from the bucket owner</p>",
          "<p>You can't configure Cross-Region Replication to operate through an access point</p>",
          "<p>Aliases for S3 Access Points are interchangeable with S3 bucket names. Aliases can be used as a logging destination for AWS CloudTrail logs and S3 server access logs. However, an alias cannot be used in AWS Identity and Access Management (IAM) policies</p>",
          "<p>After you create an access point, you can't change its virtual private cloud (VPC) configuration from the console anymore. An AWS CLI has to be used for modifying the configuration</p>",
        ],
      },
      correct_response: ["b", "c", "d"],
      section: "Data Protection",
      question_plain:
        "A media company stores all of its business data on Amazon S3 buckets. Since a massive growth in the number of customers has resulted in complicated bucket policies, the company has now hired you as an AWS Certified Security Specialist for simplifying the company's S3 buckets configuration to facilitate access for the company's customers as well as other connected applications.\n\nWhat are the important configuration characteristics to consider while defining access points for the S3 buckets? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165368,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A data analytics company processes the sensitive data of several financial institutions across the country. The company needs an automated and efficient way to identify sensitive information and operationalize security for its customers while keeping costs low. The solution should also have a security dashboard that aggregates alerts and facilitates automated remediation of security issues while having a complete view of the security architecture of the systems. A high-performing interactive query service is also needed for business purposes.</p>\n\n<p>As a Security Engineer, which options will you combine to implement a cost-optimal and high-performance solution for the given requirements? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Store the data cost-effectively on Amazon S3 buckets and use Amazon Macie to automatically discover, classify and protect the highly sensitive data</strong></p>\n\n<p>Macie uses machine learning and pattern matching to discover sensitive data at scale in a cost-efficient way. Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of your data stored in Amazon Simple Storage Service (Amazon S3). Macie gives visibility the company wouldn’t otherwise have because it wouldn’t have the means or the resources to do this type of discovery without a tool like Macie.</p>\n\n<p><strong>Configure AWS Security Hub to have a central dashboard for higher visibility of the environment and remediate issues quickly</strong></p>\n\n<p>Alongside Macie, use AWS Security Hub, a cloud security posture management service that performs security best-practice checks, aggregates alerts, and facilitates automated remediation. The security scores provided by Security Hub help to build frameworks for how to secure the environment in the cloud. Having a central dashboard where you can improve visibility into the configurations in the environment is very helpful because you can just go in there and remediate the issues.</p>\n\n<p><strong>Use Amazon Athena to analyze data in Amazon Simple Storage Service (Amazon S3) to retrieve any amount of data from anywhere—using standard SQL</strong></p>\n\n<p>Amazon Athena, an interactive query service makes it easy to analyze data in Amazon Simple Storage Service (Amazon S3)—object storage built to retrieve any amount of data from anywhere—using standard SQL. Using Macie alongside Athena provides the team with comprehensive visibility in its data security posture, including the presence of sensitive information.</p>\n\n<p>Sample reference architecture:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q52-i1.jpg">\nvia - <a href="https://aws.amazon.com/solutions/case-studies/kasasa-case-study/">https://aws.amazon.com/solutions/case-studies/kasasa-case-study/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon Detective to analyze, investigate, and quickly identify the root cause of potential security issues along with Amazon GuardDuty. Use Amazon GuardDuty to continuously monitor for malicious activity and unauthorized behavior to protect your AWS accounts, and Amazon Elastic Compute Cloud (EC2) workloads</strong> - While Amazon Detective and GaurdDuty are useful services for the use-case, AWS Security Hub gives a comprehensive view of high priority security alerts and compliance status across your AWS accounts. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions.</p>\n\n<p><strong>Use Amazon QuickSight to quickly embed interactive dashboards and visualizations into your applications without needing to build your own analytics capabilities</strong> - Amazon QuickSight has a serverless architecture that automatically scales to hundreds of thousands of users without the need to set up, configure, or manage your own servers. It also ensures that your users don’t have to deal with slow dashboards during peak hours when multiple business intelligence (BI) users are accessing the same dashboards or datasets. Amazon QuickSight offers amazing visualizations, unlike traditional Business Intelligence (BI) solutions. However, this option is not the best fit for the given use case.</p>\n\n<p><strong>Use Amazon S3 buckets to store data and include S3 Intelligent-Tiering for automatic cost savings for data with unknown or changing access patterns</strong> - Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) is the first cloud storage that automatically reduces your storage costs on a granular object level by automatically moving data to the most cost-effective access tier based on access frequency, without performance impact, retrieval fees, or operational overhead. While S3 Intelligent Tiering is extremely useful for several use cases, it is not an alternative to Amazon Macie, which can automatically discover, classify and protect highly sensitive data.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/about-aws/whats-new/2019/12/aws-security-hub-integrates-with-amazon-detective/">https://aws.amazon.com/about-aws/whats-new/2019/12/aws-security-hub-integrates-with-amazon-detective/</a></p>\n',
        answers: [
          "<p>Configure Amazon Detective to analyze, investigate, and quickly identify the root cause of potential security issues along with Amazon GuardDuty. Use Amazon GuardDuty to continuously monitor for malicious activity and unauthorized behavior to protect your AWS accounts, and Amazon Elastic Compute Cloud (EC2) workloads</p>",
          "<p>Store the data cost-effectively on Amazon S3 buckets and use Amazon Macie to automatically discover, classify and protect the highly sensitive data</p>",
          "<p>Configure AWS Security Hub to have a central dashboard for higher visibility of the environment and remediate issues quickly</p>",
          "<p>Use Amazon Athena to analyze data in Amazon Simple Storage Service (Amazon S3) to retrieve any amount of data from anywhere—using standard SQL</p>",
          "<p>Use Amazon QuickSight to quickly embed interactive dashboards and visualizations into your applications without needing to build your own analytics capabilities</p>",
          "<p>Use Amazon S3 buckets to store data and include S3 Intelligent-Tiering for automatic cost savings for data with unknown or changing access patterns</p>",
        ],
      },
      correct_response: ["b", "c", "d"],
      section: "Management and Security Governance",
      question_plain:
        "A data analytics company processes the sensitive data of several financial institutions across the country. The company needs an automated and efficient way to identify sensitive information and operationalize security for its customers while keeping costs low. The solution should also have a security dashboard that aggregates alerts and facilitates automated remediation of security issues while having a complete view of the security architecture of the systems. A high-performing interactive query service is also needed for business purposes.\n\nAs a Security Engineer, which options will you combine to implement a cost-optimal and high-performance solution for the given requirements? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165366,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>For auditing purposes, a company needs to showcase a report of changes made to the security group(s) for an Amazon Virtual Private Cloud (Amazon VPC).</p>\n\n<p>What are the different ways to review security group changes in an AWS account? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Use AWS CloudTrail Event history to review security group changes in your AWS account</strong></p>\n\n<p>You can troubleshoot operational and security incidents in the CloudTrail console by viewing Event history. The Event history provides a read-only view of the last 90 days of recorded API activity (management events) in an AWS Region.</p>\n\n<p>You can look up events related to the creation, modification, or deletion of resources (such as IAM users or Amazon EC2 instances) in your AWS account on a per-region basis. Events can be viewed and downloaded by using the AWS CloudTrail console. You can customize the view of event history in the console by selecting which columns are displayed and which are hidden. You can programmatically look up events by using the AWS SDKs or AWS Command Line Interface. You can also compare the details of events in Event history side-by-side.</p>\n\n<p><strong>Create an AWS CloudTrail trail configured to log to an Amazon Simple Storage Service (Amazon S3) bucket. Use Athena to query CloudTrail Logs over the last 30-45 days</strong></p>\n\n<p>To use Athena to query CloudTrail Logs, you must have a trail configured to log to an Amazon Simple Storage Service (Amazon S3) bucket. You can use Athena to query CloudTrail Logs over the last 90 days.</p>\n\n<p>You can create a non-partitioned Athena table for querying CloudTrail logs directly from the CloudTrail console. Creating an Athena table from the CloudTrail console requires that you be logged in with a role that has sufficient permissions to create tables in Athena.</p>\n\n<p><strong>Use AWS Config to view configuration history for security groups. You must have the AWS Config configuration recorder turned on</strong></p>\n\n<p>AWS Config records details of changes to your AWS resources to provide you with a configuration history. You can use the AWS Management Console, API, or CLI to obtain details of what a resource’s configuration looked like at any point in the past. AWS Config will also automatically deliver a configuration history file to the Amazon Simple Storage Service (S3) bucket you specify.</p>\n\n<p>AWS Config discovers, maps, and tracks AWS resource relationships in your account. For example, if a new EC2 security group is associated with an EC2 instance, AWS Config records the updated configurations of both the EC2 security group and the EC2 instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the AWS AppConfig capability of AWS Systems Manager, to create, manage, and quickly deploy application configurations and track changes in them</strong> - AWS AppConfig is a capability of AWS Systems Manager, to create, manage, and quickly deploy application configurations. A configuration is a collection of settings that influence the behavior of your application. You can use AWS AppConfig with applications hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS Lambda, containers, mobile applications, or IoT devices. You cannot monitor and track changes to security groups using AppConfig.</p>\n\n<p><strong>Use CloudTrail Lake to create trails that aggregate information from multiple AWS accounts across regions</strong> - If you want to perform SQL queries on CloudTrail event information across accounts, regions, and dates, consider using CloudTrail Lake. CloudTrail Lake is an AWS alternative to creating trails that aggregate information from an enterprise into a single, searchable event data store. Instead of using Amazon S3 bucket storage, it stores events in a data lake, which allows richer, faster queries. This option has been added as a distractor.</p>\n\n<p><strong>Configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when security group changes occur. CloudTrail supports sending only data events to CloudWatch Logs. Configure Amazon EventBridge to monitor management events</strong> - This statement is incorrect. CloudTrail supports sending data, Insights, and management events to CloudWatch Logs.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html#create-cloudtrail-table-understanding">https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html#create-cloudtrail-table-understanding</a></p>\n\n<p><a href="https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html">https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html</a></p>\n',
        answers: [
          "<p>Use AWS CloudTrail Event history to review security group changes in your AWS account</p>",
          "<p>Use AWS AppConfig capability of AWS Systems Manager, to create, manage, and quickly deploy application configurations and track changes in them</p>",
          "<p>Use CloudTrail Lake to create trails that aggregate information from multiple AWS accounts across regions</p>",
          "<p>Create an AWS CloudTrail trail configured to log to an Amazon Simple Storage Service (Amazon S3) bucket. Use Athena to query CloudTrail Logs over the last 30-45 days</p>",
          "<p>Use AWS Config to view configuration history for security groups. You must have the AWS Config configuration recorder turned on</p>",
          "<p>Configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when security group changes occur. CloudTrail supports sending only data events to CloudWatch Logs. Configure Amazon EventBridge to monitor management events</p>",
        ],
      },
      correct_response: ["a", "d", "e"],
      section: "Security Logging and Monitoring",
      question_plain:
        "For auditing purposes, a company needs to showcase a report of changes made to the security group(s) for an Amazon Virtual Private Cloud (Amazon VPC).\n\nWhat are the different ways to review security group changes in an AWS account? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165364,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A Security Engineer is planning for a DDoS-resilient architecture for a three-tier web application. What are the best practices to consider for DDoS mitigation? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>When using Amazon CloudFront and AWS WAF with Amazon API Gateway, configure the cache behavior for your distributions to forward all headers to the API Gateway regional endpoint</strong></p>\n\n<p>Configure the cache behavior for your distributions to forward all headers to the API Gateway regional endpoint. By doing this, CloudFront will treat the content as dynamic and skip caching the content.</p>\n\n<p><strong>If you are subscribed to AWS Shield Advanced, you can register Elastic IP addresses as Protected Resources</strong></p>\n\n<p>If you are subscribed to AWS Shield Advanced, you can register Elastic IP addresses as Protected Resources. DDoS attacks against Elastic IP addresses that have been registered as Protected Resources are detected more quickly, which can result in a faster time to mitigate. When an attack is detected, the DDoS mitigation systems read the network ACL that corresponds to the targeted Elastic IP and enforce it at the AWS network border. This significantly reduces your risk of impact from infrastructure layer DDoS attacks.</p>\n\n<p><strong>The security groups assigned to Application Load Balancers should be configured to not use connection tracking</strong></p>\n\n<p>Security groups are stateful, depending on the configuration. To allow the response traffic, the security group uses connection tracking to track information about traffic on the resource where the security group is applied. There is a maximum number of connections that can be tracked per resource. After the maximum is reached, both new inbound and outbound connections cannot be established. This condition exhausts the number of connections tracked by the security group that can be met during a DDoS attack.</p>\n\n<p>To improve the DDoS resilience of resources in your VPC, AWS recommends ensuring that the security groups assigned to managed resources such as Classic and Application Load Balancers are configured to not use connection tracking (untracked connections).</p>\n\n<p>DDoS-resilient reference architecture:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q50-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/mitigation-techniques.html">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/mitigation-techniques.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Network Load Balancer to route traffic to targets based on content and accept only well-formed web requests. Network Load Balancer blocks many common DDoS attacks, such as SYN floods or UDP reflection attacks</strong> - This statement is incorrect. Network Load Balancers cannot route traffic based on content since they are not at layer 7, unlike Application Load Balancers.</p>\n\n<p><strong>Use AWS WAF to configure web access control lists (Web ACLs) on your Amazon S3 buckets with critical data to filter and block requests based on request signatures</strong> - AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync. Amazon S3 bucket is not a supported target.</p>\n\n<p><strong>Configure Amazon API Gateway with edge-optimized API endpoints whenever possible and associate it with your Amazon CloudFront distribution</strong> - When you use Amazon API Gateway, you can choose from two types of API endpoints. The first is the default option: edge-optimized API endpoints that are accessed through an Amazon CloudFront distribution. The distribution is created and managed by API Gateway, however, so you don’t have control over it. The second option is to use a regional API endpoint that is accessed from the same AWS region in which your REST API is deployed. AWS recommends that you use the second type of endpoint and associate it with your own Amazon CloudFront distribution. This gives you control over the Amazon CloudFront distribution and the ability to use AWS WAF for application layer protection. This mode provides you with access to scaled DDoS mitigation capacity across the AWS global edge network.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/best-practices-for-ddos-mitigation.html">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/best-practices-for-ddos-mitigation.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/use-aws-edge-locations-for-scale-bp1-bp3.html">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/use-aws-edge-locations-for-scale-bp1-bp3.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/application-layer-defense-bp1-bp2.html">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/application-layer-defense-bp1-bp2.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/attack-surface-reduction.html">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/attack-surface-reduction.html</a></p>\n',
        answers: [
          "<p>When using Amazon CloudFront and AWS WAF with Amazon API Gateway, configure the cache behavior for your distributions to forward all headers to the API Gateway regional endpoint</p>",
          "<p>If you are subscribed to AWS Shield Advanced, you can register Elastic IP addresses as Protected Resources</p>",
          "<p>Use Network Load Balancer to route traffic to targets based on content and accept only well-formed web requests. Network Load Balancer blocks many common DDoS attacks, such as SYN floods or UDP reflection attacks</p>",
          "<p>The security groups assigned to Application Load Balancers should be configured to not use connection tracking</p>",
          "<p>Use AWS WAF to configure web access control lists (Web ACLs) on your Amazon S3 buckets with critical data to filter and block requests based on request signatures</p>",
          "<p>Configure Amazon API Gateway with edge-optimized API endpoints whenever possible and associate it with your Amazon CloudFront distribution</p>",
        ],
      },
      correct_response: ["a", "b", "d"],
      section: "Infrastructure Security",
      question_plain:
        "A Security Engineer is planning for a DDoS-resilient architecture for a three-tier web application. What are the best practices to consider for DDoS mitigation? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165362,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A Security Engineer noticed that an application layer (layer 7) DDoS attack is underway on one of the critical systems.</p>\n\n<p>What should the immediate response of the engineer be to control the damage? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Create your own AWS WAF rules in your web ACL to mitigate the attack</strong></p>\n\n<p><strong>You can contact the AWS Support Center to get help with mitigations if you\'re a Shield Advanced customer</strong> - AWS automatically mitigates network and transport layer (layer 3 and layer 4) Distributed Denial of Service (DDoS) attacks.</p>\n\n<p>For application layer (layer 7) DDoS attacks, AWS attempts to detect and notify AWS Shield Advanced customers through CloudWatch alarms. By default, it doesn\'t automatically apply mitigations, to avoid inadvertently blocking valid user traffic.</p>\n\n<p>For application layer (layer 7) resources, you have the following options available for responding to an attack.\n1. Provide your own mitigations – You can investigate and mitigate the attack on your own. To manually mitigate a potential application layer DDoS attack you can create your own AWS WAF rules in your web ACL to mitigate the attack. This is the only option available if you aren\'t a Shield Advanced customer.</p>\n\n<ol>\n<li>Contact support – If you\'re a Shield Advanced customer, you can contact the AWS Support Center to get help with mitigations. Critical and urgent cases are routed directly to DDoS experts.</li>\n</ol>\n\n<p>Manually mitigating an application layer DDoS attack:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q49-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html#ddos-responding-manual">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html#ddos-responding-manual</a></p>\n\n<p>How AWS Shield Advanced works:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q49-i2.jpg">\nvia - <a href="https://aws.amazon.com/shield/">https://aws.amazon.com/shield/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Amazon GuardDuty to automatically monitor for malicious activity and block unauthorized access</strong></p>\n\n<p><strong>Monitor the CloudWatch metrics: The maximum size of the Auto Scaling group, Amazon EC2 instance\'s CPUUtilization, and NetworkIn parameters to detect a DDoS attack and send an SNS notification to the security team</strong></p>\n\n<p>These two options can be used to proactively tighten the security of AWS infrastructure. But, these are not meant to be used as immediate responses when a DDos attack is underway.</p>\n\n<p><strong>Define an AWS Systems Manager document (SSM document) to block all vulnerable ports, lock public access to Amazon S3 buckets and stop internet traffic to affected EC2 instances. Run the SSM using AWS Systems Manager CLI</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html#ddos-responding-manual">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html#ddos-responding-manual</a></p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/operational-techniques.html">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/operational-techniques.html</a></p>\n',
        answers: [
          "<p>Create your own AWS WAF rules in your web ACL to mitigate the attack</p>",
          "<p>Enable Amazon GuardDuty to automatically monitor for malicious activity and block unauthorized access</p>",
          "<p>Define an AWS Systems Manager document (SSM document) to block all vulnerable ports, lock public access to Amazon S3 buckets, and stop internet traffic to affected EC2 instances. Run the SSM using AWS Systems Manager CLI</p>",
          "<p>You can contact the AWS Support Center to get help with mitigations if you're a Shield Advanced customer</p>",
          "<p>Monitor the CloudWatch metrics: The maximum size of the Auto Scaling group, Amazon EC2 instance's CPUUtilization and NetworkIn parameters to detect a DDoS attack and send an SNS notification to the security team</p>",
        ],
      },
      correct_response: ["a", "d"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "A Security Engineer noticed that an application layer (layer 7) DDoS attack is underway on one of the critical systems.\n\nWhat should the immediate response of the engineer be to control the damage? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165358,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A fault management application at a company connects to several other systems to monitor the status of the systems hosting the suite of flagship applications for the company. As per the security policy of the company, Cloudtrail and VPC flow logs have been enabled for all AWS resources. A recent internal error from the support team led to several minutes of outage on the fault management application and a few hours of analysis to understand the root cause of the error.</p>\n\n<p>The company is now looking for a solution that can analyze data from various logs as well as security findings to quickly triage the root-cause linked to the security issues. What is the best-fit solution for the company's requirements?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use Amazon Detective in conjunction with Amazon GuardDuty to monitor malicious activity and unauthorized behavior on the AWS resources and quickly identify the root cause of potential security issues through linked datasets</strong></p>\n\n<p>Amazon Detective makes it easy to analyze, investigate, and quickly identify the root cause of potential security issues or suspicious activities. Amazon Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to build a linked set of data that enables you to easily conduct faster and more efficient security investigations.</p>\n\n<p>Amazon Detective simplifies the investigative process and helps security teams conduct faster and more effective investigations. Amazon Detective’s prebuilt data aggregations, summaries, and context help you to quickly analyze and determine the nature and extent of possible security issues. Amazon Detective maintains up to a year of aggregated data and makes it easily available through a set of visualizations that shows changes in the type and volume of activity over a selected time window, and links those changes to security findings.</p>\n\n<p>Amazon Detective provides a variety of visualizations that present context and insights about AWS resources such as AWS accounts, EC2 instances, users, roles, IP addresses, and Amazon GuardDuty findings. Each visualization is designed to answer specific questions that may come up as you analyze findings and the related activity. Each visualization provides textual guidance that clearly explains how to interpret the panel and use its information to answer your investigative questions.</p>\n\n<p>Amazon Detective requires that you have Amazon GuardDuty enabled on your accounts for at least 48 hours before you enable Detective on those accounts. However, you can use Amazon Detective to investigate more than just your Amazon GuardDuty findings. Amazon Detective provides detailed summaries, analyses, and visualizations of the behaviors and interactions amongst your AWS accounts, EC2 instances, AWS users, roles, and IP addresses. This information can be very useful in understanding security issues or operational account activity.</p>\n\n<p>How Amazon Detective works:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q47-i1.jpg">\nvia - <a href="https://aws.amazon.com/detective/">https://aws.amazon.com/detective/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon GuardDuty that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. Integrate it into your workflow system and initiate AWS Lambda to automatically remediate the issue</strong> - As discussed above, Amazon Detective requires Amazon GuardDuty as a pre-requisite. Amazon Detective automatically groups related GuardDuty findings and the affected AWS resources like Amazon EC2 instances, AWS accounts, or Amazon S3 buckets to help you investigate a single security event rather than individual GuardDuty findings.</p>\n\n<p>Amazon Detective conforms to the AWS shared responsibility model, which includes regulations and guidelines for data protection. Once enabled, Amazon Detective will process data from AWS CloudTrail logs, Amazon VPC Flow Logs, Amazon EKS audit logs, and Amazon GuardDuty findings for any accounts where it has been turned on.</p>\n\n<p><strong>Use AWS Security Hub, a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services to help analyze the security data under one service for easy root cause analyses</strong> - With AWS Security Hub, you have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Amazon Detective simplifies the process of investigating security findings and identifying the root cause. Amazon Detective analyzes trillions of events from multiple data sources such as Amazon VPC Flow Logs, AWS CloudTrail logs, Amazon EKS audit logs, and Amazon GuardDuty findings and automatically creates a graph model that provides you with a unified, interactive view of your resources, users, and the interactions between them over time.</p>\n\n<p><strong>Use Amazon Inspector to automatically scan and manage the known vulnerabilities and integration with AWS Security Hub and Amazon EventBridge to automate workflows for root cause analysis</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. Amazon Inspector removes the operational overhead associated with deploying and configuring a vulnerability management solution by allowing you to deploy Amazon Inspector across all accounts with a single step.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/detective/faqs/">https://aws.amazon.com/detective/faqs/</a></p>\n\n<p><a href="https://aws.amazon.com/guardduty/">https://aws.amazon.com/guardduty/</a></p>\n',
        answers: [
          "<p>Configure Amazon GuardDuty that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. Integrate it into your workflow system and initiate AWS Lambda to automatically remediate the issue</p>",
          "<p>Use AWS Security Hub, a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services to help analyze the security data under one service for easy root cause analyses</p>",
          "<p>Use Amazon Inspector to automatically scan and manage the known vulnerabilities and integration with AWS Security Hub and Amazon EventBridge to automate workflows for root cause analysis</p>",
          "<p>Use Amazon Detective in conjunction with Amazon GuardDuty to monitor malicious activity and unauthorized behavior on the AWS resources and quickly identify the root cause of potential security issues through linked datasets</p>",
        ],
      },
      correct_response: ["d"],
      section: "Infrastructure Security",
      question_plain:
        "A fault management application at a company connects to several other systems to monitor the status of the systems hosting the suite of flagship applications for the company. As per the security policy of the company, Cloudtrail and VPC flow logs have been enabled for all AWS resources. A recent internal error from the support team led to several minutes of outage on the fault management application and a few hours of analysis to understand the root cause of the error.\n\nThe company is now looking for a solution that can analyze data from various logs as well as security findings to quickly triage the root-cause linked to the security issues. What is the best-fit solution for the company's requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165440,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A financial services company is evaluating storage options on Amazon S3 standard storage to meet regulatory guidelines. The data should be stored in such a way on S3 that it cannot be deleted until the regulatory period has expired.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend for the given requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use S3 Object Lock</strong></p>\n\n<p>Amazon S3 Object Lock is an Amazon S3 feature that allows you to store objects using a write once, read many (WORM) model. You can use WORM protection for scenarios where it is imperative that data is not changed or deleted after it has been written. Whether your business has a requirement to satisfy compliance regulations in the financial or healthcare sector, or you simply want to capture a golden copy of business records for later auditing and reconciliation, S3 Object Lock is the right tool for you. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q56-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q56-i2.jpg">\nvia - <a href="https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/">https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 Glacier Vault Lock</strong></p>\n\n<p>A vault is a container for storing archives on Glacier. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. Since Vault Lock is only for Glacier and not for S3 standards storage, it cannot be used for the given use case.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q56-i3.jpg"></p>\n\n<p>"Use S3 cross-Region Replication" - Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. The object may be replicated to a single destination bucket or multiple destination buckets. Both source and destination buckets must have versioning enabled. By default, when Amazon S3 Replication is enabled and an object is deleted in the source bucket, Amazon S3 adds a delete marker in the source bucket only. This action protects data from malicious deletions. If you have delete marker replication enabled, these markers are copied to the destination buckets, and Amazon S3 behaves as if the object was deleted in both source and destination buckets. However, someone with administrative access to S3 can disable cross-Region replication and then delete all versions from both source as well as the destination, so this option will not be able to safeguard your data compared to S3 Object Lock.</p>\n\n<p><strong>Activate MFA delete on the S3 bucket</strong> - When working with S3 Versioning in Amazon S3 buckets, you can optionally add another layer of security by configuring a bucket to enable MFA (multi-factor authentication) delete. When you do this, the bucket owner must include two forms of authentication in any request to delete a version or change the versioning state of the bucket. Only the root account can enable MFA delete. MFA delete cannot be used for the given use case because it just represents an additional security layer and can be disabled by anyone having access to the root account credentials.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/">https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/</a></p>\n',
        answers: [
          "<p>Use S3 cross-Region Replication</p>",
          "<p>Use S3 Glacier Vault Lock</p>",
          "<p>Activate MFA delete on the S3 bucket</p>",
          "<p>Use S3 Object Lock</p>",
        ],
      },
      correct_response: ["d"],
      section: "Management and Security Governance",
      question_plain:
        "A financial services company is evaluating storage options on Amazon S3 standard storage to meet regulatory guidelines. The data should be stored in such a way on S3 that it cannot be deleted until the regulatory period has expired.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend for the given requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165442,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>The development team at a company is moving the static content from the company's e-commerce website hosted on EC2 instances to an S3 bucket. The team wants to use a CloudFront distribution to deliver the static content. The security group used by the EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post the migration to CloudFront, access to the static content should only be allowed from the aforementioned IP addresses.</p>\n\n<p>Which options would you combine to build a solution to meet these requirements? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Configure an origin access identity (OAI) and associate it with the CloudFront distribution. Set up the permissions in the S3 bucket policy so that only the OAI can read the objects</strong></p>\n\n<p>When you use CloudFront with an Amazon S3 bucket as the origin, you can configure CloudFront and Amazon S3 in a way that provides the following benefits:</p>\n\n<p>Restricts access to the Amazon S3 bucket so that it\'s not publicly accessible</p>\n\n<p>Makes sure that viewers (users) can access the content in the bucket only through the specified CloudFront distribution—that is, prevents them from accessing the content directly from the bucket, or through an unintended CloudFront distribution</p>\n\n<p>To do this, configure CloudFront to send authenticated requests to Amazon S3, and configure Amazon S3 to only allow access to authenticated requests from CloudFront. CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI).</p>\n\n<p>Exam Alert:</p>\n\n<p>Please note that AWS recommends using OAC because it supports:</p>\n\n<p>All Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022</p>\n\n<p>Amazon S3 server-side encryption with AWS KMS (SSE-KMS)</p>\n\n<p>Dynamic requests (POST, PUT, etc.) to Amazon S3</p>\n\n<p>OAI doesn\'t work for the scenarios in the preceding list, or it requires extra workarounds in those scenarios. However, you will continue to see answers enlisting OAI as the preferred option in the actual exam as it takes about 6 months/1 year for a new feature to appear in the exam.</p>\n\n<p><strong>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the CloudFront distribution</strong></p>\n\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to your protected web application resources. You can protect the following resource types:</p>\n\n<p>Amazon CloudFront distribution</p>\n\n<p>Amazon API Gateway REST API</p>\n\n<p>Application Load Balancer</p>\n\n<p>AWS AppSync GraphQL API</p>\n\n<p>Amazon Cognito user pool</p>\n\n<p>AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response.</p>\n\n<p>If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions via your AWS WAF. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from.</p>\n\n<p>For the given use case, you should add those IP addresses that are allowed in the EC2 security group into the IP match condition.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the S3 bucket policy</strong> - You cannot associate a WAF ACL with an S3 bucket policy.</p>\n\n<p><strong>Create a new NACL that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new NACL with the CloudFront distribution</strong> - NACL is associated with a subnet within a VPC. CloudFront delivers your content through a worldwide network of data centers called edge locations. So an NACL cannot be associated with a CloudFront distribution.</p>\n\n<p><strong>Create a new security group that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new security group with the CloudFront distribution</strong> - A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. CloudFront delivers your content through a worldwide network of data centers called edge locations. So a security group cannot be associated with a CloudFront distribution.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html</a></p>\n',
        answers: [
          "<p>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the S3 bucket policy</p>",
          "<p>Create a new NACL that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new NACL with the CloudFront distribution</p>",
          "<p>Create a new security group that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new security group with the CloudFront distribution</p>",
          "<p>Configure an origin access identity (OAI) and associate it with the CloudFront distribution. Set up the permissions in the S3 bucket policy so that only the OAI can read the objects</p>",
          "<p>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the CloudFront distribution</p>",
        ],
      },
      correct_response: ["d", "e"],
      section: "Identity and Access Management",
      question_plain:
        "The development team at a company is moving the static content from the company's e-commerce website hosted on EC2 instances to an S3 bucket. The team wants to use a CloudFront distribution to deliver the static content. The security group used by the EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post the migration to CloudFront, access to the static content should only be allowed from the aforementioned IP addresses.\n\nWhich options would you combine to build a solution to meet these requirements? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165444,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The security team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you identify as an INVALID option for setting up such a configuration?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>You can use an Internet Gateway ID as the custom source for the inbound rule</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, you can use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.</p>\n\n<p>Please see this list of allowed source or destination for security group rules:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q58-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n\n<p>Therefore, you cannot use an Internet Gateway ID as the custom source for the inbound rule.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can use a security group as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use an IP address as the custom source for the inbound rule</strong></p>\n\n<p>As described in the list of allowed sources or destinations for security group rules, the above options are supported.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</p>\n',
        answers: [
          "<p>You can use a security group as the custom source for the inbound rule</p>",
          "<p>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</p>",
          "<p>You can use an IP address as the custom source for the inbound rule</p>",
          "<p>You can use an Internet Gateway ID as the custom source for the inbound rule</p>",
        ],
      },
      correct_response: ["d"],
      section: "Data Protection",
      question_plain:
        "The security team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group.\n\nAs an AWS Certified Security Specialist, which of the following would you identify as an INVALID option for setting up such a configuration?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165446,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An e-commerce company is using Amazon Macie, AWS Shield Advanced, Amazon Inspector and AWS Firewall Manager in its AWS account. The company wants to receive alerts in case a DDoS attack occurs against the account.</p>\n\n<p>As an AWS Certified Security Specialist, what would you recommend?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors Shield Advanced metrics for an active DDoS event</strong></p>\n\n<p>AWS Shield Advanced is a managed service that helps you protect your application against external threats, like DDoS attacks, volumetric bots, and vulnerability exploitation attempts. For higher levels of protection against attacks, you can subscribe to AWS Shield Advanced. When you subscribe to Shield Advanced and add protection to your resources, Shield Advanced provides expanded DDoS attack protection for those resources. The protections that you receive from Shield Advanced can vary depending on your architecture and configuration choices. Use the information in this guide to build and protect resilient applications using Shield Advanced, and to escalate when you need expert help.</p>\n\n<p>AWS Shield Advanced publishes Amazon CloudWatch event metrics for all resources that it protects. These metrics improve your ability to monitor your resources by making it possible to create and configure CloudWatch dashboards and alarms for them.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q59-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors Amazon Macie findings for an active DDoS event</strong> - Amazon Macie is a data security service that uses machine learning (ML) and pattern matching to discover and help protect your sensitive data in Amazon S3. You cannot use it to monitor active DDoS events.</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors Amazon Inspector findings for an active DDoS event</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. You cannot use it to monitor active DDoS events.</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors AWS Firewall Manager metrics for an active DDoS event</strong> - AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organization. You cannot use it to monitor active DDoS events.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/ddos-advanced-summary.html">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-advanced-summary.html</a></p>\n',
        answers: [
          "<p>Set up an Amazon CloudWatch alarm that monitors Amazon Macie findings for an active DDoS event</p>",
          "<p>Set up an Amazon CloudWatch alarm that monitors Amazon Inspector findings for an active DDoS event</p>",
          "<p>Set up an Amazon CloudWatch alarm that monitors Shield Advanced metrics for an active DDoS event</p>",
          "<p>Set up an Amazon CloudWatch alarm that monitors AWS Firewall Manager metrics for an active DDoS event</p>",
        ],
      },
      correct_response: ["c"],
      section: "Infrastructure Security",
      question_plain:
        "An e-commerce company is using Amazon Macie, AWS Shield Advanced, Amazon Inspector and AWS Firewall Manager in its AWS account. The company wants to receive alerts in case a DDoS attack occurs against the account.\n\nAs an AWS Certified Security Specialist, what would you recommend?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165448,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A social media company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running under an Application Load Balancer (ALB) with AWS WAF.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following solutions will you combine to address the given use case? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Use WAF geo match statement listing the countries that you want to block</strong></p>\n\n<p><strong>Use WAF IP set statement that specifies the IP addresses that you want to allow through</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define.</p>\n\n<p>You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on EC2, or Amazon API Gateway for your APIs.</p>\n\n<p>AWS WAF - How it Works\n<img src="https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png">\nvia - <a href="https://aws.amazon.com/waf/">https://aws.amazon.com/waf/</a></p>\n\n<p>To block specific countries, you can create a WAF geo match statement listing the countries that you want to block, and to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. You can combine the two rules as shown below:</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q60-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a deny rule for the blocked countries in the NACL associated to each of the EC2 instances</strong> - A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. NACL cannot block traffic based on geographic match conditions.</p>\n\n<p><strong>Use ALB geo match statement listing the countries that you want to block</strong></p>\n\n<p><strong>Use ALB IP set statement that specifies the IP addresses that you want to allow through</strong></p>\n\n<p>An Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing for modern application architectures, including microservices and container-based applications.</p>\n\n<p>An ALB cannot block or allow traffic based on geographic match conditions or IP-based conditions. Both these options have been added as distractors.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/">https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/</a></p>\n',
        answers: [
          "<p>Create a deny rule for the blocked countries in the NACL associated with each of the EC2 instances</p>",
          "<p>Use ALB geo match statement listing the countries that you want to block</p>",
          "<p>Use ALB IP set statement that specifies the IP addresses that you want to allow through</p>",
          "<p>Use WAF geo match statement listing the countries that you want to block</p>",
          "<p>Use WAF IP set statement that specifies the IP addresses that you want to allow through</p>",
        ],
      },
      correct_response: ["d", "e"],
      section: "Data Protection",
      question_plain:
        "A social media company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running under an Application Load Balancer (ALB) with AWS WAF.\n\nAs an AWS Certified Security Specialist, which of the following solutions will you combine to address the given use case? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165450,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A retail company has its flagship application hosted on Amazon EC2 instances that are configured in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). The application should only be accessible to users from a specific country. The company also needs the ability to monitor any prohibited requests for further analysis by the security team.</p>\n\n<p>What will you suggest as the most optimal and low-maintenance solution for the given use case?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Set up an AWS Web Application Firewall (WAF) web ACL. Create a rule to deny any requests that do not originate from the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</strong></p>\n\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon CloudFront distribution, and Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API. AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, Amazon CloudFront, Amazon API Gateway, Application Load Balancer, or AWS AppSync responds to requests either with the requested content or with an HTTP 403 status code (Forbidden).</p>\n\n<p>A web access control list (web ACL) gives you fine-grained control over all of the HTTP(S) web requests that your protected resource responds to. You can protect Amazon CloudFront, Amazon API Gateway, Application Load Balancer, and AWS AppSync resources. You can use criteria like the following to allow or block requests:\n1. IP address origin of the request\n2. Country of origin of the request\n3. String match or regular expression (regex) match in a part of the request\n4. Size of a particular part of the request\n5. Detection of malicious SQL code or scripting</p>\n\n<p>More on web request inspection and handling criteria:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q61-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a></p>\n\n<p>Geographic match rule statement - To allow or block web requests based on country of origin, create one or more geographical, or geo, match statements. You can use this to block access to your site from specific countries or to only allow access from specific countries. If you want to allow some web requests and block others based on country of origin, add a geo match statement for the countries that you want to allow and add a second one for the countries that you want to block.</p>\n\n<p>More on Geographic match rule:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q61-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an AWS WAF web ACL. Create a rule to block the requests that do not originate from the IP range defined in an IP set containing a list of IP ranges that belong to the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</strong> - The IP set match statement inspects the IP address of a web request against a set of IP addresses and address ranges. You can use this to allow or block web requests based on the IP addresses that the requests originate from. However, this is not an optimal solution. While defining an IP set, you need to enter one IP address or IP address range per line. Every time an IP address changes, it has to be manually added to this IP set. Hence, this option is not correct for the given use case.</p>\n\n<p><strong>Set up AWS Shield to block any request that does not originate from the specified country. Attach AWS Shield with the ALB</strong> - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield Standard is automatically enabled when you use AWS services like Elastic Load Balancing (ELB), Application Load Balancer, Amazon CloudFront, and Amazon Route 53. You cannot use AWS Shield to block traffic from a specified country.</p>\n\n<p><strong>Create a Global Accelerator and attach the WAF to it. Create a rule to block any requests that do not originate from the specified country. Create the Global Accelerator to front the existing ALB</strong> - AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon CloudFront distribution, and Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API. WAF cannot be configured with AWS Global Accelerator.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html</a></p>\n',
        answers: [
          "<p>Set up an AWS Web Application Firewall (WAF) web ACL. Create a rule to deny any requests that do not originate from the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</p>",
          "<p>Set up an AWS WAF web ACL. Create a rule to block the requests that do not originate from the IP range defined in an IP set containing a list of IP ranges that belong to the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</p>",
          "<p>Set up AWS Shield to block any request that does not originate from the specified country. Attach AWS Shield with the ALB</p>",
          "<p>Create a Global Accelerator and attach the WAF to it. Create a rule to block any requests that do not originate from the specified country. Create the Global Accelerator to front the existing ALB</p>",
        ],
      },
      correct_response: ["a"],
      section: "Infrastructure Security",
      question_plain:
        "A retail company has its flagship application hosted on Amazon EC2 instances that are configured in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). The application should only be accessible to users from a specific country. The company also needs the ability to monitor any prohibited requests for further analysis by the security team.\n\nWhat will you suggest as the most optimal and low-maintenance solution for the given use case?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165452,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          '<p>A security specialist with administrator permissions is using the AWS management console to access the CloudWatch logs for a Lambda function named "myFunc". However, upon choosing the option to view the logs in the AWS Lambda console, the specialist encountered an error message reading "error loading Log Streams". The specialist was unable to retrieve the logs as desired and must now find a solution to this issue.</p>\n\n<p>Following is an example IAM policy for the Lambda function\'s execution role:</p>\n\n<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": "logs:CreateLogGroup",\n            "Resource": "arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:*"\n        },\n        {\n            "Effect": "Allow",\n            "Action": [\n                "logs:PutLogEvents"\n            ],\n            "Resource": [\n                "arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:log-group:/aws/lambda/myFunc:*"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Which of the following solutions would you suggest to the specialist for addressing the issue?</p>\n',
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Add the logs:CreateLogStream action to the second Allow statement</strong></p>\n\n<p>A log stream is a sequence of log events that share the same source. Each separate source of logs in CloudWatch Logs makes up a separate log stream.</p>\n\n<p>A log group is a group of log streams that share the same retention, monitoring, and access control settings. You can define log groups and specify which streams to put into each group. There is no limit on the number of log streams that can belong to one log group.</p>\n\n<p>CreateLogStream creates a log stream for the specified log group.</p>\n\n<p>For the given use case, you must ensure that the write actions CreateLogGroup and CreateLogStream are allowed.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q62-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/</a></p>\n\n<p>Incorrect options:</p>\n\n<p>Since the security specialist already has administrator privileges as an IAM user, so there is no lack of permissions that\'s causing the error while the specialist is trying to "view" the logs.</p>\n\n<p>The root cause of the issue is that the Lambda function itself needs the CreateLogStream permission to be able to create the log stream and thereby successfully write the logs into CloudWatch Logs.</p>\n\n<p><strong>Add the logs:GetLogEvents action to the second Allow statement</strong></p>\n\n<p><strong>Add the logs:DescribeLogStreams action to the second Allow statement</strong></p>\n\n<p>The GetLogEvents and DescribeLogStreams are both "read" type of permissions which are not needed for the Lambda to successfully write the logs. Hence, both these options are incorrect.</p>\n\n<p><strong>Move the logs:CreateLogGroup action to the second Allow statement</strong> - This option is a distractor. The CreateLogGroup action needs to be in the first Allow statement only.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/permissions-reference-cwl.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/permissions-reference-cwl.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/</a></p>\n',
        answers: [
          "<p>Add the logs:GetLogEvents action to the second Allow statement</p>",
          "<p>Add the logs:CreateLogStream action to the second Allow statement</p>",
          "<p>Add the logs:DescribeLogStreams action to the second Allow statement</p>",
          "<p>Move the logs:CreateLogGroup action to the second Allow statement</p>",
        ],
      },
      correct_response: ["b"],
      section: "Security Logging and Monitoring",
      question_plain:
        'A security specialist with administrator permissions is using the AWS management console to access the CloudWatch logs for a Lambda function named "myFunc". However, upon choosing the option to view the logs in the AWS Lambda console, the specialist encountered an error message reading "error loading Log Streams". The specialist was unable to retrieve the logs as desired and must now find a solution to this issue.\n\nFollowing is an example IAM policy for the Lambda function\'s execution role:\n\n{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": "logs:CreateLogGroup",\n            "Resource": "arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:*"\n        },\n        {\n            "Effect": "Allow",\n            "Action": [\n                "logs:PutLogEvents"\n            ],\n            "Resource": [\n                "arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:log-group:/aws/lambda/myFunc:*"\n            ]\n        }\n    ]\n}\n\n\nWhich of the following solutions would you suggest to the specialist for addressing the issue?',
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165454,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The security team at an e-commerce company has noticed that several Amazon Elastic Block Store (Amazon EBS) volumes are not encrypted. These unencrypted EBS volumes are attached to Amazon EC2 instances that are provisioned with an Auto Scaling group and a launch template. You have been hired as an AWS Certified Security Specialist to implement a solution that ensures all EBS volumes are encrypted both now and in the future.</p>\n\n<p>What would you recommend?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Leverage the Auto Scaling group\'s instance refresh feature to replace existing instances with new instances</strong></p>\n\n<p>You can configure your AWS account to enforce the encryption of the new EBS volumes and snapshot copies that you create. For example, Amazon EBS encrypts the EBS volumes created when you launch an instance and the snapshots that you copy from an unencrypted snapshot. You should note that encryption by default does not affect existing EBS volumes or snapshots.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q63-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/</a></p>\n\n<p>For the given use case, you can use the instance refresh feature of an Auto Scaling group to update the instances in your Auto Scaling group instead of manually replacing instances a few at a time. This can be useful when a configuration change requires you to replace instances, and you have a large number of instances in your Auto Scaling group. Amazon EC2 Auto Scaling starts performing a rolling replacement of the instances. It takes a set of instances out of service, terminates them, and launches a set of instances with the new desired configuration. Then, it waits until the instances pass your health checks and complete warmup before it moves on to replacing other instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new launch template from the existing launch template, such that the encrypted flag for all EBS volumes is set to true in the new launch template. Update the Auto Scaling group to use the new launch template. In due course of time, let the Auto Scaling group replace all the existing instances that have unencrypted EBS volumes</strong> - Since the given use case requires all EBS volumes to be encrypted both now and in the future, so you cannot let the Auto Scaling group to replace all the existing instances (with unencrypted EBS volumes) in due course of time. Hence this option is incorrect.</p>\n\n<p><strong>Modify the launch template by setting the encrypted flag for all EBS volumes to true. Leverage the Auto Scaling group\'s instance refresh feature to replace existing instances with new instances</strong> - You cannot modify a launch template, rather you need to create a new version of the launch template and then leverage the Auto Scaling group\'s instance refresh feature to replace existing instances with new instances. This option has been added as a distractor.</p>\n\n<p><strong>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Propagate this setting to the Auto Scaling group so it will automatically replace existing instances with new instances</strong> - This option acts as a distractor, as there is no such setting in the Auto Scaling group that propagates the automatic encryption of new EBS volumes by default.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/ebs-automatic-encryption/">https://aws.amazon.com/premiumsupport/knowledge-center/ebs-automatic-encryption/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-instance-refresh.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-instance-refresh.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/manage-launch-template-versions.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/manage-launch-template-versions.html</a></p>\n',
        answers: [
          "<p>Configure a new launch template from the existing launch template, such that the encrypted flag for all EBS volumes is set to true in the new launch template. Update the Auto Scaling group to use the new launch template. In due course of time, let the Auto Scaling group replace all the old instances that have unencrypted EBS volumes</p>",
          "<p>Modify the launch template by setting the encrypted flag for all EBS volumes to true. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</p>",
          "<p>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Propagate this setting to the Auto Scaling group so it will automatically replace existing instances with new instances</p>",
          "<p>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</p>",
        ],
      },
      correct_response: ["d"],
      section: "Data Protection",
      question_plain:
        "The security team at an e-commerce company has noticed that several Amazon Elastic Block Store (Amazon EBS) volumes are not encrypted. These unencrypted EBS volumes are attached to Amazon EC2 instances that are provisioned with an Auto Scaling group and a launch template. You have been hired as an AWS Certified Security Specialist to implement a solution that ensures all EBS volumes are encrypted both now and in the future.\n\nWhat would you recommend?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165456,
      assessment_type: "multi-select",
      prompt: {
        question:
          '<p>The security team at a company has set up an IAM user with full permissions for the EC2 service, yet the user is unable to start an Amazon EC2 instance after it was stopped for maintenance purposes. The instance would change its state to "Pending" but would eventually switch back to "Stopped" with the error "client error on launch". Upon investigating the issue, it was discovered that the EC2 instance had attached Amazon EBS volumes that were encrypted using a Customer Master Key (CMK). Detaching the encrypted volumes from the EC2 instance resolved the issue and allowed the user to start the instance successfully.</p>\n\n<p>Following is a snippet of the existing IAM user policy:</p>\n\n<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": [\n                    &lt;action&gt;\n            ],\n            "Resource": "arn:aws:kms:&lt;region&gt;:&lt;accountId&gt;:key/kms-encryption-key-for-ebs",\n            "Condition": &lt;condition&gt;\n        }\n    ]\n}\n</code></pre>\n\n<p>You have been tasked to build a solution to fix this issue. What do you recommend? (Select two)</p>\n',
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Add the &lt;action&gt; as <code>kms:CreateGrant</code> to the IAM user policy</strong></p>\n\n<p>This issue occurs with EC2 instances with encrypted volumes attached if:</p>\n\n<p>The AWS Key Management Service (AWS KMS) or an AWS Identity and Access Management (IAM) user launching the instances doesn\'t have the required permissions.\nThe KMS key usage is restricted by the SourceIp condition key.\nThe IAM user must have permission from AWS KMS to decrypt the AWS KMS key.</p>\n\n<p>You should note that the default KMS key policy gives the AWS account that owns the KMS key permission to use IAM policies to allow access to all AWS KMS operations on the KMS key. To allow access to decrypt a KMS key, you must use the key policy with IAM policies or grants. IAM policies alone aren\'t sufficient to allow access to a KMS key, but you can use them in combination with a KMS key\'s policy (which is the default key policy in this case).</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q64-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html">https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html</a></p>\n\n<p><strong>Add the condition as <code>{ "Bool": { "kms:GrantIsForAWSResource": true }</code> to the IAM user policy</strong></p>\n\n<p>The condition <code>kms:GrantIsForAWSResource</code> allows or denies permission for the CreateGrant, ListGrants, or RevokeGrant operations only when an AWS service integrated with AWS KMS calls the operation on the user\'s behalf. This policy condition doesn\'t allow the user to call these grant operations directly. This policy condition can be applied to Key policies as well as IAM policies.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q64-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource">https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add the &lt;action&gt; as <code>kms:DescribeKey</code> to the IAM user policy</strong> - <code>kms:EnableKey</code> controls permission to view detailed information about an AWS KMS key. This option is not relevant to the given use case.</p>\n\n<p><strong>Add the &lt;action&gt; as <code>kms:CreateKey</code> to the IAM user policy</strong> - <code>kms:CreateKey</code> controls permission to create an AWS KMS key that can be used to protect data keys and other sensitive information. Since the key is already created, this option is not relevant.</p>\n\n<p><strong>Add the condition as <code>{ "Bool": { "kms:ViaService": "ec2.&lt;region&gt;.amazonaws.com"}</code> to the IAM user policy</strong> - The <code>kms:ViaService</code> condition key limits the use of a KMS key to requests from specified AWS services. You can specify one or more services in each kms:ViaService condition key. The operation must be a KMS key resource operation, that is, an operation that is authorized for a particular KMS key. This option has been added as a distractor since the issue is with the EBS volumes (and not the EC2 service) that were encrypted using a CMK and the lack of sufficient key-specific grants thereof.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/encrypted-volumes-stops-immediately/">https://aws.amazon.com/premiumsupport/knowledge-center/encrypted-volumes-stops-immediately/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource">https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html">https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html</a></p>\n',
        answers: [
          "<p>Add the &lt;action&gt; as <code>kms:CreateGrant</code> to the IAM user policy</p>",
          '<p>Add the condition as <code>{ "Bool": { "kms:GrantIsForAWSResource": true }</code> to the IAM user policy</p>',
          "<p>Add the &lt;action&gt; as <code>kms:DescribeKey</code> to the IAM user policy</p>",
          "<p>Add the &lt;action&gt; as <code>kms:CreateKey</code> to the IAM user policy</p>",
          '<p>Add the condition as <code>{ "Bool": { "kms:ViaService": "ec2.&lt;region&gt;.amazonaws.com"}</code> to the IAM user policy</p>',
        ],
      },
      correct_response: ["a", "b"],
      section: "Identity and Access Management",
      question_plain:
        'The security team at a company has set up an IAM user with full permissions for the EC2 service, yet the user is unable to start an Amazon EC2 instance after it was stopped for maintenance purposes. The instance would change its state to "Pending" but would eventually switch back to "Stopped" with the error "client error on launch". Upon investigating the issue, it was discovered that the EC2 instance had attached Amazon EBS volumes that were encrypted using a Customer Master Key (CMK). Detaching the encrypted volumes from the EC2 instance resolved the issue and allowed the user to start the instance successfully.\n\nFollowing is a snippet of the existing IAM user policy:\n\n{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": [\n                    &lt;action&gt;\n            ],\n            "Resource": "arn:aws:kms:&lt;region&gt;:&lt;accountId&gt;:key/kms-encryption-key-for-ebs",\n            "Condition": &lt;condition&gt;\n        }\n    ]\n}\n\n\nYou have been tasked to build a solution to fix this issue. What do you recommend? (Select two)',
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 76165458,
      assessment_type: "multi-select",
      prompt: {
        question:
          '<p>A financial services company wants to develop a solution called Financial Information System (FIS) on AWS Cloud that would allow the financial institutions and government agencies to collaborate, anticipate and navigate the changing finance landscape. While pursuing this endeavor, the company would like to decrease its IT operational overhead. The solution should help the company eliminate the bottleneck created by manual provisioning of development pipelines while adhering to crucial governance and control requirements. As a means to this end, the company has set up "AWS Organizations" to manage several of these scenarios and would like to use Service Control Policies (SCP) for central control over the maximum available permissions for the various accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines.</p>\n\n<p>Which of the following scenarios would you identify as correct regarding the given use-case? (Select three)</p>\n',
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can\'t perform that action</strong></p>\n\n<p><strong>SCPs affect all users and roles in attached accounts, including the root user</strong></p>\n\n<p><strong>SCPs do not affect service-linked role</strong></p>\n\n<p>Service control policies (SCPs) are one type of policy that can be used to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.</p>\n\n<p>In SCPs, you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access. You can also define conditions for when to restrict access to AWS services, resources, and API actions. These restrictions even override the administrators of member accounts in the organization.</p>\n\n<p>Please note the following effects on permissions vis-a-vis the SCPs:</p>\n\n<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can\'t perform that action.</p>\n\n<p>SCPs affect all users and roles in the attached accounts, including the root user.</p>\n\n<p>SCPs do not affect any service-linked role.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q65-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can still perform that action</strong></p>\n\n<p><strong>SCPs affect all users and roles in attached accounts, excluding the root user</strong></p>\n\n<p><strong>SCPs affect service-linked roles</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p>\n',
        answers: [
          "<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action</p>",
          "<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can still perform that action</p>",
          "<p>SCPs affect all users and roles in attached accounts, including the root user</p>",
          "<p>SCPs affect all users and roles in attached accounts, excluding the root user</p>",
          "<p>SCPs affect service-linked roles</p>",
          "<p>SCPs do not affect service-linked role</p>",
        ],
      },
      correct_response: ["a", "c", "f"],
      section: "Identity and Access Management",
      question_plain:
        'A financial services company wants to develop a solution called Financial Information System (FIS) on AWS Cloud that would allow the financial institutions and government agencies to collaborate, anticipate and navigate the changing finance landscape. While pursuing this endeavor, the company would like to decrease its IT operational overhead. The solution should help the company eliminate the bottleneck created by manual provisioning of development pipelines while adhering to crucial governance and control requirements. As a means to this end, the company has set up "AWS Organizations" to manage several of these scenarios and would like to use Service Control Policies (SCP) for central control over the maximum available permissions for the various accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines.\n\nWhich of the following scenarios would you identify as correct regarding the given use-case? (Select three)',
      related_lectures: [],
    },
  ],
};
