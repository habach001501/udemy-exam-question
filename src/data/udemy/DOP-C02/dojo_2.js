export const dojo_2 = {
  count: 75,
  next: null,
  previous: null,
  results: [
    {
      _class: "assessment",
      id: 134588367,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A European enterprise has developed a serverless web application hosted on AWS. The application comprises Amazon API Gateway, various AWS Lambda functions, Amazon S3, and an Amazon RDS for MySQL database. The source code consists of AWS Serverless Application Model (AWS SAM) templates and Python code and is stored in GitHub.</p><p>The enterprise's security team recently performed a security audit and discovered that user names and passwords for authentication to the database are hardcoded within GitHub repositories. The DevOps Engineer must implement a solution with the following requirements:</p><p>- Automatically detect and prevent hardcoded secrets.<br>- Automatic secrets rotation should also be implemented.</p><p>What is the MOST secure solution that meets these requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon CodeGuru</strong> helps improve code quality and automate code reviews by scanning and profiling your Java and Python applications. <strong>CodeGuru Reviewer</strong> can detect potential defects and bugs in your code. For instance, it recommends improvements regarding security vulnerabilities, resource leaks, concurrency issues, incorrect input validation, and deviation from AWS best practices.</p><p><img src="https://media.tutorialsdojo.com/public/codeguru-github-100324.png"></p><p><strong>Amazon CodeGuru Reviewer Secrets Detector </strong>is an automated tool that helps developers detect secrets in source code or configuration files, such as passwords, API keys, SSH keys, and access tokens. The detectors use machine learning (ML) to identify hardcoded secrets as part of the code review process, ensuring all new code doesn’t contain hardcoded secrets before being merged and deployed. In addition to Java and Python code, secrets detectors scan configuration and documentation files. CodeGuru Reviewer suggests remediation steps to secure secrets with AWS Secrets Manager, a managed service that lets you securely and automatically store, rotate, manage, and retrieve credentials, API keys, and all sorts of secrets.</p><p><img src="https://media.tutorialsdojo.com/public/codeguru-detected-secret.png"></p><p><strong>Secrets Manager</strong> enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can\'t be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise.</p><p>Hence, the correct answer is: <strong>Link the GitHub repositories to Amazon CodeGuru Reviewer. Perform a manual evaluation of the code review for any recommendations. Select the option to protect the secret. Revise the SAM templates and Python code to fetch the secret from AWS Secrets Manager.</strong></p><p>The option that says: <strong>Link the GitHub repositories to Amazon CodeGuru Reviewer. Perform a manual evaluation of the code review for any recommendations. Store the secret as a string in Parameter Store. Modify the SAM templates and Python code to retrieve the secret from Parameter Store</strong> is incorrect. While it utilizes CodeGuru Reviewer, storing a secret as a string in Parameter Store is not considered to be secure. It is recommended to store secrets as a secure string. Furthermore, the Parameter Store typically does not provide automatic secrets rotation. It is recommended to use AWS Secrets Manager instead.</p><p>The option that says: <strong>Integrate Amazon CodeGuru Profiler. Apply the CodeGuru Profiler function decorator &lt;code&gt;@with_lambda_profiler()&lt;/code&gt; to your handler function and review the recommendation report manually. Select the option to protect the secret. Modify the SAM templates and Python code to fetch the secret from AWS Secrets Manage</strong>r is incorrect because CodeGuru Profiler is primarily designed to enhance the performance of production applications and identify the most resource-intensive lines of code. It is not intended to automatically identify hardcoded secrets.</p><p>The option that says: <strong>Integrate Amazon CodeGuru Profiler on the AWS Lambda function by enabling the code profiling feature. Apply the CodeGuru Profiler function decorator &lt;code&gt;@with_lambda_profiler()&lt;/code&gt; to your handler function and review the recommendation report manually. Store the secret as a secure string in Parameter Store. Modify the SAM templates and Python code to retrieve the secret from Parameter Store</strong> is incorrect. Although it stores secrets in Parameter Store as a secure string, it does not fulfill the requirement for automatic secrets rotation. In addition, this option utilizes CodeGuru Profiler instead of CodeGuru Reviewer.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/aws/codeguru-reviewer-secrets-detector-identify-hardcoded-secrets/">https://aws.amazon.com/blogs/aws/codeguru-reviewer-secrets-detector-identify-hardcoded-secrets/</a></p><p><a href="https://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda-command-line.html">https://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda-command-line.html</a></p><p><a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p><p><br></p><p><strong>Check out this AWS Secrets Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-secrets-manager/?src=udemy">https://tutorialsdojo.com/aws-secrets-manager/</a></p>',
        answers: [
          "<p>Integrate Amazon CodeGuru Profiler on the AWS Lambda function by enabling the code profiling feature. Apply the CodeGuru Profiler function decorator <code>@with_lambda_profiler()</code> to your handler function and review the recommendation report manually. Store the secret as a secure string in Parameter Store. Modify the SAM templates and Python code to retrieve the secret from Parameter Store.</p>",
          "<p>Link the GitHub repositories to Amazon CodeGuru Reviewer. Perform a manual evaluation of the code review for any recommendations. Select the option to protect the secret. Revise the SAM templates and Python code to fetch the secret from AWS Secrets Manager.</p>",
          "<p>Integrate Amazon CodeGuru Profiler. Apply the CodeGuru Profiler function decorator <code>@with_lambda_profiler()</code> to your handler function and review the recommendation report manually. Select the option to protect the secret. Modify the SAM templates and Python code to fetch the secret from AWS Secrets Manager.</p>",
          "<p>Link the GitHub repositories to Amazon CodeGuru Reviewer. Perform a manual evaluation of the code review for any recommendations. Store the secret as a string in Parameter Store. Modify the SAM templates and Python code to retrieve the secret from Parameter Store.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A European enterprise has developed a serverless web application hosted on AWS. The application comprises Amazon API Gateway, various AWS Lambda functions, Amazon S3, and an Amazon RDS for MySQL database. The source code consists of AWS Serverless Application Model (AWS SAM) templates and Python code and is stored in GitHub.The enterprise's security team recently performed a security audit and discovered that user names and passwords for authentication to the database are hardcoded within GitHub repositories. The DevOps Engineer must implement a solution with the following requirements:- Automatically detect and prevent hardcoded secrets.- Automatic secrets rotation should also be implemented.What is the MOST secure solution that meets these requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588369,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A digital payment gateway system is running in AWS which serves thousands of businesses worldwide. It is hosted in an Auto Scaling Group of EC2 instances behind an Application Load Balancer with an Amazon RDS database in a Multi-AZ deployment configuration. The company is using several CloudFormation templates in deploying the new version of the system. The <code>AutoScalingRollingUpdate</code> policy is used to control how CloudFormation handles rolling updates for their Auto Scaling group which replaces the old instances based on the parameters they have set. Lately, there were a lot of failed deployments which has caused system unavailability issues and business disruptions. They want to find out what's preventing their Auto Scaling group from updating correctly during a stack update.&nbsp; </p><p>In this scenario, how should the DevOps engineer troubleshoot this issue? (Select THREE.)</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>The AWS::AutoScaling::AutoScalingGroup resource uses the UpdatePolicy attribute to define how an Auto Scaling group resource is updated when the AWS CloudFormation stack is updated. If you don\'t have the right settings configured for the UpdatePolicy attribute, your rolling update can produce unexpected results.</p><p>You can use the <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate">AutoScalingRollingUpdate policy</a> to control how AWS CloudFormation handles rolling updates for an Auto Scaling group. This common approach keeps the same Auto Scaling group, and then replaces the old instances based on the parameters that you set.</p><p>The <strong>AutoScalingRollingUpdate</strong> policy supports the following configuration options:</p><p><br></p><pre class="prettyprint linenums">    "UpdatePolicy": {\n      "AutoScalingRollingUpdate": {\n        "MaxBatchSize": Integer,\n        "MinInstancesInService": Integer,\n        "MinSuccessfulInstancesPercent": Integer,\n        "PauseTime": String,\n        "SuspendProcesses": [ List of processes ],\n        "WaitOnResourceSignals": Boolean\n      }\n    }\n</pre><p><br></p><p>Using a rolling update has a risk of system outages and performance degradation due to the decreased availability of your running EC2 instances. If you want to ensure high availability of your application, you can also use the <em>AutoScalingReplacingUpdate</em> policy to perform an immediate rollback of the stack without any possibility of failure.</p><p>To find out what\'s preventing your Auto Scaling group from updating correctly during a stack update, work through the following troubleshooting scenarios as needed:</p><p><strong>- Configure WaitOnResourceSignals and PauseTime to avoid problems with success signals</strong></p><p>In your <em>AutoScalingRollingUpdate</em> policy, set the <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate-waitonresourcesignals"><em>WaitOnResourceSignals</em></a> property to false. Take note that if <em>WaitOnResourceSignals</em> is set to true, <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate-pausetime"><em>PauseTime</em></a> changes to a timeout value. AWS CloudFormation waits to receive a success signal until the maximum time specified by the <em>PauseTime</em> value. If a signal is not received, AWS CloudFormation cancels the update. Then, AWS CloudFormation rolls back the stack with the same settings, including the same PauseTime value.</p><p><strong>- Configure MinSuccessfulInstancesPercent to avoid stack rollback</strong></p><p>If you\'re replacing a large number of instances during a rolling update and waiting for a success signal for each instance, complete the following: In your <em>AutoScalingRollingUpdate</em> policy, set the value of the <em>MinSuccessfulInstancesPercent</em> property. Take note that setting the <em>MinSuccessfulInstancesPercent</em> property prevents AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch.</p><p><strong>- Configure SuspendProcesses to avoid unexpected changes to the Auto Scaling group</strong></p><p>During a rolling update, suspend the following Auto Scaling processes: <em>HealthCheck</em>, <em>ReplaceUnhealthy</em>, <em>AZRebalance</em>, <em>AlarmNotification</em>, and <em>ScheduledActions</em>. It is quite important to know that if you\'re using your Auto Scaling group with Elastic Load Balancing (ELB), you should not suspend the following processes: <em>Launch</em>, <em>Terminate</em>, and <em>AddToLoadBalancer</em>. These processes are required to make rolling updates. Take note that if an unexpected scaling action changes the state of the Auto Scaling group during a rolling update, the update can fail. The failure can result from an inconsistent view of the group by AWS CloudFormation.</p><p>Based on the above information, the correct answers are:</p><p><strong>- In your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy, set the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> property to false.</strong></p><p><strong>- In your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy, set the value of the </strong><code><strong>MinSuccessfulInstancesPercent</strong></code><strong> property to prevent AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch</strong></p><p><strong>- During a rolling update, suspend the following Auto Scaling processes: </strong><code><strong>HealthCheck</strong></code><strong>, </strong><code><strong>ReplaceUnhealthy</strong></code><strong>, </strong><code><strong>AZRebalance</strong></code><strong>, </strong><code><strong>AlarmNotification</strong></code><strong>, and </strong><code><strong>ScheduledActions</strong></code></p><p><br></p><p>The option that says: <strong>Switch from </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> to </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy by modifying the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template. Set the </strong><code><strong>WillReplace</strong></code><strong> property to true</strong> is incorrect because although the <code><strong><em>AutoScalingReplacingUpdate</em></strong></code><strong><em> </em></strong>policy provides an immediate rollback of the stack without any possibility of failure, this solution is not warranted since the scenario asks for the options that will help troubleshoot the issue.</p><p>The option that says: <strong>Suspend the following Auto Scaling processes that are related with your ELB: </strong><code><strong>Launch</strong></code><strong>, </strong><code><strong>Terminate</strong></code><strong>, and </strong><code><strong>AddToLoadBalancer</strong></code> is incorrect because these processes are required by the ELB to make rolling updates.</p><p>The option that says: <strong>Set the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> property to true in your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy</strong> is incorrect. The <code>WaitOnResourceSignals</code> property should be set to false instead of true, to determine what prevents the Auto Scaling group from being updated correctly during a stack update.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p>',
        answers: [
          "<p>Switch from <code>AutoScalingRollingUpdate</code> to <code>AutoScalingReplacingUpdate</code> policy by modifying the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template. Set the <code>WillReplace</code> property to true. </p>",
          "<p>In your <code>AutoScalingRollingUpdate</code> policy, set the <code>WaitOnResourceSignals</code> property to false.</p>",
          "<p>In your <code>AutoScalingRollingUpdate</code> policy, set the value of the <code>MinSuccessfulInstancesPercent</code> property to prevent AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch.</p>",
          "<p>During a rolling update, suspend the following Auto Scaling processes: <code>HealthCheck</code>, <code>ReplaceUnhealthy</code>, <code>AZRebalance</code>, <code>AlarmNotification</code>, and <code>ScheduledActions</code>.</p>",
          "<p>Suspend the following Auto Scaling processes that are related with your ELB: <code>Launch</code>, <code>Terminate</code>, and <code>AddToLoadBalancer</code>.</p>",
          "<p>Set the <code>WaitOnResourceSignals</code> property to true in your <code>AutoScalingRollingUpdate</code> policy.</p>",
        ],
      },
      correct_response: ["b", "c", "d"],
      section: "SDLC Automation",
      question_plain:
        "A digital payment gateway system is running in AWS which serves thousands of businesses worldwide. It is hosted in an Auto Scaling Group of EC2 instances behind an Application Load Balancer with an Amazon RDS database in a Multi-AZ deployment configuration. The company is using several CloudFormation templates in deploying the new version of the system. The AutoScalingRollingUpdate policy is used to control how CloudFormation handles rolling updates for their Auto Scaling group which replaces the old instances based on the parameters they have set. Lately, there were a lot of failed deployments which has caused system unavailability issues and business disruptions. They want to find out what's preventing their Auto Scaling group from updating correctly during a stack update.&nbsp; In this scenario, how should the DevOps engineer troubleshoot this issue? (Select THREE.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588371,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company has a separate AWS account where developers can freely spawn its AWS resources and test new builds. Given the lax restrictions in this environment, AWS Trusted Advisor was checked, and it shows that several instances use the default security group rule that opens inbound port 22 to all IP addresses. Even for a test environment, port 22 access should be restricted to the public IP of the on-premises data center only. The company wants to be notified of any security check recommendations from Trusted Advisor and automatically resolve non-compliance based on the results.</p><p>What are the steps to set up the required solution? (Select THREE.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p><strong>AWS Config</strong> enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. You can configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic. This way, you can be notified when AWS Config evaluates your custom or managed rules against your resources.</p><p>AWS Config now includes remediation capability with AWS Config rules. This feature gives you the ability to associate and execute remediation actions with AWS Config rules to address noncompliant resources. You can choose from a list of available remediation actions. For example, you can create an AWS Config rule to check that your Amazon S3 buckets do not allow public read access. You can then associate a remediation action to disable public access for noncompliant S3 buckets.</p><p>It\'s easy to set up remediation actions through the AWS Config console or API. Just choose the remediation action you want to associate from a pre-populated list or create your own custom remediation actions using AWS Systems Manager Automation documents.</p><p><img src="https://media.tutorialsdojo.com/public/how-AWSconfig-works.png"></p><p>You can also write a scheduled Lambda function to check Trusted Advisor regularly. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression using Amazon EventBridge. You can retrieve and refresh Trusted Advisor results programmatically. The AWS Support service enables you to write applications that interact with AWS Trusted Advisor.</p><p>Hence, the correct answers are:</p><p><strong>- Create a Lambda function and integrate Amazon EventBridge and AWS Lambda to execute the function on a regular schedule to check AWS Trusted Advisor via API. Based on the results, publish a message to an SNS Topic to notify the subscribers.</strong></p><p><strong>- Set up custom AWS Config rule that checks security groups to make sure that port 22 is not open to public. Send a notification to an SNS topic for non-compliance.</strong></p><p><strong>- Set up a custom AWS Config rule to execute a remediation action using AWS Systems Manager Automation to update the security group rules for port 22 and restrict it to the office\'s public IP.</strong></p><p>The option that says: <strong>Create an AWS Config Cron job to schedule checks on all AWS security groups and send results to SNS for the non-compliance notification</strong> is incorrect because you do not schedule AWS Config with Cron jobs since this service can be configured to run periodically.</p><p>The option that says: <strong>Set up custom AWS Config rule to execute a remediation action that triggers a Lambda Function to update the publicly open port 22 in the security group and restrict to the office’s public IP </strong>is incorrect because the custom remediation actions in AWS Config is simply used in conjunction with AWS Systems Manager Automation documents.</p><p>The option that says: <strong>Create a Lambda function that executes every hour to refresh AWS Trusted Advisor scan results via API. The automated notification on AWS trusted Advisor will notify of any changes</strong> is incorrect because the notification is only sent on a weekly basis, which can be quite long if you are concerned about security issues.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/about-aws/whats-new/2019/03/use-aws-config-to-remediate-noncompliant-resources/">https://aws.amazon.com/about-aws/whats-new/2019/03/use-aws-config-to-remediate-noncompliant-resources/</a></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html ">https://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html</a></p><p><a href="https://aws.amazon.com/blogs/aws/aws-config-rules-dynamic-compliance-checking-for-cloud-resources/">https://aws.amazon.com/blogs/aws/aws-config-rules-dynamic-compliance-checking-for-cloud-resources/</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-config/?src=udemy">https://tutorialsdojo.com/aws-config/</a></p>',
        answers: [
          "<p>Create a Lambda function and integrate Amazon EventBridge and AWS Lambda to execute the function on a regular schedule to check AWS Trusted Advisor via API. Based on the results, publish a message to an SNS Topic to notify the subscribers.</p>",
          "<p>Create an AWS Config Cron job to schedule checks on all AWS security groups and send results to SNS for the non-compliance notification.</p>",
          "Set up custom AWS Config rule that checks security groups to make sure that port 22 is not open to public. Send a notification to an SNS topic for non-compliance.",
          "<p>Set up a custom AWS Config rule to execute a remediation action using AWS Systems Manager Automation to update the security group rules for port 22 and restrict it to the office's public IP.</p>",
          "<p>Set up custom AWS Config rule to execute a remediation action that triggers a Lambda Function to update the publicly open port 22 in the security group and restrict to the office’s public IP.</p>",
          "<p>Create a Lambda function that executes every hour to refresh AWS Trusted Advisor scan results via API. The automated notification on AWS trusted Advisor will notify of any changes.</p>",
        ],
      },
      correct_response: ["a", "c", "d"],
      section: "Incident and Event Response",
      question_plain:
        "A company has a separate AWS account where developers can freely spawn its AWS resources and test new builds. Given the lax restrictions in this environment, AWS Trusted Advisor was checked, and it shows that several instances use the default security group rule that opens inbound port 22 to all IP addresses. Even for a test environment, port 22 access should be restricted to the public IP of the on-premises data center only. The company wants to be notified of any security check recommendations from Trusted Advisor and automatically resolve non-compliance based on the results.What are the steps to set up the required solution? (Select THREE.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588373,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A DevOps engineer working for a company that manages multiple teams sharing a single AWS account is tasked with overseeing the production infrastructure. The teams primarily store media and images in Amazon S3 buckets; some buckets are configured for public internet access, while others are restricted to internal applications. To ensure security and compliance, the company wants to leverage AWS Trusted Advisor to identify public buckets and verify that only intended users have &lt;code&gt;List&lt;/code&gt; access. Additionally, the company needs to be notified whenever a public bucket has incorrect permissions and requires automatic remediation if needed.</p><p>Which of the following actions should the DevOps engineer implement to meet these requirements? (Select THREE.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p><strong>AWS Config</strong> enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. You can configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic. This way, you can be notified when AWS Config evaluates your custom or managed rules against your resources.</p><p>AWS Config can monitor your Amazon Simple Storage Service (S3) bucket ACLs and policies for violations that allow public read or public write access. If AWS Config finds a policy violation, it can trigger an Amazon EventBridge rule to trigger an AWS Lambda function which either corrects the S3 bucket ACL, or notifies you via Amazon Simple Notification Service (Amazon SNS) that the policy is in violation and allows public read or public write access.</p><p><img src="https://media.tutorialsdojo.com/public/how-AWSconfig-works.png"></p><p>You can use Amazon EventBridge to detect and react to changes in the status of Trusted Advisor checks. Then, based on the rules that you create, Amazon EventBridge invokes one or more target actions when a check status changes to the value you specify in a rule. Depending on the type of status change, you might want to send notifications, capture status information, take corrective action, initiate events, or take other actions.</p><p>Hence, the correct answers are:</p><p><strong>- Set up a custom Config rule that checks public S3 bucket permissions. Then, send a non-compliance notification to your subscribed Amazon SNS topic.</strong></p><p><strong>- Set up a custom Config rule to check public S3 bucket permissions and send an event to Amazon EventBridge when policy violations are detected. Configure the EventBridge rule to trigger a Lambda function, automatically updating the S3 bucket permissions.</strong></p><p><strong>- Utilize EventBridge to monitor Trusted Advisor security recommendation results and then set a trigger to send an email using SNS to notify you about the results of the check.</strong></p><p>The option that says: <strong>Set up a custom Amazon Inspector rule that checks public S3 buckets permissions. Send an action to AWS Systems Manager to correct the S3 bucket policy </strong>is incorrect because Amazon Inspector is just an automated security assessment service that is primarily used for EC2 instances. You have to use AWS Config instead.</p><p>The option that says: <strong>Set up a custom AWS Config rule to execute a default remediation action to update the permissions on the public S3 bucket </strong>is incorrect because there is no default remediation action. This should be integrated with the AWS Systems Manager Automation service where you can configure the actions for your remediation.</p><p>The option that says: <strong>Create an AWS Lambda function that executes every hour to refresh Trusted Advisor scan results via API. Subscribe to Trusted Advisor notification messages to receive the results </strong>is incorrect because it is better to use AWS Config instead of AWS Trusted Advisor in this scenario. Moreover, Trusted Advisor only sends the summary notification every week so this won\'t notify you immediately about your non-compliant resources.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/security/how-to-use-aws-config-to-monitor-for-and-respond-to-amazon-s3-buckets-allowing-public-access/ ">https://aws.amazon.com/blogs/security/how-to-use-aws-config-to-monitor-for-and-respond-to-amazon-s3-buckets-allowing-public-access/</a></p><p><a href="https://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html">https://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html</a></p><p><a href="https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p><p><br></p><p><strong>Check out these AWS Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/aws-config/?src=udemy">https://tutorialsdojo.com/aws-config/</a></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><a href="https://tutorialsdojo.com/aws-trusted-advisor/?src=udemy">https://tutorialsdojo.com/aws-trusted-advisor/</a></p>',
        answers: [
          "<p>Set up a custom Amazon Inspector rule that checks public S3 buckets permissions. Send an action to AWS Systems Manager to correct the S3 bucket policy.</p>",
          "<p>Set up a custom AWS Config rule to execute a default remediation action to update the permissions on the public S3 bucket.</p>",
          "<p>Create an AWS Lambda function that executes every hour to refresh Trusted Advisor scan results via API. Subscribe to Trusted Advisor notification messages to receive the results.</p>",
          "<p>Set up a custom Config rule that checks public S3 bucket permissions. Then, send a non-compliance notification to your subscribed Amazon SNS topic.</p>",
          "<p>Set up a custom Config rule to check public S3 bucket permissions and send an event to Amazon EventBridge when policy violations are detected. Configure the EventBridge rule to trigger a Lambda function, automatically updating the S3 bucket permissions.</p>",
          "<p>Utilize EventBridge to monitor Trusted Advisor security recommendation results and then set a trigger to send an email using SNS to notify you about the results of the check.</p>",
        ],
      },
      correct_response: ["d", "e", "f"],
      section: "Incident and Event Response",
      question_plain:
        "A DevOps engineer working for a company that manages multiple teams sharing a single AWS account is tasked with overseeing the production infrastructure. The teams primarily store media and images in Amazon S3 buckets; some buckets are configured for public internet access, while others are restricted to internal applications. To ensure security and compliance, the company wants to leverage AWS Trusted Advisor to identify public buckets and verify that only intended users have &lt;code&gt;List&lt;/code&gt; access. Additionally, the company needs to be notified whenever a public bucket has incorrect permissions and requires automatic remediation if needed.Which of the following actions should the DevOps engineer implement to meet these requirements? (Select THREE.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588375,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A startup aims to rearchitect its internal web application hosted on Amazon EC2 into serverless architecture. At present, the startup deploys changes to the application by provisioning a new Auto Scaling group of EC2 instances across multiple Availability Zones and is fronted with a new Application Load Balancer. It then shifts the traffic with the use of Amazon Route 53 weighted routing policy. The DevOps Engineer of the startup will need to design a deployment strategy for serverless architecture similar to the current process that retains the ability to test new features with a limited set of users before making the features accessible to the entire user base. The startup plans to use AWS Lambda and Amazon API Gateway for the serverless architecture.</p><p>Which of the following is the MOST suitable solution to meet the requirements?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>By introducing alias traffic shifting, implementing <strong>canary deployments</strong> of <strong>Lambda functions</strong> has become effortless. The weightings of additional version can be adjusted on an alias to route invocation traffic to new function versions based on the weight specified.</p><p>In<strong> API Gateway</strong>, a <strong>canary release deployment</strong> uses the deployment stage for the production release of the base version of an API, and attaches to the stage a canary release for the new versions, relative to the base version, of the API. The stage is associated with the initial deployment and the canary with subsequent deployments.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-canary-release-strategy.png"></p><p>Hence, the correct answer is the option that says: <strong>Deploy Lambda functions with versions and API Gateway using AWS CloudFormation. When there are code changes, update the CloudFormation stack with the new Lambda code then a canary release strategy should be used to update the API versions. Once testing is done, promote the new version.</strong></p><p>The option that says: <strong>Deploy Lambda functions and API Gateway via AWS CDK. When there are code changes, update the CloudFormation Stack and deploy the new version of the Lambda functions and APIs. Enable canary release strategy by utilizing Amazon Route 53 failover routing policy </strong>is incorrect because failover routing policy is primarily used for active-passive failover that lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. In addition, Route 53 cannot set Lambda versions as target.</p><p>The option that says: <strong>Utilize AWS Elastic Beanstalk to deploy Lambda functions and API Gateway. When there are code changes, a new version of both Lambda functions and API should be deployed. Use Elastic Beanstalk\'s blue/green deployment strategy to shift traffic gradually </strong>is incorrect because Elastic Beanstalk cannot deploy Lambda functions and API Gateway.</p><p>The option that says: <strong>Use AWS CodeDeploy to deploy the Lambda functions and the API Gateway. When there are code changes, use CodeDeploy’s All at once deployment strategy, then redirect all traffic immediately using Amazon Route 53 simple routing policy </strong>is incorrect because the All at once deployment strategy updates all instances simultaneously. All instances in your environment are out of service for a short period of time. This strategy doesn’t allow for testing new features with a limited set of users before making the features accessible to the entire user base, which is a requirement in the question. Also, Amazon Route 53 simple routing policy is used when you have a single resource that performs a given function for your domain, it doesn’t allow for gradual traffic shifting which is required in the context of the question.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html#api-gateway-canary-release-deployment-overview">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html#api-gateway-canary-release-deployment-overview</a></p><p><a href="https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/">https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/</a></p><p><a href="https://aws.amazon.com/blogs/compute/performing-canary-deployments-for-service-integrations-with-amazon-api-gateway/">https://aws.amazon.com/blogs/compute/performing-canary-deployments-for-service-integrations-with-amazon-api-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Utilize AWS Elastic Beanstalk to deploy Lambda functions and API Gateway. When there are code changes, a new version of both Lambda functions and API should be deployed. Use Elastic Beanstalk's blue/green deployment strategy to shift traffic gradually.</p>",
          "<p>Deploy Lambda functions with versions and API Gateway using AWS CloudFormation. When there are code changes, update the CloudFormation stack with the new Lambda code then a canary release strategy should be used to update the API versions. Once testing is done, promote the new version.</p>",
          "<p>Use AWS CodeDeploy to deploy the Lambda functions and the API Gateway. When there are code changes, use CodeDeploy’s All at once deployment strategy, then redirect all traffic immediately using Amazon Route 53 simple routing policy.</p>",
          "<p>Deploy Lambda functions and API Gateway via AWS CDK. When there are code changes, update the CloudFormation Stack and deploy the new version of the Lambda functions and APIs. Enable canary release strategy by utilizing Amazon Route 53 failover routing policy.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A startup aims to rearchitect its internal web application hosted on Amazon EC2 into serverless architecture. At present, the startup deploys changes to the application by provisioning a new Auto Scaling group of EC2 instances across multiple Availability Zones and is fronted with a new Application Load Balancer. It then shifts the traffic with the use of Amazon Route 53 weighted routing policy. The DevOps Engineer of the startup will need to design a deployment strategy for serverless architecture similar to the current process that retains the ability to test new features with a limited set of users before making the features accessible to the entire user base. The startup plans to use AWS Lambda and Amazon API Gateway for the serverless architecture.Which of the following is the MOST suitable solution to meet the requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588377,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A DevOps engineer is managing an application deployed across multiple AWS Availability Zones using an Application Load Balancer (ALB). The application is hosted on EC2 instances spread across these zones to ensure high availability. Recently, the engineer noticed that some instances in one Availability Zone are experiencing issues.</p><p>The engineer must isolate the impacted instances and use failover mechanisms to direct traffic to healthy zones.</p><p>Which of the following solutions should the DevOps engineer implement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>When cross-zone load balancing is enabled, each load balancer node distributes traffic across all registered targets in the enabled Availability Zones. This means that if one Availability Zone is experiencing issues, traffic would still be sent to that zone, potentially leading to further problems.</p><p>Application Load Balancers always enable cross-zone load balancing at the load balancer level. However, at the target group level, this feature can be disabled.</p><p><img src="https://media.tutorialsdojo.com/public/cross-zone-load-balancing-110524.png"></p><p>Route 53 ARC allows you to implement highly reliable routing controls to manage traffic during application recovery scenarios. These routing controls act as simple on/off switches hosted on a high-availability cluster. The cluster provides five redundant Regional endpoints, enabling API calls to manage the state of the routing controls. To handle failover, you can set one routing control to ON and another to OFF. This action reroutes traffic from the impacted Availability Zone to a healthy one, ensuring continuous application availability.</p><p><img src="https://media.tutorialsdojo.com/public/amazon-application-recovery-controller-110524.png"></p><p>Hence, the correct answer is: <strong>Disable Cross-Zone Load Balancing on the ALB and use Amazon Route 53 Application Recovery Controller to initiate a zonal shift to avoid the impacted zone entirely.</strong> This solution ensures that traffic is not sent to the unhealthy zone by disabling Cross-Zone Load Balancing and effectively shifts traffic to the healthy zones using the Application Recovery Controller. This combination addresses both the isolation of the unhealthy zone and the rerouting of traffic to maintain application availability.</p><p>The option that says: <strong>Enable Cross-Zone Load Balancing on the ALB and use Amazon Route 53 Application Recovery Controller to route traffic to the impacted zone</strong> is incorrect as this would simply direct traffic to the impacted instances, defeating the purpose of isolating the issue.</p><p>The option that says: <strong>Implement auto-scaling policies to automatically replace unhealthy instances within the affected zone</strong> is incorrect. Although it usually helps maintain the number of instances, it doesn\'t directly address the need to reroute traffic away from the unhealthy zone. It focuses more on instance replacement rather than traffic management.</p><p>The option that says: <strong>Enable Cross-Zone Load Balancing on the ALB and configure Amazon Route 53 Failover Routing to ensure traffic is evenly distributed across all zones</strong> is incorrect because enabling Cross-Zone Load Balancing would still distribute traffic to the unhealthy instances. Failover Routing alone isn\'t sufficient if Cross-Zone Load Balancing is enabled, as it won\'t entirely avoid the impacted zone.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p><p><a href="https://docs.aws.amazon.com/routing-control/latest/APIReference/Welcome.html">https://docs.aws.amazon.com/routing-control/latest/APIReference/Welcome.html</a></p><p><br></p><p><strong>Check out these AWS Elastic Load Balancing and Amazon Route53 Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p><p><a href="https://tutorialsdojo.com/amazon-route-53/?src=udemy">https://tutorialsdojo.com/amazon-route-53/</a></p>',
        answers: [
          "<p>Enable Cross-Zone Load Balancing on the ALB and use Amazon Route 53 Application Recovery Controller to route traffic to the impacted zone.</p>",
          "<p>Implement auto-scaling policies to automatically replace unhealthy instances within the affected zone.</p>",
          "<p>Disable Cross-Zone Load Balancing on the ALB and use Amazon Route 53 Application Recovery Controller to initiate a zonal shift to avoid the impacted zone entirely.</p>",
          "<p>Enable Cross-Zone Load Balancing on the ALB and configure Amazon Route 53 Failover Routing to ensure traffic is evenly distributed across all zones.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A DevOps engineer is managing an application deployed across multiple AWS Availability Zones using an Application Load Balancer (ALB). The application is hosted on EC2 instances spread across these zones to ensure high availability. Recently, the engineer noticed that some instances in one Availability Zone are experiencing issues.The engineer must isolate the impacted instances and use failover mechanisms to direct traffic to healthy zones.Which of the following solutions should the DevOps engineer implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588379,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A developer is developing a mobile quiz app hosted on AWS and is using AWS CodeDeploy to deploy the application on a cluster of EC2 instances. A hotfix needs to be applied after the scheduled deployment. These are files not included in the current application revision, so the developer uploaded the files manually to the target instances. On the next application release, the hotfix files were included in the new revision, but the deployment fails, and CodeDeploy auto-rolls back to the previous version. However, it was noticed that the hotfix files that were manually added were missing, and the application was not working properly.</p><p>Which of the following is the possible cause, and how will it be prevented in the future?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>As part of the deployment process, the CodeDeploy agent removes from each instance all the files installed by the most recent deployment. If files that weren’t part of a previous deployment appear in target deployment locations, you can choose what CodeDeploy does with them during the next deployment:</p><p><strong>Fail the deployment</strong> — An error is reported and the deployment status is changed to Failed.</p><p><strong>Overwrite the content</strong> — The version of the file from the application revision replaces the version already on the instance.</p><p><strong>Retain the content</strong> — The file in the target location is kept and the version in the application revision is not copied to the instance.</p><p><img src="https://media.tutorialsdojo.com/public/Deployment_2AUG2023.png"></p><p>Hence, the correct answer is: <strong>By default, CodeDeploy removes all files on the deployment location and the auto rollback will deploy the old revision files cleanly. Choose “Retain the content” option for future deployments so that only the files included in the old app revision will be deployed and the existing contents will be retained.</strong></p><p>The option that says: <strong>By default, CodeDeploy removes all files on the deployment location and the auto rollback will deploy the old revision files cleanly. Choose “Overwrite the content” option for future deployments so that only the files included in the old app revision will be overwritten and the existing contents will be retained</strong> is incorrect. Since you want to retain the already existing hotfix files, you should use the “Retain the content” option instead.</p><p>The option that says: <strong>By default, CodeDeploy retains all files on the deployment location but the auto rollback will deploy the old revision files cleanly. Choose “Overwrite the content” option for future deployments so that only the files included in the old app revision will be overwritten and the existing contents will be retained</strong> is incorrect. The “Overwrite content” option will simply remove the files and not retain them. As part of the deployment process, the CodeDeploy agent removes from each instance all the files installed by the most recent deployment and does not retain them. Moreover, the “Retain the content” option is a more suitable option to choose in this scenario for future deployments.</p><p>The option that says: <strong>By default, CodeDeploy retains all files on the deployment location but the auto rollback will deploy the old revision files cleanly. Choose "Fail the deployment" option for future deployments so that only the files included in the old app revision will be deployed and the existing contents will be retained</strong> is incorrect because, as part of the deployment process, the CodeDeploy agent will just remove the file from each instance installed by the most recent deployment and not retain them. You should also choose the “Retain the content” option for future deployments.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#deployments-rollback-and-redeploy-content-options">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#deployments-rollback-and-redeploy-content-options</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codedeploy/?src=udemy">https://tutorialsdojo.com/aws-codedeploy/</a></p>',
        answers: [
          "<p>By default, CodeDeploy removes all files on the deployment location and the auto rollback will deploy the old revision files cleanly. Choose “Overwrite the content” option for future deployments so that only the files included in the old app revision will be overwritten and the existing contents will be retained.</p>",
          "<p>By default, CodeDeploy retains all files on the deployment location but the auto rollback will deploy the old revision files cleanly. Choose “Overwrite the content” option for future deployments so that only the files included in the old app revision will be overwritten and the existing contents will be retained.</p>",
          "<p>By default, CodeDeploy removes all files on the deployment location and the auto rollback will deploy the old revision files cleanly. Choose “Retain the content” option for future deployments so that only the files included in the old app revision will be deployed and the existing contents will be retained.</p>",
          "<p>By default, CodeDeploy retains all files on the deployment location but the auto rollback will deploy the old revision files cleanly. Choose “Fail the deployment” option for future deployments so that only the files included in the old app revision will be deployed and the existing contents will be retained.</p>",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "A developer is developing a mobile quiz app hosted on AWS and is using AWS CodeDeploy to deploy the application on a cluster of EC2 instances. A hotfix needs to be applied after the scheduled deployment. These are files not included in the current application revision, so the developer uploaded the files manually to the target instances. On the next application release, the hotfix files were included in the new revision, but the deployment fails, and CodeDeploy auto-rolls back to the previous version. However, it was noticed that the hotfix files that were manually added were missing, and the application was not working properly.Which of the following is the possible cause, and how will it be prevented in the future?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588381,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has a CI/CD pipeline built in AWS CodePipeline for deployment updates. Part of the deployment process is to perform database schema updates and is performed via AWS CodeBuild. A recent security audit has discovered that the AWS CodeBuild is downloading the database scripts via Amazon S3 in an unauthenticated manner. The security team requires a solution that will enhance the security of the company's CI/CD pipeline.</p><p>What action should the DevOps Engineer take to address the issue in the MOST secure way?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Service role</strong>: A service role is an AWS Identity and Access Management (IAM) that grants permissions to an AWS service so that the service can access AWS resources. You need an AWS CodeBuild service role so that CodeBuild can interact with dependent AWS services on your behalf. You can create a CodeBuild service role by using the CodeBuild or AWS CodePipeline consoles.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-codebuild-service-role.png"></p><p>In this scenario, the S3 bucket will be safeguarded from unauthorized access by utilizing a bucket policy. Moreover, CodeBuild leverages the service role for executing S3 actions on your behalf.</p><p>Hence, the correct answer is: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</strong></p><p>The option that says: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI</strong> is incorrect. While the use of IAM access key and secret access key can provide S3 access to CodeBuild, it is not the most secure approach to address the issue.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because an IAM policy alone cannot secure an S3 bucket from unauthorized access. A bucket policy must be used instead. Furthermore, this option uses IAM access key and secret access key, which is not the most secure way.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. It is not used for removing unauthenticated access to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html</a></p><p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role</a></p><p><a href="https://aws.amazon.com/codebuild/">https://aws.amazon.com/codebuild/</a></p>',
        answers: [
          "<p>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
          "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>",
          "<p>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
          "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>",
        ],
      },
      correct_response: ["d"],
      section: "SDLC Automation",
      question_plain:
        "A company has a CI/CD pipeline built in AWS CodePipeline for deployment updates. Part of the deployment process is to perform database schema updates and is performed via AWS CodeBuild. A recent security audit has discovered that the AWS CodeBuild is downloading the database scripts via Amazon S3 in an unauthenticated manner. The security team requires a solution that will enhance the security of the company's CI/CD pipeline.What action should the DevOps Engineer take to address the issue in the MOST secure way?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588383,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A global cloud-based payment processing system is hosted in AWS which accepts credit card payments as well as cryptocurrencies such as Bitcoin. It is deployed in AWS which uses EC2, DynamoDB, S3, and CloudFront to process the payments. Since they are accepting credit card information from the users, they are required to be compliant with the Payment Card Industry Data Security Standard (PCI DSS). It was found that the credit card numbers are not properly encrypted on the recent 3rd-party audit and hence, their system failed the PCI DSS compliance test. You were hired by the company to solve this issue so they can release the product in the market as soon as possible. In addition, you also have to improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content.</p><p>Which of the following is the BEST option to protect and encrypt the sensitive credit card information of the users and improve the cache hit ratio of your CloudFront distribution?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can already configure CloudFront to help enforce secure end-to-end connections to origin servers by using HTTPS. Field-level encryption adds an additional layer of security along with HTTPS that lets you protect specific data throughout system processing so that only certain applications can see it. Field-level encryption allows you to securely upload user-submitted sensitive information to your web servers. The sensitive information provided by your clients is encrypted at the edge closer to the user and remains encrypted throughout your entire application stack, ensuring that only applications that need the data—and have the credentials to decrypt it—are able to do so.</p><p><img src="https://media.tutorialsdojo.com/public/fleoverview_2AUG2023.png"></p><p>To use field-level encryption, you configure your CloudFront distribution to specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request.</p><p>You can improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content; that is, by improving the cache hit ratio for your distribution. To increase your cache hit ratio, you can configure your origin to add a <code>Cache-Control max-age</code> directive to your objects and specify the longest practical value for <code>max-age</code>. The shorter the cache duration, the more frequently CloudFront forwards another request to your origin to determine whether the object has changed and, if so, to get the latest version.</p><p>Hence, the correct answer is: <strong>Secure end-to-end connections to the origin servers in your CloudFront distribution by using HTTPS and field-level encryption. Set up your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects. Apply the longest practical value for </strong><code><strong>max-age</strong></code><strong> to increase your cache hit ratio.</strong></p><p>The option that says: <strong>Use a custom SSL in the CloudFront distribution. Configure your origin to add </strong><code><strong>User-Agent</strong></code><strong> and </strong><code><strong>Host</strong></code><strong> headers to your objects to increase your cache hit ratio<em> </em></strong>is incorrect. Although it provides secure end-to-end connections to origin servers, it is better to add field-level encryption to protect the credit card information.</p><p>The option that says: <strong>Modify the CloudFront distribution to use Signed URLs. Configure your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects, and specify the longest practical value for </strong><code><strong>max-age</strong></code><strong> to increase your cache hit ratio<em> </em></strong>is incorrect because a Signed URL provides a way to distribute private content but it doesn\'t encrypt the sensitive credit card information.</p><p>The option that says: <strong>Set up an origin access identity (OAI) and add it to the CloudFront distribution. Configure your origin to add </strong><code><strong>User-Agent</strong></code><strong> and </strong><code><strong>Host</strong></code><strong> headers to your objects to increase your cache hit ratio </strong>is incorrect because OAI is mainly used to restrict access to objects in S3 bucket, but does not provide encryption to specific fields.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-setting-up">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-setting-up</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudfront/?src=udemy">https://tutorialsdojo.com/amazon-cloudfront/</a></p>',
        answers: [
          "<p>Use a custom SSL in the CloudFront distribution. Configure your origin to add <code>User-Agent</code> and <code>Host</code> headers to your objects to increase your cache hit ratio.</p>",
          "<p>Modify the CloudFront distribution to use Signed URLs. Configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code> to increase your cache hit ratio.</p>",
          "<p>Set up an origin access identity (OAI) and add it to the CloudFront distribution. Configure your origin to add <code>User-Agent</code> and <code>Host</code> headers to your objects to increase your cache hit ratio.</p>",
          "<p>Secure end-to-end connections to the origin servers in your CloudFront distribution by using HTTPS and field-level encryption. Set up your origin to add a <code>Cache-Control max-age</code> directive to your objects. Apply the longest practical value for <code>max-age</code> to increase your cache hit ratio.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A global cloud-based payment processing system is hosted in AWS which accepts credit card payments as well as cryptocurrencies such as Bitcoin. It is deployed in AWS which uses EC2, DynamoDB, S3, and CloudFront to process the payments. Since they are accepting credit card information from the users, they are required to be compliant with the Payment Card Industry Data Security Standard (PCI DSS). It was found that the credit card numbers are not properly encrypted on the recent 3rd-party audit and hence, their system failed the PCI DSS compliance test. You were hired by the company to solve this issue so they can release the product in the market as soon as possible. In addition, you also have to improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content.Which of the following is the BEST option to protect and encrypt the sensitive credit card information of the users and improve the cache hit ratio of your CloudFront distribution?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588385,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is planning to host their enterprise application in an ECS Cluster which uses the Fargate launch type. The database credentials should be provided to the AMI by using environment variables for security purposes. A DevOps engineer was instructed to ensure that the credentials are secure when passed to the image and that the sensitive passwords cannot be viewed on the cluster itself. In addition, the credentials must be kept in a dedicated storage with lifecycle management and key rotation. </p><p>Which of the following is the MOST suitable solution that the engineer should implement with the LEAST amount of effort?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon ECS</strong> enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</p><p><img src="https://media.tutorialsdojo.com/public/diagram3-1_2AUG2023.png"></p><p>Within your container definition, specify <code>secrets</code> with the name of the environment variable to set in the container and the full ARN of either the Secrets Manager secret or Systems Manager Parameter Store parameter containing the sensitive data to present to the container. The parameter that you reference can be from a different Region than the container using it, but must be from within the same account.</p><p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises.</p><p>If you want a single store for configuration and secrets, you can use Parameter Store. If you want a dedicated secrets store with lifecycle management, use Secrets Manager.</p><p>Hence, the correct answer is the option that says: <strong>Store the database credentials using the AWS Secrets Manager. Encrypt the credentials using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret, which contains the sensitive data, to present to the container<em>.</em></strong></p><p>The option that says: <strong>Upload and manage the database credentials using AWS Systems Manager Parameter Store then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container </strong>is incorrect. Although the use of Systems Manager Parameter Store in securing sensitive data in ECS is valid, this service doesn\'t provide dedicated storage with lifecycle management and key rotation, unlike Secrets Manager.</p><p>The option that says: <strong>Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt them with KMS. Store the task definition JSON file in a private Amazon S3 bucket. Ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Set up an IAM role to the ECS task definition script that allows access to the specific S3 bucket and then pass the </strong><code><strong>--cli-input-json</strong></code><strong> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials</strong><em> </em>is incorrect. Although the solution may work, it is not recommended to store sensitive credentials in S3. This entails a lot of overhead and manual configuration steps which can be simplified by simply using the Secrets Manager or Systems Manager Parameter Store.</p><p>The option that says: <strong>Store the database credentials using Docker Secrets in the ECS task definition file of the ECS Cluster where you can centrally manage sensitive data and securely transmit it to only those containers that need access to it. Ensure that the secrets are encrypted during transit and at rest<em> </em></strong>is incorrect. Although you can use Docker Secrets to secure the sensitive database credentials, this feature is only applicable in Docker Swarm. In AWS, the recommended way to secure sensitive data is either through the use of Secrets Manager or Systems Manager Parameter Store.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html</a></p><p><a href="https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/">https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><br></p><p><strong>Check out this AWS Secrets Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-secrets-manager/?src=udemy">https://tutorialsdojo.com/aws-secrets-manager/</a></p>',
        answers: [
          "<p>Upload and manage the database credentials using AWS Systems Manager Parameter Store then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container.</p>",
          "<p>Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt them with KMS. Store the task definition JSON file in a private Amazon S3 bucket. Ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Set up an IAM role to the ECS task definiton script that allows access to the specific S3 bucket and then pass the <code>--cli-input-json</code> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials.</p>",
          "<p>Store the database credentials using the AWS Secrets Manager. Encrypt the credentials using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret, which contains the sensitive data, to present to the container.</p>",
          "<p>Store the database credentials using Docker Secrets in the ECS task definition file of the ECS Cluster where you can centrally manage sensitive data and securely transmit it to only those containers that need access to it. Ensure that the secrets are encrypted during transit and at rest.</p>",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "A company is planning to host their enterprise application in an ECS Cluster which uses the Fargate launch type. The database credentials should be provided to the AMI by using environment variables for security purposes. A DevOps engineer was instructed to ensure that the credentials are secure when passed to the image and that the sensitive passwords cannot be viewed on the cluster itself. In addition, the credentials must be kept in a dedicated storage with lifecycle management and key rotation. Which of the following is the MOST suitable solution that the engineer should implement with the LEAST amount of effort?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588387,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has its on-premises data network connected to their AWS VPC via a Direct Connect connection. Their DevOps team is maintaining their Media Asset Management (MAM) system which uses a repository of over 50-TB digital videos and media files that are stored on their on-premises tape library. Due to the sheer size of their data, they want to implement an automated catalog system that will enable them to search their files using facial recognition. A catalog will store the faces of the people who are present in these videos including a still image of each person. Eventually, the media company would like to migrate these media files to AWS including the MAM video contents. </p><p>Which of the following provides a solution which uses the LEAST amount of ongoing management overhead and will cause MINIMAL disruption to the existing system?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon Rekognition</strong> can store information about detected faces in server-side containers known as collections. You can use the facial information that\'s stored in a collection to search for known faces in images, stored videos, and streaming videos. Amazon Rekognition supports the <a href="https://docs.aws.amazon.com/rekognition/latest/dg/API_IndexFaces.html">IndexFaces</a> operation. You can use this operation to detect faces in an image and persist information about facial features that are detected in a collection. This is an example of a <em>storage-based</em> API operation because the service persists information on the server.</p><p><img src="https://media.tutorialsdojo.com/public/detect-analyze-faces-rekognition_3AUG2023.png"></p><p><strong>AWS Storage Gateway</strong> offers file-based, volume-based, and tape-based storage solutions. With a tape gateway, you can cost-effectively and durably archive backup data in GLACIER or DEEP_ARCHIVE. A tape gateway provides a virtual tape infrastructure that scales seamlessly with your business needs and eliminates the operational burden of provisioning, scaling, and maintaining a physical tape infrastructure.</p><p>You can run AWS Storage Gateway either on-premises as a VM appliance, as a hardware appliance or in AWS as an Amazon Elastic Compute Cloud (Amazon EC2) instance. You deploy your gateway on an EC2 instance to provision iSCSI storage volumes in AWS. You can use gateways hosted on EC2 instances for disaster recovery, data mirroring, and providing storage for applications hosted on Amazon EC2.</p><p>Hence, the correct answer is: <strong>Connect the on-premises file system to AWS Storage Gateway by setting up a file gateway appliance on-premises. Use the MAM solution to extract the media files from the current data store and send them into the file gateway. Populate a collection using Amazon Rekognition by building a catalog of faces from the processed media files. Launch a Lambda function to invoke Amazon Rekognition Javascript SDK to have it fetch the media files from the S3 bucket which is backing the file gateway. Retrieve the needed metadata using the Lambda function and store the information into the MAM solution.</strong></p><p>The option that says: <strong>Move all of the media files from the on-premises library into an EBS volume mounted on a large Amazon EC2 instance. Set up an open-source facial recognition tool in the instance. Process the media files to retrieve the metadata and store this information into the MAM solution. Copy the media files to an Amazon S3 bucket</strong> is incorrect because it entails a lot of ongoing management overhead instead of just using Amazon Rekognition. Moreover, it is more suitable to use the AWS Storage Gateway service rather than an EBS Volume.</p><p>The option that says: <strong>Launch a tape gateway appliance in your on-premises data center and connect it to your AWS Storage Gateway service. Set up the MAM solution to fetch the media files from the current archive and store them into the tape gateway in the AWS Cloud. Build a collection from the catalog of faces using Amazon Rekognition. Set up an AWS Lambda function which invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video directly from the tape gateway in real-time. Retrieve the required metadata and store them into the MAM solution</strong> is incorrect. Although this is using the right combination of AWS Storage Gateway and Amazon Rekognition, take note that you can\'t directly fetch the media files from your tape gateway in real-time since this is backed up using Glacier. Although the on-premises data center is using a tape gateway, you can still set up a solution to use a file gateway in order to properly process the videos using Amazon Rekognition. Keep in mind that the tape gateway in AWS Storage Gateway service is primarily used as an archive solution.</p><p>The option that says: <strong>Using Amazon Kinesis Video Streams, create a video ingestion stream and build a collection of faces with Amazon Rekognition. Stream the media files from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed files. Set up a stream consumer to retrieve the required metadata, and store them into the MAM solution. Configure the stream to store the files in an Amazon S3 bucket </strong>is incorrect<strong> </strong>because you won\'t be able to connect your tape gateway directly to your Kinesis Video Streams service. You need to use the AWS Storage Gateway first.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/rekognition/latest/dg/collections.html">https://docs.aws.amazon.com/rekognition/latest/dg/collections.html</a></p><p><a href="https://aws.amazon.com/storagegateway/file/">https://aws.amazon.com/storagegateway/file/</a></p><p><br></p><p><strong>Check out this Amazon Rekognition Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-rekognition/?src=udemy"><strong>https://tutorialsdojo.com/amazon-rekognition/</strong></a></p>',
        answers: [
          "<p>Connect the on-premises file system to AWS Storage Gateway by setting up a file gateway appliance on-premises. Use the MAM solution to extract the media files from the current data store and send them into the file gateway. Populate a collection using Amazon Rekognition by building a catalog of faces from the processed media files. Launch a Lambda function to invoke Amazon Rekognition Javascript SDK to have it fetch the media files from the S3 bucket which is backing the file gateway. Retrieve the needed metadata using the Lambda function and store the information into the MAM solution.</p>",
          "<p>Using Amazon Kinesis Video Streams, create a video ingestion stream and build a collection of faces with Amazon Rekognition. Stream the media files from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed files. Set up a stream consumer to retrieve the required metadata, and store them into the MAM solution. Configure the stream to store the files in an Amazon S3 bucket.</p>",
          "<p>Launch a tape gateway appliance in your on-premises data center and connect it to your AWS Storage Gateway service. Set up the MAM solution to fetch the media files from the current archive and store them into the tape gateway in the AWS Cloud. Build a collection from the catalog of faces using Amazon Rekognition. Set up an AWS Lambda function which invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video directly from the tape gateway in real-time. Retrieve the required metadata and store them into the MAM solution.</p>",
          "<p>Move all of the media files from the on-premises library into an EBS volume mounted on a large Amazon EC2 instance. Set up an open-source facial recognition tool in the instance. Process the media files to retrieve the metadata and store this information into the MAM solution. Copy the media files to an Amazon S3 bucket.</p>",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "A company has its on-premises data network connected to their AWS VPC via a Direct Connect connection. Their DevOps team is maintaining their Media Asset Management (MAM) system which uses a repository of over 50-TB digital videos and media files that are stored on their on-premises tape library. Due to the sheer size of their data, they want to implement an automated catalog system that will enable them to search their files using facial recognition. A catalog will store the faces of the people who are present in these videos including a still image of each person. Eventually, the media company would like to migrate these media files to AWS including the MAM video contents. Which of the following provides a solution which uses the LEAST amount of ongoing management overhead and will cause MINIMAL disruption to the existing system?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588389,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A national university has launched its serverless online learning portal using AWS Lambda and API Gateway in AWS that enables its students to enroll, see grades online as well as manage the class schedule. The portal abruptly stopped working after a few weeks and lost all of its data. The university hired a DevOps consultant and based on the investigation, the outage was due to an SQL injection vulnerability on the portal's login page in which the attacker simply injected the malicious SQL code. The consultant also emphasized the system's inability to track historical changes to the rules and metrics associated with its firewall.</p><p>Which of the following is the MOST suitable and cost-effective solution to avoid another SQL Injection attack against the infrastructure in AWS?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. With AWS Config, you can track changes to WAF web access control lists (web ACLs). For example, you can record the creation and deletion of rules and rule actions, as well as updates to WAF rule configurations.</p><p><img src="https://media.tutorialsdojo.com/public/waf-archs2_3AUG2023.png"></p><p>AWS WAF gives you control over which traffic to allow or block to your web applications by defining customizable web security rules. You can use AWS WAF to create custom rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that are designed for your specific application. New rules can be deployed within minutes, letting you respond quickly to changing traffic patterns. Also, AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of web security rules.</p><p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting.</p><p><img src="https://media.tutorialsdojo.com/public/TD-AWS-WAF-02-21-2025.png"></p><p>In this scenario, integrating WAF in front of the API Gateway and using AWS Config are the ones that should be implemented in order to improve the security of the online learning portal.</p><p>Hence, the correct answer is: <strong>Add a web access control list in front of the API Gateway using AWS WAF to block requests that contain malicious SQL code. Implement a scope-down statement in the WAF rule to limit inspection to specific request components and improve performance. Track the changes to web access control lists (web ACLs), such as the creation and deletion of rules, including updates to WAF rule configurations, using AWS Config.</strong></p><p>The option that says: <strong>Add a web access control list in front of the Lambda functions using AWS WAF to block requests that contain malicious SQL code. Track the changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations using AWS Firewall Manager</strong> is incorrect because you have to use AWS WAF in front of the API Gateway and not directly to the Lambda functions. AWS Firewall Manager is primarily used to manage your Firewall across multiple AWS accounts under your AWS Organizations and hence, it is not suitable for tracking changes to WAF web access control lists. You should use AWS Config instead.</p><p>The option that says: <strong>Configure the Network Access Control List of your Amazon VPC to block the IP address of the attacker and then create an Amazon CloudFront web distribution. Configure AWS WAF to add a web access control list (web ACL) in front of the CloudFront distribution to block requests that contain malicious SQL code. Track the changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations using AWS Config</strong> is incorrect because even though it is valid to use AWS WAF with CloudFront, it entails an additional and unnecessary cost to launch a CloudFront distribution for this scenario. There is no requirement that the serverless online portal should be scalable and be accessible around the globe hence, a CloudFront distribution is not relevant.</p><p>The option that says: <strong>Launch a new Application Load Balancer and set up AWS WAF to block requests that contain malicious SQL code. Place the API Gateway behind the ALB and use the AWS Firewall Manager to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations</strong> is incorrect because launching a new Application Load Balancer entails additional cost and is not cost-effective. In addition, AWS Firewall Manager is primarily used to manage your Firewall across multiple AWS accounts under your AWS Organizations. Using AWS Config is much more suitable for tracking changes to WAF web access control lists.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/waf/">https://aws.amazon.com/waf/</a></p><p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p><p><a href="https://aws.amazon.com/config/">https://aws.amazon.com/config/</a></p><p><br></p><p><strong>Check out this AWS WAF Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-waf/?src=udemy">https://tutorialsdojo.com/aws-waf/</a></p>',
        answers: [
          "<p>Add a web access control list in front of the API Gateway using AWS WAF to block requests that contain malicious SQL code. Implement a scope-down statement in the WAF rule to limit inspection to specific request components and improve performance. Track the changes to web access control lists (web ACLs), such as the creation and deletion of rules, including updates to WAF rule configurations, using AWS Config.</p>",
          "<p>Add a web access control list in front of the Lambda functions using AWS WAF to block requests that contain malicious SQL code. Track the changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations using AWS Firewall Manager.</p>",
          "<p>Configure the Network Access Control List of your Amazon VPC to block the IP address of the attacker and then create an Amazon CloudFront web distribution. Configure AWS WAF to add a web access control list (web ACL) in front of the CloudFront distribution to block requests that contain malicious SQL code. Track the changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations using AWS Config.</p>",
          "<p>Launch a new Application Load Balancer and set up AWS WAF to block requests that contain malicious SQL code. Place the API Gateway behind the ALB and use the AWS Firewall Manager to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A national university has launched its serverless online learning portal using AWS Lambda and API Gateway in AWS that enables its students to enroll, see grades online as well as manage the class schedule. The portal abruptly stopped working after a few weeks and lost all of its data. The university hired a DevOps consultant and based on the investigation, the outage was due to an SQL injection vulnerability on the portal's login page in which the attacker simply injected the malicious SQL code. The consultant also emphasized the system's inability to track historical changes to the rules and metrics associated with its firewall.Which of the following is the MOST suitable and cost-effective solution to avoid another SQL Injection attack against the infrastructure in AWS?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588391,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A DevOps Engineer has been assigned to develop an automated workflow to ensure that the required patches of all of their Windows EC2 instances are properly applied. It is of utmost importance that the EC2 instance reboots do not occur at the same time on all of their Windows instances in order to maintain their system uptime requirements. Any unavailability issues of their systems would likely cause a loss of revenue in the company since the customer transactions will not be processed in a timely manner. </p><p>How can the engineer meet the above requirements?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications.</p><p>You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src="https://media.tutorialsdojo.com/public/image1v2-1_3AUG2023.png"></p><p>Patch Manager uses <em>patch baselines</em>, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p>You can use a <em>patch group</em> to associate instances with a specific patch baseline. Patch groups help ensure that you are deploying the appropriate patches, based on the associated patch baseline rules, to the correct set of instances. Patch groups can also help you avoid deploying patches before they have been adequately tested. For example, you can create patch groups for different environments (such as Development, Test, and Production) and register each patch group to an appropriate patch baseline.</p><p><img src="https://media.tutorialsdojo.com/public/patch-groups-how-it-works_3AUG2023.png"></p><p>When you run <code>AWS-RunPatchBaseline</code>, you can target managed instances using their instance ID or tags. SSM Agent and Patch Manager will then evaluate which patch baseline to use based on the patch group value that you added to the instance.</p><p>You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group <em>must</em> be defined with the tag key: <strong>Patch Group</strong>. Note that the key is case-sensitive. You can specify any value, for example, "web servers," but the key must be <strong>Patch Group</strong>.</p><p>The <code>AWS-DefaultPatchBaseline</code> baseline is primarily used to approve all Windows Server operating system patches that are classified as "CriticalUpdates" or "SecurityUpdates" and that have an MSRC severity of "Critical" or "Important". Patches are auto-approved seven days after release.</p><p>Hence, the option that says: <strong>Set up two Patch Groups with unique tags that you will assign to all of your Amazon EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on both patch groups. Set up two non-overlapping maintenance windows and associate each with a different patch group. Register targets with specific maintenance windows using Patch Group tags. Assign the </strong><code><strong>AWS-RunPatchBaseline</strong></code><strong> document as a task within each maintenance window which has a different processing start time</strong> is the correct answer as it properly uses two Patch Groups, non-overlapping maintenance windows, and the <code>AWS-DefaultPatchBaseline</code> baseline to ensure that the EC2 instance reboots do not occur at the same time.</p><p>The option that says: <strong>Set up a Patch Group with unique tags that you will assign to all of your Amazon EC2 Windows Instances and then associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on your patch group. Create a new maintenance window and associate it with your patch group. Assign the </strong><code><strong>AWS-RunPatchBaseline</strong></code><strong> document as a task within your maintenance window</strong> is incorrect. Although it is correct to use a Patch Group, you must create another Patch Group to avoid any unavailability issues. Having two non-overlapping maintenance windows will ensure that there will be another set of running Windows EC2 instances while the other set is being patched.</p><p>The option that says:<strong> Set up two Patch Groups with unique tags that you will assign to all of your Amazon EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on both patch groups. Integrate AWS Systems Manager Run Command and CloudWatch Events to set up a cron expression that will automate the patch execution for the two Patch Groups. Use an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution.</strong> is incorrect because the AWS Systems Manager Run Command is primarily used to remotely manage the configuration of your managed instances while AWS Systems Manager State Manager is just a configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. These two services, including CloudWatch Events, are not suitable for this scenario. The better solution would be to use AWS Systems Manager Maintenance Windows which lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches.</p><p>The option that says: <strong>Set up a Patch Group with unique tags that you will assign to all of your Amazon EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on both patch groups. Set up a CloudWatch Events rule configured to use a cron expression to automate the execution of patching in a given schedule using the AWS Systems Manager Run command. Use an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution</strong> is incorrect because, just as what is mentioned in the previous option, you have to use Maintenance Windows for scheduling the patches and you also need to set up two Patch Groups in this scenario instead of one.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><a href="https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/">https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-ssm-documents.html</a></p><p><a href="https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-scheduletasks.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>Set up two Patch Groups with unique tags that you will assign to all of your Amazon EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code> baseline on both patch groups. Set up two non-overlapping maintenance windows and associate each with a different patch group. Register targets with specific maintenance windows using Patch Group tags. Assign the <code>AWS-RunPatchBaseline</code> document as a task within each maintenance window which has a different processing start time.</p>",
          "<p>Set up a Patch Group with unique tags that you will assign to all of your Amazon EC2 Windows Instances and then associate the predefined <code>AWS-DefaultPatchBaseline</code> baseline on your patch group. Create a new maintenance window and associate it with your patch group. Assign the <code>AWS-RunPatchBaseline</code> document as a task within your maintenance window.</p>",
          "<p>Set up two Patch Groups with unique tags that you will assign to all of your Amazon EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code> baseline on both patch groups. Integrate AWS Systems Manager Run Command and CloudWatch Events to set up a cron expression that will automate the patch execution for the two Patch Groups. Use an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution.</p>",
          "<p>Set up a Patch Group with unique tags that you will assign to all of your Amazon EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code> baseline on both patch groups. Set up a CloudWatch Events rule configured to use a cron expression to automate the execution of patching in a given schedule using the AWS Systems Manager Run command. Use an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A DevOps Engineer has been assigned to develop an automated workflow to ensure that the required patches of all of their Windows EC2 instances are properly applied. It is of utmost importance that the EC2 instance reboots do not occur at the same time on all of their Windows instances in order to maintain their system uptime requirements. Any unavailability issues of their systems would likely cause a loss of revenue in the company since the customer transactions will not be processed in a timely manner. How can the engineer meet the above requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588393,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. </p><p>As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src="https://media.tutorialsdojo.com/ssm-patch-baselines.jpg"></p><p><strong>Patch Manager</strong> uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p><strong>AWS Config</strong> provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements. Hence, the correct option is: <strong>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</strong></p><p>The option that says: <strong>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, it is stated in the scenario that the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC </strong>is incorrect because Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise, however, it does not check if your EC2 instances are using an approved AMI or not.</p><p>The option that says: <strong>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect because the AWS Shield Advanced service is more suitable in preventing DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href="https://tutorialsdojo.com/aws-config/?src=udemy">https://tutorialsdojo.com/aws-config/</a></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
          "<p>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
          "<p>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
          "<p>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Security and Compliance",
      question_plain:
        "A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588395,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company leverages API Gateway REST API to streamline SDK generation and simplify distribution to its partners. Employing an AWS CodePipeline pipeline, updates are made to the REST API multiple times daily. In the event of a deployment failure, the REST API reverts to the previous version, ensuring continuous service. However, engineers still have to manually upload corresponding SDKs to an Amazon S3 bucket. The company wants to eliminate this manual process as it is quite error-prone.</p><p>Which option will fit the requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS CodePipeline</strong> is a continuous delivery service that models, visualizes, and automates the steps required to release software. It allows you to quickly model and configure the different stages of a software release process and automate the steps required to continuously release your software changes.</p><p><img src="https://media.tutorialsdojo.com/public/AWSCodePipeline_with_APIGateway_21Mar2024.png"></p><p><strong>Amazon API Gateway</strong> is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.</p><p>More specifically, the API Gateway REST API offering includes several features that make the distribution and operation of APIs more scalable. One of these is the generation of client SDKs via its <code>GetSdk</code> API. Another useful feature is that API operations, such as <code>UpdateStage</code> create observable lifecycle events, allowing for effects, like retrieving SDKs and uploading them to S3 in this scenario, to be programmatically triggered and decoupled.</p><p>Therefore, the correct answer is: <strong>Create a Lambda function to generate and download the SDKs via API Gateway </strong><code><strong>GetSdk</strong></code><strong> and upload them to Amazon S3. Set up an EventBridge rule to filter for API Gateway </strong><code><strong>UpdateStage</strong></code><strong> operation and to invoke the function on such event.</strong></p><p>The option that says: <strong>Create a Lambda function to download the generated SDKs via API Gateway </strong><code><strong>GetSdk</strong></code><strong> and upload them to Amazon S3. Set up a separate CodePipeline action to invoke the function. Configure the new action to be triggered by a deploy action failure</strong> is incorrect because CodePipeline does not provide a feature for conditional actions. A pipeline action can be retried if it fails at some stage but, in general, a pipeline only "moves forward" and either succeeds or fails.</p><p>The option that says: <strong>Set up a separate action in the pipeline. Create a Lambda function to generate language-specific SDKs and upload them to Amazon S3. Configure the action to invoke this function.</strong> is incorrect. Aside from unnecessarily implementing code generation, since the function is now part of the pipeline, it will also not be triggered unless the pipeline is manually triggered and parameterized with an older stable version of the API.</p><p>The option that says: <strong>Set up an EventBridge rule to filter for API Gateway </strong><code><strong>CreateDeployment</strong></code><strong> operation. Create a Lambda function to generate the SDKs and upload them to Amazon S3. Configure the EventBridge rule to invoke the function</strong> is incorrect. Reverting to a previous version of an API/stage does not create a new deployment, so the <code>CreateDeployment</code>-related event will not be emitted, and the EventBridge rule will never be triggered.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-retry.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-retry.html</a></p><p><a href="https://docs.aws.amazon.com/apigateway/latest/api/API_GetSdk.html">https://docs.aws.amazon.com/apigateway/latest/api/API_GetSdk.html</a></p><p><a href="https://docs.aws.amazon.com/apigateway/latest/api/API_UpdateStage.html">https://docs.aws.amazon.com/apigateway/latest/api/API_UpdateStage.html</a></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p>',
        answers: [
          "<p>Create a Lambda function to download the generated SDKs via API Gateway <code>GetSdk</code> and upload them to Amazon S3. Set up a separate CodePipeline action to invoke the function. Configure the new action to be triggered by a deploy action failure.</p>",
          "<p>Set up a separate action in the pipeline. Create a Lambda function to generate language-specific SDKs and upload them to Amazon S3. Configure the action to invoke this function.</p>",
          "<p>Create a Lambda function to generate and download the SDKs via API Gateway <code>GetSdk</code> and upload them to Amazon S3. Set up an EventBridge rule to filter for API Gateway <code>UpdateStage</code> operation and to invoke the function on such event.</p>",
          "<p>Set up an EventBridge rule to filter for API Gateway <code>CreateDeployment</code> operation. Create a Lambda function to generate the SDKs and upload them to Amazon S3. Configure the EventBridge rule to invoke the function.</p>",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "A company leverages API Gateway REST API to streamline SDK generation and simplify distribution to its partners. Employing an AWS CodePipeline pipeline, updates are made to the REST API multiple times daily. In the event of a deployment failure, the REST API reverts to the previous version, ensuring continuous service. However, engineers still have to manually upload corresponding SDKs to an Amazon S3 bucket. The company wants to eliminate this manual process as it is quite error-prone.Which option will fit the requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588397,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has a PROD, DEV, and TEST environment in its software development department, each contains hundreds of EC2 instances and other AWS services. There was a series of security patches that have been released on the official Ubuntu operating system for a critical flaw that was recently discovered. Although this is an urgent matter, there is no guarantee that these patches will be bug-free and production-ready. This is why a DevOps engineer was instructed to immediately patch all of their affected EC2 instances in all the environments, except for the PROD environment. The EC2 instances in their PROD environment will only be patched after the initial patches have been verified to work effectively in their non-PROD environments. Each environment also has different baseline patch requirements that you will need to satisfy. </p><p>How should the DevOps engineer perform this task with the LEAST amount of effort?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type.</p><p><em>Patch Manager</em> uses patch baselines<em>,</em> which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. For each auto-approval rule that you create, you can specify an auto-approval delay. This delay is the number of days to wait after the patch was released before the patch is automatically approved for patching.</p><p><img src="https://media.tutorialsdojo.com/public/AWS-Systems-Manager-Patch-Manager_3AUG2023.png"></p><p>A patch group is an optional means of organizing instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested.</p><p><img src="https://media.tutorialsdojo.com/public/patch-groups-how-it-works_3AUG2023.png"></p><p>You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: <code><strong>Patch Group</strong></code>. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution.</p><p>Hence, the correct answer is: <strong>Tag each instance based on its environment, business unit, and operating system. Set up a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize each Amazon EC2 instance based on its tags using Patch Groups. Apply the required patches specified in the corresponding patch baseline to each Patch Group.</strong></p><p>The option that says: <strong>Develop various shell scripts for each environment that specifies which patch will serve as its baseline. Tag each instance based on its environment, business unit, and operating system. Add the Amazon EC2 instances into Target Groups using the AWS Systems Manager Run Command and then execute the script corresponding to each Target Group</strong> is incorrect as this option takes more effort to perform because you are using Systems Manager Run Command instead of Patch Manager. The Run Command service enables you to automate common administrative tasks and perform ad hoc configuration changes at scale, however, it takes a lot of effort to implement this solution. You can use Patch Manager instead to perform the task required by the scenario since you need to perform this task with the least amount of effort.</p><p>The option that says: <strong>Set up a new patch baseline in AWS Systems Manager Patch Manager for each environment. Tag each Amazon EC2 instance based on its operating system. Categorize EC2 instances based on their tags using Patch Groups. Apply the patches specified in their corresponding patch baseline to each Patch Group. Use Patch Compliance to ensure that the patches have been installed correctly. Using AWS Config, record all of the changes to patch and association compliance statuses </strong>is incorrect because you should be tagging instances based on the environment and its OS type in which they belong and not just its OS type. This is because the type of patches that will be applied varies between the different environments. With this option, the Ubuntu EC2 instances in <strong>all</strong> of your environments, including in production, will automatically be patched.</p><p>The option that says: <strong>Use the AWS Systems Manager Maintenance Windows to set up a scheduled maintenance period for each environment, where the period is after business hours so as not to affect daily operations. The Systems Manager will execute a cron job that will install the required patches for each Amazon EC2 instance in each environment during the maintenance period. Use the Systems Manager Managed Instances to verify that your environments are fully patched and compliant </strong>is incorrect because this is not the simplest way to address the issue using AWS Systems Manager. The AWS Systems Manager Maintenance Windows feature lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Each Maintenance Window has a schedule, a maximum duration, a set of registered targets (the instances that are acted upon), and a set of registered tasks. Although this solution may work, it entails a lot of configuration and effort to implement.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.htmll</a></p><p><a href="https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>Develop various shell scripts for each environment that specifies which patch will serve as its baseline. Tag each instance based on its environment, business unit, and operating system. Add the Amazon EC2 instances into Target Groups using the AWS Systems Manager Run Command and then execute the script corresponding to each Target Group.</p>",
          "<p>Set up a new patch baseline in AWS Systems Manager Patch Manager for each environment. Tag each Amazon EC2 instance based on its operating system. Categorize EC2 instances based on their tags using Patch Groups. Apply the patches specified in their corresponding patch baseline to each Patch Group. Use Patch Compliance to ensure that the patches have been installed correctly. Using AWS Config, record all of the changes to patch and association compliance statuses.</p>",
          "<p>Tag each instance based on its environment, business unit, and operating system. Set up a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize each Amazon EC2 instance based on its tags using Patch Groups. Apply the required patches specified in the corresponding patch baseline to each Patch Group.</p>",
          "<p>Use the AWS Systems Manager Maintenance Windows to set up a scheduled maintenance period for each environment, where the period is after business hours so as not to affect daily operations. The Systems Manager will execute a cron job that will install the required patches for each Amazon EC2 instance in each environment during the maintenance period. Use the Systems Manager Managed Instances to verify that your environments are fully patched and compliant.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Security and Compliance",
      question_plain:
        "A company has a PROD, DEV, and TEST environment in its software development department, each contains hundreds of EC2 instances and other AWS services. There was a series of security patches that have been released on the official Ubuntu operating system for a critical flaw that was recently discovered. Although this is an urgent matter, there is no guarantee that these patches will be bug-free and production-ready. This is why a DevOps engineer was instructed to immediately patch all of their affected EC2 instances in all the environments, except for the PROD environment. The EC2 instances in their PROD environment will only be patched after the initial patches have been verified to work effectively in their non-PROD environments. Each environment also has different baseline patch requirements that you will need to satisfy. How should the DevOps engineer perform this task with the LEAST amount of effort?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588399,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is planning to launch a mobile marketplace using AWS Amplify and AWS Mobile Hub which will serve millions of users worldwide. The backend APIs will be launched to multiple AWS regions to process the sales and financial transactions in the region closest to the users to lower the latency. A DevOps Engineer was instructed to design the system architecture to ensure that the transactions made in one region are automatically replicated to other regions. In the coming months ahead, it is expected that the marketplace will have millions of users across North America, South America, Europe, and Asia.</p><p>Which of the following is the MOST scalable, cost-effective, and highly available architecture that the Engineer should implement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Amazon DynamoDB is a fully managed, key-value and document database that delivers single-digit millisecond performance at any scale. It is a multi-Region, multi-active database that offers built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and support peaks of more than 20 million requests per second.</p><p><img alt="Amazon DynamoDB Global Table" height="770" src="https://media.tutorialsdojo.com/amazon-dynamodb-global-table-dop-c01.png" width="1000"></p><p>DynamoDB Global Tables build upon DynamoDB’s global footprint to provide a fully managed, multi-Region, and multi-active (active-active) database. They automatically replicate your DynamoDB table data across the AWS Regions you select, eliminating the need to build and maintain your own replication logic. With Global Tables, your applications get fast, local read and write performance while AWS takes care of data replication and conflict resolution in the background.</p><p>If a Region becomes unavailable, your application can continue reading and writing in other Regions without downtime, providing high availability and resiliency.</p><p>Hence, the correct answer is: <strong>Create an Amazon DynamoDB Global Table across the required AWS Regions. Store the individual transactions in the local region’s replica table. Any changes made in one of the replica tables are automatically replicated across all other replica tables worldwide.</strong></p><p>The option says: <strong>Store the individual transactions in each local region in an Amazon DynamoDB table. Use an AWS Lambda function to read recent writes from the primary DynamoDB table and replay the data to tables in all other regions</strong> is incorrect because this requires building and maintaining custom replication logic, which introduces additional operational complexity, higher latency, and greater potential for data inconsistency. DynamoDB Global Tables already provide automatic, fully managed, multi-Region replication without requiring custom Lambda-based replication pipelines, making this option less scalable and more costly.</p><p>The option says: <strong>Set up an Amazon DynamoDB Global table in your preferred AWS region and enable the DynamoDB Streams option. Set up replica tables in the other AWS regions where you want to replicate your data. In each local region, store the individual transactions in a DynamoDB replica table in the same region</strong> is incorrect because DynamoDB Global Tables already enable and use DynamoDB Streams automatically to handle change data capture and cross-Region replication. Customers do not need to manually enable Streams or configure replication themselves. This option incorrectly suggests that manual setup is required, while in reality AWS manages the Streams integration and replication process in the background.</p><p>The option says: <strong>Set up an Amazon Aurora Global Database with a primary writer in one AWS Region and read-only replicas in other Regions. Store the transactions in the local Aurora writer instance. Aurora will replicate data asynchronously across Regions via the storage layer, and on failover you can promote a replica to be the new writer</strong> is incorrect because Aurora Global Database is a single-writer design; only the primary Region accepts writes, and all secondary Regions are read-only until a switchover or failover. This means users in secondary Regions can’t perform low-latency local writes and must route write traffic cross-Region to the primary, introducing latency; replication to secondaries is asynchronous (typically under ~1 second) and promotion of a secondary is an operational event, not simultaneous multi-Region writes. Therefore, it doesn’t meet the requirement for active-active, multi-Region write capability with automatic replication in every Region.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html</a></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-dynamodb/?src=udemy">https://tutorialsdojo.com/amazon-dynamodb/</a></p>',
        answers: [
          "<p>Store the individual transactions in each local region in an Amazon DynamoDB table. Use an AWS Lambda function to read recent writes from the primary DynamoDB table and replay the data to tables in all other regions.</p>",
          "<p>Create an Amazon DynamoDB Global Table across the required AWS Regions. Store the individual transactions in the local region’s replica table. Any changes made in one of the replica tables are automatically replicated across all other replica tables worldwide.</p>",
          "<p>Set up an Amazon DynamoDB Global table in your preferred AWS region and enable the DynamoDB Streams option. Set up replica tables in the other AWS regions where you want to replicate your data. In each local region, store the individual transactions in a DynamoDB replica table in the same region.</p>",
          "<p>Set up an Amazon Aurora Global Database with a primary writer in one AWS Region and read-only replicas in other Regions. Store the transactions in the local Aurora writer instance. Aurora will replicate data asynchronously across Regions via the storage layer, and on failover you can promote a replica to be the new writer.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A company is planning to launch a mobile marketplace using AWS Amplify and AWS Mobile Hub which will serve millions of users worldwide. The backend APIs will be launched to multiple AWS regions to process the sales and financial transactions in the region closest to the users to lower the latency. A DevOps Engineer was instructed to design the system architecture to ensure that the transactions made in one region are automatically replicated to other regions. In the coming months ahead, it is expected that the marketplace will have millions of users across North America, South America, Europe, and Asia.Which of the following is the MOST scalable, cost-effective, and highly available architecture that the Engineer should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588401,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>An IT consulting firm with various teams and departments uses AWS Organizations to manage multiple AWS accounts grouped into several organizational units (OUs). The IT Security team reported a suspected breach in the environment where a third-party AWS account was suddenly added to the organization without prior approval. The external account has high-level access privileges to the accounts that the firm owns, but fortunately, no detrimental action was taken.</p><p>Which of the following is the MOST appropriate monitoring setup that notifies of any changes to the AWS accounts? (Select TWO.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p><strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an <em>organization</em> that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.</p><p>AWS Organizations can work with Amazon EventBridge to raise events when administrator-specified actions occur in an organization. For example, because of the sensitivity of such actions, most administrators would want to be warned every time someone creates a new account in the organization or when an administrator of a member account attempts to leave the organization. You can configure EventBridge rules that look for these actions and then send the generated events to administrator-defined targets. Targets can be an Amazon SNS topic that emails or text messages its subscribers. You could also create an AWS Lambda function that logs the details of the action for your later review.</p><p>Multi-account, multi-region data aggregation in AWS Config enables you to aggregate AWS Config data from multiple accounts and regions into a single account. Multi-account, multi-region data aggregation is useful for central IT administrators to monitor compliance for multiple AWS accounts in the enterprise. An aggregator is a new resource type in AWS Config that collects AWS Config data from multiple source accounts and regions.</p><p><img src="https://media.tutorialsdojo.com/public/td-tutorial-cwe-01-07-25.png"></p><p>Hence, the correct answers are:</p><p><strong>- Launch a new trail in Amazon CloudTrail to capture all API calls to your AWS Organizations, including calls from the AWS Organizations console. Also, track all code calls to the AWS Organizations APIs. Integrate Amazon EventBridge and Amazon SNS to raise events when administrator-specified actions occur in an organization and configure it to send a notification.</strong></p><p><strong>- Monitor the compliance of your AWS Organizations using AWS Config. Launch a new SNS Topic or Amazon Amazon EventBridge rule to send alerts for any changes.</strong></p><p>The option that says:<strong> Use the AWS Systems Manager and Amazon EventBridge to monitor all organizational changes and notify you of any new activities or configurations made to your account</strong> is incorrect because AWS Systems Manager is a collection of capabilities for configuring and managing your Amazon EC2 instances, on-premises servers and virtual machines, and other AWS resources at scale. This can\'t be used to monitor the changes to the set up of AWS Organizations.</p><p>The option that says:<strong> Create an Amazon CloudWatch Dashboard to monitor any changes to your organization. Launch a new Amazon SNS topic that will notify you and your team </strong>is incorrect because a CloudWatch Dashboard is primarily used to monitor your AWS resources and not the configuration of your AWS Organizations. While dashboards offer visualization, they are not tailored for detecting specific API calls or account additions.</p><p>The option that says: <strong>Launch an AWS-approved third-party monitoring tool from the AWS Marketplace that would send alerts if a breach was detected. Analyze any possible breach using AWS GuardDuty. Use Amazon SNS to notify the administrators</strong> is incorrect because this option only entails a lot of configuration, which is not fit for the scenario. GuardDuty might not determine similar future incidents as malicious if it was performed by an authenticated user already within the organization.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_monitoring.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_monitoring.html</a></p><p><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_tutorials_cwe.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_tutorials_cwe.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-organizations/?src=udemy">https://tutorialsdojo.com/aws-organizations/</a></p>',
        answers: [
          "<p>Use the AWS Systems Manager and Amazon EventBridge to monitor all organizational changes and notify you of any new activities or configurations made to your account.</p>",
          "<p>Create an Amazon CloudWatch Dashboard to monitor any changes to your organization. Launch a new Amazon SNS topic that will notify you and your team.</p>",
          "<p>Launch a new trail in Amazon CloudTrail to capture all API calls to your AWS Organizations, including calls from the AWS Organizations console. Also, track all code calls to the AWS Organizations APIs. Integrate Amazon EventBridge and Amazon SNS to raise events when administrator-specified actions occur in an organization and configure it to send a notificatio</p>",
          "<p>Launch an AWS-approved third-party monitoring tool from the AWS Marketplace that would send alerts if a breach was detected. Analyze any possible breach using AWS GuardDuty. Use Amazon SNS to notify the administrators.</p>",
          "<p>Monitor the compliance of your AWS Organizations using AWS Config. Launch a new SNS Topic or Amazon Amazon EventBridge rule to send alerts for any changes.</p>",
        ],
      },
      correct_response: ["c", "e"],
      section: "Monitoring and Logging",
      question_plain:
        "An IT consulting firm with various teams and departments uses AWS Organizations to manage multiple AWS accounts grouped into several organizational units (OUs). The IT Security team reported a suspected breach in the environment where a third-party AWS account was suddenly added to the organization without prior approval. The external account has high-level access privileges to the accounts that the firm owns, but fortunately, no detrimental action was taken.Which of the following is the MOST appropriate monitoring setup that notifies of any changes to the AWS accounts? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588403,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A business has its AWS accounts managed by AWS Organizations and has employees in different countries. The business is reviewing its AWS account security policies and is looking for a way to monitor its AWS accounts for unusual behavior that is associated with an IAM identity. The business wants to:</p><ul><li><p>send a notification to any employee for whom the unusual activity is detected.</p></li><li><p>send a notification to the user's team leader.</p></li><li><p>an external messaging platform will send the notifications. The platform requires a target user-id for each recipient.</p></li></ul><p>The business already has an API that can be used to retrieve the team leader's and the employee's user-id from IAM user names.<br><br>Which solution will satisfy the requirements?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon GuardDuty </strong>is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. A <strong>GuardDuty finding</strong> represents a potential security issue detected within your network. GuardDuty generates a finding whenever it detects unexpected and potentially malicious activity in your AWS environment.</p><p><img src="https://media.tutorialsdojo.com/public/amazon-guardduty-dop-c02.png"></p><p>In this scenario, findings from Amazon GuardDuty are published to Amazon EventBridge as events that can be used to trigger a Lambda function which will send notifications to the external messaging platform.</p><p>Hence, the correct answer is: <strong>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business\' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and</strong><br><strong>invoke the Lambda function.</strong></p><p>The option that says: <strong>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business\' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function</strong> is incorrect because Amazon Detective will not by itself detect unusual activity. Detective provides analysis information related to a given finding.</p><p>The option that says: <strong>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business\' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function</strong> is incorrect because Amazon SNS can filter messages by attributes and not by message contents. An EventBridge rule would be required to publish to the SNS topic.</p><p>The option that says: <strong>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business\' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function</strong> is incorrect because Amazon Detective will not by itself detect unusual activity. In addition, Amazon SNS can filter messages by attributes and not by message contents. An EventBridge rule would be required to publish to the SNS topic.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/guardduty/">https://aws.amazon.com/guardduty/</a></p><p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings.html">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings.html</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-guardduty/?src=udemy">https://tutorialsdojo.com/amazon-guardduty/</a></p>',
        answers: [
          "<p>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
          "<p>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
          "<p>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
          "<p>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Security and Compliance",
      question_plain:
        "A business has its AWS accounts managed by AWS Organizations and has employees in different countries. The business is reviewing its AWS account security policies and is looking for a way to monitor its AWS accounts for unusual behavior that is associated with an IAM identity. The business wants to:send a notification to any employee for whom the unusual activity is detected.send a notification to the user's team leader.an external messaging platform will send the notifications. The platform requires a target user-id for each recipient.The business already has an API that can be used to retrieve the team leader's and the employee's user-id from IAM user names.Which solution will satisfy the requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588405,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A business wants to leverage AWS CloudFormation to deploy its infrastructure. The business would like to restrict deployment to two particular regions and wants to implement a strict tagging requirement. Developers are expected to deploy various versions of the same application and want to guarantee that resources are deployed in compliance with the business policy while still enabling developers to deploy different versions of the application.</p><p>Which of the following is the MOST suitable solution?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>With <strong>AWS Service Catalog</strong>, cloud resources can be centrally managed to achieve infrastructure as code (IaC) template governance at scale, whether written in CloudFormation or Terraform. Compliance requirements can be met while ensuring customers can efficiently deploy the necessary cloud resources.</p><p><img alt="Service Catalog" height="540" src="https://media.tutorialsdojo.com/public/dop-c02-service-catalog.png" width="1000"></p><p>Template constraints can be applied when limiting end-users\' options during a product launch. This ensures that the organization\'s compliance requirements are not breached.</p><p>A product must be present within a Service Catalog portfolio to apply template constraints. A template constraint includes rules that narrow the allowable values for parameters in the underlying AWS CloudFormation template of the product. These parameters define the set of values available to users when creating a stack. For instance, an instance type parameter can be specified to limit the types of instances that users can choose from when launching a stack containing EC2 instances.</p><p>Hence, the correct answer is: <strong>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</strong></p><p>The option that says:<strong> Utilize approved CloudFormation templates and launch CloudFormation StackSets </strong>is incorrect because StackSets manage deployments across accounts and regions but do not enforce tagging or region restrictions. They do not typically provide governance to prevent non-compliant implementations.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks </strong>is incorrect because Trusted Advisor does not support checks for unauthorized StackSets or enforce CloudFormation template compliance.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation </strong>is incorrect because drift detection only identifies changes after deployment, but cannot prevent non-compliant resource creation or enforce policies before deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/servicecatalog/">https://aws.amazon.com/servicecatalog/</a></p><p><a href="https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html</a></p><p><a href="https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/">https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/</a></p><p><br></p><p><strong>Check out this AWS Service Catalog Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-service-catalog/?src=udemy">https://tutorialsdojo.com/aws-service-catalog/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/</a></p>',
        answers: [
          "<p>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</p>",
          "<p>Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks.</p>",
          "<p>Utilize approved CloudFormation templates and launch CloudFormation StackSets.</p>",
          "<p>Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Security and Compliance",
      question_plain:
        "A business wants to leverage AWS CloudFormation to deploy its infrastructure. The business would like to restrict deployment to two particular regions and wants to implement a strict tagging requirement. Developers are expected to deploy various versions of the same application and want to guarantee that resources are deployed in compliance with the business policy while still enabling developers to deploy different versions of the application.Which of the following is the MOST suitable solution?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588407,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A multinational company is using multiple AWS accounts for its global cloud architecture. The AWS resources in their production account are shared among various business units of the company. A single business unit may have one or more AWS accounts that have resources in the production account. Recently, there were a lot of incidents in which the developers from a specific business unit accidentally terminated the Amazon EC2 instances owned by another business unit. A DevOps Engineer was tasked to come up with a solution to only allow a specific business unit who owns the EC2 instances and other AWS resources to terminate their own resources. </p><p>How should the Engineer implement a multi-account strategy to satisfy this requirement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an <em>organization</em> that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.</p><p>You can use organizational units (OUs) to group accounts together to administer as a single unit. This greatly simplifies the management of your accounts. For example, you can attach a policy-based control to an OU, and all accounts within the OU automatically inherit the policy. You can create multiple OUs within a single organization, and you can create OUs within other OUs. Each OU can contain multiple accounts, and you can move accounts from one OU to another. However, OU names must be unique within a parent OU or root.</p><p><img src="https://media.tutorialsdojo.com/aws-organizations.jpg"></p><p>Resource-level permissions refer to the ability to specify which resources users are allowed to perform actions on. Amazon EC2 has partial support for resource-level permissions. This means that for certain Amazon EC2 actions, you can control when users are allowed to use those actions based on conditions that have to be fulfilled or specific resources that users are allowed to use. For example, you can grant users permissions to launch instances, but only of a specific type and only using a specific AMI.</p><p>Hence, the correct answer is: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</strong></p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access</strong> is incorrect. The AWSServiceRoleForOrganizations service-linked role is primarily used to only allow AWS Organizations to create service-linked roles for other AWS services. This service-linked role is present in all organizations and not just in a specific OU.</p><p>The option that says: <strong>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources<em> </em></strong>is incorrect. Although the use of the AWS Control Tower is right, the aggregator feature is simply an AWS Config resource type that collects AWS Config configuration and compliance data from the following various AWS accounts. In addition, you have to launch the Customizations for AWS Control Tower (CfCT) on the same AWS region where your AWS Control Tower landing zone is deployed, and not on a different account, to put it in effect.</p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts</strong> is incorrect. AWS Service Catalog simply allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A more suitable service to use here is AWS Organizations.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html</a></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html</a></p><p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-organizations/?src=udemy">https://tutorialsdojo.com/aws-organizations/</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href="https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>',
        answers: [
          "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access.</p>",
          "<p>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources.</p>",
          "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</p>",
          "<p>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A multinational company is using multiple AWS accounts for its global cloud architecture. The AWS resources in their production account are shared among various business units of the company. A single business unit may have one or more AWS accounts that have resources in the production account. Recently, there were a lot of incidents in which the developers from a specific business unit accidentally terminated the Amazon EC2 instances owned by another business unit. A DevOps Engineer was tasked to come up with a solution to only allow a specific business unit who owns the EC2 instances and other AWS resources to terminate their own resources. How should the Engineer implement a multi-account strategy to satisfy this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588409,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A technology consulting company has an Oracle Real Application Clusters (RAC) database on their on-premises network which they want to migrate to AWS Cloud. Their Chief Technology Officer instructed the DevOps team to automate the patch management process of the operating system in which their database will run. They are also mandated as well to set up scheduled backups to comply with the disaster recovery plan of the company. </p><p>What should the DevOps team do to satisfy the requirement for this scenario with the LEAST amount of effort?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src="https://media.tutorialsdojo.com/public/AWS-Systems-Manager-Patch-Manager_3AUG2023.png"></p><p><strong>Amazon Data Lifecycle Manager (DLM)</strong> for EBS Snapshots provides a simple, automated way to back up data stored on Amazon EBS volumes. You can define backup and retention schedules for EBS snapshots by creating lifecycle policies based on tags. With this feature, you no longer have to rely on custom scripts to create and manage your backups.</p><p><strong>Oracle RAC</strong> is supported via the deployment using Amazon EC2 only since Amazon RDS and Aurora do not support it. Amazon RDS does not support certain features in Oracle such as Multitenant Database, Real Application Clusters (RAC), Unified Auditing, Database Vault and many more. You can use AWS Systems Manager Patch Manager to automate the process of patching managed instances with security-related updates.</p><p>Hence, the correct answer is: <strong>Migrate the Oracle RAC database to a large EBS-backed Amazon EC2 instance then install the SSM agent. Use the AWS Systems Manager Patch Manager to automate the patch management process. Set up the Amazon Data Lifecycle Manager service to automate the creation of Amazon EBS snapshots from the EBS volumes of the EC2 instance.</strong></p><p>The option that says: <strong>Migrate the database that is hosted on-premises to Amazon RDS which provides a multi-AZ failover feature for your Oracle RAC cluster. The RPO and RTO will be reduced in the event of system failure since Amazon RDS offers features such as patch management and maintenance of the underlying host</strong> is incorrect because Amazon RDS doesn\'t support Oracle RAC.</p><p>The option that says: <strong>Migrate the on-premises database to Amazon Aurora. Enable automated backups for your Aurora RAC cluster. With Amazon Aurora, patching is automatically handled during the system maintenance window</strong> is incorrect because, just like Amazon RDS, the Amazon Aurora doesn\'t support Oracle RAC as well.</p><p>The option that says: <strong>Migrate the Oracle RAC database to a large EBS-backed Amazon EC2 instance. Launch an AWS Lambda function that will call the </strong><code><strong>CreateSnapshot</strong></code><strong> EC2 API to automate the creation of Amazon EBS snapshots of the database. Integrate CloudWatch Events and Lambda in order to run the function on a regular basis. Set up the CodeDeploy and CodePipeline services to automate the patch management process of the database </strong>is incorrect because CodeDeploy and CodePipeline are CI/CD services and are not suitable for patch management. You should use AWS Systems Manager Patch Manager instead. In addition, the Amazon Data Lifecycle Manager service is the recommended way to automate the creation of Amazon EBS snapshots and not a combination of Lambda and CloudWatch Events.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href="https://aws.amazon.com/rds/oracle/faqs/">https://aws.amazon.com/rds/oracle/faqs/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>',
        answers: [
          "<p>Migrate the Oracle RAC database to a large EBS-backed Amazon EC2 instance then install the SSM agent. Use the AWS Systems Manager Patch Manager to automate the patch management process. Set up the Amazon Data Lifecycle Manager service to automate the creation of Amazon EBS snapshots from the EBS volumes of the EC2 instance.</p>",
          "<p>Migrate the database that is hosted on-premises to Amazon RDS which provides a multi-AZ failover feature for your Oracle RAC cluster. The RPO and RTO will be reduced in the event of system failure since Amazon RDS offers features such as patch management and maintenance of the underlying host.</p>",
          "<p>Migrate the on-premises database to Amazon Aurora. Enable automated backups for your Aurora RAC cluster. With Amazon Aurora, patching is automatically handled during the system maintenance window.</p>",
          "<p>Migrate the Oracle RAC database to a large EBS-backed Amazon EC2 instance. Launch an AWS Lambda function that will call the <code>CreateSnapshot</code> EC2 API to automate the creation of Amazon EBS snapshots of the database. Integrate CloudWatch Events and Lambda in order to run the function on a regular basis. Set up the CodeDeploy and CodePipeline services to automate the patch management process of the database.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Security and Compliance",
      question_plain:
        "A technology consulting company has an Oracle Real Application Clusters (RAC) database on their on-premises network which they want to migrate to AWS Cloud. Their Chief Technology Officer instructed the DevOps team to automate the patch management process of the operating system in which their database will run. They are also mandated as well to set up scheduled backups to comply with the disaster recovery plan of the company. What should the DevOps team do to satisfy the requirement for this scenario with the LEAST amount of effort?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588411,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A multinational investment bank has several AWS accounts across the globe that host its cloud-based applications. A DevOps Engineer has been tasked with securing all of the AWS resources and web applications against common web vulnerabilities and cyberattacks.</p><p>A major concern is Distributed Denial of Service (DDoS) attacks, where a large volume of traffic from multiple locations targets the company’s web application, flooding its network and causing potential service disruption.</p><p>To reduce the DDoS attack surface and minimize the blast radius of such threats, the engineer must implement a cost-effective and scalable strategy.</p><p>Which options should the Engineer implement for the company’s DDoS mitigation strategy? (Select TWO.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p><strong>A Distributed Denial of Service (DDoS)</strong> attack is a malicious attempt to make a targeted system, such as a website or application, unavailable to end-users. To achieve this, attackers use a variety of techniques that consume network or other resources, interrupting access for legitimate end-users.</p><p>Another important consideration when architecting on AWS is to limit the opportunities that an attacker may have to target your application. For example, if you do not expect an end-user to directly interact with certain resources, you will want to make sure that those resources are not accessible from the Internet. Similarly, if you do not expect end-users or external applications to communicate with your application on certain ports or protocols, you will want to make sure that traffic is not accepted. This concept is known as attack surface reduction. Resources that are not exposed to the Internet are more difficult to attack, which limits the options an attacker might have to target the availability of your application.</p><p><img src="https://media.tutorialsdojo.com/public/web-app-ddos-mitigation_3AUG2023.png"></p><p>AWS Shield is a managed DDoS protection service that is available in two tiers: Standard and Advanced. AWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency. AWS Shield Standard is included automatically and transparently to your Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 resources at no additional cost. When you use these services that include AWS Shield Standard, you receive comprehensive availability protection against all known infrastructure layer attacks.</p><p>Customers who have the technical expertise to manage their own monitoring and mitigation of application-layer attacks can use AWS Shield together with AWS WAF rules to create a comprehensive DDoS attack mitigation strategy.</p><p>Hence, the correct answers are:</p><p><strong>- Use AWS Shield Advanced to enable enhanced DDoS attack detection and monitoring for application-layer traffic of the company\'s AWS resources. Ensure that every security group in the Amazon VPC only allows certain ports and traffic from authorized servers or services. Protect your origin servers by putting it behind an Amazon CloudFront distribution.</strong></p><p><strong>- Set up AWS WAF rules that identify and block common DDoS request patterns to effectively mitigate an attack. Implement an IP set match rule statement in WAF to block requests from malicious IP addresses associated with DDoS attacks. Ensure that Network Access Control Lists (ACLs) only allow the required ports and network addresses within the Amazon VPC.</strong></p><p>The option that says: <strong>Set up AWS Systems Manager Session Manager to filter all client-side web sessions of their Amazon EC2 instances. Use extra-large EC2 instances to accommodate a surge of incoming traffic caused by a DDoS attack. Configure Elastic Load Balancing and Auto Scaling to your EC2 instances across multiple Availability Zones to improve availability and scalability to your compute capacity</strong> is incorrect. Although it improves the scalability of your network in case of an ongoing DDoS attack, it simply absorbs the heavy application layer traffic and doesn\'t minimize the attack surface in your cloud architecture. In addition, the AWS Systems Manager Session Manager is primarily used to provide secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys, but not to filter client-side web sessions.</p><p>The option that says: <strong>Set up a Multi-Account Multi-Region Data Aggregation using AWS Config to monitor all of the company\'s AWS accounts. Enable the Versioning feature on all of the Amazon S3 buckets. Automate the OS patching of all of the company\'s Amazon EC2 instances using Systems Manager Patch Manager </strong>is incorrect. Although it is recommended that all of your instances are properly patched using the Systems Manager Patch Manager, it is still not enough to protect your cloud infrastructure against DDoS attacks. The S3 Versioning feature is primarily used to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket, but not as DDoS attack mitigation.</p><p>The option that says: <strong>Enable Multi-Factor Authentication (MFA) to all of the IAM users as well as Amazon S3 buckets. Improve the security of your AWS resources using Systems Manager State Manager, AWS Config, and Trusted Advisor</strong> is incorrect because MFA doesn\'t minimize the DDoS attack surface area. The AWS Systems Manager State Manager is just a configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This service will not help you minimize the blast radius of a security attack.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/">https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/</a></p><p><a href="https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf">https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a></p><p><a href="https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf">https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf</a></p><p><br></p><p><strong>Check out these AWS Shield and AWS WAF Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/aws-shield/?src=udemy">https://tutorialsdojo.com/aws-shield/</a></p>',
        answers: [
          "<p>Set up AWS Systems Manager Session Manager to filter all client-side web sessions of their Amazon EC2 instances. Use extra-large EC2 instances to accommodate a surge of incoming traffic caused by a DDoS attack. Configure Elastic Load Balancing and Auto Scaling to your EC2 instances across multiple Availability Zones to improve availability and scalability to your compute capacity.</p>",
          "<p>Use AWS Shield Advanced to enable enhanced DDoS attack detection and monitoring for application-layer traffic of the company's AWS resources. Ensure that every security group in the Amazon VPC only allows certain ports and traffic from authorized servers or services. Protect your origin servers by putting it behind an Amazon CloudFront distribution.</p>",
          "<p>Set up AWS WAF rules that identify and block common DDoS request patterns to effectively mitigate an attack. Implement an IP set match rule statement in WAF to block requests from malicious IP addresses associated with DDoS attacks. Ensure that Network Access Control Lists (ACLs) only allow the required ports and network addresses within the Amazon VPC.</p>",
          "<p>Set up a Multi-Account Multi-Region Data Aggregation using AWS Config to monitor all of the company's AWS accounts. Enable the Versioning feature on all of the Amazon S3 buckets. Automate the OS patching of all of the company's Amazon EC2 instances using Systems Manager Patch Manager.</p>",
          "<p>Enable Multi-Factor Authentication (MFA) to all of the IAM users as well as Amazon S3 buckets. Improve the security of your AWS resources using Systems Manager State Manager, AWS Config, and Trusted Advisor</p>",
        ],
      },
      correct_response: ["b", "c"],
      section: "Incident and Event Response",
      question_plain:
        "A multinational investment bank has several AWS accounts across the globe that host its cloud-based applications. A DevOps Engineer has been tasked with securing all of the AWS resources and web applications against common web vulnerabilities and cyberattacks.A major concern is Distributed Denial of Service (DDoS) attacks, where a large volume of traffic from multiple locations targets the company’s web application, flooding its network and causing potential service disruption.To reduce the DDoS attack surface and minimize the blast radius of such threats, the engineer must implement a cost-effective and scalable strategy.Which options should the Engineer implement for the company’s DDoS mitigation strategy? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588413,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A leading IT consulting firm is building a GraphQL API service and a mobile application that lets people post photos and videos of the traffic situations and other issues in the city's public roads. Users can include a text report and constructive feedback to the authorities. The department of public works shall rectify the problems based on the data gathered by the system. In order for the mobile app to run on various mobile and tablet devices, the firm decided to develop it using the React Native mobile framework, which will consume and send data to the GraphQL API. The backend service will be responsible for storing the photos and videos in an Amazon S3 bucket. The API will also need access to the Amazon DynamoDB database to store the text reports. The firm has recently deployed the mobile app prototype, however, during testing, the GraphQL API showed a lot of issues. The team decided to remove the API to proceed with the project and refactor the mobile application instead so that it will directly connect to both DynamoDB and S3 as well as handle user authentication. </p><p>Which of the following options provides a cost-effective and scalable architecture for this project? (Select TWO.)</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>With web identity federation, you don\'t need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any of your OpenID Connect (OIDC)-compatible IdP. They can receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Using an IdP helps you keep your AWS account secure because you don\'t have to embed and distribute long-term security credentials with your application.</p><p>The preferred way to use web identity federation is to use <a href="https://aws.amazon.com/cognito/">Amazon Cognito</a>. For example, You are a developer that builds a game for a mobile device where each user data such as scores and profiles are stored in Amazon S3 and Amazon DynamoDB. You could also store this data locally on the device and use Amazon Cognito to keep it synchronized across devices. You know that for security and maintenance reasons, long-term AWS security credentials should not be distributed with the game. You might also know that the game might have a large number of users. For all of these reasons, you don\'t want to create new user identities in IAM for each player. Instead, you build the game so that users can sign in using an identity that they\'ve already established with a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible IdP. Your game can take advantage of the authentication mechanism from one of these providers to validate the user\'s identity.</p><p><img src="https://media.tutorialsdojo.com/aws-cognito-diagram-tutorialsdojo.png"></p><p>To enable the mobile app to access your AWS resources, you should first register for a developer ID with your chosen IdPs. You can also configure the application with each of these providers. In your AWS account that contains the Amazon S3 bucket and DynamoDB table for the game, you should use Amazon Cognito to create IAM roles that precisely define permissions that the game needs. If you are using an OIDC IdP, you can also create an IAM OIDC identity provider entity to establish trust between your AWS account and the IdP.</p><p>In the app\'s code, you can call the sign-in interface for the IdP that you configured previously. The IdP handles all the details of letting the user sign in, and the app gets an OAuth access token or OIDC ID token from the provider. Your mobile app can trade this authentication information for a set of temporary security credentials that consist of an AWS access key ID, a secret access key, and a session token. The app can then use these credentials to access web services offered by AWS. The app is limited to the permissions that are defined in the role that it assumes.</p><p>In this scenario, you have a mobile app that needs to have access to the DynamoDB and S3 bucket. You can achieve this by using Web Identity Federation with AssumeRoleWithWebIdentity API which provides temporary security credentials and an IAM role. You can also use Amazon Cognito to simplify the process.</p><p>Hence, the correct answers are:</p><p><strong>- Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with social identity providers like Facebook, Google or any other OpenID Connect (OIDC)-compatible IdP. Create a new IAM Role and grant permissions to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table.</strong></p><p><strong>- Create an identity pool in Amazon Cognito that will be used to store the end-user identities organized for your mobile app. Amazon Cognito will automatically create the required IAM roles for authenticated identities as well as for unauthenticated "guest" identities that define permissions for Amazon Cognito users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use Amazon Cognito. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client.</strong></p><p>The option that says: <strong>Create an identity pool in AWS Identity and Access Management (IAM) that will be used to store the end-user identities organized for your mobile app. IAM will automatically create the required IAM roles for authenticated identities as well as for unauthenticated "guest" identities that define permissions for the users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use IAM. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client</strong> is incorrect because you cannot create identity pools with guest identities using the AWS Identity and Access Management (IAM) service. You can only implement this using Amazon Cognito.</p><p>The option that says: <strong>Using the STS AssumeRoleWithSAML API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile app to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table</strong> is incorrect because you should have used the AssumRoleWithWebIdentity API instead of <em>AssumeRoleWithSAML</em>, as this is used in SAML authentication response and not for web identity authentication.</p><p>The option that says: <strong>Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the text-based report to a DynamoDB table</strong> is incorrect because even though the use of Amazon Cognito is valid, it is wrong to store and use the AWS access and secret keys from the mobile app itself. This is a security risk and you should use the temporary security credentials instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html</a></p><p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></p><p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>',
        answers: [
          "<p>Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with social identity providers like Facebook, Google or any other OpenID Connect (OIDC)-compatible IdP. Create a new IAM Role and grant permissions to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table.</p>",
          '<p>Create an identity pool in AWS Identity and Access Management (IAM) that will be used to store the end-user identities organized for your mobile app. IAM will automatically create the required IAM roles for authenticated identities as well as for unauthenticated "guest" identities that define permissions for the users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use IAM. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client.</p>',
          "<p>Using the STS AssumeRoleWithSAML API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile app to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table.</p>",
          "<p>Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the text-based report to a DynamoDB table.</p>",
          '<p>Create an identity pool in Amazon Cognito that will be used to store the end-user identities organized for your mobile app. Amazon Cognito will automatically create the required IAM roles for authenticated identities as well as for unauthenticated "guest" identities that define permissions for Amazon Cognito users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use Amazon Cognito. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client.</p>',
        ],
      },
      correct_response: ["a", "e"],
      section: "Security and Compliance",
      question_plain:
        "A leading IT consulting firm is building a GraphQL API service and a mobile application that lets people post photos and videos of the traffic situations and other issues in the city's public roads. Users can include a text report and constructive feedback to the authorities. The department of public works shall rectify the problems based on the data gathered by the system. In order for the mobile app to run on various mobile and tablet devices, the firm decided to develop it using the React Native mobile framework, which will consume and send data to the GraphQL API. The backend service will be responsible for storing the photos and videos in an Amazon S3 bucket. The API will also need access to the Amazon DynamoDB database to store the text reports. The firm has recently deployed the mobile app prototype, however, during testing, the GraphQL API showed a lot of issues. The team decided to remove the API to proceed with the project and refactor the mobile application instead so that it will directly connect to both DynamoDB and S3 as well as handle user authentication. Which of the following options provides a cost-effective and scalable architecture for this project? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588415,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A startup is developing an AI-powered traffic monitoring portal that will be hosted in AWS Cloud. The design of the cloud architecture should be highly available and fault-tolerant to avoid unnecessary outages that can affect the users. A DevOps Engineer was instructed to implement the architecture and also set up a system that automatically assesses applications for exposure, vulnerabilities, and deviations from the AWS best practices. </p><p>Among the options below, which is the MOST appropriate architecture that you should implement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon Inspector</strong> is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. These findings can be reviewed directly or as part of detailed assessment reports, which are available via the Amazon Inspector console or API.</p><p>Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Amazon Inspector assessments are offered to you as pre-defined rules packages mapped to common security best practices and vulnerability definitions. Examples of built-in rules include checking for access to your EC2 instances from the internet, remote root login being enabled, or vulnerable software versions installed. These rules are regularly updated by AWS security researchers.</p><p>If you host a website on multiple Amazon EC2 instances, you can distribute traffic to your website across the instances by using an Elastic Load Balancing (ELB) load balancer. The ELB service automatically scales the load balancer as traffic to your website changes over time. The load balancer can also monitor the health of its registered instances and route domain traffic only to healthy instances. To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. An alias record is a Route 53 extension to DNS. It\'s similar to a CNAME record, but you can create an alias record both for the root domain, such as tutorialsdojo.com, and for subdomains, such as <strong>www</strong>.tutorialsdojo.com. (You can create CNAME records only for subdomains.)</p><p><img src="https://media.tutorialsdojo.com/public/elb-tutorial-architecture-diagram_3AUG2023.png"></p><p>You can use Amazon EC2 Auto Scaling to maintain a minimum number of running instances for your application at all times. Amazon EC2 Auto Scaling can detect when your instance or application is unhealthy and replace it automatically to maintain the availability of your application. You can also use Amazon EC2 Auto Scaling to scale your Amazon EC2 capacity up or down automatically based on demand, using criteria that you specify.</p><p>In this scenario, all of the options are highly available architectures. The main difference here is how they use Amazon Route 53. Keep in mind that you have to create an alias record in Amazon Route 53 in order to point to your load balancer.</p><p>Hence, the correct answer is: <strong>Use Amazon Inspector for automated security assessment to help improve the security and compliance of your applications. Launch an Auto Scaling group of Amazon EC2 instances on three Availability Zones. Set up an Application Load Balancer to distribute the incoming traffic. Set up an Amazon Aurora as the database tier. Using Amazon Route 53, create an alias record for the root domain to point to the load balancer.</strong></p><p>The option that says: <strong>Use Amazon Shield for automated security assessment to help improve the security and compliance of your applications. Launch an Auto Scaling group of Amazon EC2 instances on two Availability Zones with an Application Load Balancer in front. Set up a MySQL RDS database instance with Multi-AZ deployments configuration. Using Amazon Route 53, create a non-alias A record for the root domain to point to the load balancer</strong> is incorrect because AWS Shield is a managed Distributed Denial of Service (DDoS) protection service and not an automated security assessment. Moreover, you need to create an Alias record with the root DNS name and not an A record.</p><p>The option that says: <strong>Use Amazon GuardDuty for automated security assessment to help improve the security and compliance of your applications. Set up an Amazon ElastiCache cluster for the database caching of the portal. Launch an Auto Scaling group of Amazon EC2 instances on four Availability Zones then associate it to an Application Load Balancer. Set up a MySQL RDS database instance with Multi-AZ deployments configuration and Read Replicas. Using Amazon Route 53, create a CNAME record for the root domain to point to the load balancer</strong> is incorrect because Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts. The correct service that you should use is Amazon Inspector since this is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. In addition, you can create CNAME records only for subdomains and not for the zone apex or root domain.</p><p>The option that says: <strong>Use Amazon Macie for automated security assessment to help improve the security and compliance of your applications. Set up Amazon DynamoDB as the database of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones with an Application Load Balancer in front to distribute the incoming traffic. Using Amazon Route 53, create a non-alias A record for the root domain to point to the load balancer</strong> is incorrect because you should use Amazon Inspector instead of Amazon Macie since this is just a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. In addition, you should create an alias record with the root DNS name and not a non-alias A record.</p><p><br></p><p><strong>References:</strong></p><p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-route-53/?src=udemy">https://tutorialsdojo.com/amazon-route-53/</a></p>',
        answers: [
          "<p>Use AWS Shield for automated security assessment to help improve the security and compliance of your applications. Launch an Auto Scaling group of Amazon EC2 instances on two Availability Zones with an Application Load Balancer in front. Set up a MySQL RDS database instance with Multi-AZ deployments configuration. Using Amazon Route 53, create a non-alias A record for the root domain to point to the load balancer.</p>",
          "<p>Use Amazon Inspector for automated security assessment to help improve the security and compliance of your applications. Launch an Auto Scaling group of Amazon EC2 instances on three Availability Zones. Set up an Application Load Balancer to distribute the incoming traffic. Set up an Amazon Aurora as the database tier. Using Amazon Route 53, create an alias record for the root domain to point to the load balancer.</p>",
          "<p>Use Amazon GuardDuty for automated security assessment to help improve the security and compliance of your applications. Set up an Amazon ElastiCache cluster for the database caching of the portal. Launch an Auto Scaling group of Amazon EC2 instances on four Availability Zones then associate it to an Application Load Balancer. Set up a MySQL RDS database instance with Multi-AZ deployments configuration and Read Replicas. Using Amazon Route 53, create a CNAME record for the root domain to point to the load balancer.</p>",
          "<p>Use Amazon Macie for automated security assessment to help improve the security and compliance of your applications. Set up Amazon DynamoDB as the database of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones with an Application Load Balancer in front to distribute the incoming traffic. Using Amazon Route 53, create a non-alias A record for the root domain to point to the load balancer.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A startup is developing an AI-powered traffic monitoring portal that will be hosted in AWS Cloud. The design of the cloud architecture should be highly available and fault-tolerant to avoid unnecessary outages that can affect the users. A DevOps Engineer was instructed to implement the architecture and also set up a system that automatically assesses applications for exposure, vulnerabilities, and deviations from the AWS best practices. Among the options below, which is the MOST appropriate architecture that you should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588417,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A cloud-based payments company is heavily using Amazon EC2 instances to host their applications in AWS Cloud. They would like to improve the security of their cloud resources by ensuring that all of their EC2 instances were launched from pre-approved AMIs only. The list of AMIs is set and managed by their IT Security team. Their Software Development team has an automated CI/CD process that launches several EC2 instances with new and untested AMIs for testing. The development process must not be affected by the new solution, which will be implemented by their Lead DevOps Engineer. </p><p>Which of the following can the Engineer implement to satisfy the requirement with the LEAST impact on the development process? (Select TWO.)</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p><strong>AWS Config</strong> is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting.</p><p>When you run your applications on AWS, you usually use AWS resources, which you must create and manage collectively. As the demand for your application keeps growing, so does your need to keep track of your AWS resources. AWS Config is designed to help you oversee your application resources.</p><p><img src="https://media.tutorialsdojo.com/public/how-AWSconfig-works.png"></p><p>AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p>AWS Config provides AWS managed rules which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. In this scenario, you can use the <code><strong>approved-amis-by-id</strong></code> AWS manage rule which checks whether running instances are using specified AMIs. You can also use a Lambda function which is scheduled to run regularly to scan all of the running EC2 instances in your VPC and check if there is an instance that was launched using an unauthorized AMI.</p><p>In this scenario, we have to balance two things: security and development operations. The former should always be our utmost priority, this is why the scenario says that all of the company\'s EC2 instances must be launched from pre-approved AMIs only.</p><p>In the first place, why should the development team deploy a non-approved AMI? That\'s a clear violation of the company policy as stated above.<em> </em>As a Solutions Architect, a small security risk is still considered a risk and it must always be dealt with. In some organizations, such as banks and financial institutions, the IT Security team has the power to stop or revert back any recent deployments if there is a security issue.</p><p>Hence, the correct answers are:</p><p><strong>- Use AWS Config with a Lambda function that periodically evaluates whether there are EC2 instances launched based on non-approved AMIs. Set up a remediation action using AWS Systems Manager Automation that will automatically terminate the EC2 instance. Publish a message to an Amazon SNS topic to inform the IT Security and Development teams about the occurrence.</strong></p><p><strong>- Integrate AWS Lambda and CloudWatch Events to schedule a daily process that will search through the list of running Amazon EC2 instances within your VPC. Configure the function to determine if any of these are based on unauthorized AMIs. Publish a new message to an Amazon SNS topic to inform the Security and Development teams that the issue occurred and then automatically terminate the EC2 instance.</strong></p><p>Remember that the question asks for the LEAST impact solution. The development teams will still be able to continue deploying their applications without any disruption. If you are using the Atlassian suite, your Bamboo build and deployment plans would still execute successfully. Take note that the two solutions will not immediately terminate the EC2 instances running with a non-approved AMI. The first solution uses AWS Config with a Lambda function that periodically evaluates the instances while the second has a once-a-day (daily) process. Therefore, these two answers provide the LEAST impact solution while keeping the architecture secure.</p><p>The option that says: <strong>Set up a centralized IT Systems Operations team that has the required policies, roles, and permissions. The team will manually process the security approval steps to ensure that Amazon EC2 instances are launched from pre-approved AMIs only </strong>is incorrect because having manual information security approval will impact the development process. A better solution is to implement an automated process using AWS Config or a scheduled job using AWS Lambda and CloudWatch Events.</p><p>The option that says: <strong>Set up IAM policies to restrict the ability of users to launch Amazon EC2 instances based on a specific set of pre-approved AMIs which were tagged by the IT Security team</strong> is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact their CI/CD process. The scenario clearly says that the solution should not have any interruption in the company\'s development process.</p><p>The option that says: <strong>Do regular scans using Amazon Inspector via a custom assessment template that determines if the Amazon EC2 instance is based upon a pre-approved AMI or not. Terminate the instances and inform the IT Security team by email about the security breach </strong>is incorrect because Amazon Inspector is just an automated security assessment service that helps improve the security and compliance of applications deployed in AWS. It does not have the capability to detect EC2 instances that are using unapproved AMIs, unlike AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-config/?src=udemy">https://tutorialsdojo.com/aws-config/</a></p>',
        answers: [
          "<p>Set up a centralized IT Systems Operations team that has the required policies, roles, and permissions. The team will manually process the security approval steps to ensure that Amazon EC2 instances are launched from pre-approved AMIs only.</p>",
          "<p>Use AWS Config with a Lambda function that periodically evaluates whether there are EC2 instances launched based on non-approved AMIs. Set up a remediation action using AWS Systems Manager Automation that will automatically terminate the EC2 instance. Publish a message to an Amazon SNS topic to inform the IT Security and Development teams about the occurrence.</p>",
          "<p>Integrate AWS Lambda and CloudWatch Events to schedule a daily process that will search through the list of running Amazon EC2 instances within your VPC. Configure the function to determine if any of these are based on unauthorized AMIs. Publish a new message to an Amazon SNS topic to inform the Security and Development teams that the issue occurred and then automatically terminate the EC2 instance.</p>",
          "<p>Set up IAM policies to restrict the ability of users to launch Amazon EC2 instances based on a specific set of pre-approved AMIs which were tagged by the IT Security team.</p>",
          "<p>Do regular scans using Amazon Inspector via a custom assessment template that determines if the Amazon EC2 instance is based upon a pre-approved AMI or not. Terminate the instances and inform the IT Security team by email about the security breach.</p>",
        ],
      },
      correct_response: ["b", "c"],
      section: "Security and Compliance",
      question_plain:
        "A cloud-based payments company is heavily using Amazon EC2 instances to host their applications in AWS Cloud. They would like to improve the security of their cloud resources by ensuring that all of their EC2 instances were launched from pre-approved AMIs only. The list of AMIs is set and managed by their IT Security team. Their Software Development team has an automated CI/CD process that launches several EC2 instances with new and untested AMIs for testing. The development process must not be affected by the new solution, which will be implemented by their Lead DevOps Engineer. Which of the following can the Engineer implement to satisfy the requirement with the LEAST impact on the development process? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588419,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A rental payment startup has developed a web portal that enables users to pay for their rent using both their credit and debit cards online. They are using a third-party payment service to handle and process credit card payments on their platform since the portal is not fully compliant with the Payment Card Industry Data Security Standard (PCI DSS). The application is hosted in an Auto Scaling group of Amazon EC2 instances, which are launched in private subnets behind an internal-facing Application Load Balancer. The system must connect to an external payment service over the Internet to complete the transaction for every user payment. </p><p>As a DevOps Engineer, what would be the MOST suitable option to implement to satisfy the above requirement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can use a <strong>network address translation (NAT) gateway</strong> to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.</p><p>To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed once you associate it with the NAT Gateway. After you\'ve created a NAT gateway, you must update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet.</p><p><img src="https://media.tutorialsdojo.com/public/nat-gateway-diagram_6AUG2023.png"></p><p>Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone. You have a limit on the number of NAT gateways you can create in an Availability Zone.</p><p>Remember the difference between NAT Instance and NAT Gateways. A NAT Instance needs to use a script to manage failover between instances while this is done automatically in NAT gateways.</p><p>Hence, the correct answer is: <strong>Using a NAT Gateway, route credit card payment requests from the EC2 instances to the external payment service. Associate an Elastic IP address to the NAT Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway.</strong></p><p>The option that says: <strong>In the Security Group, whitelist the Public IP of the Internet Gateway. Route the user payment requests through the Internet Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the Internet Gateway</strong> is incorrect because you cannot whitelist an IP address using a Security Group. You should use a NAT Gateway instead to enable instances in a private subnet to connect to the Internet.</p><p>The option that says: <strong>Use the Application Load Balancer to route payment requests from the application servers through the Customer Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the Customer Gateway</strong> is incorrect because what you need here is a NAT Gateway and not a Customer Gateway. The use of an ELB is also not suitable for this scenario.</p><p>The option that says: <strong>Develop a shell script to automatically assign Elastic IP addresses to the Amazon EC2 instances. Add the script in the User Data of the EC2 instances, which automatically adds the Elastic IP address to the Network Access List upon launch. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to a VPC Endpoint</strong> is incorrect because neither the required NAT Gateway nor the NAT instance is mentioned in this option. Moreover, a VPC endpoint simply enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink. This doesn\'t allow you to connect to the public Internet.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-comparison.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-comparison.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-vpc/?src=udemy">https://tutorialsdojo.com/amazon-vpc/</a></p>',
        answers: [
          "<p>Using a NAT Gateway, route credit card payment requests from the EC2 instances to the external payment service. Associate an Elastic IP address to the NAT Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway.</p>",
          "<p>In the Security Group, whitelist the Public IP of the Internet Gateway. Route the user payment requests through the Internet Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the Internet Gateway.</p>",
          "<p>Use the Application Load Balancer to route payment requests from the application servers through the Customer Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the Customer Gateway.</p>",
          "<p>Develop a shell script to automatically assign Elastic IP addresses to the Amazon EC2 instances. Add the script in the User Data of the EC2 instances, which automatically adds the Elastic IP address to the Network Access List upon launch. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to a VPC Endpoint.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A rental payment startup has developed a web portal that enables users to pay for their rent using both their credit and debit cards online. They are using a third-party payment service to handle and process credit card payments on their platform since the portal is not fully compliant with the Payment Card Industry Data Security Standard (PCI DSS). The application is hosted in an Auto Scaling group of Amazon EC2 instances, which are launched in private subnets behind an internal-facing Application Load Balancer. The system must connect to an external payment service over the Internet to complete the transaction for every user payment. As a DevOps Engineer, what would be the MOST suitable option to implement to satisfy the above requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588421,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An international IT consulting firm has multiple on-premises data centers across the globe. Their technical team regularly uploads financial and regulatory files from each of their respective data centers to a centralized web portal hosted in AWS. It uses an Amazon S3 bucket named <code>financial-tdojo-reports</code> to store the data. Another team downloads various reports from a CloudFront web distribution that uses the same Amazon S3 bucket as the origin. A DevOps Engineer noticed that the staff are using both the CloudFront link and the direct Amazon S3 URLs to download the reports. The IT Security team of the company considered this as a security risk, and they recommended to re-design the architecture. A new system must be implemented that prevents anyone from bypassing the CloudFront distribution and disable direct access from Amazon S3 URLs. </p><p>What should the Engineer do to meet the above requirement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon CloudFront</strong> is a content delivery network (CDN) service provided by AWS. It securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. Amazon CloudFront provides various options to secure content delivery from S3 buckets, including preventing direct access to S3 and forcing downloads through the CloudFront distribution.</p><p><strong>Origin Access Control</strong> is a feature of Amazon CloudFront that enhances security for accessing S3 buckets and other supported origins. It allows you to restrict access to your origin resources, ensuring that they can only be accessed through designated CloudFront distributions. Some of the key points about OAC are:</p><ul><li><p>It uses AWS Identity and Access Management (IAM) service principals for authentication.</p></li><li><p>OAC employs short-term credentials with frequent rotations for improved security.</p></li><li><p>It supports comprehensive HTTP methods and server-side encryption with AWS KMS (SSE-KMS).</p></li><li><p>OAC can be used with S3 buckets in all AWS regions, as well as with AWS Lambda function URL origins.</p></li></ul><p><img src="https://media.tutorialsdojo.com/public/CloudFront_6AUG2023.png"></p><p>By creating an Origin Access Control (OAC) setting in the CloudFront distribution and updating the S3 bucket policy to grant access only to the CloudFront OAC, you effectively prevent direct access to the S3 bucket from any other source, including direct S3 URLs. All access to the objects in the S3 bucket must go through the CloudFront distribution, meeting the security requirement.</p><p>Hence, the correct answer is: <strong>Create an Origin Access Control (OAC) and associate it with the S3 bucket origin in the CloudFront distribution. Update the bucket policy for the </strong><code><strong>financial-tdojo-reports</strong></code><strong> bucket to allow access only from the specified CloudFront distribution.</strong></p><p>The option that says: <strong>Set up a custom SSL in your CloudFront web distribution instead of the default SSL. For each user, revoke the existing permission to access Amazon S3 URLs to download the objects</strong> is incorrect because SSL is not needed in this particular scenario. What you need to implement is an OAC.</p><p>The option that says: <strong>In the CloudFront web distribution, set up a field-level encryption configuration and for each user, revoke the existing permission to access Amazon S3 URLs to download the objects</strong> is incorrect because the field-level encryption configuration is primarily used for safeguarding sensitive fields in your CloudFront. Therefore, it is not suitable for this scenario.</p><p>The option that says: <strong>Configure the distribution to use Signed URLs and create a Signed URL signing key pair (a public/private key pair). Grant permission to the public key to access the objects in the S3 bucket</strong> is incorrect because Signed URLs are used to provide temporary access to specific objects in an S3 bucket, typically for a limited time period. While this approach can restrict direct access to the S3 bucket, it requires generating and managing Signed URLs for each object, which can be cumbersome and not scalable, especially if there are many objects or frequent updates.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html</a></p><p><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudfront/?src=udemy">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Create an Origin Access Control (OAC) and associate it with the S3 bucket origin in the CloudFront distribution. Update the bucket policy for the <code>financial-tdojo-reports</code> bucket to allow access only from the specified CloudFront distribution.</p>",
          "<p>Set up a custom SSL in your CloudFront web distribution instead of the default SSL. For each user, revoke the existing permission to access Amazon S3 URLs to download the objects.</p>",
          "<p>In the CloudFront web distribution, set up a field-level encryption configuration and for each user, revoke the existing permission to access Amazon S3 URLs to download the objects.</p>",
          "<p>Configure the distribution to use Signed URLs and create a Signed URL signing key pair (a public/private key pair). Grant permission to the public key to access the objects in the S3 bucket.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "An international IT consulting firm has multiple on-premises data centers across the globe. Their technical team regularly uploads financial and regulatory files from each of their respective data centers to a centralized web portal hosted in AWS. It uses an Amazon S3 bucket named financial-tdojo-reports to store the data. Another team downloads various reports from a CloudFront web distribution that uses the same Amazon S3 bucket as the origin. A DevOps Engineer noticed that the staff are using both the CloudFront link and the direct Amazon S3 URLs to download the reports. The IT Security team of the company considered this as a security risk, and they recommended to re-design the architecture. A new system must be implemented that prevents anyone from bypassing the CloudFront distribution and disable direct access from Amazon S3 URLs. What should the Engineer do to meet the above requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588423,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company runs a popular e-commerce website that provides discounts and weekly sales promotions on various products. It was recently moved from its previous cloud hosting provider to AWS. For its architecture, it uses an Auto Scaling group of Reserved EC2 instances with an Application Load Balancer in front. Their DevOps team needs to set up a CloudFront web distribution that uses a custom domain name where the origin is set to point to the ALB. </p><p>How can the DevOps Engineers properly implement an end-to-end HTTPS connection from the origin to the CloudFront viewers?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so that connections are encrypted when CloudFront communicates with viewers. You also can configure CloudFront to use HTTPS to get objects from your origin, so that connections are encrypted when CloudFront communicates with your origin.</p><p>Remember that there are rules on which type of SSL Certificate to use if you are using an EC2 or an ELB as your origin. This question is about setting up an end-to-end HTTPS connection between the Viewers, CloudFront, and your custom origin, which is an ALB instance.</p><p><img src="https://media.tutorialsdojo.com/end-to-end-application-load-balancer-to-cloudfront-encryption.png"></p><p>The certificate issuer you must use depends on whether you want to require HTTPS between viewers and CloudFront or between CloudFront and your origin:</p><p><strong>HTTPS between viewers and CloudFront</strong></p><p>- You can use a certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, Symantec, or other third-party providers.</p><p>- You can use a certificate provided by AWS Certificate Manager (ACM)</p><p><strong>HTTPS between CloudFront and a custom origin</strong></p><p>- If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- If your origin is an ELB load balancer, you can also use a certificate provided by ACM.</p><p>If you\'re using your own domain name, such as tutorialsdojo.com, you need to change several CloudFront settings. You also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store. Lastly, you should set the Viewer Protocol Policy to <strong>HTTPS Only</strong> in CloudFront.</p><p>Hence, the correct answer is: <strong>Generate an SSL certificate that is signed by a trusted third-party certificate authority. Import the certificate into AWS Certificate Manager and use it for the Application Load balancer. Set the </strong><code><strong>Viewer Protocol Policy</strong></code><strong> to </strong><code><strong>HTTPS Only</strong></code><strong> in CloudFront and then use an SSL/TLS certificate from a 3rd party certificate authority which was imported to either AWS Certificate Manager or the IAM certificate store.</strong></p><p>The option says:<strong> Generate an SSL self-signed certificate for the Application Load Balancer. Configure the Viewer Protocol Policy setting to </strong><code><strong>HTTPS Only</strong></code><strong> in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority that was imported to either AWS Certificate Manager or the IAM certificate store</strong> is incorrect because you cannot directly upload a self-signed certificate in your ALB.</p><p>The option that says: <strong>Use a certificate that is signed by a trusted 3rd party certificate authority for the Application Load Balancer, which is then imported into AWS Certificate Manager. Choose the </strong><code><strong>Match Viewer</strong></code><strong> setting for the Viewer Protocol Policy to support both HTTP or HTTPS in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority which was imported to either AWS Certificate Manager or the IAM certificate store</strong> is incorrect because you have to set the Viewer Protocol Policy to <code>HTTPS Only</code>. With<code><strong><em> Match Viewer</em></strong></code><strong><em>, </em></strong>CloudFront communicates with your custom origin using HTTP or HTTPS, depending on the protocol of the viewer request. For example, if you choose Match Viewer for Origin Protocol Policy and the viewer uses HTTP to request an object from CloudFront, CloudFront also uses the same protocol (HTTP) to forward the request to your origin.</p><p>The option that says: <strong>Utilize an SSL certificate that is signed by a trusted 3rd party certificate authority for the ALB, which is then imported into AWS Certificate Manager. Set the Viewer Protocol Policy to </strong><code><strong>HTTPS Only</strong></code><strong> in CloudFront. Use an SSL/TLS certificate from 3rd party certificate authority which was imported to an Amazon S3 bucket</strong> is incorrect because you cannot use an SSL/TLS certificate from a third-party certificate authority which was imported to S3.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html ">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudfront/?src=udemy">https://tutorialsdojo.com/amazon-cloudfront/</a></p>',
        answers: [
          "<p>Generate an SSL certificate that is signed by a trusted third-party certificate authority. Import the certificate into AWS Certificate Manager and use it for the Application Load balancer. Set the <code>Viewer Protocol Policy</code> to <code>HTTPS Only</code> in CloudFront and then use an SSL/TLS certificate from a 3rd party certificate authority which was imported to either AWS Certificate Manager or the IAM certificate store.</p>",
          "<p>Generate an SSL self-signed certificate for the Application Load Balancer. Configure the Viewer Protocol Policy setting to <code>HTTPS Only</code> in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority that was imported to either AWS Certificate Manager or the IAM certificate store.</p>",
          "<p>Use a certificate that is signed by a trusted 3rd party certificate authority for the Application Load Balancer, which is then imported into AWS Certificate Manager. Choose the <code>Match Viewer</code> setting for the Viewer Protocol Policy to support both HTTP or HTTPS in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority that was imported to either AWS Certificate Manager or the IAM certificate store.</p>",
          "<p>Utilize an SSL certificate that is signed by a trusted 3rd party certificate authority for the ALB, which is then imported into AWS Certificate Manager. Set the Viewer Protocol Policy to <code>HTTPS Only</code> in CloudFront. Use an SSL/TLS certificate from 3rd party certificate authority which was imported to an Amazon S3 bucket.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company runs a popular e-commerce website that provides discounts and weekly sales promotions on various products. It was recently moved from its previous cloud hosting provider to AWS. For its architecture, it uses an Auto Scaling group of Reserved EC2 instances with an Application Load Balancer in front. Their DevOps team needs to set up a CloudFront web distribution that uses a custom domain name where the origin is set to point to the ALB. How can the DevOps Engineers properly implement an end-to-end HTTPS connection from the origin to the CloudFront viewers?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588425,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company is receiving a high number of bad reviews from their customers lately because their website takes a lot of time to load. Upon investigation, there is a significant latency experienced whenever a user logs into their site. The DevOps Engineer configured CloudFront to serve the static content to its users around the globe, yet the problem still persists. There are times when their users get HTTP 504 errors in the login process. The engineers were tasked to fix this problem immediately to prevent users from unsubscribing on their website which will result in financial loss to the company. </p><p>Which combination of options below should the DevOps Engineer use together to set up a solution for this scenario with MINIMAL costs? (Select TWO.)</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p><strong>Amazon CloudFront</strong> is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you\'re serving with CloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p><p><strong>Lambda@Edge</strong> lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:</p><p>- After CloudFront receives a request from a viewer (viewer request)</p><p>- Before CloudFront forwards the request to the origin (origin request)</p><p>- After CloudFront receives the response from the origin (origin response)</p><p>- Before CloudFront forwards the response to the viewer (viewer response)</p><p><img src="https://media.tutorialsdojo.com/public/cloudfront-events-that-trigger-lambda-functions.png"></p><p>In the given scenario, you can use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. In addition, you can set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin, which CloudFront automatically switches to when the primary origin fails. This will alleviate the occasional HTTP 504 errors that users are experiencing.</p><p>Hence, the correct answers in this scenario are:</p><p><strong>- Use Lambda@Edge to customize the content that the CloudFront distribution delivers to your users across the globe. This will allow the Lambda functions to execute the authentication process in a specific AWS location that is proximate to the user.</strong></p><p><strong>- Create an origin group with two origins to set up an origin failover in Amazon CloudFront. Specify one as the primary origin and the other as the second origin. This configuration will cause the CloudFront service to automatically switch to the second origin in the event that the primary origin returns specific HTTP status code failure responses.</strong></p><p>The option that says: <strong>Replicate your application stack to multiple AWS regions to serve your users around the world. Set up a Route 53 record with Latency routing policy that will automatically route traffic to the region with the best latency to the user<em> </em></strong>is incorrect. Although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will improve the performance of the application with minimal cost.</p><p>The option that says: <strong>Deploy your application stack to multiple and geographically disperse VPCs on various AWS regions. Set up a Transit VPC to easily connect all your VPCs and resources. Using the AWS Serverless Application Model (SAM) service, deploy several AWS Lambda functions in each region to improve the overall application performance </strong>is incorrect. Although setting up multiple VPCs across various regions which are connected with a transit VPC is valid, this solution still entails higher setup and maintenance costs. A more cost-effective option would be to use Lambda@Edge instead.</p><p>The option that says: <strong>In CloudFront, add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects. Improve the cache hit ratio by setting the longest practical value for </strong><code><strong>max-age</strong></code><strong> of your CloudFront distribution</strong> is incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the slow authentication process of your global users and not just the caching of the static objects.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p><p><a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html">https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html</a></p><p><br></p><p><strong>Check out these Amazon CloudFront and AWS Lambda Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudfront/?src=udemy">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href="https://tutorialsdojo.com/aws-lambda/?src=udemy">https://tutorialsdojo.com/aws-lambda/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Replicate your application stack to multiple AWS regions to serve your users around the world. Set up a Route 53 record with Latency routing policy that will automatically route traffic to the region with the best latency to the user.</p>",
          "<p>Deploy your application stack to multiple and geographically disperse VPCs on various AWS regions. Set up a Transit VPC to easily connect all your VPCs and resources. Using the AWS Serverless Application Model (SAM) service, deploy several AWS Lambda functions in each region to improve the overall application performance.</p>",
          "<p>In CloudFront, add a <code>Cache-Control max-age</code> directive to your objects. Improve the cache hit ratio by setting the longest practical value for <code>max-age</code> of your CloudFront distribution.</p>",
          "<p>Use Lambda@Edge to customize the content that the CloudFront distribution delivers to your users across the globe. This will allow the Lambda functions to execute the authentication process in a specific AWS location that is proximate to the user.</p>",
          "<p>Create an origin group with two origins to set up an origin failover in Amazon CloudFront. Specify one as the primary origin and the other as the second origin. This configuration will cause the CloudFront service to automatically switch to the second origin in the event that the primary origin returns specific HTTP status code failure responses.</p>",
        ],
      },
      correct_response: ["d", "e"],
      section: "SDLC Automation",
      question_plain:
        "A company is receiving a high number of bad reviews from their customers lately because their website takes a lot of time to load. Upon investigation, there is a significant latency experienced whenever a user logs into their site. The DevOps Engineer configured CloudFront to serve the static content to its users around the globe, yet the problem still persists. There are times when their users get HTTP 504 errors in the login process. The engineers were tasked to fix this problem immediately to prevent users from unsubscribing on their website which will result in financial loss to the company. Which combination of options below should the DevOps Engineer use together to set up a solution for this scenario with MINIMAL costs? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588427,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An IT consulting firm has a Docker application hosted on an Amazon ECS cluster in their AWS VPC. It has been encountering intermittent unavailability issues and time outs lately, which affects their production environment. A DevOps engineer was instructed to instrument the application to detect where high latencies are occurring and to determine the specific services and paths impacting application performance. </p><p>Which of the following steps should the Engineer do to accomplish this task properly?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>The <strong>AWS X-Ray SDK</strong> does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the SDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the daemon alongside your application.</p><p>To properly instrument your applications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container.</p><p><img src="https://media.tutorialsdojo.com/public/AWS-X-Ray-daemon_6AUG2023.png"></p><p>The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service.</p><p>Hence, the correct steps to properly instrument the application is to: <strong>Produce a Docker image that runs the X-Ray daemon. Upload the image to a Docker image repository, and then deploy it to your Amazon ECS cluster. Configure the network mode settings and port mappings in your task definition file to allow traffic on UDP port 2000.</strong></p><p>The option that says: <strong>Produce a Docker image that runs the X-Ray daemon. Upload the image to a Docker image repository, and then deploy it to your Amazon ECS cluster. Configure the network mode settings and port mappings in the container agent to allow traffic on TCP port 2000 </strong>is incorrect because this should be done in the task definition and not in the container agent. Moreover, X-Ray is primarily using the UDP port 2000 so this should also be added alongside the TCP port mapping.</p><p>The option that says: <strong>In the Amazon ECS container agent configuration, pass a user data script in the </strong><code><strong>/etc/ecs/ecs.config</strong></code><strong> file that will install the X-Ray daemon. The script will automatically run when the Amazon ECS container instance is launched. Configure the network mode settings and port mappings in the container agent to allow traffic on TCP port 3000 </strong>is incorrect because X-Ray is using the UDP port 2000 and not TCP port 3000. In addition, it is better to configure your ECS task definitions instead of the Amazon ECS container agent to enable X-Ray.</p><p>The option that says: <strong>Add the </strong><code><strong>xray-daemon.config</strong></code><strong> configuration file in your Docker image. Set up the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000</strong> is incorrect because this step is not suitable for ECS. The <code>xray-daemon.config</code> configuration file is primarily used in Elastic Beanstalk.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html</a></p><p><a href="https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html</a></p><p><a href="https://docs.aws.amazon.com/xray/latest/devguide/scorekeep-ecs.html">https://docs.aws.amazon.com/xray/latest/devguide/scorekeep-ecs.html</a></p><p><br></p><p><strong>Check out this AWS X-Ray Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-x-ray/?src=udemy">https://tutorialsdojo.com/aws-x-ray/</a></p><p><br></p><p><strong>Instrumenting your Application with AWS X-Ray:</strong></p><p><a href="https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/?src=udemy">https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/</a></p>',
        answers: [
          "<p>Produce a Docker image that runs the X-Ray daemon. Upload the image to a Docker image repository, and then deploy it to your Amazon ECS cluster. Configure the network mode settings and port mappings in your task definition file to allow traffic on UDP port 2000.</p>",
          "<p>Produce a Docker image that runs the X-Ray daemon. Upload the image to a Docker image repository and then deploy it to your Amazon ECS cluster. Configure the network mode settings and port mappings in the container agent to allow traffic on TCP port 2000.</p>",
          "<p>In the Amazon ECS container agent configuration, pass a user data script in the <code>/etc/ecs/ecs.config</code> file that will install the X-Ray daemon. The script will automatically run when the Amazon ECS container instance is launched. Configure the network mode settings and port mappings in the container agent to allow traffic on TCP port 3000.</p>",
          "<p>Add the <code>xray-daemon.config</code> configuration file in your Docker image. Set up the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.</p>",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "An IT consulting firm has a Docker application hosted on an Amazon ECS cluster in their AWS VPC. It has been encountering intermittent unavailability issues and time outs lately, which affects their production environment. A DevOps engineer was instructed to instrument the application to detect where high latencies are occurring and to determine the specific services and paths impacting application performance. Which of the following steps should the Engineer do to accomplish this task properly?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588429,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A multinational insurance firm has recently consolidated its multiple AWS accounts using AWS Organizations with several Organizational units (OUs) that group each department of the firm. Their IT division consists of two teams: the DevOps team and the Release &amp; Deployment team. The first one is responsible for protecting its cloud infrastructure and ensuring that all of its AWS resources are compliant, while the latter is responsible for deploying new applications to AWS Cloud. The DevOps team has been instructed to set up a system that regularly checks if all of the running EC2 instances are using an approved AMI. However, the solution should not stop the Release &amp; Deployment team from deploying an EC2 instance running on a non-approved AMI for their release process. The DevOps team must build a notification system that sends the compliance state of the AWS resources to improve and maintain the security of their cloud resources. </p><p>Which of the following is the MOST suitable solution that the DevOps team should implement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>When you run your applications on AWS, you usually use AWS resources, which you must create and manage collectively. As the demand for your application keeps growing, so does your need to keep track of your AWS resources. AWS Config is designed to help you oversee your application resources.</p><p><img src="https://media.tutorialsdojo.com/public/how-AWSconfig-works.png"></p><p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p>AWS Config provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. In this scenario, you can use the <code><strong>approved-amis-by-id</strong></code> AWS manage rule, which checks whether running instances are using specified AMIs.</p><p>Hence, the correct answer is: <strong>Set up an AWS Config Managed Rule and specify a list of approved AMI IDs. Modify the rule to check whether the running Amazon EC2 instances are using specified AMIs. Configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic which will send a notification to both teams for non-compliant instances.</strong></p><p>The option that says: <strong>Assign a Service Control Policy and an IAM policy that restricts the AWS accounts and the development team from launching an EC2 instance using an unapproved AMI. Set up a CloudWatch alarm that will automatically notify the DevOps team if there are non-compliant EC2 instances running in their VPCs</strong> is incorrect because setting up an SCP and IAM Policy will totally restrict the Release &amp; Deployment team from launching EC2 instances with unapproved AMIs. The scenario clearly says that the solution should not have this kind of restriction.</p><p>The option that says: <strong>Automatically check all of the AMIs that are being used by your EC2 instances using the Amazon Inspector service. Use an SNS topic that will send a notification to both the DevOps and Release &amp; Deployment teams if there is a non-compliant EC2 instance running in their VPCs</strong> is incorrect because the Amazon Inspector service is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It does not have the capability to detect EC2 instances that are using unapproved AMIs, unlike AWS Config.</p><p>The option that says: <strong>Verify whether the running EC2 instances in your VPCs are using approved AMIs using Trusted Advisor checks. Set up a CloudWatch alarm and integrate it with the Trusted Advisor metrics that will check all of the AMIs being used by your Amazon EC2 instances and that will send a notification to both teams if there is a running instance which uses an unapproved AMI</strong> is incorrect because AWS Trusted Advisor is primarily used to check if your cloud infrastructure is in compliance with the best practices and recommendations across five categories: cost optimization, security, fault tolerance, performance, and service limits. Their security checks for EC2 do not cover the checking of individual AMIs that are being used by your EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-config/?src=udemy">https://tutorialsdojo.com/aws-config/</a></p>',
        answers: [
          "<p>Assign a Service Control Policy and an IAM policy that restricts the AWS accounts and the development team from launching an EC2 instance using an unapproved AMI. Set up a CloudWatch alarm that will automatically notify the DevOps team if there are non-compliant EC2 instances running in their VPCs.</p>",
          "<p>Automatically check all of the AMIs that are being used by your EC2 instances using the Amazon Inspector service. Use an SNS topic that will send a notification to both the DevOps and Release &amp; Deployment teams if there is a non-compliant EC2 instance running in their VPCs.</p>",
          "<p>Verify whether the running EC2 instances in your VPCs are using approved AMIs using Trusted Advisor checks. Set up a CloudWatch alarm and integrate it with the Trusted Advisor metrics that will check all of the AMIs being used by your Amazon EC2 instances and that will send a notification to both teams if there is a running instance which uses an unapproved AMI.</p>",
          "<p>Set up an AWS Config Managed Rule and specify a list of approved AMI IDs. Modify the rule to check whether the running Amazon EC2 instances are using specified AMIs. Configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic which will send a notification to both teams for non-compliant instances.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Security and Compliance",
      question_plain:
        "A multinational insurance firm has recently consolidated its multiple AWS accounts using AWS Organizations with several Organizational units (OUs) that group each department of the firm. Their IT division consists of two teams: the DevOps team and the Release &amp; Deployment team. The first one is responsible for protecting its cloud infrastructure and ensuring that all of its AWS resources are compliant, while the latter is responsible for deploying new applications to AWS Cloud. The DevOps team has been instructed to set up a system that regularly checks if all of the running EC2 instances are using an approved AMI. However, the solution should not stop the Release &amp; Deployment team from deploying an EC2 instance running on a non-approved AMI for their release process. The DevOps team must build a notification system that sends the compliance state of the AWS resources to improve and maintain the security of their cloud resources. Which of the following is the MOST suitable solution that the DevOps team should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588431,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A mobile phone manufacturer hosts a suite of enterprise resource planning (ERP) solutions to several Amazon EC2 instances in their AWS VPC. Its DevOps team is using AWS CloudFormation templates to design, launch, and deploy resources to their cloud infrastructure. Each template is manually updated to map the latest AMI IDs of the ERP solution. This process takes a significant amount of time to execute, which is why the team was tasked to automate this process. </p><p>In this scenario, which of the following options is the MOST suitable solution that can satisfy the requirement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.</p><p>If the parameter being referenced in the template does not exist in Systems Manager, a synchronous validation error is thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.</p><p><img src="https://media.tutorialsdojo.com/public/Systems-Manager-parameters_6AUG2023.png"></p><p>Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html">Parameters</a> section in the output for Describe API will show an additional ‘ResolvedValue’ field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation.</p><p>Hence, the correct answer is: <strong>Use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template. Call the update-stack API in CloudFormation in your template whenever you decide to update the Amazon EC2 instances.</strong></p><p>The option that says: <strong>Set up and configure the Systems Manager State Manager service to store the latest AMI IDs and integrate it with your AWS CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template</strong> is incorrect because the Systems Manager State Manager service simply automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This can\'t be used as a parameter store that refers to the latest AMI of your application.</p><p>The option that says: <strong>Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments</strong> is incorrect because using AWS Service Catalog is not suitable in this scenario. This service just allows organizations to create and manage catalogs of IT services that are approved for use on AWS.</p><p>The option that says: <strong>Integrate AWS CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments</strong> is incorrect because AWS Service Catalog just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A better solution is to use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>Set up and configure the Systems Manager State Manager service to store the latest AMI IDs and integrate it with your AWS CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.</p>",
          "<p>Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.</p>",
          "<p>Integrate AWS CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.</p>",
          "<p>Use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template. Call the update-stack API in CloudFormation in your template whenever you decide to update the Amazon EC2 instances.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A mobile phone manufacturer hosts a suite of enterprise resource planning (ERP) solutions to several Amazon EC2 instances in their AWS VPC. Its DevOps team is using AWS CloudFormation templates to design, launch, and deploy resources to their cloud infrastructure. Each template is manually updated to map the latest AMI IDs of the ERP solution. This process takes a significant amount of time to execute, which is why the team was tasked to automate this process. In this scenario, which of the following options is the MOST suitable solution that can satisfy the requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588433,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has encountered instances where DevOps engineers cannot connect to certain Amazon EC2 Windows instances and experience boot issues. This process involves a lot of manual steps before it can regain access. The team has been assigned to design a solution to automatically recover impaired EC2 instances in the company's Amazon VPC. To meet compliance requirements, it should automatically fix an EC2 instance that has become unreachable due to network misconfigurations, RDP issues, firewall settings, and other related problems. The solution should also leverage &lt;code&gt;aws.trustedadvisor&lt;/code&gt; to monitor best practices.</p><p>Which is the MOST suitable solution that the DevOps engineers should implement to satisfy this requirement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>EC2Rescue</strong> can help you diagnose and troubleshoot problems on Amazon EC2 Linux and Windows Server instances. You can run the tool manually or you can run the tool automatically by using Systems Manager Automation and the <em>AWSSupport-ExecuteEC2Rescue</em> document. The <em>AWSSupport-ExecuteEC2Rescue </em>document is designed to perform a combination of Systems Manager actions, AWS CloudFormation actions, and Lambda functions that automate the steps normally required to use EC2Rescue.</p><p>Systems Manager Automation simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following:</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon CloudWatch Events.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p><img src="https://media.tutorialsdojo.com/public/TD-AWS-Systems-Manager-02-21-2025.png"></p><p>Hence, the correct answer is: <strong>Diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances using the EC2Rescue tool. Automatically run the tool by using the AWS Systems Manager Automation and the </strong><code><strong>AWSSupport-ExecuteEC2Rescue</strong></code><strong> document.</strong></p><p>The option that says: <strong>Integrate AWS Config and the AWS Systems Manager State Manager to self diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Use AWS Systems Manager Maintenance Windows to automate the recovery process </strong>is incorrect because AWS Config is a service that is primarily used to assess, audit, and evaluate the configurations of your AWS resources but not to diagnose and troubleshoot problems in your EC2 instances. In addition, AWS Systems Manager State Manager is primarily used as a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define but does not help you in troubleshooting your EC2 instances.</p><p>The option that says: <strong>Use a combination of AWS Config and the AWS Systems Manager Session Manager to self diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Automate the recovery process by setting up a monitoring system using Amazon CloudWatch, AWS Lambda, and the AWS Systems Manager Run Command to automatically monitor and recover impaired EC2 instances</strong> is incorrect because, just like the previous option, AWS Config does not help you in troubleshooting the problems in your EC2 instances. Moreover, the AWS Systems Manager Sessions Manager simply provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys for your EC2 instances but it does not provide the capability of helping you diagnose and troubleshoot problems in your instance like what the EC2Rescue tool can do. In addition, setting up a CloudWatch, AWS Lambda, and the AWS Systems Manager Run Command that will automatically monitor and recover impaired EC2 instances is an operational overhead that can be easily done by using AWS Systems Manager Automation.</p><p>The option that says: <strong>Use AWS Trusted Advisor to monitor and resolve common security and performance issues with EC2 instances, ensuring compliance and automated recovery </strong>is incorrect because AWS Trusted Advisor simply provides best practices and recommendations for improving security and performance but does not offer automated recovery capabilities for the specific issues mentioned in the scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>Integrate AWS Config and the AWS Systems Manager State Manager to self diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Use AWS Systems Manager Maintenance Windows to automate the recovery process.</p>",
          "<p>Diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances using the EC2Rescue tool. Automatically run the tool by using the AWS Systems Manager Automation and the <code>AWSSupport-ExecuteEC2Rescue</code> document.</p>",
          "<p>Use a combination of AWS Config and the AWS Systems Manager Session Manager to self diagnose and troubleshoot problems on your EC2 Linux and Windows Server instances. Automate the recovery process by setting up a monitoring system using Amazon CloudWatch, AWS Lambda, and the AWS Systems Manager Run Command to automatically monitor and recover impaired EC2 instances.</p>",
          "<p>Use AWS Trusted Advisor to monitor and resolve common security and performance issues with EC2 instances, ensuring compliance and automated recovery.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company has encountered instances where DevOps engineers cannot connect to certain Amazon EC2 Windows instances and experience boot issues. This process involves a lot of manual steps before it can regain access. The team has been assigned to design a solution to automatically recover impaired EC2 instances in the company's Amazon VPC. To meet compliance requirements, it should automatically fix an EC2 instance that has become unreachable due to network misconfigurations, RDP issues, firewall settings, and other related problems. The solution should also leverage &lt;code&gt;aws.trustedadvisor&lt;/code&gt; to monitor best practices.Which is the MOST suitable solution that the DevOps engineers should implement to satisfy this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588435,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading technology company is planning to build a document management portal that will utilize an existing Amazon DynamoDB table. The portal will be launched in Kubernetes and managed via AWS App Runner for easier deployment. The table has an attribute of <code>DocumentName</code> that acts as the partition key and another attribute called <code>Category</code> as its sort key. A DevOps Engineer was instructed to develop a feature that queries the <code>DocumentName</code> attribute yet uses a different sort key other than the existing one. To fetch the latest data, strong read consistency must be used in the database tier.</p><p>Which of the following solutions below should the engineer implement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>A <strong><em>local secondary index</em></strong> maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.</p><p>Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to <code>Scan</code> the entire <em>Thread </em>table and discard any posts that were not within the specified time frame. With a local secondary index, a <code>Query</code> operation could use <em>LastPostDateTime</em> as a sort key and find the data quickly.</p><p>To create a Local Secondary Index, make sure that the primary key of the index is the same as the primary key/partition key of the table, just as shown below. Then you must select an alternative sort key that is different from the sort key of the table.</p><p><img src="https://media.tutorialsdojo.com/public/Local-Secondary-Index_9AUG2023.png"></p><p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful. A strongly consistent read might not be available if there is a network delay or outage. Strongly consistent reads are not supported on global secondary indexes.</p><p>The primary key of a local secondary index must be composite (partition key and sort key). A local secondary index lets you query over a single partition, as specified by the partition key value in the query.</p><p>Local secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist.</p><p>Hence, the correct answer is: <strong>Set up a new DynamoDB table with a Local Secondary Index that uses the </strong><code><strong>DocumentName</strong></code><strong> attribute with a different sort key. Migrate the data from the existing table to the new table.</strong></p><p>The option that says: <strong>Add a Global Secondary Index that uses the </strong><code><strong>DocumentName</strong></code><strong> attribute and a different sort key</strong><em> </em>is incorrect. Although it is possible to query data without using a scan command, it is still not enough because GSI does not support strong read consistency which is required in the scenario.</p><p>The option that says: <strong>Add a Global Secondary Index which uses the </strong><code><strong>DocumentName</strong></code><strong> attribute and use an alternative sort key as projected attributes</strong> is incorrect because using a local secondary index is a more appropriate solution to be used in this scenario just as explained above. Moreover, projected attributes are just attributes stored in the index that can be returned by queries and scans performed on the index, hence, these are not useful in satisfying the provided requirement.</p><p>The option that says: <strong>Add a Local Secondary Index that uses the </strong><code><strong>DocumentName</strong></code><strong> attribute and a different sort key</strong> is incorrect. Although it is using the correct type of index, you cannot add a local secondary index to an already existing table.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-dynamodb/?src=udemy">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Add a Global Secondary Index which uses the <code>DocumentName</code> attribute and use an alternative sort key as projected attributes.</p>",
          "<p>Set up a new DynamoDB table with a Local Secondary Index that uses the <code>DocumentName</code> attribute with a different sort key. Migrate the data from the existing table to the new table.</p>",
          "<p>Add a Global Secondary Index that uses the <code>DocumentName</code> attribute and a different sort key.</p>",
          "<p>Add a Local Secondary Index that uses the <code>DocumentName</code> attribute and a different sort key.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A leading technology company is planning to build a document management portal that will utilize an existing Amazon DynamoDB table. The portal will be launched in Kubernetes and managed via AWS App Runner for easier deployment. The table has an attribute of DocumentName that acts as the partition key and another attribute called Category as its sort key. A DevOps Engineer was instructed to develop a feature that queries the DocumentName attribute yet uses a different sort key other than the existing one. To fetch the latest data, strong read consistency must be used in the database tier.Which of the following solutions below should the engineer implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588437,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A financial company has a total of over a hundred Amazon EC2 instances running across their development, testing, and production environments in AWS. Based on a recent IT review, the company initiated a new compliance rule that mandates a monthly audit of every Linux and Windows EC2 instances check for system performance issues. Each instance must have a logging function that collects various system details and retrieve custom metrics from installed applications or services. The DevOps team will periodically review these logs and analyze their contents using AWS Analytics tools, and the result will be stored in an S3 bucket. </p><p>Which is the MOST recommended way to collect and analyze logs from the instances with MINIMAL effort?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>To collect logs from your Amazon EC2 instances and on-premises servers into CloudWatch Logs, AWS offers both a new unified CloudWatch agent, and an older CloudWatch Logs agent. It is recommended to use the unified CloudWatch agent, which has the following advantages:</p><p>- You can collect both logs and advanced metrics with the installation and configuration of just one agent.</p><p>- The unified agent enables the collection of logs from servers running Windows Server.</p><p>- If you are using the agent to collect CloudWatch metrics, the unified agent also enables the collection of additional system metrics, for in-guest visibility.</p><p>- The unified agent provides better performance.</p><p><img src="https://media.tutorialsdojo.com/public/LogsInsights-workflow_6AUG2023.png"></p><p>CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you quickly and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p><p>CloudWatch Logs Insights includes a purpose-built query language with a few simple but powerful commands. CloudWatch Logs Insights provides sample queries, command descriptions, query autocompletion, and log field discovery to help you get started quickly. Sample queries are included for several types of AWS service logs.</p><p>Hence, the correct answer is: <strong>Configure and install the unified CloudWatch Logs agent in each Amazon EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</strong></p><p>The option that says: <strong>Configure and install AWS SDK in each Amazon EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and analyze the log data of all instances using CloudWatch Logs Insights</strong> is incorrect. Although this is a valid solution, this entails a lot of effort to implement as you have to allocate time to install the AWS SDK to each instance and develop a custom monitoring solution. Remember that the question is specifically looking for a solution that can be implemented with minimal effort. In addition, it is unnecessary and not cost-efficient to enable detailed monitoring in CloudWatch in order to meet the requirements of this scenario since this can be done using CloudWatch Logs.</p><p>The option that says: <strong>Configure and install the AWS Systems Manager Agent (SSM Agent) in each EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights</strong> is incorrect. Although this is also a valid solution, it is more efficient to use CloudWatch agent than an SSM agent. Manually connecting to an instance to view log files and troubleshoot an issue with SSM Agent is time-consuming hence, for more efficient instance monitoring, you can use the CloudWatch Agent instead to send the log data to Amazon CloudWatch Logs.</p><p>The option that says: <strong>Configure and install AWS Inspector Agent in each Amazon EC2 instance that will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all EC2 instances </strong>is incorrect because AWS Inspector is simply a security assessment service that only helps you in checking for unintended network accessibility of your EC2 instances and for vulnerabilities on those EC2 instances. Furthermore, setting up an Amazon CloudWatch dashboard is not suitable since it\'s primarily used for scenarios where you have to monitor your resources in a single view, even those resources that are spread across different AWS Regions. It is better to use CloudWatch Logs Insights instead since it enables you to interactively search and analyze your log data.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html ">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>CloudWatch Agent vs SSM Agent vs Custom Daemon Scripts:</strong></p><p><a href="https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/?src=udemy">https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/</a></p>',
        answers: [
          "<p>Configure and install the unified CloudWatch Logs agent in each Amazon EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
          "<p>Configure and install AWS SDK in each Amazon EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and analyze the log data of all instances using CloudWatch Logs Insights.</p>",
          "<p>Configure and install the AWS Systems Manager Agent (SSM Agent) in each EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
          "<p>Configure and install AWS Inspector Agent in each Amazon EC2 instance that will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all EC2 instances.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A financial company has a total of over a hundred Amazon EC2 instances running across their development, testing, and production environments in AWS. Based on a recent IT review, the company initiated a new compliance rule that mandates a monthly audit of every Linux and Windows EC2 instances check for system performance issues. Each instance must have a logging function that collects various system details and retrieve custom metrics from installed applications or services. The DevOps team will periodically review these logs and analyze their contents using AWS Analytics tools, and the result will be stored in an S3 bucket. Which is the MOST recommended way to collect and analyze logs from the instances with MINIMAL effort?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588439,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          '<p>An educational startup has developed an e-learning platform hosted in AWS, where they sell their online courses. The CTO is planning to release an online forum that will allow students to interact with each other and post their questions. This new feature requires a new DynamoDB table named <code>Thread</code> in which the partition key is <code>ForumName</code> and the sort key is <code>Subject</code>. Below is a diagram that shows how the items in the table must be organized:</p><p><img src="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/LSI_01.png "></p><p>The forum must also find all of the threads that were posted in a particular forum within the last three months as part of the system\'s quarterly reporting.</p><p>Which of the following is the MOST suitable solution that you should implement?</p>',
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>DynamoDB</strong> supports two types of secondary indexes:</p><p>- <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html">Global secondary index</a> — an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered "global" because queries on the index can span all of the data in the base table, across all partitions.</p><p>- <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html">Local secondary index</a> — an index that has the same partition key as the base table, but a different sort key. A local secondary index is "local" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.</p><p><br></p><p>A local secondary index maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.</p><p>Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to <code>Scan</code> the entire <em>Thread </em>table and discard any posts that were not within the specified time frame. With a local secondary index, a <code>Query</code> operation could use <em>LastPostDateTime</em> as a sort key and find the data quickly.</p><p>In the provided scenario, you can create a local secondary index named <em>LastPostIndex </em>to meet the requirements. Note that the partition key is the same as that of the <em>Thread</em> table, but the sort key is <em>LastPostDateTime </em>as shown in the diagram below:</p><p><img src="https://media.tutorialsdojo.com/public/LastPostIndex%C2%A0.png"></p><p>Hence, the most effective solution in this scenario is to: <strong>Create a new DynamoDB table with a local secondary index. Refactor the e-learning platform to use the </strong><code><strong>Query</strong></code><strong> operation for search and utilize the </strong><code><strong>LastPostDateTime</strong></code><strong> attribute as the sort key.</strong></p><p>The option that says: <strong>Refactor the e-learning platform to do a </strong><code><strong>Scan</strong></code><strong> operation in the entire </strong><code><strong>Thread</strong></code><strong> table. Discard any posts that were not within the specified time frame</strong> is incorrect. Although this option is valid, this solution would consume a large amount of provisioned read-throughput and take a long time to complete. This is not a scalable solution, and the time it takes to fetch the data will continue to increase as the table grows.</p><p>The option that says: <strong>Configure the e-learning platform to use a </strong><code><strong>Query</strong></code><strong> operation for the entire </strong><code><strong>Thread table</strong></code><strong>. Discard any posts that were not within the specified time frame</strong><em> </em>is incorrect because using the Query operation is not sufficient to meet this requirement. You have to create a local secondary index when you create the table to narrow down the results and to improve the performance of your application.</p><p>The option that says: <strong>Create a new DynamoDB table with a global secondary index. Refactor the e-learning platform to use a </strong><code><strong>Query</strong></code><strong> operation for search and to utilize the </strong><code><strong>LastPostDateTime</strong></code><strong> attribute as the sort key</strong> is incorrect because using a local secondary index is a more appropriate solution to be used in this scenario. Take note that in this scenario, it is still using the same partition key (<code>ForumName</code>) but with an alternate sort key (<code>LastPostDateTime</code>) which warrants the use of a local secondary index.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-dynamodb/?src=udemy">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Refactor the e-learning platform to do a <code>Scan</code> operation in the entire <code>Thread</code> table. Discard any posts that were not within the specified time frame.</p>",
          "<p>Create a new DynamoDB table with a local secondary index. Refactor the e-learning platform to use the <code>Query</code> operation for search and utilize the <code>LastPostDateTime</code> attribute as the sort key.</p>",
          "<p>Configure the e-learning platform to use a <code>Query</code> operation for the entire <code>Thread</code> table. Discard any posts that were not within the specified time frame.</p>",
          "<p>Create a new DynamoDB table with a global secondary index. Refactor the e-learning platform to use a <code>Query</code> operation for search and to utilize the <code>LastPostDateTime</code> attribute as the sort key.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "An educational startup has developed an e-learning platform hosted in AWS, where they sell their online courses. The CTO is planning to release an online forum that will allow students to interact with each other and post their questions. This new feature requires a new DynamoDB table named Thread in which the partition key is ForumName and the sort key is Subject. Below is a diagram that shows how the items in the table must be organized:The forum must also find all of the threads that were posted in a particular forum within the last three months as part of the system's quarterly reporting.Which of the following is the MOST suitable solution that you should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588441,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A PHP web application is uploaded to the AWS Elastic Beanstalk of the company's development account to automatically handle the deployment, capacity provisioning, load balancing, auto-scaling, and application health monitoring. Amazon RDS is used as the database, and it is tightly coupled to the Elastic Beanstalk environment. A DevOps Engineer noticed that if you terminate the environment, its database goes down as well. This issue prevents you from performing seamless updates with blue-green deployments. Moreover, this poses a critical security risk if the company decides to deploy the application in its production environment. </p><p>How can the DevOps Engineer decouple the database instance from the environment with the LEAST amount of data loss?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Elastic Beanstalk</strong> provides support for running Amazon Relational Database Service (Amazon RDS) instances in your Elastic Beanstalk environment. This works great for development and testing environments. However, it isn\'t ideal for a production environment because it ties the lifecycle of the database instance to the lifecycle of your application\'s environment.</p><p>If you haven\'t used a DB instance with your application before, try adding one to a test environment with the Elastic Beanstalk console first. This lets you verify that your application is able to read environment properties, construct a connection string, and connect to a DB instance before you add Amazon Virtual Private Cloud (Amazon VPC) and security group configuration to the mix.</p><p><img src="https://media.tutorialsdojo.com/public/aeb-architecture2.png"></p><p>To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch. This enables you to connect multiple environments to a database, terminate an environment without affecting the database, and perform seamless updates with blue-green deployments.</p><p>To allow the Amazon EC2 instances in your environment to connect to an outside database, you can configure the environment\'s Auto Scaling group with an additional security group. The security group that you attach to your environment can be the same one that is attached to your database instance, or a separate security group from which the database\'s security group allows ingress.</p><p>You can connect your environment to a database by adding a rule to your database\'s security group that allows ingress from the autogenerated security group that Elastic Beanstalk attaches to your environment\'s Auto Scaling group. However, doing so creates a dependency between the two security groups. Subsequently, when you attempt to terminate the environment, Elastic Beanstalk will be unable to delete the environment\'s security group because the database\'s security group is dependent on it.</p><p>Hence, the correct answer is: <strong>Decouple the Amazon RDS instance from your Elastic Beanstalk environment using the blue/green deployment strategy to decouple. Take an RDS DB snapshot of the database and enable deletion protection. Set up a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment, remove its security group rule first before proceeding.</strong></p><p>The option that says: <strong>Decouple the Amazon RDS instance from your Elastic Beanstalk environment using a blue/green deployment strategy. Enable deletion protection and take an RDS DB snapshot of the database. Set up a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment </strong>is incorrect. Although the deployment strategy being used here is valid, the existing security group rule is not yet removed which hinders the deletion of the old environment.</p><p>The option that says: <strong>Decouple the Amazon RDS instance from your Elastic Beanstalk environment using the Canary deployment strategy. Take an RDS DB snapshot of the database and enable deletion protection. Set up a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment</strong> is incorrect because there is no Canary deployment configuration in Elastic Beanstalk. This type of deployment strategy is usually used in Lambda.</p><p>The option that says: <strong>Decouple the Amazon RDS instance from your Elastic Beanstalk environment using the Canary deployment strategy. Take an RDS DB snapshot of the database and then set up a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance</strong> is incorrect because you should use a blue/green deployment strategy instead. This will also cause data loss since the deletion protection for the database is not enabled. Moreover, there is no Canary deployment configuration in Elastic Beanstalk.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p><p><br></p><p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p><p><a href="https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/?src=udemy">https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</a></p>',
        answers: [
          "<p>Decouple the Amazon RDS instance from your Elastic Beanstalk environment using a blue/green deployment strategy. Enable deletion protection and take an RDS DB snapshot of the database. Set up a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment.</p>",
          "<p>Decouple the Amazon RDS instance from your Elastic Beanstalk environment using the Canary deployment strategy. Take an RDS DB snapshot of the database and enable deletion protection. Set up a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment.</p>",
          "<p>Decouple the Amazon RDS instance from your Elastic Beanstalk environment using the blue/green deployment strategy to decouple. Take an RDS DB snapshot of the database and enable deletion protection. Set up a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment, remove its security group rule first before proceeding.</p>",
          "<p>Decouple the Amazon RDS instance from your Elastic Beanstalk environment using the Canary deployment strategy. Take an RDS DB snapshot of the database and then set up a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A PHP web application is uploaded to the AWS Elastic Beanstalk of the company's development account to automatically handle the deployment, capacity provisioning, load balancing, auto-scaling, and application health monitoring. Amazon RDS is used as the database, and it is tightly coupled to the Elastic Beanstalk environment. A DevOps Engineer noticed that if you terminate the environment, its database goes down as well. This issue prevents you from performing seamless updates with blue-green deployments. Moreover, this poses a critical security risk if the company decides to deploy the application in its production environment. How can the DevOps Engineer decouple the database instance from the environment with the LEAST amount of data loss?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588443,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>You have migrated your application API server from a cluster of EC2 instances to a combination of API gateway and AWS Lambda. You are used to canary deployments on your EC2 cluster where you carefully check any errors on the application before doing the full deployment. However, you can’t do this on your current AWS Lambda setup since the deployment switches quickly from one version to another. </p><p>How can you implement the same functionality on AWS Lambda?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>By default, an alias points to a single Lambda function version. When the alias is updated to point to a different function version, incoming request traffic in turn instantly points to the updated version. This exposes that alias to any potential instabilities introduced by the new version. To minimize this impact, you can implement the routing-config parameter of the Lambda alias that allows you to point to two different versions of the Lambda function and dictate what percentage of incoming traffic is sent to each version.</p><p>With the introduction of alias traffic shifting, it is now possible to trivially implement canary deployments of Lambda functions. By updating additional version weights on an alias, invocation traffic is routed to the new function versions based on the weight specified. Detailed CloudWatch metrics for the alias and version can be analyzed during the deployment, or other health checks performed, to ensure that the new version is healthy before proceeding.</p><p><img src="https://media.tutorialsdojo.com/public/deployProgress_2AUG2023.png"></p><p>Hence, the correct answer is:<strong>Deploy your app using Traffic shifting with AWS Lambda aliases.</strong></p><p>The option that says: <strong>Use CodeDeploy to perform rolling update of the latest Lambda function </strong>is incorrect because AWS Lambda does not support native rolling update. Deployments of a Lambda function could only be performed in a single flip by updating the function code for version $LATEST, or by updating an alias to target a different function version.</p><p>The option that says: <strong>Deploy your app using Traffic shifting with Amazon Route 53</strong> is incorrect because Route 53 does not support traffic shifting for Lambda deployments.</p><p>The option that says: <strong>Use Route 53 weighted routing policy with API Gateway</strong> is incorrect. Although technically valid, this is not an efficient solution to perform canary deployments of Lambda functions. Traffic splitting uses an alias to switch between two functions in the backend allowing you to maintain a single instance of API Gateway + Lambda function. With weighted routing, you need to deploy the new version on a new instance of API Gateway + Lambda.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/ ">https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/</a></p><p><a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html">https://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html</a></p><p><a href="https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><br></p><p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-lambda/?src=udemy">https://tutorialsdojo.com/aws-lambda/</a></p>',
        answers: [
          "<p>Use CodeDeploy to perform rolling update of the latest Lambda function.</p>",
          "<p>Deploy your app using Traffic shifting with AWS Lambda aliases.</p>",
          "<p>Deploy your app using Traffic shifting with Amazon Route 53.</p>",
          "<p>Use Route 53 weighted routing policy with API Gateway.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "You have migrated your application API server from a cluster of EC2 instances to a combination of API gateway and AWS Lambda. You are used to canary deployments on your EC2 cluster where you carefully check any errors on the application before doing the full deployment. However, you can’t do this on your current AWS Lambda setup since the deployment switches quickly from one version to another. How can you implement the same functionality on AWS Lambda?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588445,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>You have developed an app that lets users quickly upload and share image snippets across various platforms such as mobile app, desktop web browser, and third party instant messaging apps. Your app relies on a Lambda function that detects each request’s User-Agent and automatically sends a corresponding image resolution based on the device. You want to have a consistent and fast experience for all your users around the world. </p><p>Which of the following options will you implement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Lambda@Edge</strong> runs your code in response to events generated by the Amazon CloudFront content delivery network (CDN). Lambda@Edge lets you run Node.js Lambda functions to customize content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers.</p><p>Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. Lambda@Edge lets you use CloudFront triggers to invoke a Lambda function. When you associate a CloudFront distribution with a Lambda function, CloudFront intercepts requests and responses at CloudFront edge locations and runs the function. Lambda functions can improve security or customize information close to your viewers, to improve performance.</p><p><img src="https://media.tutorialsdojo.com/public/AWS-Lambda-at-Edge_How-It-Works-diagram_2AUG2023.png"></p><p>Hence, the correct answer is: <strong>Create your function on Lambda and deploy it with Lambda@Edge.</strong></p><p>The option that says: <strong><em>Create your function on AWS Lambda and set it as a CloudFront origin</em> </strong>is incorrect because in AWS, you don\'t usually set a Lambda function as the origin for your CloudFront distribution. You have to use Lambda@Edge instead.</p><p>The option that says: <strong>Upload your function on S3 and set it as CloudFront origin to run it </strong>is incorrect. Although you can set S3 as the origin of your CloudFront distribution, you won\'t be able to trigger the function since S3 has no compute capability.</p><p>The option that says: <strong>Deploy your function on Amazon API Gateway and set up an Edge-optimized API endpoint </strong>is incorrect because you can\'t run your function using API Gateway alone. You have to integrate your Lambda functions with API Gateway first. Moreover, The Edge-optimized API is just a default hostname in API Gateway that is deployed to the specified region while using a CloudFront distribution to facilitate client access typically from across AWS regions.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html ">https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html</a></p><p><a href="https://aws.amazon.com/lambda/edge/ ">https://aws.amazon.com/lambda/edge/</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-how-it-works-tutorial.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-how-it-works-tutorial.html</a></p><p><br></p><p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-lambda/?src=udemy">https://tutorialsdojo.com/aws-lambda/</a></p>',
        answers: [
          "<p>Create your function on AWS Lambda and set it as a CloudFront origin.</p>",
          "<p>Create your function on Lambda and deploy it with Lambda@Edge.</p>",
          "<p>Upload your function on S3 and set it as CloudFront origin to run it.</p>",
          "<p>Deploy your function on Amazon API Gateway and set up an Edge-optimized API endpoint.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "You have developed an app that lets users quickly upload and share image snippets across various platforms such as mobile app, desktop web browser, and third party instant messaging apps. Your app relies on a Lambda function that detects each request’s User-Agent and automatically sends a corresponding image resolution based on the device. You want to have a consistent and fast experience for all your users around the world. Which of the following options will you implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588447,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company developed a web portal for gathering Census data within the city. The household information uploaded on the portal contains personally identifiable information (PII) and is stored in encrypted files on an Amazon S3 bucket. The object indexes are saved on an Amazon DynamoDB table.</p><p>Data security is a top priority, and S3 access logs along with AWS CloudTrail have been enabled to track access to S3 objects. To further enhance security, there is a need to verify that data access meets compliance standards and to receive alerts in case of any risk of unauthorized access or inadvertent data leaks.</p><p>Which of the following AWS services enables this?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon Macie</strong> is an ML-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization.</p><p>Amazon Macie continuously monitors data access activity for anomalies and delivers alerts when it detects the risk of unauthorized access or inadvertent data leaks. Amazon Macie has the ability to detect global access permissions inadvertently being set on sensitive data, detect uploading of API keys inside source code, and verify sensitive customer data is being stored and accessed in a manner that meets their compliance standards.</p><p><img src="https://media.tutorialsdojo.com/public/MacieAlerts_2AUG2023.jpg">Hence, the correct answer is:<strong><em> </em>Use Amazon Macie to monitor and detect usage patterns on your S3 data.</strong></p><p>The option that says: <strong>Use AWS Security Hub to aggregate security findings from multiple AWS services</strong> is incorrect because AWS Security Hub simply aggregates security findings from services like Macie and GuardDuty but does not directly monitor or classify sensitive data in S3. While useful for a centralized security overview, it does not proactively identify PII exposure risks.</p><p>The option that says: <strong>Use Amazon GuardDuty to monitor malicious activity on your S3 data </strong>is incorrect. Although GuardDuty can continuously monitor malicious activity and unauthorized behavior in your Amazon S3 bucket, it still is not capable of recognizing sensitive data such as personally identifiable information (PII), which is required in the scenario.</p><p>The option that says: <strong>Use Amazon Inspector to alert you whenever a security violation is detected on your S3 data<em> </em></strong>is incorrect because Inspector is typically an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html ">https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html</a></p><p><a href="https://aws.amazon.com/macie/faq/">https://aws.amazon.com/macie/faq/</a></p><p><a href="https://docs.aws.amazon.com/macie/index.html ">https://docs.aws.amazon.com/macie/index.html</a></p><p><br></p><p><strong>Check out this Amazon Macie Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-macie/?src=udemy">https://tutorialsdojo.com/amazon-macie/</a></p>',
        answers: [
          "<p>Use Amazon Macie to monitor and detect usage patterns on your S3 data.</p>",
          "<p>Use AWS Security Hub to aggregate security findings from multiple AWS services.</p>",
          "<p>Use Amazon GuardDuty to monitor malicious activity on your S3 data.</p>",
          "<p>Use Amazon Inspector to alert you whenever a security violation is detected on your S3 data.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Monitoring and Logging",
      question_plain:
        "A company developed a web portal for gathering Census data within the city. The household information uploaded on the portal contains personally identifiable information (PII) and is stored in encrypted files on an Amazon S3 bucket. The object indexes are saved on an Amazon DynamoDB table.Data security is a top priority, and S3 access logs along with AWS CloudTrail have been enabled to track access to S3 objects. To further enhance security, there is a need to verify that data access meets compliance standards and to receive alerts in case of any risk of unauthorized access or inadvertent data leaks.Which of the following AWS services enables this?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588449,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>You’re running a cluster of EC2 instances on AWS that serves authentication processes and session handling for your new application. After three weeks of operation, your monitoring team flagged your instances with constantly high CPU usage, and the login module response is very slow. Upon checking, it was discovered that your Amazon EC2 instances were hacked and exploited for mining Bitcoins. You have immediately taken down the cluster and created a new one with your custom AMI. You want to have a solution to detect compromised EC2 instances as well as detect malicious activity within your AWS environment. </p><p>Which of the following will help you with this?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon GuardDuty</strong> offers threat detection that enables you to continuously monitor and protect your AWS accounts and workloads. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately. This can include issues like escalations of privileges, uses of exposed credentials, or communication with malicious IPs, URLs, or domains.</p><p><img src="https://media.tutorialsdojo.com/public/product-page-diagram-Amazon-GuardDuty_how-it-works_2AUG2023.png"></p><p>For example, GuardDuty can detect compromised EC2 instances serving malware or mining bitcoin. It also monitors AWS account access behavior for signs of compromise, such as unauthorized infrastructure deployments, like instances deployed in a region that has never been used, or unusual API calls, like a password policy change to reduce password strength.</p><p>Hence, the correct answer is: <strong>Enable Amazon GuardDuty<em>.</em></strong></p><p>The option that says: <strong>Use Amazon Macie </strong>is incorrect because this service is available to protect data stored in Amazon S3 only. It uses machine learning to automatically discover, classify, and protect sensitive data in AWS.</p><p>The option that says: <strong>Use AWS Inspector</strong> is incorrect because this service is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It will not detect possible malicious activity on your instances.</p><p>The option that says: <strong>Enable VPC Flow Logs </strong>is incorrect because this feature only collects traffic logs flowing to your VPC. It does not provide an analysis of the traffic logs.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/guardduty/faqs/ ">https://aws.amazon.com/guardduty/faqs/</a></p><p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html ">https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html</a></p><p><a href="https://aws.amazon.com/guardduty/">https://aws.amazon.com/guardduty/</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-guardduty/?src=udemy">https://tutorialsdojo.com/amazon-guardduty/</a></p>',
        answers: [
          "<p>Use Amazon Macie</p>",
          "<p>Use AWS Inspector</p>",
          "<p>Enable Amazon GuardDuty</p>",
          "<p>Enable VPC Flow Logs</p>",
        ],
      },
      correct_response: ["c"],
      section: "Monitoring and Logging",
      question_plain:
        "You’re running a cluster of EC2 instances on AWS that serves authentication processes and session handling for your new application. After three weeks of operation, your monitoring team flagged your instances with constantly high CPU usage, and the login module response is very slow. Upon checking, it was discovered that your Amazon EC2 instances were hacked and exploited for mining Bitcoins. You have immediately taken down the cluster and created a new one with your custom AMI. You want to have a solution to detect compromised EC2 instances as well as detect malicious activity within your AWS environment. Which of the following will help you with this?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588451,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A custom web dashboard has been developed for the company that displays all instances running on AWS, including the details of each instance. The web application relies on an Amazon DynamoDB table that is updated whenever a new instance is created or terminated. Several auto-scaling groups of Amazon EC2 instances are in use, and there is a need for an effective method to update the DynamoDB table whenever an instance is created or terminated.</p><p>Which of the following steps should be implemented?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Adding lifecycle hooks to your Auto Scaling group gives you greater control over how instances launch and terminate. Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them.</p><p>For example, your newly launched instance completes its startup sequence, and a lifecycle hook pauses the instance. While the instance is in a wait state, you can install or configure software on it, making sure that your instance is fully ready before it starts receiving traffic. For another example of the use of lifecycle hooks, when a scale-in event occurs, the terminating instance is first deregistered from the load balancer (if the Auto Scaling group is being used with Elastic Load Balancing). Then, a lifecycle hook pauses the instance before it is terminated. While the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p><p><img alt="Auto Scaling group - Lifecycle hooks" height="636" src="https://media.tutorialsdojo.com/public/lifecycle_hooks_2AUG2023.png" width="1000"></p><p>After you add lifecycle hooks to your Auto Scaling group, the workflow is shown as follows:</p><ol><li><p>The Auto Scaling group responds to scale-out events by launching instances and scale-in events by terminating instances.</p></li><li><p>The lifecycle hook puts the instance into a wait state (<code>Pending:Wait</code> or <code>Terminating:Wait</code>). The instance is paused until you continue or the timeout period ends.</p></li><li><p>You can perform a custom action using one or more of the following options:</p></li></ol><p>- Define an Amazon EventBridge (CloudWatch Events) target to invoke a Lambda function when a lifecycle action occurs. The Lambda function is invoked when Amazon EC2 Auto Scaling submits an event for a lifecycle action to EventBridge. The event contains information about the instance that is launching or terminating, and a token that you can use to control the lifecycle action.</p><p>- Define a notification target for the lifecycle hook. Amazon EC2 Auto Scaling sends a message to the notification target. The message contains information about the instance that is launching or terminating and a token that you can use to control the lifecycle action.</p><p>- Create a script that runs on the instance as the instance starts. The script can control the lifecycle action using the ID of the instance on which it runs.</p><p>4. By default, the instance remains in a wait state for one hour, and then the Auto Scaling group continues the launch or terminate process (<code>Pending:Proceed</code> or <code>Terminating:Proceed</code>). If you need more time, you can restart the timeout period by recording a heartbeat. If you finish before the timeout period ends, you can complete the lifecycle action, which continues the launch or termination process.</p><p>Hence, the correct answer is: <strong>Configure an Amazon EventBridge target for your auto scaling Group that will trigger an AWS Lambda function when a lifecycle action occurs. Configure the function to update the DynamoDB table with the necessary instance details.</strong></p><p>The option that says: <strong>Create a custom script that runs on the instances that will run on scale-in and scale-out events to update the DynamoDB table with the necessary details </strong>is incorrect. Although this is possible, it will be harder to execute since a custom script needs to be developed first and will be run only at instance creation and termination.</p><p>The option that says: <strong>Define an AWS Lambda function as a notification target for the lifecycle hook for the auto scaling group. In the event of a scale-out or scale-in, the lifecycle hook will trigger the Lambda function to update the DynamoDB table with the necessary details</strong> is incorrect because you cannot use AWS Lambda as a notification target for the lifecycle hook of an Auto Scaling group. You can only configure Amazon EventBridge, Amazon SNS, or Amazon SQS as notification targets.</p><p>The option that says: <strong>Configure an Amazon CloudWatch alarm that will monitor the number of instances on your auto scaling group and trigger an AWS Lambda function to update the DynamoDB table with the necessary details</strong> is incorrect because this option doesn\'t track the lifecycle action of the Auto Scaling group. In this scenario, it\'s better to create a lifecycle hook integrated with Amazon EventBridge and Lambda instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#adding-lifecycle-hooks ">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#adding-lifecycle-hooks</a></p><p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-overview">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-overview</a></p><p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-auto-scaling/?src=udemy">https://tutorialsdojo.com/aws-auto-scaling/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Create a custom script that runs on the instances that will run on scale-in and scale-out events to update the DynamoDB table with the necessary details.</p>",
          "<p>Configure an Amazon EventBridge target for your auto scaling Group that will trigger an AWS Lambda function when a lifecycle action occurs. Configure the function to update the DynamoDB table with the necessary instance details.</p>",
          "<p>Configure an Amazon CloudWatch alarm that will monitor the number of instances on your auto scaling group and trigger an AWS Lambda function to update the DynamoDB table with the necessary details.</p>",
          "<p>Define an AWS Lambda function as a notification target for the lifecycle hook for the auto scaling group. In the event of a scale-out or scale-in, the lifecycle hook will trigger the Lambda function to update the DynamoDB table with the necessary details.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Monitoring and Logging",
      question_plain:
        "A custom web dashboard has been developed for the company that displays all instances running on AWS, including the details of each instance. The web application relies on an Amazon DynamoDB table that is updated whenever a new instance is created or terminated. Several auto-scaling groups of Amazon EC2 instances are in use, and there is a need for an effective method to update the DynamoDB table whenever an instance is created or terminated.Which of the following steps should be implemented?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588453,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          '<p>A business is developing a serverless Angular application in AWS for internal use. As part of the deployment process, the application is built and packaged using AWS CodeBuild and then copied to an S3 bucket.</p><p>The buildspec.yml file includes the subsequent code:</p><pre class="prettyprint linenums">version: 0.2\n phases:\n  install:\n   runtime-versions:\n    nodejs: 14\n  commands:\n   - npm install -g @angular/cli\nbuild:\n commands:\n  - ng build --prod\n post_build:\n  commands:\n   - aws s3 cp dist s3://nueva-ecija-angular-internal --acl \nauthenticated-read\n</pre><p>After deployment, the security team discovered that any individual with an AWS account could access the objects within the S3 bucket despite the application being designed solely for internal purposes.</p><p>Which of the following solutions should the DevOps Engineer implement to resolve the issue in the MOST secure manner?</p>',
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon S3 has a set of predefined groups</strong>. When granting account access to a group, you specify one of our URIs instead of a canonical user ID. AWS provides the following predefined groups:</p><p><strong>-Authenticated Users group</strong> – Represented by /AuthenticatedUsers. This group represents all AWS accounts. Access permission to this group allows any AWS account to access the resource. However, all requests must be signed (authenticated).</p><p><strong>-All Users group</strong> – Represented by /AllUsers. Access permission to this group allows anyone in the world access to the resource. The requests can be signed (authenticated) or unsigned (anonymous). Unsigned requests omit the Authentication header in the request.</p><p><strong>-Log Delivery group</strong> – Represented by /LogDelivery. WRITE permission on a bucket enables this group to write server access logs to the bucket.</p><p><strong>Access control lists (ACLs)</strong> are one of resource-based options that you can use to manage access to your buckets and objects. You can use ACLs to grant basic read/write permissions to other AWS accounts.</p><p>Amazon S3 supports a set of predefined grants known as canned ACLs. Each canned ACL has a predefined set of grantees and permissions. The following table lists the set of canned ACLs and the associated predefined grants.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-canned-acl.png"></p><p>However, there are limits to managing permissions using ACLs. For example, you can grant permissions only to other AWS accounts; you cannot grant permissions to users in your account. You cannot grant conditional permissions, nor can you explicitly deny permissions. ACLs are suitable for specific scenarios. For example, if a bucket owner allows other AWS accounts to upload objects, permissions to these objects can only be managed using object ACL by the AWS account that owns the object.</p><p>In this scenario, removing the <code>--acl authenticated-read</code> will prevent AuthenticatedUsers group (All AWS accounts) from reading the objects in the S3 bucket. This will resolve the issue of anyone with an AWS account being able to access the objects. In addition, attaching a bucket policy that only grants the AWS accounts relevant to the business will make the bucket more secure.</p><p>Hence, the correct answer is: <strong>Add a bucket policy that allows access exclusively to the AWS accounts affiliated with the business. Remove the </strong><code><strong>--acl authenticated-read</strong></code><strong> from the post_build command specified in the buildspec.yml.</strong></p><p>The option that says: <strong>Replace the </strong><code><strong>--acl authenticated-read</strong></code><strong> from the post_build command specified in the buildspec.yml with </strong><code><strong>--acl public-read</strong></code> is incorrect because using the <code>--acl public-read</code> will grant READ access to AllUsers group (anyone in the world). This will not resolve the issue of anyone with an AWS account being able to access the objects.</p><p>The option that says: <strong>Replace the </strong><code><strong>--acl authenticated-read</strong></code><strong> from the post_build command specified in the buildspec.yml with </strong><code><strong>--acl public-read-write</strong></code> is incorrect because using the <code>public-read-write</code> will make the issue much worst because this will grant AllUsers group (anyone in the world) READ and WRITE access. Granting this on a bucket is generally not recommended.</p><p>The option that says: <strong>Modify the post_build command stated in the buildspec.yml by including </strong><code><strong>--sse AES256</strong></code><strong> to encrypt the objects</strong> is incorrect because this will only apply encryption at rest in the objects. It does not resolve the issue of anyone with an AWS account being able to access the objects since <code>--acl authenticated-read</code> still exist in the command.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/acls.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/acls.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/managing-acls.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/managing-acls.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-s3/?src=udemy">https://tutorialsdojo.com/amazon-s3/</a></p>',
        answers: [
          "<p>Replace the <code>--acl authenticated-read</code>from the post_build command specified in the buildspec.yml with <code>--acl public-read</code>.</p>",
          "<p>Add a bucket policy that allows access exclusively to the AWS accounts affiliated with the business. Remove the <code>acl authenticated-read</code> from the post_build command specified in the buildspec.yml.</p>",
          "<p>Replace the <code>--acl authenticated-read</code> from the post_build command specified in the buildspec.yml with <code>--acl public-read-write</code>.</p>",
          "<p>Modify the post_build command stated in the buildspec.yml by including <code>--sse AES256</code> to encrypt the objects.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A business is developing a serverless Angular application in AWS for internal use. As part of the deployment process, the application is built and packaged using AWS CodeBuild and then copied to an S3 bucket.The buildspec.yml file includes the subsequent code:version: 0.2\n phases:\n  install:\n   runtime-versions:\n    nodejs: 14\n  commands:\n   - npm install -g @angular/cli\nbuild:\n commands:\n  - ng build --prod\n post_build:\n  commands:\n   - aws s3 cp dist s3://nueva-ecija-angular-internal --acl \nauthenticated-read\nAfter deployment, the security team discovered that any individual with an AWS account could access the objects within the S3 bucket despite the application being designed solely for internal purposes.Which of the following solutions should the DevOps Engineer implement to resolve the issue in the MOST secure manner?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588455,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>You are developing a store module that lets users choose which plugins they want to activate on your mobile app that is deployed on ECS Fargate. When the cluster was created, the service was launched with the LATEST platform version which is 1.2.0. However, there is a new update on the platform version (1.3.0) that supports a Splunk log driver which you are planning to use. </p><p>What is the recommended way of updating the platform version of the service on the AWS Console?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Fargate</strong> platform versions are used to refer to a specific runtime environment for Fargate task infrastructure. It is a combination of the kernel and container runtime versions. New platform versions are released as the runtime environment evolves, for example, if there are kernel or operating system updates, new features, bug fixes, or security updates. Security updates and patches are deployed automatically for your Fargate tasks. If a security issue is found that affects a platform version, AWS patches the platform version. In some cases, you may be notified that your Fargate tasks have been scheduled for retirement</p><p>You can update a running service to change the number of tasks that are maintained by a service, which task definition is used by the tasks, or if your tasks are using the Fargate launch type, you can change the platform version your service uses. If you have an application that needs more capacity, you can scale up your service. If you have the unused capacity to scale down, you can reduce the number of desired tasks in your service and free up resources.</p><p>If your updated Docker image uses the same tag as what is in the existing task definition for your service (for example, <code>my_image:latest</code>), you do not need to create a new revision of your task definition. You can update your service with your custom configuration, keep the current settings for your service, and select <strong>Force new deployment</strong>. The new tasks launched by the deployment pull the current image/tag combination from your repository when they start. The <strong>Force new deployment</strong> option is also used when updating a Fargate task to use a more current platform version when you specify <code>LATEST</code>. For example, if you specified <code>LATEST</code> that your running tasks are using the <code>1.0.0</code> platform version and you want them to relaunch using a newer platform version.</p><p><img src="https://media.tutorialsdojo.com/public/overview-fargate.png"></p><p>By default, deployments are not forced but you can use the <em>forceNewDeployment</em> request parameter (or the --force-new-deployment parameter if you are using the AWS CLI) to force a new deployment of the service. You can use this option to trigger a new deployment with no service definition changes. For example, you can update a service\'s tasks to use a newer Docker image with the same image/tag combination (my_image:latest) or to roll Fargate tasks onto a newer platform version.</p><p>Hence, the correct answer is: <strong>Update the service on ECS and select “Force new deployment” so that the cluster will be re-deployed with the new platform version.</strong></p><p>The option that says: <strong>Update the service on ECS and select “Redeploy” on the deployment strategy so that the cluster will be re-deployed with the new platform version</strong> is incorrect because there is no "Redeploy" deployment strategy option in ECS.</p><p>The option that says: <strong>Update the task definition with the new platform version ARN so that ECS re-deploys the cluster with the new platform version<em> </em></strong>is incorrect because modifying the task definition will cause a new version to be deployed, but since the ARN or the image revision did not change, no further deployments will be made by ECS.</p><p>The option that says: <strong>Enable the automatic platform version upgrade feature in ECS </strong>is incorrect because there is no such feature in Amazon ECS.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform_versions.html ">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform_versions.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/update-service.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/update-service.html</a></p><p><a href="https://docs.aws.amazon.com/cli/latest/reference/ecs/update-service.html">https://docs.aws.amazon.com/cli/latest/reference/ecs/update-service.html</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p>',
        answers: [
          "<p>Update the service on ECS and select “Force new deployment” so that the cluster will be re-deployed with the new platform version.</p>",
          "<p>Update the service on ECS and select “Redeploy” on the deployment strategy so that the cluster will be re-deployed with the new platform version.</p>",
          "<p>Update the task definition with the new platform version ARN so that ECS re-deploys the cluster with the new platform version.</p>",
          "<p>Enable the automatic platform version upgrade feature in ECS.</p>",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "You are developing a store module that lets users choose which plugins they want to activate on your mobile app that is deployed on ECS Fargate. When the cluster was created, the service was launched with the LATEST platform version which is 1.2.0. However, there is a new update on the platform version (1.3.0) that supports a Splunk log driver which you are planning to use. What is the recommended way of updating the platform version of the service on the AWS Console?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588457,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>You have created a new Elastic Beanstalk environment to be used as a pre-production stage for load testing new code version. Since code changes are committed on a regular basis, you sometimes need to deploy new versions 2 to 3 times each day. You need to deploy a new version as quickly as possible in a cost-effective way to give ample time for the QA team to test it. </p><p>Which of the following implementations is suited for this scenario?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>In ElasticBeanstalk, you can choose from a variety of deployment methods:</p><p><strong>All at once</strong> – Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. This is the method that provides the least amount of time for deployment.</p><p><strong>Rolling</strong> – Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment\'s capacity by the number of instances in a batch.</p><p><strong>Rolling with additional batch</strong> – Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process.</p><p><strong>Immutable</strong> – Deploy the new version to a fresh group of instances by performing an <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html">immutable update</a>.</p><p><strong>Blue/Green</strong> - Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.</p><p><br></p><p>Refer to the table below for the characteristics of each deployment method as well as the amount of time it takes to do the deployment, as seen in the <strong>Deploy Time</strong> column:</p><p><img src="https://media.tutorialsdojo.com/public/DeploymentMethods_2AUG2023.png"></p><p>Hence, the correct answer is:<strong><em> </em>Use </strong><code><strong>All at once</strong></code><strong> as the deployment policy to deploy new versions.</strong></p><p>The option that says: <strong>Use </strong><code><strong>Rolling</strong></code><strong> as the deployment policy to deploy new versions</strong> is incorrect because this deployment type is not as fast as “All at once” deployment since the code is deployed in batches.</p><p>The option that says: <strong>Use </strong><code><strong>Immutable</strong></code><strong> as the deployment policy to deploy code on new instances</strong> is incorrect because this deployment type takes time as new instances are being provisioned during the deployment.</p><p>The option that says: <strong>Implement a blue/green deployment strategy to have the new version ready for quick switching</strong> is incorrect because just like the Immutable deployment, this type also takes time since the new instances are provisioned first before the actual deployment starts. Plus, it incurs additional cost as 2 sets of instances are running at the same time.</p><p><br><strong>References:</strong> <br><br><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>',
        answers: [
          "<p>Use <code>Rolling</code> as the deployment policy to deploy new versions.</p>",
          "<p>Use <code>All at once</code> as deployment policy to deploy new versions.</p>",
          "<p>Use <code>Immutable</code> as the deployment policy to deploy code on new instances.</p>",
          "<p>Implement a blue/green deployment strategy to have the new version ready for quick switching.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "You have created a new Elastic Beanstalk environment to be used as a pre-production stage for load testing new code version. Since code changes are committed on a regular basis, you sometimes need to deploy new versions 2 to 3 times each day. You need to deploy a new version as quickly as possible in a cost-effective way to give ample time for the QA team to test it. Which of the following implementations is suited for this scenario?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588459,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>You are deploying a critical web application with Elastic Beanstalk using the “Rolling” deployment policy. Your Elastic Beanstalk environment configuration has an RDS DB instance attached to it and used by your application servers. The deployment failed when you deployed a major version. And it took even more time to rollback changes because you have to manually redeploy the old version. </p><p>Which of the following options will you implement to prevent this from happening in future deployments?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Immutable</strong> environment updates are an alternative to rolling updates. Immutable environment updates ensure that configuration changes that require replacing instances are applied efficiently and safely. If an immutable environment update fails, the rollback process requires only terminating an Auto Scaling group. A failed rolling update, on the other hand, requires performing an additional rolling update to roll back the changes.</p><p>To perform an immutable environment update, Elastic Beanstalk creates a second, temporary Auto Scaling group behind your environment\'s load balancer to contain the new instances. First, Elastic Beanstalk launches a single instance with the new configuration in the new group. This instance serves traffic alongside all of the instances in the original Auto Scaling group that are running the previous configuration.</p><p>When the first instance passes health checks, Elastic Beanstalk launches additional instances with the new configuration, matching the number of instances running in the original Auto Scaling group. When all of the new instances pass health checks, Elastic Beanstalk transfers them to the original Auto Scaling group, and terminates the temporary Auto Scaling group and old instances.</p><p>Refer to the table below for the characteristics of each deployment method as well as the amount of time it takes to do the deployment, as seen in the <strong>Deploy Time</strong> column:</p><p><img src="https://media.tutorialsdojo.com/public/DeploymentMethods_2AUG2023.png"></p><p>Hence, the correct answer is: <strong>Configure </strong><code><strong>Immutable</strong></code><strong> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</strong></p><p>The option that says: <strong>Configure </strong><code><strong>Rolling with additional batch</strong></code><strong> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application </strong>is incorrect because this deployment type is just similar to rolling deployments and hence, this will not help alleviate the root cause of the issue in this scenario.</p><p>The option that says: <strong>Configure </strong><code><strong>All at once</strong></code><strong> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application<em> </em></strong>is incorrect because this will cause a brief downtime during deployment and hence, this is not ideal for deploying your critical production applications.</p><p>The option that says: <strong>Implement a Blue/green deployment strategy in your Elastic Beanstalk environment for future deployments of your web application. Ensure that the RDS DB instance is still tightly coupled with the environment</strong> is incorrect because a Blue/green deployment requires that your environment runs independently of your production database. This means that you have to decouple your RDS database from your environment. If your Elastic Beanstalk environment has an attached Amazon RDS DB instance, the data will be lost if you terminate the original (blue) environment.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rollingupdates.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rollingupdates.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>',
        answers: [
          "<p>Configure <code>Rolling with additional batch</code> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</p>",
          "<p>Configure <code>All at once</code> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</p>",
          "<p>Configure <code>Immutable</code> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</p>",
          "<p>Implement a Blue/green deployment strategy in your Elastic Beanstalk environment for future deployments of your web application. Ensure that the RDS DB instance is still tightly coupled with the environment.</p>",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "You are deploying a critical web application with Elastic Beanstalk using the “Rolling” deployment policy. Your Elastic Beanstalk environment configuration has an RDS DB instance attached to it and used by your application servers. The deployment failed when you deployed a major version. And it took even more time to rollback changes because you have to manually redeploy the old version. Which of the following options will you implement to prevent this from happening in future deployments?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588461,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading e-commerce company has a payment portal that handles the payment and refund transactions of its online platform. The portal is hosted in an Auto Scaling group of On-Demand Amazon EC2 instances across three multiple Availability Zones in the US West (N California) region. There is a new requirement to improve the system monitoring of the application as well to track the number of payment and refund transactions being done every minute. The DevOps team should also be notified if this metric breaches the specified threshold.</p><p>Which of the following options provides the MOST cost-effective and automated solution that will satisfy the above requirement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Metrics are data about the performance of your systems. By default, several services provide free metrics for resources (such as Amazon EC2 instances, Amazon EBS volumes, and Amazon RDS DB instances). You can also enable detailed monitoring for some resources, such as your Amazon EC2 instances, or publish your own application metrics. Amazon CloudWatch can load all the metrics in your account (both AWS resource metrics and application metrics that you provide) for search, graphing, and alarms.</p><p>Metric data is kept for 15 months, enabling you to view both up-to-the-minute data and historical data. You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console.</p><p>CloudWatch stores data about a metric as a series of data points. Each data point has an associated timestamp. You can even publish an aggregated set of data points called a <em>statistic set</em>.</p><p><img src="https://media.tutorialsdojo.com/public/CW-Overview_6AUG2023.png">You can aggregate your data before you publish it to CloudWatch. When you have multiple data points per minute, aggregating data minimizes the number of calls to <strong>put-metric-data</strong>. For example, instead of calling <strong>put-metric-data</strong> multiple times for three data points that are within 3 seconds of each other, you can aggregate the data into a statistic set that you publish with one call, using the <code>--statistic-values</code> parameter.</p><p>Hence, the correct answer is: <strong>Configure the instances to push the number of payments and refund transactions to Amazon CloudWatch as a custom metric. Set up a CloudWatch alarm to notify the DevOps team using Amazon SNS when the threshold is breached. View statistical graphs of your published metrics with the AWS Management Console.</strong></p><p>The option that says: <strong>Set up an ELK Stack in AWS using Amazon OpenSearch service for log processing. Store the payments and refund transactions in each instance and configure Logstash to send the logs to OpenSearch. Set up a Kibana dashboard to view the data and the metric graphs</strong> is incorrect. Although this solution may work, it entails a lot of effort to set up and costs more to maintain. A more cost-effective solution is to primarily use custom metrics in CloudWatch.</p><p>The option that says: <strong>Configure the instances to push the number of payments and refund transactions to Amazon CloudWatch Logs as a custom metric. Develop a custom monitoring application using a Python-based Flask web application to view the metrics and host it in an EC2 instance</strong> is incorrect because it will typically take you a considerable amount of time to set up the Flask web app. Running this solution also entails an added monthly cost.</p><p>The option that says: <strong>Configure the instances to push the entire log of each payment and refund transactions to Amazon EventBridge as a custom metric. Set up a CloudWatch alarm to notify the DevOps team using Amazon SNS when the threshold is breached. View statistical graphs of your published metrics with the AWS Management Console</strong> is incorrect because you can\'t push a custom metric using Amazon EventBridge.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>',
        answers: [
          "<p>Set up an ELK Stack in AWS using Amazon OpenSearch service for log processing. Store the payments and refund transactions in each instance and configure Logstash to send the logs to OpenSearch. Set up a Kibana dashboard to view the data and the metric graphs.</p>",
          "<p>Configure the instances to push the number of payments and refund transactions to Amazon CloudWatch as a custom metric. Set up a CloudWatch alarm to notify the DevOps team using Amazon SNS when the threshold is breached. View statistical graphs of your published metrics with the AWS Management Console.</p>",
          "<p>Configure the instances to push the number of payments and refund transactions to Amazon CloudWatch Logs as a custom metric. Develop a custom monitoring application using a Python-based Flask web application to view the metrics and host it in an EC2 instance.</p>",
          "<p>Configure the instances to push the entire log of each payment and refund transactions to Amazon EventBridge as a custom metric. Set up a CloudWatch alarm to notify the DevOps team using Amazon SNS when the threshold is breached. View statistical graphs of your published metrics with the AWS Management Console.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Monitoring and Logging",
      question_plain:
        "A leading e-commerce company has a payment portal that handles the payment and refund transactions of its online platform. The portal is hosted in an Auto Scaling group of On-Demand Amazon EC2 instances across three multiple Availability Zones in the US West (N California) region. There is a new requirement to improve the system monitoring of the application as well to track the number of payment and refund transactions being done every minute. The DevOps team should also be notified if this metric breaches the specified threshold.Which of the following options provides the MOST cost-effective and automated solution that will satisfy the above requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588463,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading telecommunications company is migrating a multi-tier enterprise application to AWS, which must be hosted on a single Amazon EC2 Dedicated Instance with an Elastic Fabric Adapter (EFA) and instance store volumes. The app cannot use Auto Scaling due to server licensing constraints.</p><p>For its database tier, Amazon Aurora will be used to store the application's data and transactions. Automatic recovery must be configured to ensure high availability even in the event of EC2 or Aurora database outages.</p><p>Which of the following options provides the MOST cost-effective solution for this migration task?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group.</p><p>When the <code>StatusCheckFailed_System</code> alarm is triggered, and the recovery action is initiated, you will be notified by the Amazon SNS topic that you selected when you created the alarm and associated the recovery action. During instance recovery, the instance is migrated during an instance reboot, and any in-memory data is lost. When the process is complete, information is published to the SNS topic you\'ve configured for the alarm. Anyone subscribed to this SNS topic will receive an email notification that includes the status of the recovery attempt and any further instructions. You will notice an instance reboot on the recovered instance.<br>Examples of problems that cause system status checks to fail include:</p><p>- Loss of network connectivity</p><p>- Loss of system power</p><p>- Software issues on the physical host</p><p>- Hardware issues on the physical host that impact network reachability</p><p>If your instance has a public IPv4 address, it retains the public IPv4 address after recovery.</p><p><img alt="Amazon EventBridge" height="593" src="https://media.tutorialsdojo.com/aws_eventbridge_13AUG2023.png" width="1000"></p><p>You can configure a CloudWatch alarm to automatically recover impaired EC2 instances and notify you through Amazon SNS. However, the SNS notification doesn\'t include the results of the automatic recovery action.</p><p>You must also configure an Amazon EventBridge rule to monitor AWS Personal Health Dashboard (AWS Health) events for your instance. Then, you are notified of the results of automatic recovery actions, for example.</p><p>While Amazon Aurora Read Replicas incur additional costs because each replica is a separate database instance, they are necessary to provide high availability and failover capability, critical for enterprise applications requiring auto-healing and minimal downtime.</p><p>Hence, the correct answer is: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Set up an Amazon EventBridge rule to trigger an AWS Lambda function to start a new EC2 instance in an available Availability Zone when the instance status reaches a failure state. Configure an Aurora database with a Read Replica in the other Availability Zone. In the event that the primary database instance fails, promote the read replica to a primary database instance.</strong></p><p>The option that says: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch a single-instance Aurora database. Create an AWS Config conformance pack containing the Config rules and remediation actions to automatically recover the EC2 instance during system outages</strong> is incorrect because launching a single-instance Aurora database is simply not a highly available architecture. You have to set up at least a Read Replica that you can configure to be the new primary instance during outages. In addition, AWS Config rules alone cannot recover your EC2 instances automatically. This must be integrated with the AWS Systems Manager Automation first.</p><p>The option that says: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Set up an EC2 instance and enable the built-in instance recovery feature. Create an Aurora database with a Read Replica on the other Availability Zone. Promote the replica as the primary in the event that the primary database instance fails</strong> is incorrect because the built-in instance recovery failure feature for Amazon EC2 doesn\'t apply to instances that use Elastic Fabric Adapter (EFA) and instance store volumes. The scenario explicitly mentioned that the application uses a Dedicated Instance with both EFA and instance store volumes, so EC2 auto-recovery won\'t take place. You have to use a combination of Amazon EventBridge and a Lambda function to recover the EC2 instance from failure automatically.</p><p>The option that says: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch an Elastic IP address and attach it to the dedicated instance. Set up a second EC2 instance in the other Availability Zone. Create an Amazon EventBridge rule to trigger an AWS Lambda function to move the EIP to the second instance when the first instance fails. Set up a single-instance Aurora database</strong> is incorrect because setting up a second EC2 instance in another Availability Zone will only entail an additional cost. Launching a single-instance Aurora database is not a highly available architecture as well.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html</a></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-sns-ec2-automatic-recovery/">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-sns-ec2-automatic-recovery/</a></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/automatic-recovery-ec2-cloudwatch/">https://aws.amazon.com/premiumsupport/knowledge-center/automatic-recovery-ec2-cloudwatch/</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>',
        answers: [
          "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch a single-instance Aurora database. Create an AWS Config conformance pack containing the Config rules and remediation actions to automatically recover the EC2 instance during system outages.</p>",
          "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch a single-instance Aurora database. Create an AWS Config conformance pack containing the Config rules and remediation actions to automatically recover the EC2 instance during system outages.</p>",
          "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Set up an Amazon EventBridge rule to trigger an AWS Lambda function to start a new EC2 instance in an available Availability Zone when the instance status reaches a failure state. Configure an Aurora database with a Read Replica in the other Availability Zone. In the event that the primary database instance fails, promote the read replica to a primary database instance.</p>",
          "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch an Elastic IP address and attach it to the dedicated instance. Set up a second EC2 instance in the other Availability Zone. Create an Amazon EventBridge rule to trigger an AWS Lambda function to move the EIP to the second instance when the first instance fails. Set up a single-instance Aurora database.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Monitoring and Logging",
      question_plain:
        "A leading telecommunications company is migrating a multi-tier enterprise application to AWS, which must be hosted on a single Amazon EC2 Dedicated Instance with an Elastic Fabric Adapter (EFA) and instance store volumes. The app cannot use Auto Scaling due to server licensing constraints.For its database tier, Amazon Aurora will be used to store the application's data and transactions. Automatic recovery must be configured to ensure high availability even in the event of EC2 or Aurora database outages.Which of the following options provides the MOST cost-effective solution for this migration task?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588465,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>An insurance firm has recently undergone digital transformation and AWS cloud adoption. Its app development team has four environments, namely DEV, TEST, PRE-PROD, and PROD, for its flagship application that is configured with AWS CodePipeline. After several weeks, they noticed that there were several outages caused by misconfigured files or faulty code blocks that were deployed into the PROD environment. A DevOps Engineer has been assigned to add the required steps to identify issues in the application before it is released. </p><p>Which of the following is the MOST appropriate combination of steps that the Engineer should implement to identify functional issues during the deployment process? (Select TWO.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Continuous delivery is a release practice in which code changes are automatically built, tested, and prepared for release to production. With AWS CloudFormation and CodePipeline, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. This release process lets you rapidly and reliably make changes to your AWS infrastructure.</p><p>In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p><img src="https://media.tutorialsdojo.com/public/PipelineFlow.png"></p><p>If the action is approved, the pipeline execution resumes. If the action is rejected—or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping — the result is the same as an action failing, and the pipeline execution does not continue.</p><p>Hence, the correct answers are:</p><p><strong>- Add a test action to the pipeline to run both the unit and functional tests using AWS CodeBuild. Verify that the test results passed before deploying the new application revision to the PROD environment.</strong></p><p><strong>- In the pipeline, add an AWS CodeDeploy action to deploy the latest version of the application to the PRE-PROD environment. Set up a manual approval action in the pipeline so that the QA team can perform the required tests. Add another CodeDeploy action that deploys the verified code to the PROD environment after the manual approval action.</strong></p><p>The option that says: <strong>Add a test action that uses Amazon Inspector to the pipeline. Run an assessment using the Runtime Behavior Analysis rules package to verify that the deployed code complies with the strict security standards of the company before deploying it to the PROD environment</strong> is incorrect because Amazon inspector just checks the security vulnerabilities of the EC2 instances and not the application functionality itself.</p><p>The option that says: <strong>Add a test action that uses Amazon GuardDuty to the pipeline. Run an assessment using the Runtime Behavior Analysis rules package to verify that the deployed code complies with the strict security standards of the company before deploying it to the PROD environment</strong> is incorrect because there is no Runtime Behavior Analysis rules package in Amazon GuardDuty.</p><p>The option that says: <strong>Add a test action that uses Amazon Macie to the pipeline. Run an assessment using the Runtime Behavior Analysis rules package to verify that the deployed code complies with the strict security standards of the company before deploying it to the PROD environment</strong> is incorrect because Amazon Macie focuses on discovering and protecting sensitive data within AWS, not on running functional tests for application code. Moreover, there is no Runtime Behavior Analysis rules package in Amazon Macie.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/best-practices.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/best-practices.html</a></p><p><br></p><p><strong>Check out these AWS CloudFormation and CodePipeline Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p>',
        answers: [
          "<p>Add a test action that uses Amazon Inspector to the pipeline. Run an assessment using the Runtime Behavior Analysis rules package to verify that the deployed code complies with the strict security standards of the company before deploying it to the PROD environment.</p>",
          "<p>Add a test action to the pipeline to run both the unit and functional tests using AWS CodeBuild. Verify that the test results passed before deploying the new application revision to the PROD environment.</p>",
          "<p>Add a test action that uses Amazon GuardDuty to the pipeline. Run an assessment using the Runtime Behavior Analysis rules package to verify that the deployed code complies with the strict security standards of the company before deploying it to the PROD environment.</p>",
          "<p>Add a test action that uses Amazon Macie to the pipeline. Run an assessment using the Runtime Behavior Analysis rules package to verify that the deployed code complies with the strict security standards of the company before deploying it to the PROD environment.</p>",
          "<p>In the pipeline, add an AWS CodeDeploy action to deploy the latest version of the application to the PRE-PROD environment. Set up a manual approval action in the pipeline so that the QA team can perform the required tests. Add another CodeDeploy action that deploys the verified code to the PROD environment after the manual approval action.</p>",
        ],
      },
      correct_response: ["b", "e"],
      section: "SDLC Automation",
      question_plain:
        "An insurance firm has recently undergone digital transformation and AWS cloud adoption. Its app development team has four environments, namely DEV, TEST, PRE-PROD, and PROD, for its flagship application that is configured with AWS CodePipeline. After several weeks, they noticed that there were several outages caused by misconfigured files or faulty code blocks that were deployed into the PROD environment. A DevOps Engineer has been assigned to add the required steps to identify issues in the application before it is released. Which of the following is the MOST appropriate combination of steps that the Engineer should implement to identify functional issues during the deployment process? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588467,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is planning to launch its Node.js application to AWS to better serve its clients around the globe. A hybrid deployment is required to be implemented wherein the application will run on both on-premises application servers and On-Demand Amazon EC2 instances. The application instances require secure access to database credentials, which must be encrypted both at rest and in transit. </p><p>As a DevOps Engineer, how can you automate the deployment process of the application in the MOST secure manner?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Systems Manager Parameter Store</strong> provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Highly scalable, available, and durable, Parameter Store is backed by the AWS Cloud.</p><p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants <code>AssumeRole</code> trust to the Systems Manager service. You only need to create the service role for a hybrid environment once for each AWS account.</p><p>Users in your company or organization who will use Systems Manager on your hybrid machines must be granted permission in IAM to call the SSM API.</p><p><img src="https://media.tutorialsdojo.com/public/getParamsbypath_6AUG2023.png"></p><p>You can use CodeDeploy to deploy to both Amazon EC2 instances and on-premises instances. An on-premises instance is any physical device that is not an Amazon EC2 instance that can run the CodeDeploy agent and connect to public AWS service endpoints. You can use CodeDeploy to simultaneously deploy an application to Amazon EC2 instances in the cloud and to desktop PCs in your office or servers in your own data center.</p><p><img src="https://media.tutorialsdojo.com/public/CodeDeploy-6AUG2023.png"></p><p>To register an on-premises instance, you must use an IAM identity to authenticate your requests. You can choose from the following options for the IAM identity and registration method you use:</p><p>- Use an <strong>IAM User</strong> ARN to authenticate requests</p><p>- Use an <strong>IAM Role</strong> ARN to authenticate requests</p><p>For maximum control over the authentication and registration of your on-premises instances, you can use the <code>register-on-premises-instance</code> command and periodically refreshed temporary credentials generated with the AWS Security Token Service (AWS STS). A static IAM role for the instance assumes the role of these refreshed AWS STS credentials to perform CodeDeploy deployment operations. This method is most useful when you need to register a large number of instances. It allows you to automate the registration process with CodeDeploy. You can use your own identity and authentication system to authenticate on-premises instances and distribute IAM session credentials from the service to the instances for use with CodeDeploy.</p><p>Take note you cannot directly attach an IAM Role to your on-premises servers. You have to set up your on-premises servers as "on-premises instances" in CodeDeploy with a static IAM Role that your servers can assume.</p><p>Hence, the correct answer is: <strong>Using AWS Systems Manager Parameter Store, upload and manage the database credentials with a Secure String data type. Create an IAM role with an attached policy that allows access and decryption of the database credentials. Attach this role to the instance profile of the CodeDeploy-managed instances as well as to the on-premises instances using the </strong><code><strong>register-on-premises-instance</strong></code><strong> command. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy.</strong></p><p>The option that says: <strong>Using AWS Systems Manager Parameter Store, upload and manage the database credentials with a Secure String data type. Create an IAM role that allows access and decryption of the database credentials. Associate this role to all the Amazon EC2 instances. Upload the application in AWS Elastic Beanstalk with a Node.js platform configuration and deploy the application revisions to both on-premises servers and EC2 instances using blue/green deployment </strong>is incorrect. You can\'t deploy an application to your on-premises servers using Elastic Beanstalk. This is only applicable to your Amazon EC2 instances.</p><p>The option that says: <strong>Using AWS Systems Manager Parameter Store, upload and manage the database credentials with a Secure String data type. Create an IAM policy that allows access and decryption of the database credentials. Attach the IAM policy to the instance profile for CodeDeploy-managed instances as well as to the on-premises instances using the </strong><code><strong>register-on-premises-instance</strong></code><strong> command. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy</strong> is incorrect. You have to use an IAM Role and not an IAM Policy to grant access to AWS Systems Manager Parameter Store. Alternatively, you can use your IAM User credentials too, but this method isn\'t secure.</p><p>The option that says: <strong>Store database credentials in the </strong><code><strong>appspec.yml</strong></code><strong> configuration file. Create an IAM policy for allowing access to only the database credentials. Attach the IAM policy to the role associated with the instance profile for CodeDeploy-managed instances and the IAM role used for on-premises instances registration on CodeDeploy. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy</strong> is incorrect. It is not secure to store sensitive database credentials in an <code><strong><em>appspec.yml</em></strong></code><strong><em> </em></strong>configuration file. You have to use AWS Systems Manager Parameter Store instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/on-premises-instances-register.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/on-premises-instances-register.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>Using AWS Systems Manager Parameter Store, upload and manage the database credentials with a Secure String data type. Create an IAM role that allows access and decryption of the database credentials. Associate this role to all the Amazon EC2 instances. Upload the application in AWS Elastic Beanstalk with a Node.js platform configuration and deploy the application revisions to both on-premises servers and EC2 instances using blue/green deployment.</p>",
          "<p>Using AWS Systems Manager Parameter Store, upload and manage the database credentials with a Secure String data type. Create an IAM Policy that allows access and decryption of the database credentials. Attach the IAM policy to the instance profile for CodeDeploy-managed instances as well as to the on-premises instances using the <code>register-on-premises-instance</code> command. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy.</p>",
          "<p>Using AWS Systems Manager Parameter Store, upload and manage the database credentials with a Secure String data type. Create an IAM role with an attached policy that allows access and decryption of the database credentials. Attach this role to the instance profile of the CodeDeploy-managed instances as well as to the on-premises instances using the <code>register-on-premises-instance</code> command. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy.</p>",
          "<p>Store database credentials in the <code>appspec.yml</code> configuration file. Create an IAM policy for allowing access to only the database credentials. Attach the IAM policy to the role associated with the instance profile for CodeDeploy-managed instances and the IAM role used for on-premises instances registration on CodeDeploy. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company is planning to launch its Node.js application to AWS to better serve its clients around the globe. A hybrid deployment is required to be implemented wherein the application will run on both on-premises application servers and On-Demand Amazon EC2 instances. The application instances require secure access to database credentials, which must be encrypted both at rest and in transit. As a DevOps Engineer, how can you automate the deployment process of the application in the MOST secure manner?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588469,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An international tours and travel company is planning to launch a multi-tier Node.js web portal with a MySQL database to AWS. The portal must be highly available during the deployment of new portal versions in the future and have the ability to roll back the changes if necessary, to improve user experience. There are third-party applications that will also use the same MySQL database that the portal is using. The architecture should allow the IT Operations team to centrally view all the server logs from various EC2 instances and store the data for three months. It should also have a feature that allows the team to search and filter server logs in near-real-time for monitoring purposes. The solution should be cost-effective and preferably has less operational overhead. </p><p>As a DevOps Engineer, which of the following is the BEST solution that you should implement to satisfy the above requirements?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p><p>Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as Amazon EC2 instances, to run your application.</p><p>You can interact with Elastic Beanstalk by using the AWS Management Console, the AWS Command Line Interface (AWS CLI), or <strong>eb</strong>, a high-level CLI designed specifically for Elastic Beanstalk.</p><p><img src="https://media.tutorialsdojo.com/public/aeb-architecture2.png"></p><p>An Amazon RDS instance attached to an Elastic Beanstalk environment is ideal for development and testing environments. However, it\'s not ideal for production environments because the lifecycle of the database instance is tied to the lifecycle of your application environment. If you terminate the environment, then you will lose your data because the Amazon RDS instance is deleted by the environment.</p><p>Hence, the correct answer is: <strong>Using AWS Elastic Beanstalk, host the multi-tier Node.js web portal in a load-balancing and autoscaling environment. Set up an Amazon RDS MySQL database with a Multi-AZ deployments configuration that is decoupled from the Elastic Beanstalk stack. Set the log options to stream the application logs to Amazon CloudWatch Logs with a 90-day retention.</strong></p><p>The option that says: <strong>Using AWS Elastic Beanstalk, host the multi-tier Node.js web portal in a load-balancing and autoscaling environment. Set up an Amazon RDS MySQL instance with a Multi-AZ deployments configuration within the Elastic Beanstalk stack. Set the log options to stream the application logs to Amazon CloudWatch Logs with 90-day data retention</strong> is incorrect because it\'s not ideal to place the database within the Elastic Beanstalk environment because the lifecycle of the database instance is tied to the lifecycle of your application environment in production.</p><p>The option that says: <strong>Host the multi-tier Node.js web portal in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Set up an Amazon RDS MySQL database instance. Configure the CloudWatch Log agent to send the application logs to Amazon CloudWatch Logs with 90-day data retention</strong> is incorrect because it is easier to upload the web portal to Elastic Beanstalk instead. Moreover, you have to enable Multi-AZ deployments in RDS to improve the availability of your database tier.</p><p>The option that says: <strong>Host the multi-tier Node.js web portal in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Set up an Amazon RDS MySQL database instance. For log monitoring and analysis, configure CloudWatch Logs to fetch the application logs from the instances and send them to an Amazon ES cluster. Purge the contents of the Amazon ES domain every 90 days and then recreate it again</strong> is incorrect. Although this solution may work, it entails a lot of operational overhead to maintain the Amazon ES cluster.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>',
        answers: [
          "<p>Using AWS Elastic Beanstalk, host the multi-tier Node.js web portal in a load-balancing and autoscaling environment. Set up an Amazon RDS MySQL instance with a Multi-AZ deployments configuration within the Elastic Beanstalk stack. Set the log options to stream the application logs to Amazon CloudWatch Logs with 90-day data retention.</p>",
          "<p>Host the multi-tier Node.js web portal in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Set up an Amazon RDS MySQL database instance. Configure the CloudWatch Log agent to send the application logs to Amazon CloudWatch Logs with 90-day data retention.</p>",
          "<p>Host the multi-tier Node.js web portal in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Set up an Amazon RDS MySQL database instance. For log monitoring and analysis, configure CloudWatch Logs to fetch the application logs from the instances and send them to an Amazon ES cluster. Purge the contents of the Amazon ES domain every 90 days and then recreate it again.</p>",
          "<p>Using AWS Elastic Beanstalk, host the multi-tier Node.js web portal in a load-balancing and autoscaling environment. Set up an Amazon RDS MySQL database with a Multi-AZ deployments configuration that is decoupled from the Elastic Beanstalk stack. Set the log options to stream the application logs to Amazon CloudWatch Logs with 90-day retention.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "An international tours and travel company is planning to launch a multi-tier Node.js web portal with a MySQL database to AWS. The portal must be highly available during the deployment of new portal versions in the future and have the ability to roll back the changes if necessary, to improve user experience. There are third-party applications that will also use the same MySQL database that the portal is using. The architecture should allow the IT Operations team to centrally view all the server logs from various EC2 instances and store the data for three months. It should also have a feature that allows the team to search and filter server logs in near-real-time for monitoring purposes. The solution should be cost-effective and preferably has less operational overhead. As a DevOps Engineer, which of the following is the BEST solution that you should implement to satisfy the above requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588471,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading digital consultancy company has two teams in its IT department: the DevOps team and the Security team, that are working together on different components of its cloud architecture. AWS CloudFormation is used to manage its resources across all of its AWS accounts, including AWS Config for configuration management. The Security team applies the operating system-level updates and patches while the DevOps team manages application-level dependencies and updates. The DevOps team must use the latest AMI when launching new EC2 instances and deploying its flagship application. </p><p>Which of the following options is the MOST scalable method for integrating the two processes and teams?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Dynamic references</strong> provide a compact, powerful way for you to specify external values that are stored and managed in other services, such as the Systems Manager Parameter Store, in your stack templates. When you use a dynamic reference, CloudFormation retrieves the value of the specified reference when necessary during stack and change set operations.</p><p>CloudFormation currently supports the following dynamic reference patterns:</p><p>- ssm, for plaintext values stored in AWS Systems Manager Parameter Store</p><p>- ssm-secure, for secure strings stored in AWS Systems Manager Parameter Store</p><p>- secretsmanager, for entire secrets or specific secret values that are stored in AWS Secrets Manager</p><p><img src="https://media.tutorialsdojo.com/public/AWS-Systems-Manager-Parameter-Store_6AUG2023.png"></p><p>Some considerations when using dynamic references:</p><p>- You can include up to 60 dynamic references in a stack template.</p><p>- For transforms, such as AWS::Include and AWS::Serverless, AWS CloudFormation does not resolve dynamic references prior to invoking any transforms. Rather, AWS CloudFormation passes the literal string of the dynamic reference to the transform. Dynamic references (including those inserted into the processed template as the result of a transform) are resolved when you execute the change set using the template.</p><p>- Dynamic references for secure values, such as <code>ssm-secure</code> and <code>secretsmanager</code>, are not currently supported in custom resources.</p><p>Hence, the correct answer is: <strong>Instruct the Security team to set up an AWS CloudFormation stack that creates an AWS CodePipeline pipeline that builds new Amazon Machine Images. Then, store the AMI ARNs as parameters in AWS Systems Manager Parameter Store as part of the pipeline output. Order the DevOps team to use the </strong><code><strong>AWS::SSM::Parameter</strong></code><strong> section in their CloudFormation stack to obtain the most recent AMI ARN from the Parameter Store.</strong></p><p>The option that says: <strong>Instruct the Security team to set up an AWS CloudFormation template that creates new versions of their AMIs and lists the Amazon Resource names (ARNs) of the AMIs in an encrypted S3 object as part of the stack output section. Direct the DevOps team to use the cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs </strong>is incorrect because it is better to store the parameters in AWS Systems Manager Parameter Store.</p><p>The option that says: <strong>Instruct the Security team to maintain a nested stack in AWS CloudFormation that includes both the OS and the templates from the DevOps team. Order the Security team to use the stack update action to deploy updates to the application stack whenever the DevOps team changes the application code</strong> is incorrect because using a nested stack will not decouple the responsibility of the two teams. Integrating AWS Systems Manager Parameter Store to store the ARN of the AMIs is a better solution.</p><p>The option that says: <strong>Instruct the Security team to use a CloudFormation stack that launches an AWS CodePipeline pipeline that builds new AMIs, then store the latest AMI ARNs in an encrypted S3 object as part of the pipeline output. Order the DevOps team to use a cross-stack reference within their own CloudFormation template to get that S3 object location and obtain the most recent AMI ARNs to use when deploying their application</strong> is incorrect. Although this is a valid solution, it entails a lot of effort to set up a cross-stack reference within the DevOps team\'s own CloudFormation template to get that S3 object location and obtain the most recent AMI ARNs. You also have to ensure that the AMI ARN on Amazon S3 is the latest one.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html</a></p><p><a href="https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p>',
        answers: [
          "<p>Instruct the Security team to set up an AWS CloudFormation template that creates new versions of their AMIs and lists the Amazon Resource names (ARNs) of the AMIs in an encrypted S3 object as part of the stack output section. Direct the DevOps team to use the cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs.</p>",
          "<p>Instruct the Security team to set up an AWS CloudFormation stack that creates an AWS CodePipeline pipeline that builds new Amazon Machine Images. Then, store the AMI ARNs as parameters in AWS Systems Manager Parameter Store as part of the pipeline output. Order the DevOps team to use the <code>AWS::SSM::Parameter</code> section in their CloudFormation stack to obtain the most recent AMI ARN from the Parameter Store.</p>",
          "<p>Instruct the Security team to maintain a nested stack in AWS CloudFormation that includes both the OS and the templates from the DevOps team. Order the Security team to use the stack update action to deploy updates to the application stack whenever the DevOps team changes the application code.</p>",
          "<p>Instruct the Security team to use a CloudFormation stack that launches an AWS CodePipeline pipeline that builds new AMIs then store the latest AMI ARNs in an encrypted S3 object as part of the pipeline output. Order the DevOps team to use a cross-stack reference within their own CloudFormation template to get that S3 object location and obtain the most recent AMI ARNs to use when deploying their application.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A leading digital consultancy company has two teams in its IT department: the DevOps team and the Security team, that are working together on different components of its cloud architecture. AWS CloudFormation is used to manage its resources across all of its AWS accounts, including AWS Config for configuration management. The Security team applies the operating system-level updates and patches while the DevOps team manages application-level dependencies and updates. The DevOps team must use the latest AMI when launching new EC2 instances and deploying its flagship application. Which of the following options is the MOST scalable method for integrating the two processes and teams?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588473,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A multinational corporation has multiple AWS accounts that are consolidated using AWS Organizations. A new system should be configured for security purposes to detect suspicious activities in any of its accounts, such as SSH brute force attacks or compromised Amazon EC2 instances that serve malware. All gathered information must be centrally stored in its dedicated security account for audit purposes, and the events should be stored in an Amazon S3 bucket.</p><p>Which solution should a DevOps Engineer implement to meet this requirement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon GuardDuty</strong> is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. The cloud simplifies the collection and aggregation of account and network activities. Still, it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in the AWS Cloud. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats. GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs. With a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with AWS EventBridge, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.</p><p><img alt="Amazon GuardDuty" height="378" src="https://media.tutorialsdojo.com/public/product-page-diagram-Amazon-GuardDuty_how-it-works_2AUG2023.png" width="1000"></p><p>GuardDuty enables and manages across multiple accounts efficiently. All member account findings can be aggregated through the multi-account feature with a GuardDuty administrator account. This allows the security team to manage all GuardDuty findings across the organization in one account. The aggregated findings are also available through EventBridge, making integration with an existing enterprise event management system easy.</p><p>Hence, the correct answer is: <strong>Automatically detect SSH brute force or malware attacks by enabling Amazon GuardDuty in every account. Configure the security account as the GuardDuty Administrator for all member accounts in the organization. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</strong></p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by enabling Amazon Macie in every account. Set up the security account as the Macie Administrator for every member account of the organization. Create an Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, which should push the findings to an S3 bucket</strong> is incorrect because you have to use Amazon GuardDuty instead of Amazon Macie. Note that Amazon Macie cannot detect SSH brute force or malware attacks.</p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by only enabling Amazon Macie in the security account. Configure the security account as the Macie Administrator for every member account. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket</strong> is incorrect because you don\'t need to create a custom shell script in Lambda or use Kinesis Data Streams. You can just configure the EventBridge rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by only enabling Amazon GuardDuty in the security account. Set up the security account as the GuardDuty Administrator for every member account. Create a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket </strong>is incorrect. Although using Amazon GuardDuty in this scenario is valid, the implementation for storing the findings is incorrect. You can typically configure the EventBridge rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/guardduty/">https://aws.amazon.com/guardduty/</a></p><p><a href="https://aws.amazon.com/blogs/security/how-to-manage-amazon-guardduty-security-findings-across-multiple-accounts/">https://aws.amazon.com/blogs/security/how-to-manage-amazon-guardduty-security-findings-across-multiple-accounts/</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-guardduty/?src=udemy">https://tutorialsdojo.com/amazon-guardduty/</a></p>',
        answers: [
          "<p>Automatically detect SSH brute force or malware attacks by enabling Amazon Macie in every account. Set up the security account as the Macie Administrator for every member account of the organization. Create an Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, which should push the findings to an S3 bucket.</p>",
          "<p>Automatically detect SSH brute force or malware attacks by only enabling Amazon Macie in the security account. Configure the security account as the Macie Administrator for every member account. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket.</p>",
          "<p>Automatically detect SSH brute force or malware attacks by enabling Amazon GuardDuty in every account. Configure the security account as the GuardDuty Administrator for all member accounts in the organization. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p>",
          "<p>Automatically detect SSH brute force or malware attacks by only enabling Amazon GuardDuty in the security account. Set up the security account as the GuardDuty Administrator for every member account. Create a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Monitoring and Logging",
      question_plain:
        "A multinational corporation has multiple AWS accounts that are consolidated using AWS Organizations. A new system should be configured for security purposes to detect suspicious activities in any of its accounts, such as SSH brute force attacks or compromised Amazon EC2 instances that serve malware. All gathered information must be centrally stored in its dedicated security account for audit purposes, and the events should be stored in an Amazon S3 bucket.Which solution should a DevOps Engineer implement to meet this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588475,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An organization runs its web application on EC2 instances within an Auto Scaling group. The EC2 instances is behind an Application Load Balancer and is deployed across multiple Availability Zones. The developers of the organization have introduced fresh features to the web application, but require to be tested before implementation to prevent any interruptions. The organization requires that the deployment strategy should:</p><ul><li><p>Deploy a duplicate fleet of instances with an equivalent capacity to the primary fleet.</p></li><li><p>Keep the original fleet unaltered while the secondary fleet is being launched.</p></li><li><p>Shift traffic to the secondary fleet once it is completely deployed.</p></li><li><p>Automatically terminate the original fleet one hour after the transition.</p></li></ul><p>Which of the following is the MOST suitable solution that the DevOps Engineer should implement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS CodeDeploy</strong> is a fully managed deployment service that automates software deployments to various compute services, such as Amazon Elastic Compute Cloud (EC2), Amazon Elastic Container Service (ECS), AWS Lambda, and your on-premises servers.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-codedeploy.png"></p><p>A <strong>deployment group</strong> is the AWS CodeDeploy entity for grouping EC2 instances or AWS Lambda functions in a CodeDeploy deployment. For EC2 deployments, it is a set of instances associated with an application that you target for a deployment.</p><p><strong>BlueInstanceTerminationOption</strong> contains information about whether instances in the original environment are terminated when a blue/green deployment is successful.</p><p><strong>-action<br></strong></p><p>The action to take on instances in the original environment after a successful blue/green deployment.</p><p><code>TERMINATE</code>: Instances are terminated after a specified wait time.</p><p><code>KEEP_ALIVE</code>: Instances are left running after they are deregistered from the load balancer and removed from the deployment group.</p><p><strong>-terminationWaitTimeInMinutes</strong></p><p>The number of minutes to wait after a successful blue/green deployment before terminating instances from the original environment.</p><p><br></p><p>Hence, the correct answer is the option that says: <strong>Utilize AWS CodeDeploy and set up a deployment group that has a blue/green deployment configuration. Set the BlueInstanceTerminationOption </strong><code><strong>action</strong></code><strong> to TERMINATE and </strong><code><strong>terminationWaitTimeInMinutes</strong></code><strong> with a 1-hour waiting period.</strong></p><p>The option that says: <strong>Deploy an AWS CloudFormation template that includes a retention policy of 1 hour for the ALB. Then, update the Amazon Route 53 record to reflect the updated ALB </strong>is incorrect because you cannot set a retention policy in CloudFormation.</p><p>The option that says: <strong>Create two AWS Elastic Beanstalk environments to execute a blue/green deployment from the original environment to the new one. Configure an application version lifecycle policy to terminate the primary environment in 1 hour </strong>is incorrect because the application version lifecycle policy is not used for EC2 instances and only deletes old .config files. In addition, the minimum age limit is set in days, not hours.</p><p>The option that says: <strong>Configure AWS Elastic Beanstalk with an Immutable setting, then create a .ebextension file using the Resources key to establish a deletion policy of 1 hour for the ALB, then deploy the application </strong>is incorrect because deletion policy is primarily used to preserve, and in some cases, backup a resource when the stack is deleted. In addition, the deletion policy cannot be set to delete a resource after 1 hour.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html">https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html</a></p><p><a href="https://aws.amazon.com/codedeploy/">https://aws.amazon.com/codedeploy/</a></p><p><a href="https://aws.amazon.com/codedeploy/faqs/">https://aws.amazon.com/codedeploy/faqs/</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codedeploy/?src=udemy">https://tutorialsdojo.com/aws-codedeploy/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Utilize AWS CodeDeploy and set up a deployment group that has a blue/green deployment configuration. Set the BlueInstanceTerminationOption <code>action</code> to TERMINATE and <code>terminationWaitTimeInMinutes</code> with a 1-hour waiting period.</p>",
          "<p>Deploy an AWS CloudFormation template that includes a retention policy of 1 hour for the ALB. Then, update the Amazon Route 53 record to reflect the updated ALB.</p>",
          "<p>Create two AWS Elastic Beanstalk environments to execute a blue/green deployment from the original environment to the new one. Configure an application version lifecycle policy to terminate the primary environment in 1 hour</p>",
          "<p>Configure AWS Elastic Beanstalk with an Immutable setting, then create a .ebextension file using the Resources key to establish a deletion policy of 1 hour for the ALB, then deploy the application.</p>",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "An organization runs its web application on EC2 instances within an Auto Scaling group. The EC2 instances is behind an Application Load Balancer and is deployed across multiple Availability Zones. The developers of the organization have introduced fresh features to the web application, but require to be tested before implementation to prevent any interruptions. The organization requires that the deployment strategy should:Deploy a duplicate fleet of instances with an equivalent capacity to the primary fleet.Keep the original fleet unaltered while the secondary fleet is being launched.Shift traffic to the secondary fleet once it is completely deployed.Automatically terminate the original fleet one hour after the transition.Which of the following is the MOST suitable solution that the DevOps Engineer should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588477,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A stock trading company has recently built a Node.js web application with a GraphQL API-backed service that stores and retrieves financial transactions. The application is currently hosted in an on-premises server with a local MySQL database as its data store. For testing purposes, the app will have a series of updates based on the feedback of the QA and UX teams. There should be no downtime or degraded performance in the application while the DevOps team is building, testing, and deploying the new release versions. The architecture should also be scalable to meet the surge of application requests.</p><p>Which of the following actions will allow the DevOps team to quickly migrate the application to AWS?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Because <strong>AWS Elastic Beanstalk</strong> performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. A blue/green deployment is also required when you want to update an environment to an incompatible platform version.</p><p>Blue/green deployments require that your environment run independently of your production database, if your application uses one. If your environment has an Amazon RDS DB instance attached to it, the data will not transfer over to your second environment, and will be lost if you terminate the original environment.</p><p><img src="https://media.tutorialsdojo.com/aws-beanstalk-blue-green.png"></p><p>AWS CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. You can use CodeBuild together with the EB CLI to automate building your application from its source code. Environment creation and each deployment thereafter start with a build step, and then deploy the resulting application.</p><p>Hence, the correct answer is: <strong>Migrate the application source code to GitHub and set up the AWS Connector for GitHub. Use CodeBuild to set up automatic unit and functional tests. Set up two stacks in Elastic Beanstalk with an external Amazon RDS database with Multi-AZ deployments configuration. Deploy the current application version on the two environments. Configure CodeBuild to deploy the succeeding application revision to Elastic Beanstalk. Use a blue/green strategy for deployment.</strong></p><p>The option that says: <strong>Migrate the application source code to GitHub and set up the AWS Connector for GitHub. Use CodeBuild to set up automatic unit and functional tests. Set up two stacks in Elastic Beanstalk with an external Amazon RDS database with Multi-AZ deployments configuration. Deploy the current application version on the two environments. Configure CodeBuild to deploy the succeeding application revision to Elastic Beanstalk. Use an in-place strategy for deployment </strong>is incorrect because it is better to use a blue/green deployment configuration to ensure the high-availability of your application even during deployment.</p><p>The option that says: <strong>Migrate the application source code to GitHub and set up the AWS Connector for GitHub. Use CodeBuild to set up automatic unit and functional tests. Set up two stacks in Elastic Beanstalk with a separate Amazon RDS database with Multi-AZ deployments configuration for each stack. Deploy the current application version on the two environments. Configure CodeBuild to deploy the succeeding application revision to Elastic Beanstalk. Use a blue/green strategy for deployment </strong>is incorrect because it is not appropriate to launch a separate Amazon RDS database with Multi-AZ deployments configuration on each Elastic Beanstalk environment. It is recommended to decouple your database from your Elastic Beanstalk environment.</p><p>The option that says: <strong>Migrate the application source code to Amazon ECR and use CodeDeploy to set up the automatic unit and functional tests. Set up two stacks in Elastic Beanstalk with an external Amazon RDS database with Multi-AZ deployments configuration. Deploy the current application version on the two environments. Configure CodeBuild to deploy the succeeding application revision to Elastic Beanstalk. Use an in-place strategy for deployment</strong> is incorrect because the Amazon ECR service is primarily used for containerized applications. You also have to use CodeBuild to set up the automatic unit and functional tests and not CodeDeploy. In addition, it is better to use a blue/green deployment configuration to ensure the high availability of your application even during deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli-codebuild.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli-codebuild.html</a></p><p><a href="https://docs.aws.amazon.com/dtconsole/latest/userguide/connections-create-github.html">https://docs.aws.amazon.com/dtconsole/latest/userguide/connections-create-github.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>',
        answers: [
          "<p>Migrate the application source code to GitHub and set up the AWS Connector for GitHub. Use CodeBuild to set up automatic unit and functional tests. Set up two stacks in Elastic Beanstalk with an external Amazon RDS database with Multi-AZ deployments configuration. Deploy the current application version on the two environments. Configure CodeBuild to deploy the succeeding application revision to Elastic Beanstalk. Use an in-place strategy for deployment.</p>",
          "<p>Migrate the application source code to GitHub and set up the AWS Connector for GitHub. Use CodeBuild to set up automatic unit and functional tests. Set up two stacks in Elastic Beanstalk with an external Amazon RDS database with Multi-AZ deployments configuration. Deploy the current application version on the two environments. Configure CodeBuild to deploy the succeeding application revision to Elastic Beanstalk. Use a blue/green strategy for deployment.</p>",
          "<p>Migrate the application source code to GitHub and set up the AWS Connector for GitHub. Use CodeBuild to set up automatic unit and functional tests. Set up two stacks in Elastic Beanstalk with a separate Amazon RDS database with Multi-AZ deployments configuration for each stack. Deploy the current application version on the two environments. Configure CodeBuild to deploy the succeeding application revision to Elastic Beanstalk. Use a blue/green strategy for deployment.</p>",
          "<p>Migrate the application source code to Amazon ECR and use CodeDeploy to set up the automatic unit and functional tests. Set up two stacks in Elastic Beanstalk with an external Amazon RDS database with Multi-AZ deployments configuration. Deploy the current application version on the two environments. Configure CodeBuild to deploy the succeeding application revision to Elastic Beanstalk. Use an in-place strategy for deployment.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A stock trading company has recently built a Node.js web application with a GraphQL API-backed service that stores and retrieves financial transactions. The application is currently hosted in an on-premises server with a local MySQL database as its data store. For testing purposes, the app will have a series of updates based on the feedback of the QA and UX teams. There should be no downtime or degraded performance in the application while the DevOps team is building, testing, and deploying the new release versions. The architecture should also be scalable to meet the surge of application requests.Which of the following actions will allow the DevOps team to quickly migrate the application to AWS?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588479,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading software development company is using GitHub as the source control repository of one of its cloud-based applications. There is a new requirement to automate its continuous integration and continuous deployment pipeline on its development, testing, and production environments. A security code review must be done to ensure that there is no leaked Personally Identifiable Information (PII) or other sensitive data. Each change should go through both unit testing as well as functional testing. Any code push to GItHub should automatically trigger the CI/CD pipeline, and an email notification to devops-administrators@tutorialsdojo.com should be sent in the event of build or deployment failures. In addition, an approval to stage the assets to Amazon S3 after tests should also be performed. The solution must strictly follow the CI/CD best practices.</p><p>Which among the following options should a DevOps Engineer implement to satisfy all of the requirements above?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Monitoring is an important part of maintaining the reliability, availability, and performance of AWS CodePipeline. You should collect monitoring data from all parts of your AWS solution so that you can more easily debug a multi-point failure if one occurs.</p><p>You can use the following tools to monitor your CodePipeline pipelines and their resources:</p><p><strong>EventBridge event bus events</strong> — You can monitor CodePipeline events in EventBridge, which detects changes in your pipeline, stage, or action execution status. EventBridge routes that data to targets such as AWS Lambda and Amazon Simple Notification Service.</p><p><strong>Notifications for pipeline events in the Developer Tools console</strong> — You can monitor CodePipeline events with notifications that you set up in the console and then create an Amazon Simple Notification Service topic and subscription for.</p><p><strong>AWS CloudTrail</strong> — Use CloudTrail to capture API calls made by or on behalf of CodePipeline in your AWS account and deliver the log files to an Amazon S3 bucket. You can choose to have CloudWatch publish Amazon SNS notifications when new log files are delivered so you can take quick action.</p><p><strong>Console and CLI</strong> — You can use the CodePipeline console and CLI to view details about the status of a pipeline or a particular pipeline execution.</p><p><img src="https://media.tutorialsdojo.com/public/td-codepipeline-and-eventbridge-integration-01-07-25.png"></p><p><strong>Amazon Eventbridge </strong>is a web service that monitors your AWS resources and the applications you run on AWS. You can use Amazon Eventbridge to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, Amazon Eventbridge invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions.</p><p>Hence, the correct answer is: <strong>Configure the AWS CodePipeline to have a trigger to start off the pipeline when a new code change is committed on a certain code branch. In the pipeline, set up the required stages for security review, unit tests, functional tests, and manual approval. Use Amazon Eventbridge to detect changes in pipeline stages. Send an email notification to devops-administrators@tutorialsdojo.com using Amazon SNS.</strong></p><p>The option that says: <strong>Configure AWS CodePipeline to have a trigger to start off the pipeline when a new code change is committed on a certain branch. Add the required stages in the pipeline for security review, unit tests, functional tests, and manual approval. Use Amazon Eventbridge to detect changes in pipeline stages. Send an email notification to devops-administrators@tutorialsdojo.com using Amazon SES</strong> is incorrect because Amazon SES is primarily a bulk or transactional email service and requires additional configurations, such as setting up identities and templates. Amazon SNS is better suited for this use case.</p><p>The option that says: <strong>Configure the AWS CodePipeline to have a trigger to start off the pipeline when a new code change is committed on the master branch. In the pipeline, set up the required stages for security review, unit tests, functional tests, and manual approval. Use CloudTrail to detect changes in pipeline stages. Send an email notification to devops-administrators@tutorialsdojo.com using Amazon SNS</strong> is incorrect because AWS CloudTrail is primarily designed for tracking API calls and events for security auditing and compliance. It does not provide real-time detection of pipeline stage changes or failures. Amazon EventBridge is better suited for monitoring CodePipeline stages and acting on stage changes.</p><p>The option that says: <strong>Configure AWS CodePipeline to have a trigger to start off the pipeline when a new code change is committed on the master branch. Add the required stages in the pipeline for security review, unit tests, functional tests, and manual approval. Use CloudWatch Logs to detect changes in pipeline stages. Send an email notification to devops-administrators@tutorialsdojo.com using Amazon SES</strong> is incorrect. While CloudWatch Logs can capture log data from pipeline stages, it does not inherently provide event-based mechanisms to detect and act on pipeline stage changes or failures. Using it would require custom solutions to parse logs and trigger notifications, which adds unnecessary complexity.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring.html</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p>',
        answers: [
          "<p>Configure AWS CodePipeline to have a trigger to start off the pipeline when a new code change is committed on a certain branch. Add the required stages in the pipeline for security review, unit tests, functional tests, and manual approval. Use Amazon Eventbridge to detect changes in pipeline stages. Send an email notification to devops-administrators@tutorialsdojo.com using Amazon SES.</p>",
          "<p>Configure the AWS CodePipeline to have a trigger to start off the pipeline when a new code change is committed on the master branch. In the pipeline, set up the required stages for security review, unit tests, functional tests, and manual approval. Use CloudTrail to detect changes in pipeline stages. Send an email notification to devops-administrators@tutorialsdojo.com using Amazon SNS.</p>",
          "<p>Configure AWS CodePipeline to have a trigger to start off the pipeline when a new code change is committed on the master branch. Add the required stages in the pipeline for security review, unit tests, functional tests, and manual approval. Use CloudWatch Logs to detect changes in pipeline stages. Send an email notification to devops-administrators@tutorialsdojo.com using Amazon SES.</p>",
          "<p>Configure the AWS CodePipeline to have a trigger to start off the pipeline when a new code change is committed on a certain code branch. In the pipeline, set up the required stages for security review, unit tests, functional tests, and manual approval. Use Amazon Eventbridge to detect changes in pipeline stages. Send an email notification to devops-administrators@tutorialsdojo.com using Amazon SNS.</p>",
        ],
      },
      correct_response: ["d"],
      section: "SDLC Automation",
      question_plain:
        "A leading software development company is using GitHub as the source control repository of one of its cloud-based applications. There is a new requirement to automate its continuous integration and continuous deployment pipeline on its development, testing, and production environments. A security code review must be done to ensure that there is no leaked Personally Identifiable Information (PII) or other sensitive data. Each change should go through both unit testing as well as functional testing. Any code push to GItHub should automatically trigger the CI/CD pipeline, and an email notification to devops-administrators@tutorialsdojo.com should be sent in the event of build or deployment failures. In addition, an approval to stage the assets to Amazon S3 after tests should also be performed. The solution must strictly follow the CI/CD best practices.Which among the following options should a DevOps Engineer implement to satisfy all of the requirements above?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588481,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A DevOps Engineer is designing a service that aggregates clickstream data in real-time. The service should also deliver a report to its subscribers once a week via email only. The data being handled are geographically distributed, high volume, and unpredictable. The service should identify and create sessions from real-time clickstream events with a feature to do an ad hoc analysis. </p><p>Which among the options below is the MOST suitable solution that the Engineer should implement with the LEAST amount of cost?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon Kinesis</strong> makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.</p><p><img alt="Amazon Kinesis Data Stream" height="493" src="https://media.tutorialsdojo.com/public/010524_Amazon-Kinesis-Data-Streams.png" width="1000"></p><p>Hence, the correct answer is: <strong>Collect the real-time clickstream data using Amazon Kinesis Data Stream then build and analyze the sessions using Amazon Managed Service for Apache Flink Studio. The aggregated analytics will trigger the real-time events on AWS Lambda and then send them to Data Firehose which in turn, sends data to an Amazon S3 bucket. The clickstream data is ingested to a table by an AWS Glue crawler that will be used by Amazon Athena for running queries and ad hoc analysis.</strong></p><p>The option that says: <strong>Collect the real-time clickstream data using Amazon CloudWatch Events then build and analyze the sessions using Amazon Managed Service for Apache Flink Studio. The aggregated analytics will trigger the real-time events on AWS Lambda and then send them to Data Firehose which in turn, sends data to an Amazon S3 bucket. The clickstream data is ingested to a table by an AWS Glue crawler that will be used by Amazon Athena for running queries and ad hoc analysis</strong> is incorrect because you can\'t primarily collect real-time clickstream data using Amazon CloudWatch Events. You have to use Amazon Kinesis Data Streams instead.</p><p>The option that says: <strong>Collect the real-time clickstream data using a custom Amazon EMR application hosted on extra-large Amazon EC2 instances then build and analyze the sessions using Amazon Managed Service for Apache Flink Studio. The aggregated analytics will trigger the real-time events on AWS Lambda and then send them to Amazon SQS which in turn, sends data to an Amazon S3 bucket. The clickstream data is ingested to a table by an AWS Glue crawler that will be used by Amazon Athena for running queries and ad hoc analysis</strong> is incorrect. Although it is typically possible to collect the clickstream using Amazon EMR, it entails a significant amount of cost due to the use of extra-large Amazon EC2 instances. Using Amazon Kinesis is a more cost-effective option.</p><p>The option that says: <strong>Collect the real-time clickstream data using Amazon SQS then build and analyze the sessions using Amazon Managed Service for Apache Flink Studio. The aggregated analytics will trigger the real-time events on AWS Lambda and then send them to Data Firehose which in turn, sends data to an Instance Store-backed Amazon EC2 instance. The clickstream data is ingested to a table by an AWS Glue crawler that will be used by Amazon Athena for running queries and ad hoc analysis</strong> is incorrect because using Amazon SQS is not the appropriate service to use for collecting clickstream data in real-time.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/solutions/latest/real-time-web-analytics-with-kinesis/architecture.html">https://docs.aws.amazon.com/solutions/latest/real-time-web-analytics-with-kinesis/architecture.html</a></p><p><a href="https://aws.amazon.com/kinesis/data-streams/">https://aws.amazon.com/kinesis/data-streams/</a></p><p><br></p><p><strong>Check out these Amazon Kinesis and Athena Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-kinesis/?src=udemy">https://tutorialsdojo.com/amazon-kinesis/</a></p><p><a href="https://tutorialsdojo.com/amazon-athena/?src=udemy">https://tutorialsdojo.com/amazon-athena/</a></p>',
        answers: [
          "<p>Collect the real-time clickstream data using Amazon CloudWatch Events then build and analyze the sessions using Amazon Managed Service for Apache Flink Studio. The aggregated analytics will trigger the real-time events on AWS Lambda and then send them to Data Firehose which in turn sends data to an Amazon S3 bucket. The clickstream data is ingested to a table by an AWS Glue crawler that will be used by Amazon Athena for running queries and ad hoc analysis.</p>",
          "<p>Collect the real-time clickstream data using Amazon Kinesis Data Stream then build and analyze the sessions using Amazon Managed Service for Apache Flink Studio. The aggregated analytics will trigger the real-time events on AWS Lambda and then send them to Data Firehose which in turn, sends data to an Amazon S3 bucket. The clickstream data is ingested to a table by an AWS Glue crawler that will be used by Amazon Athena for running queries and ad hoc analysis.</p>",
          "<p>Collect the real-time clickstream data using a custom Amazon EMR application hosted on extra-large Amazon EC2 instances then build and analyze the sessions using Amazon Managed Service for Apache Flink Studio. The aggregated analytics will trigger the real-time events on AWS Lambda and then send them to Amazon SQS which in turn, sends data to an Amazon S3 bucket. The clickstream data is ingested to a table by an AWS Glue crawler that will be used by Amazon Athena for running queries and ad hoc analysis.</p>",
          "<p>Collect the real-time clickstream data using Amazon SQS then build and analyze the sessions using Amazon Managed Service for Apache Flink Studio. The aggregated analytics will trigger the real-time events on AWS Lambda and then send them to Data Firehose which in turn, sends data to an Instance Store-backed Amazon EC2 instance. The clickstream data is ingested to a table by an AWS Glue crawler that will be used by Amazon Athena for running queries and ad hoc analysis.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A DevOps Engineer is designing a service that aggregates clickstream data in real-time. The service should also deliver a report to its subscribers once a week via email only. The data being handled are geographically distributed, high volume, and unpredictable. The service should identify and create sessions from real-time clickstream events with a feature to do an ad hoc analysis. Which among the options below is the MOST suitable solution that the Engineer should implement with the LEAST amount of cost?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588483,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading software company is testing an application that is hosted on an Auto Scaling group of EC2 instances across multiple Availability Zones behind an Application Load Balancer. For its deployment process, the company uses the blue/green deployment process with immutable instances to avoid any service degradation. Recently, the DevOps team noticed that the users are being automatically logged out of the application intermittently. Whenever a new version of the application is deployed, all users are automatically logged out which affects the overall user experience. The DevOps team needs a solution to ensure users remain logged in across scaling events and application deployments.</p><p>Which among the following options is the MOST efficient way to solve this issue?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management.</p><p>In order to address scalability and to provide a shared data storage for sessions that can be accessible from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution for this is to leverage an <a href="https://aws.amazon.com/elasticache/">In-Memory Key/Value store</a> such as <a href="https://aws.amazon.com/redis/">Redis</a> and <a href="https://aws.amazon.com/memcached/">Memcached</a>.<br><br>While Key/Value data stores are known to be extremely fast and provide sub-millisecond latency, the added network latency and added cost are the drawbacks. An added benefit of leveraging Key/Value stores is that they can also be utilized to cache any data, not just HTTP sessions, which can help boost the overall performance of your applications.<br><img src="https://media.tutorialsdojo.com/public/caching-session-management-diagram_6AUG2023.png"></p><p>A consideration when choosing a distributed cache for session management is determining how many nodes may be needed in order to manage the user sessions. Generally speaking, this decision can be determined by how much traffic is expected and/or how much risk is acceptable. In a distributed session cache, the sessions are divided by the number of nodes in the cache cluster. In the event of a failure, only the sessions that are stored on the failed node are affected. If reducing risk is more important than cost, adding additional nodes to further reduce the percentage of stored sessions on each node may be ideal even when fewer nodes are sufficient.<br><br>Another consideration may be whether or not the sessions need to be replicated or not. Some key/value stores offer replication via read replicas. In the event of a node failure, the sessions would not be entirely lost. Whether replica nodes are important in your individual architecture may inform as to which key/value store should be used. <a href="https://aws.amazon.com/elasticache/">ElastiCache</a> offerings for In-Memory key/value stores include <a href="https://aws.amazon.com/elasticache/redis/">ElastiCache for Redis</a>, which can support replication, and <a href="https://aws.amazon.com/elasticache/memcached/">ElastiCache for Memcached</a> which does not support replication.</p><p>Hence, the correct answer is: <strong>Launch a new Amazon ElastiCache for Redis cluster with nodes across multiple Availability Zones. Configure the application to store user-session information in ElastiCache.</strong></p><p>The option that says: <strong>Launch a Memcached session store on a large EC2 instance on a single Availability Zone. Configure the application to store user-session information in its corresponding Memcached node</strong> is incorrect as this will not solve the problem. The session store is located in a single instance and in a single Availability Zone. You should set up a distributed cache for session management using an ElastiCache cluster that spans across multiple AZs.</p><p>The option that says: <strong>Launch a Redis session store on a large EC2 instance for each supported Availability Zone that are independent from each other. Configure the application to store user-session information in its corresponding Redis node </strong>is incorrect because you have to create several session store nodes that are related to each other and are not stand-alone nodes.</p><p>The option that says: <strong>Enable sticky sessions in the Application Load Balancer. Configure custom application-based cookies for each target group</strong> is incorrect. Sticky sessions just bind user sessions to a specific target group, thus, you\'re still persistently storing session data locally on the instance. This means that if an instance fails, you will most likely lose the session data that are stored on the failed instance. Furthermore, if the number of your web servers increases, as in a scale-up situation, traffic may be unequally distributed throughout the web servers because active sessions may reside on certain servers.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/elasticache/">https://aws.amazon.com/elasticache/</a></p><p><a href="https://aws.amazon.com/elasticache/">https://aws.amazon.com/caching/session-management/</a></p><p><br></p><p><strong>Check out this Amazon Elasticache Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-elasticache/?src=udemy">https://tutorialsdojo.com/amazon-elasticache/</a></p>',
        answers: [
          "<p>Launch a Memcached session store on a large EC2 instance on a single Availability Zone. Configure the application to store user-session information in its corresponding Memcached node.</p>",
          "<p>Launch a Redis session store on a large EC2 instance for each supported Availability Zone that are independent from each other. Configure the application to store user-session information in its corresponding Redis node.</p>",
          "<p>Enable sticky sessions in the Application Load Balancer. Configure custom application-based cookies for each target group.</p>",
          "<p>Launch a new Amazon ElastiCache for Redis cluster with nodes across multiple Availability Zones. Configure the application to store user-session information in ElastiCache.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A leading software company is testing an application that is hosted on an Auto Scaling group of EC2 instances across multiple Availability Zones behind an Application Load Balancer. For its deployment process, the company uses the blue/green deployment process with immutable instances to avoid any service degradation. Recently, the DevOps team noticed that the users are being automatically logged out of the application intermittently. Whenever a new version of the application is deployed, all users are automatically logged out which affects the overall user experience. The DevOps team needs a solution to ensure users remain logged in across scaling events and application deployments.Which among the following options is the MOST efficient way to solve this issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588485,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has a mission-critical website hosted on-premises that is written in .NET and Angular. Its DevOps team is planning to host the site to AWS Elastic Beanstalk. The application should maintain its availability at all times to avoid any loss of revenue for the company. The existing EC2 instances should remain in service during the deployment of the succeeding application versions. A new fleet of EC2 instances should be provisioned to host the new application version. The new instances should be placed in service and the old ones should be removed after the new application version is successfully deployed in the new fleet of instances. No DNS change should also be made on the underlying resources of the environment especially the Elastic Beanstalk DNS CNAME. In the event of deployment failure, the new fleet of instances should be terminated and the current instances should continue serving traffic as normal. </p><p>As a DevOps Engineer, what deployment strategy should you implement to satisfy the requirements?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>In ElasticBeanstalk, you can choose from a variety of deployment methods:</p><p><strong>All at once</strong> – Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. This is the method that provides the least amount of time for deployment.</p><p><strong>Rolling</strong> – Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment\'s capacity by the number of instances in a batch.</p><p><strong>Rolling with additional batch</strong> – Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process.</p><p><strong>Immutable</strong> – Deploy the new version to a fresh group of instances by performing an <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html">immutable update</a>.</p><p><strong>Blue/Green</strong> - Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.</p><p>Refer to the table below for the characteristics of each deployment method as well as the amount of time it takes to do the deployment, as seen in the <strong>Deploy Time</strong> column:</p><p><img src="https://media.tutorialsdojo.com/public/DeploymentMethods_2AUG2023.png"></p><p>Immutable environment updates are an alternative to <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rollingupdates.html">rolling updates</a>. Immutable environment updates ensure that configuration changes that require replacing instances are applied efficiently and safely. If an immutable environment update fails, the rollback process requires only terminating an Auto Scaling group. A failed rolling update, on the other hand, requires performing an additional rolling update to roll back the changes.</p><p>To perform an immutable environment update, Elastic Beanstalk creates a second, temporary Auto Scaling group behind your environment\'s load balancer to contain the new instances. First, Elastic Beanstalk launches a single instance with the new configuration in the new group. This instance serves traffic alongside all of the instances in the original Auto Scaling group that are running the previous configuration.</p><p>When the first instance passes health checks, Elastic Beanstalk launches additional instances with the new configuration, matching the number of instances running in the original Auto Scaling group. When all of the new instances pass health checks, Elastic Beanstalk transfers them to the original Auto Scaling group, and terminates the temporary Auto Scaling group and old instances.</p><p>Hence, the correct answer is:<strong> Configure the deployment setting of the Elastic Beanstalk environment to use immutable environment updates.</strong></p><p>The option that says: <strong>Configure the deployment setting of the Elastic Beanstalk environment to use rolling deployments</strong><em> </em>is incorrect because it doesn\'t launch a fresh group of EC2 instances to host the new application version.</p><p>The option that says: <strong>Configure the deployment setting of the Elastic Beanstalk environment to use rolling deployments with additional batch</strong> is incorrect because the rollback process of this configuration does not meet the specified requirement. In the event of deployment failure, the existing instances are affected and could be unavailable.</p><p>The option that says: <strong>Configure the deployment setting of the Elastic Beanstalk environment to use blue/green deployment. Swap the CNAME records of the old and new environments to redirect the traffic from the old version to the new version</strong> is incorrect because the scenario explicitly mentioned that there should be no DNS change made on the underlying resources of the environment, especially the Elastic Beanstalk DNS CNAME.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>',
        answers: [
          "<p>Configure the deployment setting of the Elastic Beanstalk environment to use rolling deployments.</p>",
          "<p>Configure the deployment setting of the Elastic Beanstalk environment to use rolling deployments with additional batch.</p>",
          "<p>Configure the deployment setting of the Elastic Beanstalk environment to use blue/green deployment. Swap the CNAME records of the old and new environments to redirect the traffic from the old version to the new version.</p>",
          "<p>Configure the deployment setting of the Elastic Beanstalk environment to use immutable environment updates.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company has a mission-critical website hosted on-premises that is written in .NET and Angular. Its DevOps team is planning to host the site to AWS Elastic Beanstalk. The application should maintain its availability at all times to avoid any loss of revenue for the company. The existing EC2 instances should remain in service during the deployment of the succeeding application versions. A new fleet of EC2 instances should be provisioned to host the new application version. The new instances should be placed in service and the old ones should be removed after the new application version is successfully deployed in the new fleet of instances. No DNS change should also be made on the underlying resources of the environment especially the Elastic Beanstalk DNS CNAME. In the event of deployment failure, the new fleet of instances should be terminated and the current instances should continue serving traffic as normal. As a DevOps Engineer, what deployment strategy should you implement to satisfy the requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588487,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A JavaScript-based online salary calculator hosted on-premises is slated to be migrated to AWS. The application has no server-side code and is just composed of a UI powered by Vue.js and Bootstrap. Since the online calculator may contain sensitive financial data, adding HTTP response headers such as <code>X-Content-Type-Options</code>, <code>X-Frame-Options</code> and <code>X-XSS-Protection</code> should be implemented to comply with the Open Web Application Security Project (OWASP) standards.</p><p>Which of the following is the MOST suitable solution to implement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon CloudFront</strong> now supports adding HTTP security headers using response headers policies. This feature allows you to configure CloudFront to include headers like <code>X-Content-Type-Options</code>, <code>X-Frame-Options</code>, and <code>X-XSS-Protection</code> in HTTP responses. You can attach these headers by creating a custom policy or using an AWS-managed policy such as <code>SecurityHeadersPolicy</code>. This setup does not require writing code or using Lambda@Edge, making it easier to manage.</p><p><img alt="Response Headers Policy" height="423" src="https://media.tutorialsdojo.com/public/td-response-headers-policy-13May2025.png" width="1000">CloudFront is the content delivery network for static websites hosted on Amazon S3 and can handle header manipulation through its response behavior. Attaching a response headers policy to a cache behavior in your CloudFront distribution adds the headers automatically to all responses from the origin. This approach meets OWASP security requirements and simplifies maintenance because the configuration is handled entirely within CloudFront.</p><p>Therefore, using CloudFront response headers policies is the recommended and most suitable solution for adding security headers to static web applications hosted on S3.</p><p>Hence, the correct answer is: <strong>Host the application on an Amazon S3 bucket configured for website hosting. Set up an Amazon CloudFront web distribution and set the S3 bucket as the origin. Set a custom Request and Response Behavior in CloudFront that automatically adds the required security headers in the HTTP response.</strong></p><p>The option that says: <strong>Host the application on an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3 bucket to track user activity. Enable S3 client-side encryption and configure it to return the required security headers</strong> is incorrect because Amazon S3 does not typically allow adding custom HTTP response headers through client-side encryption. Encryption protects stored data but does not change how responses are sent to users. S3 alone cannot add security headers to the HTTP response.</p><p>The option that says:<strong> Host the application on an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3 bucket to track user activity. Configure the bucket policy of the S3 bucket to return the required security headers</strong> is incorrect because bucket policies control access to S3 content, not HTTP response behavior. You cannot add or modify HTTP headers using a bucket policy. This method cannot meet the requirement of adding OWASP-recommended security headers.</p><p>The option that says: <strong>Host the application on an Amazon S3 bucket configured for website hosting. Set up an Amazon CloudFront web distribution and set the S3 bucket as the origin with the origin response event set to trigger a Lambda@Edge function. Add the required security headers in the HTTP response using the AWS Lambda function</strong> is incorrect. While this method works, it only adds unnecessary complexity. Lambda@Edge requires extra setup, code, and management. CloudFront’s built-in Response Headers Policies now provide a more straightforward way to add security headers, making Lambda@Edge unnecessary for this case.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-response-headers-policies.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-response-headers-policies.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/creating-response-headers-policies.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/creating-response-headers-policies.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudfront/?src=udemy">https://tutorialsdojo.com/amazon-cloudfront/</a></p>',
        answers: [
          "<p>Host the application on an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3 bucket to track user activity. Enable S3 client-side encryption and configure it to return the required security headers.</p>",
          "<p>Host the application on an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3 bucket to track user activity. Configure the bucket policy of the S3 bucket to return the required security headers.</p>",
          "<p>Host the application on an Amazon S3 bucket configured for website hosting. Set up an Amazon CloudFront web distribution and set the S3 bucket as the origin with the origin response event set to trigger a Lambda@Edge function. Add the required security headers in the HTTP response using the AWS Lambda function.</p>",
          "<p>Host the application on an Amazon S3 bucket configured for website hosting. Set up an Amazon CloudFront web distribution and set the S3 bucket as the origin. Set a custom Request and Response Behavior in CloudFront that automatically adds the required security headers in the HTTP response.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A JavaScript-based online salary calculator hosted on-premises is slated to be migrated to AWS. The application has no server-side code and is just composed of a UI powered by Vue.js and Bootstrap. Since the online calculator may contain sensitive financial data, adding HTTP response headers such as X-Content-Type-Options, X-Frame-Options and X-XSS-Protection should be implemented to comply with the Open Web Application Security Project (OWASP) standards.Which of the following is the MOST suitable solution to implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588489,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading technology company with a hybrid cloud architecture has a suite of web applications that is composed of 50 modules. Each of the module is a multi-tiered application hosted in an Auto Scaling group of On-Demand Amazon EC2 instances behind an ALB with an external Amazon RDS. The Application Security team is mandated to block access from external IP addresses and only allow access to the 50 applications from the corporate data center. A group of 10 proxy servers with an associated IP address each are used for the corporate network to connect to the Internet. The 10 proxy IP addresses are being refreshed twice a month. The Network team uploads a CSV file that contains the latest proxy IP addresses into a private Amazon S3 bucket. The DevOps Engineer must build a solution to ensure that the applications are accessible from the corporate network in the most cost-effective way and with minimal operational effort.</p><p>Which of the following options will meet the above requirements</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Lambda</strong> is a <a href="https://aws.amazon.com/serverless/">serverless compute</a> service that runs your code in response to events and automatically manages the underlying compute resources for you. You can use AWS Lambda to extend other AWS services with custom logic, or create your own back-end services that operate at AWS scale, performance, and security. AWS Lambda can automatically run code in response to <a href="http://docs.aws.amazon.com/lambda/latest/dg/intro-core-components.html#intro-core-components-event-sources">multiple events</a>, such as HTTP requests via <a href="https://aws.amazon.com/api-gateway/">Amazon API Gateway</a>, modifications to objects in <a href="https://aws.amazon.com/s3/">Amazon S3</a> buckets, table updates in <a href="https://aws.amazon.com/dynamodb/">Amazon DynamoDB</a>, and state transitions in <a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a>.</p><p>Lambda runs your code on high-availability compute infrastructure and performs all the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, code and security patch deployment, and code monitoring and logging. All you need to do is supply the code.</p><p>Lambda does not enforce any restrictions on your function logic – if you can code for it, you can run it within a Lambda function. As part of your function, you may need to call other APIs, or access other AWS services like databases.</p><p><img src="https://media.tutorialsdojo.com/public/fig1-add-DNS_filtering_NAT_instance_with_Squid_9AUG2023.png"></p><p>By default, your service or API must be accessible over the public internet for AWS Lambda to access it. However, you may have APIs or services that are not exposed this way. Typically, you create these resources inside Amazon Virtual Private Cloud (Amazon VPC) so that they cannot be accessed over the public Internet. These resources could be AWS service resources, such as Amazon Redshift data warehouses, Amazon ElastiCache clusters, or Amazon RDS instances. They could also be your own services running on your own EC2 instances. By default, resources within a VPC are not accessible from within a Lambda function.</p><p>AWS Lambda runs your function code securely within a VPC by default. However, to enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within your private VPC.</p><p>Hence, the correct answer is: <strong>Launch an AWS Lambda function to read the list of proxy IP addresses from the S3 bucket. Configure the function to update the ELB security groups to allow HTTPS requests only from the given IP addresses. Use the S3 Event Notification to automatically invoke the Lambda function when the CSV file is updated.</strong></p><p>The option that says: <strong>Host all of the applications and modules in the same Virtual Private Cloud (Amazon VPC). Set up a Direct Connect connection with an active/standby configuration. Update the ELB security groups to allow only inbound HTTPS connections from the corporate network IP addresses</strong> is incorrect because setting up a Direct Connect connection costs a significant amount of money. Remember that the scenario says that you have to ensure that the applications are accessible from the corporate network in the most cost-effective way and with minimal operational effort.</p><p>The option that says: <strong>Develop a custom Python-based Bolo script using the AWS SDK for Python. Configure the script to download the CSV file that contains the proxy IP addresses and update the ELB security groups to allow only HTTPS inbound from the given IP addresses. Host the script in an AWS Lambda function and run it every minute using Amazon EventBridge</strong> is incorrect because running the Lambda function every minute will increase your compute costs. A better solution is to use Amazon S3 Event Notification to automatically invoke the Lambda function when the CSV file is updated.</p><p>The option that says: <strong>Configure the ELB security groups to allow HTTPS inbound access from the Internet. Set up Amazon Cognito to integrate the company\'s Active Directory as the identity provider. Integrate all of the 50 modules with Cognito to ensure that only the company employees can log into the application. Store the user access logs to Amazon CloudWatch Logs to record user access activities. Use AWS Config for configuration management that runs twice a month to update the settings accordingly</strong> is incorrect because using Amazon Cognito in this scenario is not warranted and is unnecessary as well as the use of AWS Config. Using AWS Lambda can fulfill the requirements in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/security/how-to-set-up-an-outbound-vpc-proxy-with-domain-whitelisting-and-content-filtering/">https://aws.amazon.com/blogs/security/how-to-set-up-an-outbound-vpc-proxy-with-domain-whitelisting-and-content-filtering/</a></p><p><a href="https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/">https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/</a></p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><p><br></p><p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-lambda/?src=udemy">https://tutorialsdojo.com/aws-lambda/</a></p>',
        answers: [
          "<p>Launch an AWS Lambda function to read the list of proxy IP addresses from the S3 bucket. Configure the function to update the ELB security groups to allow HTTPS requests only from the given IP addresses. Use the S3 Event Notification to automatically invoke the Lambda function when the CSV file is updated.</p>",
          "<p>Host all of the applications and modules in the same Virtual Private Cloud (Amazon VPC). Set up a Direct Connect connection with an active/standby configuration. Update the ELB security groups to allow only inbound HTTPS connections from the corporate network IP addresses.</p>",
          "<p>Develop a custom Python-based Bolo script using the AWS SDK for Python. Configure the script to download the CSV file that contains the proxy IP addresses and update the ELB security groups to allow only HTTPS inbound from the given IP addresses. Host the script in an AWS Lambda function and run it every minute using Amazon EventBridge.</p>",
          "<p>Configure the ELB security groups to allow HTTPS inbound access from the Internet. Set up Amazon Cognito to integrate the company's Active Directory as the identity provider. Integrate all of the 50 modules with Cognito to ensure that only the company employees can log into the application. Store the user access logs to Amazon CloudWatch Logs to record user access activities. Use AWS Config for configuration management that runs twice a month to update the settings accordingly.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A leading technology company with a hybrid cloud architecture has a suite of web applications that is composed of 50 modules. Each of the module is a multi-tiered application hosted in an Auto Scaling group of On-Demand Amazon EC2 instances behind an ALB with an external Amazon RDS. The Application Security team is mandated to block access from external IP addresses and only allow access to the 50 applications from the corporate data center. A group of 10 proxy servers with an associated IP address each are used for the corporate network to connect to the Internet. The 10 proxy IP addresses are being refreshed twice a month. The Network team uploads a CSV file that contains the latest proxy IP addresses into a private Amazon S3 bucket. The DevOps Engineer must build a solution to ensure that the applications are accessible from the corporate network in the most cost-effective way and with minimal operational effort.Which of the following options will meet the above requirements",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588491,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A telecommunications company has a web portal that requires a cross-region failover. The portal stores its data in an Amazon Aurora database in the primary region (us-west-1) and the Parallel Query feature is also enabled in the database to optimize some of the I/O and computation involved in processing data-intensive queries. The portal uses Amazon Route 53 to direct customer traffic to the active region.</p><p>Which among the options below should be taken to MINIMIZE downtime of the portal in the event that the primary database fails?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Amazon RDS uses the Amazon Simple Notification Service (Amazon SNS) to provide notification when an Amazon RDS event occurs. These notifications can be in any notification form supported by Amazon SNS for an AWS Region, such as an email, a text message, or a call to an HTTP endpoint.</p><p>Amazon RDS groups these events into categories that you can subscribe to so that you can be notified when an event in that category occurs. You can subscribe to an event category for a DB instance, DB cluster, DB cluster snapshot, DB parameter group, or DB security group. For example, if you subscribe to the Backup category for a given DB instance, you are notified whenever a backup-related event occurs that affects the DB instance. If you subscribe to a configuration change category for a DB security group, you are notified when the DB security group is changed. You also receive a notification when an event notification subscription changes.</p><p>For Amazon Aurora, events occur at both the DB cluster and the DB instance level, so you can receive events if you subscribe to an Aurora DB cluster or an Aurora DB instance.</p><p>Event notifications are sent to the addresses that you provide when you create the subscription. You might want to create several different subscriptions, such as one subscription receiving all event notifications and another subscription that includes only critical events for your production DB instances. You can easily turn off notifications without deleting a subscription by choosing <strong>No</strong> for <strong>Enabled</strong> in the Amazon RDS console or by setting the <code>Enabled</code> parameter to <code>false</code> using the AWS CLI or Amazon RDS API.</p><p><img src="https://media.tutorialsdojo.com/public/Amazon-RDS_9AUG2023.png"></p><p>When you copy a snapshot to an AWS Region that is different from the source snapshot\'s AWS Region, the first copy is a full snapshot copy, even if you copy an incremental snapshot. A full snapshot copy contains all of the data and metadata required to restore the DB instance. After the first snapshot copy, you can copy incremental snapshots of the same DB instance to the same destination region within the same AWS account.</p><p>Depending on the AWS Regions involved and the amount of data to be copied, <em>a cross-region snapshot copy can take hours to complete.</em> In some cases, there might be a large number of cross-region snapshot copy requests from a given source AWS Region. In these cases, Amazon RDS might put new cross-region copy requests from that source AWS Region into a queue until some in-progress copies are complete. No progress information is displayed about copy requests while they are in the queue. Progress information is displayed when the copy starts.</p><p>This means that a cross-region snapshot doesn\'t provide a high RPO compared with a Read Replica since the snapshot takes significant time to complete. Although this is better than Multi-AZ deployments since you can replicate your database across AWS Regions, using a Read Replica is still the best choice for providing a high RTO and RPO for disaster recovery.</p><p>Hence, the correct answer is: <strong>Launch a read replica of the primary database to the second region. Set up Amazon RDS Event Notification to publish status updates to an Amazon SNS topic. Create an AWS Lambda function subscribed to the topic to monitor database health. Configure the Lambda function to promote the read replica as the primary in the event of a failure. Update the Route 53 record to redirect traffic from the primary region to the secondary region.</strong></p><p>The option that says: <strong>Set up an Amazon CloudWatch to monitor the status of the Aurora database. Create an Amazon EventBridge rule to send a Slack message to the SysOps Team using Amazon SNS in the event of a database outage. Instruct the SysOps team to redirect traffic to an Amazon S3 static website that displays a downtime message. Manually promote the read replica as the primary instance and verify the portal\'s status. Redirect traffic from the S3 website to the secondary region</strong> is incorrect because this solution just entails a lot of manual steps. It is possible that the SysOps Team might not respond to the Slack message immediately.</p><p>The option that says: <strong>Create an Amazon EventBridge rule to periodically invoke a AWS Lambda function that checks the health of the primary Aurora database every hour. Configure the Lambda function to promote the read replica as the primary if a failure was detected. Update the Route 53 record to redirect traffic from the primary to the secondary region</strong> is incorrect. Although this is a valid option, there will still be a delay on the database failover since the Amazon EventBridge rule only runs every hour. A better solution is to use Amazon RDS Event Notification instead.</p><p>The option that says: <strong>Configure the Route 53 record to balance traffic between both regions equally using the Failover routing policy. Enable the Aurora multi-master option and set up a Route 53 health check to analyze the health of the databases. Set the Route 53 record to automatically direct all traffic to the secondary region when a primary database fails</strong>is incorrect. Although an Aurora multi-master improves the availability of the database, the application will still experience downtime in the event of an AWS Region outage. Much like Amazon RDS Multi-AZ, the Aurora multi-master has its data replicated across multiple Availability Zones only but not to another AWS Region. You should use the Amazon Aurora Global Database instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/ ">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions</a></p><p><a href="https://aws.amazon.com/rds/details/read-replicas/">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>',
        answers: [
          "<p>Set up an Amazon CloudWatch to monitor the status of the Aurora database. Create an Amazon EventBridge rule to send a Slack message to the SysOps Team using Amazon SNS in the event of a database outage. Instruct the SysOps team to redirect traffic to an Amazon S3 static website that displays a downtime message. Manually promote the read replica as the primary instance and verify the portal's status. Redirect traffic from the S3 website to the secondary region.</p>",
          "<p>Launch a read replica of the primary database to the second region. Set up Amazon RDS Event Notification to publish status updates to an Amazon SNS topic. Create an AWS Lambda function subscribed to the topic to monitor database health. Configure the Lambda function to promote the read replica as the primary in the event of a failure. Update the Route 53 record to redirect traffic from the primary region to the secondary region.</p>",
          "<p>Create an Amazon EventBridge rule to periodically invoke a AWS Lambda function that checks the health of the primary Aurora database every hour. Configure the Lambda function to promote the read replica as the primary if a failure was detected. Update the Route 53 record to redirect traffic from the primary to the secondary region.</p>",
          "<p>Configure the Route 53 record to balance traffic between both regions equally using the Failover routing policy. Enable the Aurora multi-master option and set up a Route 53 health check to analyze the health of the databases. Set the Route 53 record to automatically direct all traffic to the secondary region when a primary database fails.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A telecommunications company has a web portal that requires a cross-region failover. The portal stores its data in an Amazon Aurora database in the primary region (us-west-1) and the Parallel Query feature is also enabled in the database to optimize some of the I/O and computation involved in processing data-intensive queries. The portal uses Amazon Route 53 to direct customer traffic to the active region.Which among the options below should be taken to MINIMIZE downtime of the portal in the event that the primary database fails?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588493,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A DevOps engineer has deployed an Amazon Elastic Kubernetes Service (Amazon EKS) cluster using managed node groups. The engineer has integrated an OpenID Connect (OIDC) issuer with the cluster and is configuring Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp3) volumes for persistent storage. The engineer also enabled Amazon CloudWatch Container Insights to collect metrics and logs from the cluster.</p><p>When trying to create a PersistentVolumeClaim (PVC), the engineer encounters the following errors after running the <code>kubectl describe pvc</code> command, which are failed to provision volume with StorageClass and could not create volume in <code>EC2: UnauthorizedOperation</code>.</p><p>What approach should be taken to resolve these errors?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Persistent storage is a critical component in Kubernetes clusters, especially when applications require data to persist beyond the lifecycle of a pod. In Amazon Elastic Kubernetes Service (Amazon EKS), persistent storage is often provisioned using Amazon Elastic Block Store (Amazon EBS) volumes. The Amazon EBS Container Storage Interface (CSI) driver is responsible for integrating Kubernetes with EBS, enabling dynamic provisioning and management of volumes as per application requirements.</p><p><img alt="Amazon EBS CSI driver" src="https://media.tutorialsdojo.com/public/TD_Amazon_EBS_CSI_driver_15Nov2024.jpg" width="1000"></p><p>When integrating the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver with an Amazon EKS cluster, the <strong>EBS CSI driver</strong> requires appropriate permissions to dynamically provision and manage EBS volumes. These permissions are managed using an <strong>IAM role</strong>, often attached to the driver itself. The error <code>&lt;code&gt;EC2:UnauthorizedOperation&lt;/code&gt;</code> indicates insufficient permissions for the driver to interact with the Amazon EC2 service, which is required for creating or managing EBS volumes.</p><p>AWS recommends assigning an IAM role to the EBS CSI driver with the necessary permissions to provision and manage EBS volumes. This ensures that the driver can perform operations such as creating volumes, attaching them to nodes, and managing lifecycle events.</p><p>Hence, the correct answer is: <strong>Configure an IAM role specifically for the EBS Container Storage Interface (CSI) driver, assigning it the necessary permissions and trust policies. Attach this role to the EBS CSI driver add-on to enable it to provision and manage EBS volumes.</strong></p><p>The option that says: <strong>Set up a Kubernetes cluster role to grant persistent volumes the ability to perform actions such as get, list, watch, create, and delete. Ensure this role also allows storage-related operations like get, list, and watch within the cluster </strong>is incorrect because this would simply help manage permissions within the Kubernetes control plane but does not solve the IAM permissions issue needed for interaction with AWS services like Amazon EC2. This option addresses Kubernetes RBAC, but does not resolve the <code>UnauthorizedOperation error</code>.</p><p>The option that says: <strong>Update the Kubernetes service account used by the EBS CSI driver to use the IAM role created for the CSI driver, ensuring proper permissions for EBS volume operations</strong> is incorrect. While updating the Kubernetes service account to use the IAM role for the EBS CSI driver is part of the solution, this only assumes the IAM role has already been created and assigned permissions. It is only a partial step and does not directly address the permission issue.</p><p>The option that says: <strong>Re-deploy the cluster using EKS Distro (EKS-D), which provides extended Kubernetes support and assume it will resolve IAM authorization issues for provisioning EBS volumes</strong> is incorrect because Amazon EKS Distro (EKS-D) is an open-source Kubernetes distribution maintained by AWS for running clusters outside of the managed Amazon EKS service (EKS-D docs). While EKS-D provides version consistency and extended support, it does not integrate with AWS IAM in the same managed way as Amazon EKS. Redeploying on EKS-D would not resolve the <code>UnauthorizedOperation</code> error, which stems from missing IAM permissions on the EBS CSI driver.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html">https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html</a></p><p><a href="https://aws.amazon.com/blogs/containers/amazon-ebs-csi-driver-is-now-generally-available-in-amazon-eks-add-ons/">https://aws.amazon.com/blogs/containers/amazon-ebs-csi-driver-is-now-generally-available-in-amazon-eks-add-ons/</a></p><p><br></p><p><strong>Check out this Amazon Elastic Kubernetes Service Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-elastic-kubernetes-service-eks/?src=udemy">https://tutorialsdojo.com/amazon-elastic-kubernetes-service-eks/</a></p>',
        answers: [
          "<p>Set up a Kubernetes cluster role to grant persistent volumes the ability to perform actions such as get, list, watch, create, and delete. Ensure this role also allows storage-related operations like get, list, and watch within the cluster.</p>",
          "<p>Configure an IAM role specifically for the EBS Container Storage Interface (CSI) driver, assigning it the necessary permissions and trust policies. Attach this role to the EBS CSI driver add-on to enable it to provision and manage EBS volumes.</p>",
          "<p>Update the Kubernetes service account used by the EBS CSI driver to use the IAM role created for the CSI driver, ensuring proper permissions for EBS volume operations.</p>",
          "<p>Re-deploy the cluster using EKS Distro (EKS-D), which provides extended Kubernetes support and assume it will resolve IAM authorization issues for provisioning EBS volumes.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A DevOps engineer has deployed an Amazon Elastic Kubernetes Service (Amazon EKS) cluster using managed node groups. The engineer has integrated an OpenID Connect (OIDC) issuer with the cluster and is configuring Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp3) volumes for persistent storage. The engineer also enabled Amazon CloudWatch Container Insights to collect metrics and logs from the cluster.When trying to create a PersistentVolumeClaim (PVC), the engineer encounters the following errors after running the kubectl describe pvc command, which are failed to provision volume with StorageClass and could not create volume in EC2: UnauthorizedOperation.What approach should be taken to resolve these errors?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588495,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company needs to transform AWS Network Firewall flow logs and add extra data before storing the flow logs in an existing Amazon S3 bucket. Currently, the flow logs go directly to the S3 bucket, and the company uses Amazon Athena to analyze the data.</p><p>The goal is to process the logs before delivering the processed logs to the S3 bucket.</p><p>How can the company transform the flow logs and deliver them to the existing S3 bucket?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon Data Firehose</strong> is a fully managed service for real-time data streaming to destinations such as Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and third-party providers. It enables on-the-fly data transformation using AWS Lambda and supports automatic scaling to handle varying data loads. Data Firehose is commonly used for streaming log data, analytics, and ETL pipelines due to its reliability, scalability, and low-latency delivery.</p><p><img src="https://media.tutorialsdojo.com/public/TD_Amazon_Data_Firehose-15Nov2024.png"></p><p>Amazon Data Firehose allows you to capture, transform, and send data in near real-time. Logs can be pre-processed before being stored in the destination S3 bucket by adding a Lambda function to the delivery stream as a transformer. Data Firehose also natively interfaces with various AWS services, including S3, and includes built-in retry and scaling methods to ensure reliability and efficiency while managing large-scale log ingestion and transformations.</p><p>Hence, the correct answer is: <strong>Set up an Amazon Data Firehose delivery stream with an AWS Lambda transformer. Designate the current S3 bucket as the destination. Modify the Network Firewall logging to use Data Firehose instead of Amazon S3.</strong></p><p>The option that says: <strong>Set up Amazon SQS to queue the logs before processing. Use a Lambda function triggered by SQS to transform the logs and save the results to the existing S3 bucket. This setup provides a buffer to handle spikes in log volume efficiently</strong> is incorrect because using Amazon SQS with a Lambda function can only process logs but does not natively handle the direct integration and delivery pipeline to S3 like Data Firehose. This solution adds operational complexity, requiring custom logic for retries and scaling, which Data Firehose inherently provides.</p><p>The option that says: <strong>Use Amazon S3 Event Notifications to trigger an AWS Glue job. Configure the job to transform the logs and write the processed data back to the existing S3 bucket. Ensure the Glue job is set to handle structured and unstructured log formats</strong> is incorrect because triggering an AWS Glue job with S3 Event Notifications would simply process logs after they are already stored in the bucket, which violates the requirement to transform the logs before storage. Glue is suitable for batch transformations but not for real-time processing.</p><p>The option that says: <strong>Develop an AWS Lambda function to transform the data and save a new object in the existing S3 bucket. Set up an S3 trigger on this bucket to activate the Lambda function, specifying all object creation events as the trigger type. Ensure that recursive invocation is accounted for</strong> is incorrect because setting up a Lambda function with an S3 trigger introduces the risk of recursive invocation which the Lambda function creating new events by writing transformed objects back to the bucket. Additionally, this approach processes logs after they are written to S3, not before, failing to meet the requirement.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html">https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html</a></p><p><a href="https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html">https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html</a></p><p><a href="https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html">https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-kinesis/#data-firehose/?src=udemy">https://tutorialsdojo.com/amazon-kinesis/#data-firehose</a></p>',
        answers: [
          "<p>Set up Amazon SQS to queue the logs before processing. Use a Lambda function triggered by SQS to transform the logs and save the results to the existing S3 bucket. This setup provides a buffer to handle spikes in log volume efficiently.</p>",
          "<p>Use Amazon S3 Event Notifications to trigger an AWS Glue job. Configure the job to transform the logs and write the processed data back to the existing S3 bucket. Ensure the Glue job is set to handle structured and unstructured log formats.</p>",
          "<p>Develop an AWS Lambda function to transform the data and save a new object in the existing S3 bucket. Set up an S3 trigger on this bucket to activate the Lambda function, specifying all object creation events as the trigger type. Ensure that recursive invocation is accounted for.</p>",
          "<p>Set up an Amazon Data Firehose delivery stream with an AWS Lambda transformer. Designate the current S3 bucket as the destination. Modify the Network Firewall logging to use Data Firehose instead of Amazon S3.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Monitoring and Logging",
      question_plain:
        "A company needs to transform AWS Network Firewall flow logs and add extra data before storing the flow logs in an existing Amazon S3 bucket. Currently, the flow logs go directly to the S3 bucket, and the company uses Amazon Athena to analyze the data.The goal is to process the logs before delivering the processed logs to the S3 bucket.How can the company transform the flow logs and deliver them to the existing S3 bucket?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588497,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A DevOps Engineer manages multiple AWS accounts using AWS Organizations. The engineer must ensure that logs from all existing and future Amazon CloudWatch Logs log groups are automatically forwarded to a dedicated Amazon S3 bucket in a central logging account.</p><p>Which solution will meet these requirements most effectively?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>CloudWatch Logs Subscription Filters </strong>are a feature of CloudWatch Logs that allow you to distribute log data from one log group to other AWS services or destinations, such as Amazon Data Streams, Amazon Data Firehose, AWS Lambda, or Amazon S3.</p><p><strong>Amazon Data Firehose</strong> is a fully managed service that captures, transforms, and delivers streaming data to destinations like Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk.</p><p><img src="https://media.tutorialsdojo.com/public/td-kinesis-firehose-stream-plus-cloudwatchlogs-subs-filter-299-10-24.png"></p><p><strong>AWS Lambda</strong> is a serverless computing service that lets you run your code without provisioning or managing servers. You can create Lambda functions and configure them to be triggered by various events, such as API calls, database events, or changes to data in Amazon S3 or Amazon Kinesis.</p><p><strong>Amazon EventBridge</strong>: Amazon EventBridge is a serverless event bus service that makes it easier to build event-driven applications at scale. It delivers a stream of real-time data from your own applications, Software-as-a-Service (SaaS) applications, and AWS services and routes that data to targets like AWS Lambda functions or Amazon Kinesis streams.</p><p><img src="https://media.tutorialsdojo.com/public/td-createloggroup-eventbridge-rule-triggered-lambda3-29-10-24.png"></p><p>Amazon Data Firehose is the ideal solution for streaming CloudWatch Logs data directly into Amazon S3, providing a seamless, automated, and near real-time data transfer without requiring intermediary storage or manual scheduling of data transfer jobs. By setting up subscription filters on each log group, CloudWatch Logs data can be streamed directly to Data Firehose, which then automatically writes the data into the S3 bucket in the central logging account.</p><p>To ensure that all new log groups are included, the solution deploys a Lambda function that uses the <code>PutSubscriptionFilter</code> API to create subscription filters on new log groups as they are created. This Lambda function is triggered by an EventBridge rule set to detect the <code>CreateLogGroup</code> event, which automates the process of linking new log groups with the Data Firehose stream. This setup allows for centralized logging with minimal operational overhead and supports the automatic inclusion of future log groups.</p><p>Hence, the correct answer is: <strong>Configure subscription filters on all log groups to stream data to Data Firehose. Set up an Amazon Data Firehose stream to target the S3 bucket in the logging account. Deploy an AWS Lambda function to invoke the </strong><code><strong>PutSubscriptionFilter</strong></code><strong> API for new log groups triggered by an Amazon EventBridge rule for </strong><code><strong>CreateLogGroup</strong></code><strong> events.</strong></p><p>The option that says: <strong>Set up an S3 Batch Operations job that runs periodically to import log files from each CloudWatch Logs group and move them to the central S3 bucket in the logging account </strong>is incorrect because S3 Batch Operations is primarily designed to perform operations on existing objects in an S3 bucket, not to import data from other AWS services like CloudWatch Logs. Additionally, it would require manually configuring the job for each log group, which goes against the requirement of automatically forwarding logs from all existing and future log groups.</p><p>The option that says: <strong>Configure AWS DataSync to transfer log files from CloudWatch Logs to the central S3 bucket in the logging account. Schedule DataSync tasks to run on all CloudWatch Logs log groups across accounts</strong> is incorrect because AWS DataSync is typically designed for transferring data between on-premises storage and AWS storage services, or between AWS storage services. It is not intended for transferring data from other AWS services like CloudWatch Logs to S3.</p><p>The option that says: <strong>Subscribe an S3 bucket to a CloudWatch Logs subscription filter on each log group in all accounts. Use an Amazon EventBridge rule to invoke the AWS Lambda function whenever new log groups are created, ensuring they are linked to the subscription filter </strong>is incorrect. CloudWatch Logs cannot directly subscribe an S3 bucket to receive logs. Subscription filters in CloudWatch Logs are only designed to work with AWS services like Data Firehose or Lambda, not directly with S3.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample</a></p><p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule.html</a></p><p><br></p><p><strong>Check out the following Amazon Kinesis, Amazon Cloudwatch, and AWS lambda Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-kinesis/?src=udemy">https://tutorialsdojo.com/amazon-kinesis/</a></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><a href="https://tutorialsdojo.com/aws-lambda/?src=udemy">https://tutorialsdojo.com/aws-lambda/</a></p>',
        answers: [
          "<p>Set up an S3 Batch Operations job that runs periodically to import log files from each CloudWatch Logs group and move them to the central S3 bucket in the logging account.</p>",
          "<p>Configure AWS DataSync to transfer log files from CloudWatch Logs to the central S3 bucket in the logging account. Schedule DataSync tasks to run on all CloudWatch Logs log groups across accounts.</p>",
          "<p>Configure subscription filters on all log groups to stream data to Data Firehose. Set up an Amazon Data Firehose stream to target the S3 bucket in the logging account. Deploy an AWS Lambda function to invoke the <code>PutSubscriptionFilter</code> API for new log groups triggered by an Amazon EventBridge rule for <code>CreateLogGroup</code>.</p>",
          "<p>Subscribe an S3 bucket to a CloudWatch Logs subscription filter on each log group in all accounts. Use an Amazon EventBridge rule to invoke the AWS Lambda function whenever new log groups are created, ensuring they are linked to the subscription filter.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Monitoring and Logging",
      question_plain:
        "A DevOps Engineer manages multiple AWS accounts using AWS Organizations. The engineer must ensure that logs from all existing and future Amazon CloudWatch Logs log groups are automatically forwarded to a dedicated Amazon S3 bucket in a central logging account.Which solution will meet these requirements most effectively?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588499,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A web application is hosted on an Auto Scaling group (ASG) of On-Demand EC2 instances. It has a separate Linux EC2 instance used primarily for batch processing. This particular instance needs to update its configuration file based on the list of the active IP addresses of the instances within the ASG. This is needed in order to run a batch job properly.</p><p>Which of the following options allows effective updating of the configuration file whenever the Auto Scaling group scales in or out?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can use <strong>Amazon EventBridge (Amazon CloudWatch Events)</strong> to invoke AWS Systems Manager Run Command and perform actions on Amazon EC2 instances when certain events happen.</p><p><img src="https://media.tutorialsdojo.com/public/td-eventbridge-rule-event-pattern-10Jan2025.png"></p><p>You can use SSM Run Command to configure instances without having to login to an instance. This setup requires the SSM agent to be installed on your EC2 instance and that proper IAM permission has been granted. Additionally, create an Amazon EventBridge rule to monitor the launch or termination events of the Auto Scaling group. Set the Systems Manager Run Command as the target for this rule.</p><p>On the parameters of SSM Run Command, define the commands you need to update the configuration file. It is important that your target EC2 instance is properly tagged and defined on the SSM Run command as it will be the basis of SSM to identify your instance.</p><p>Hence, the correct answer is: <strong>Create an Amazon EventBridge rule for the Launch/Terminate events of the ASG. Set the target to an SSM Run Command that will update the configuration file on the target EC2 instance.</strong></p><p>The option that says: <strong>Develop a custom script that runs on the background to query active EC2 instances and IP addresses of the ASG. Then the script will update the configuration whenever it detects a change </strong>is incorrect. Although this may be a possible solution, you will still have to write and maintain your own script and that creates an unnecessary operational overhead. Also, running a script with regular intervals is not a good approach if you are just waiting for events.</p><p>The option that says: <strong>Create a CloudWatch Logs to monitor the Launch/Terminate events of the ASG. Set the target to a Lambda function that will update the configuration file inside the EC2 instance. Add a proper IAM permission to Lambda to access the EC2 instance </strong>is incorrect because you can\'t simply monitor the Auto Scaling Group events using CloudWatch Logs. You have to use Amazon EventBridge instead.</p><p>The option that says: <strong>Create an Amazon EventBridge rule that is scheduled to run every minute. Set the target to a Lambda function to query the ASG instances and update the configuration file on an S3 bucket. Automatically establish an SSH connection to the Linux EC2 instances by using another Lambda function and then download the master configuration file from S3 to update the local configuration file stored in the instance</strong> is incorrect. It is better to create an Amazon EventBridge rule that tracks the Launch/Terminate events of the Auto Scaling group (ASG) instead of running it every minute. AWS Lambda can’t directly establish an SSH connection and run a command inside an EC2 instance to update the configuration file, even with proper IAM permissions. A better solution would be to use the Systems Manager Run Command to let you remotely and securely manage the configuration of your managed instances.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-prereqs.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/rc-console.html</a></p><p><br></p><p><strong>Check out these Amazon CloudWatch and AWS Systems Manager Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>',
        answers: [
          "<p>Develop a custom script that runs on the background to query active EC2 instances and IP addresses of the ASG. Then the script will update the configuration whenever it detects a change.</p>",
          "<p>Create an Amazon EventBridge rule that is scheduled to run every minute. Set the target to a Lambda function to query the ASG instances and update the configuration file on an S3 bucket. Automatically establish an SSH connection to the Linux EC2 instances by using another Lambda function and then download the master configuration file from S3 to update the local configuration file stored in the instance.</p>",
          "<p>Create a CloudWatch Logs to monitor the Launch/Terminate events of the ASG. Set the target to a Lambda function that will update the configuration file inside the EC2 instance. Add a proper IAM permission to Lambda to access the EC2 instance.</p>",
          "<p>Create an Amazon EventBridge rule for the Launch/Terminate events of the ASG. Set the target to an SSM Run Command that will update the configuration file on the target EC2 instance.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Monitoring and Logging",
      question_plain:
        "A web application is hosted on an Auto Scaling group (ASG) of On-Demand EC2 instances. It has a separate Linux EC2 instance used primarily for batch processing. This particular instance needs to update its configuration file based on the list of the active IP addresses of the instances within the ASG. This is needed in order to run a batch job properly.Which of the following options allows effective updating of the configuration file whenever the Auto Scaling group scales in or out?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588501,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The company operates a fleet of 400 Amazon EC2 instances to support a High-Performance Computing (HPC) application. The fleet is configured with target tracking scaling and is managed behind an Application Load Balancer (ALB). There is a requirement to create a simple web page hosted on an Amazon S3 bucket that displays the status of the EC2 instances in the fleet. This web page must be updated whenever a new instance is launched or terminated. Additionally, a searchable log of these events is required for review at a later time.</p><p>Which of the following options should be implemented to meet these requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can create an <strong>AWS Lambda function</strong> that logs the changes in state for an Amazon EC2 instance. You can create a rule that runs the function whenever there is a state transition or a transition to one or more states that are of interest. Be sure to assign proper permissions to your Lambda function to write to S3 and to send logs to CloudWatch Logs. After creating the Lambda function, create a rule on Amazon EventBridge that will watch for the scale-in/scale-out events. Set a trigger to run your Lambda function which will then update the S3 webpage and send logs to CloudWatch Logs.</p><p><img src="https://media.tutorialsdojo.com/public/td-amazon-eventbridge-event-pattern-27Jan2025.png"></p><p>Hence, the correct answer is: <strong>Write your AWS Lambda function to update the simple webpage on S3 and send event logs to CloudWatch Logs. Create an EventBridge rule to invoke the Lambda for scale-in/scale-out events.</strong></p><p>The option that says: <strong>Create an Amazon EventBridge rule for the scale-in/scale-out event and deliver these logs to Amazon CloudWatch Logs. Manually export the CloudWatch Log group to the S3 bucket to view them </strong>is incorrect because you have to export the logs manually on an S3 bucket. This action is not recommended since you can simply use a Lambda function to log the changes of the EC2 instances to a file in the S3 bucket.</p><p>The option that says: <strong>Create an EventBridge rule for the scale-in/scale-out event and create two targets for the event - one target to the webpage S3 bucket and another to deliver event logs to CloudWatch Logs </strong>is incorrect because you cannot set an S3 bucket or object as a target for an EventBridge rule.</p><p>The option that says: <strong>No need to do anything. AWS CloudTrail records these events and you can view and search the logs on the console </strong>is incorrect. Although this is true, it would be hard to search all the relevant logs from the trail. Moreover, it typically doesn’t provide a solution for the required S3 web page.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Tutorials.html ">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Tutorials.html</a></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Create an Amazon EventBridge rule for the scale-in/scale-out event and deliver these logs to Amazon CloudWatch Logs. Manually export the CloudWatch Log group to the S3 bucket to view them.</p>",
          "<p>Write your AWS Lambda function to update the simple webpage on S3 and send event logs to CloudWatch Logs. Create an EventBridge rule to invoke the Lambda for scale-in/scale-out events.</p>",
          "<p>Create an EventBridge rule for the scale-in/scale-out event and create two targets for the event - one target to the webpage S3 bucket and another to deliver event logs to CloudWatch Logs.</p>",
          "<p>No need to do anything. AWS CloudTrail records these events and you can view and search the logs on the console.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Monitoring and Logging",
      question_plain:
        "The company operates a fleet of 400 Amazon EC2 instances to support a High-Performance Computing (HPC) application. The fleet is configured with target tracking scaling and is managed behind an Application Load Balancer (ALB). There is a requirement to create a simple web page hosted on an Amazon S3 bucket that displays the status of the EC2 instances in the fleet. This web page must be updated whenever a new instance is launched or terminated. Additionally, a searchable log of these events is required for review at a later time.Which of the following options should be implemented to meet these requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588503,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has confidential files containing patent information stored in an Amazon S3 bucket in the US East (Ohio) region. Among the team members working on the project, actions need to be monitored on the objects, such as PUT, GET, and DELETE operations. The goal is to search and review these actions for auditing purposes easily.</p><p>Which solution will meet this requirement MOST cost-effectively?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>When an event occurs in your account, CloudTrail evaluates whether the event matches the settings for your trails. Only events that match your trail settings are delivered to your Amazon S3 bucket and Amazon CloudWatch Logs log group.</p><p>You can configure your trails to log the following:</p><p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#logging-data-events"><strong>Data events</strong></a>: These events provide insight into the resource operations performed on or within a resource. These are also known as data plane operations.</p><p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#logging-management-events"><strong>Management events</strong></a>: Management events provide insight into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Management events can also include non-API events that occur in your account. For example, when a user logs in to your account, CloudTrail logs the <code>ConsoleLogin</code> event.</p><p>You can configure multiple trails differently so that the trails process and log only the events that you specify. For example, one trail can log read-only data and management events so that all read-only events are delivered to one S3 bucket. Another trail can log only write-only data and management events, so that all write-only events are delivered to a separate S3 bucket.</p><p>You can also configure your trails to have one trail log and deliver all management events to one S3 bucket, and configure another trail to log and deliver all data events to another S3 bucket.</p><p><img alt="AWS CloudTrail" src="https://media.tutorialsdojo.com/public/td-aws-cloudtrail-13Feb2025.jpg" width="1000"></p><p>Data events provide insight into the resource operations performed on or within a resource. These are also known as data plane operations. Data events are often high-volume activities.</p><p>Example data events include:</p><p>- Amazon S3 object-level API activity (for example, <code>GetObject</code>, <code>DeleteObject</code>, and <code>PutObject</code> API operations)</p><p>- AWS Lambda function execution activity (the <code>Invoke</code> API)</p><p>Data events are disabled by default when you create a trail. To record CloudTrail data events, you must explicitly add the supported resources or resource types for which you want to collect activity to a trail.</p><p>You can log the object-level API operations on your S3 buckets. Before Amazon EventBridge can match these events, you must use AWS CloudTrail to set up a trail configured to receive these events. To log data events for an S3 bucket to AWS CloudTrail and EventBridge, create a trail. A trail captures API calls and related events in your account and delivers the log files to an S3 bucket that you specify. After you create a trail and configure it to capture the log files you want, you need to be able to find the log files and interpret the information they contain.</p><p>Typically, log files appear in your bucket within 15 minutes of the recorded AWS API call or other AWS event. Then you need to create a Lambda function to log data events for your S3 bucket. Finally, you need to create a trigger to run your Lambda function in response to an Amazon S3 data event. You can create this rule on Amazon EventBridge and setting Lambda function as the target. Your logs will show up on the CloudWatch log group, which you can view and search as needed.</p><p>Hence, the correct answer is: <strong>Create an AWS CloudTrail trail to track and store your S3 API call logs in an S3 bucket. Create an AWS Lambda function that logs data events of your S3 bucket. Trigger this Lambda function using the Amazon EventBridge rule for every action taken on your S3 objects. View the logs on the Amazon CloudWatch Logs group.</strong></p><p>The option that says: <strong>Enable logging on your Amazon S3 bucket to save the access logs on a separate S3 bucket. Import logs to Amazon OpenSearch service to easily search and query the logs</strong> is incorrect because this would simply incur more cost if you provision an OpenSearch cluster. Take note that the scenario asks for the most cost-effective solution.</p><p>The option that says: <strong>Enable logging on your S3 bucket. Create an Amazon EventBridge rule that watches the object-level events of your S3 bucket. Create a target on the rule to send the event logs to an Amazon CloudWatch Log group. View and search the logs on CloudWatch Logs</strong> is incorrect because before, Amazon EventBridge could match API call events. You must use AWS CloudTrail first to set up a trail configured to receive these events.</p><p>The option that says: <strong>Create an Amazon CloudWatch Log group and configure S3 logging to send object-level events to this log group. View and search the event logs on the created CloudWatch log group</strong> is incorrect because you can\'t typically configure an S3 bucket to directly send access logs to a CloudWatch log group.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/aws/cloudtrail-update-capture-and-process-amazon-s3-object-level-api-activity/">https://aws.amazon.com/blogs/aws/cloudtrail-update-capture-and-process-amazon-s3-object-level-api-activity/</a></p><p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#example-logging-all-S3-objects">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#example-logging-all-S3-objects</a></p><p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudtrail/?src=udemy">https://tutorialsdojo.com/aws-cloudtrail/</a></p>',
        answers: [
          "<p>Enable logging on your Amazon S3 bucket to save the access logs on a separate S3 bucket. Import logs to Amazon OpenSearch service to easily search and query the logs.</p>",
          "<p>Enable logging on your S3 bucket. Create an Amazon EventBridge rule that watches the object-level events of your S3 bucket. Create a target on the rule to send the event logs to an Amazon CloudWatch Log group. View and search the logs on CloudWatch Logs.</p>",
          "<p>Create an AWS CloudTrail trail to track and store your S3 API call logs in an S3 bucket. Create an AWS Lambda function that logs data events of your S3 bucket. Trigger this Lambda function using the Amazon EventBridge rule for every action taken on your S3 objects. View the logs on the Amazon CloudWatch Logs group.</p>",
          "<p>Create an Amazon CloudWatch Log group and configure S3 logging to send object-level events to this log group. View and search the event logs on the created CloudWatch log group.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Monitoring and Logging",
      question_plain:
        "A company has confidential files containing patent information stored in an Amazon S3 bucket in the US East (Ohio) region. Among the team members working on the project, actions need to be monitored on the objects, such as PUT, GET, and DELETE operations. The goal is to search and review these actions for auditing purposes easily.Which solution will meet this requirement MOST cost-effectively?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588505,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A DevOps Engineer has recently imported a Linux VM hosted on-premises to Amazon EC2. This instance is running a legacy application that is difficult to replicate or back up. However, this instance still requires backups since it holds important data. The solution is to take an Amazon EBS snapshot of the volume of the instance every day.</p><p>Which of the following is the EASIEST way to automate the backup process?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can call the EBS Create Snapshot directly as a target from Amazon EventBridge. You can also configure Amazon EventBridge rules to create automated snapshots of an existing Amazon Elastic Block Store (Amazon EBS) volume on a schedule. EventBridge allows you to specify a fixed rate for creating snapshots at regular intervals or use a cron expression to schedule snapshots at specific times of the day.</p><p><img src="https://media.tutorialsdojo.com/public/td-ebs-create-snapshot-23Jan2025.png"></p><p>Creating rules with built-in targets is supported only in the AWS Management Console. From the Amazon EventBridge console, create a new rule scheduled to run at midnight every day. In the Targets section, select EBS Create Snapshot as the target, and input the Amazon EBS volume ID that you need to snapshot. You can specify multiple targets if you need to create snapshots of multiple EBS volumes.</p><p>Alternatively, you can also use the Amazon Data Lifecycle Manager (DLM) for EBS Snapshots. This service provides a simple, automated way to back up data stored on Amazon EBS volumes. You can define backup and retention schedules for EBS snapshots by creating lifecycle policies based on tags. With this feature, you no longer have to rely on custom scripts to create and manage your backups.</p><p>Hence, the correct answer is:<strong> Create an EventBridge rule that is scheduled to run at midnight. Set the target to directly call the EBS Create Snapshot to create a snapshot of the needed EBS volumes.</strong></p><p>The option that says: <strong>Create an Amazon EventBridge rule that is scheduled every midnight. Set the target to an AWS Lambda function that will run the EBS Create Snapshot to your defined EBS volumes </strong>is incorrect. Although this solution is valid, you actually don\'t need to launch a custom Lambda function for this scenario. Remember that the scenario asks for the solution that is easiest to implement.</p><p>The option that says: <strong>Create an EventBridge rule that is scheduled every midnight. Set the target to an AWS Systems Manager Run command to run the EBS Create Snapshot call of the EBS volumes from inside the EC2 instance</strong> is incorrect. Although this solution meets the requirement, it entails an extra step of using the Systems Manager Run command. This is not required at all since you can simply directly call the EBS Create Snapshot from the EventBridge rule as a target.</p><p>The option that says: <strong>Create a script from inside the instance that will run the </strong><code><strong>aws ec2 create-snapshot</strong></code><strong> command. Schedule it to run every midnight using crontab</strong> is incorrect because it entails a lot of manual scripting and configuration. A better solution is just to use the EventBridge rule or use the Amazon Data Lifecycle Manager (DLM) for EBS Snapshots instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html</a></p><p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html</a></p><p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule.html</a></p><p><br></p><p><strong>Check out this Amazon EventBridge Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-eventbridge/?src=udemy">https://tutorialsdojo.com/amazon-eventbridge/</a></p>',
        answers: [
          "<p>Create an Amazon EventBridge rule that is scheduled every midnight. Set the target to an AWS Lambda function that will run the EBS Create Snapshot to your defined EBS volumes.</p>",
          "<p>Create an EventBridge rule that is scheduled every midnight. Set the target to an AWS Systems Manager Run command to run the EBS Create Snapshot call of the EBS volumes from inside the EC2 instance.</p>",
          "<p>Create an EventBridge rule that is scheduled to run at midnight. Set the target to directly call the EBS Create Snapshot to create a snapshot of the needed EBS volumes.</p>",
          "<p>Create a script from inside the instance that will run the <code>aws ec2 create-snapshot</code> command. Schedule it to run every midnight using crontab.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A DevOps Engineer has recently imported a Linux VM hosted on-premises to Amazon EC2. This instance is running a legacy application that is difficult to replicate or back up. However, this instance still requires backups since it holds important data. The solution is to take an Amazon EBS snapshot of the volume of the instance every day.Which of the following is the EASIEST way to automate the backup process?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588507,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is running a batch job hosted on AWS Fargate to process large ZIP files. The job is triggered whenever files are uploaded on an Amazon S3 bucket. To save costs, the company wants to set the minimum number of ECS Tasks to 1, and only increase the task count when objects are uploaded to the S3 bucket again. Once processing is complete, the S3 bucket should be emptied out and all ECS tasks should be stopped. The object-level logging has already been enabled in the bucket.</p><p>Which of the following options is the EASIEST way to implement this?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can use Amazon EventBridge (formerly Amazon CloudWatch Events) to run Amazon ECS tasks when certain AWS events occur. You can set up an EventBridge rule that runs an Amazon ECS task whenever a file is uploaded to a certain Amazon S3 bucket using the Amazon S3 PUT operation.</p><p>First, you must create an Amazon EventBridge rule for the S3 service that will watch for object-level operations – PUT and DELETE objects. For object-level operations, it is required to create a CloudTrail trail first. You need two rules – one for running a task whenever a file is uploaded to S3 and another for stopping all the ECS tasks. For the first rule, select “ECS task” as the target and input the needed values such as the cluster name, task definition, and the task count. For the second rule, select a Lambda function as the target. To stop a running task, you need to call the StopTask API, which can be done in a Lambda function.</p><p><img src="https://media.tutorialsdojo.com/public/overview-fargate.png"></p><p>Hence, the correct answer is: <strong>Create an EventBridge rule to detect S3 object PUT operations and set the target to the ECS cluster to run a new ECS task. Create another EventBridge rule to detect S3 DELETE operations. Set the target to a Lambda function that will stop all ECS tasks.</strong></p><p>The option that says: <strong>Create an Amazon EventBridge rule to detect S3 object PUT operations and set the target to an AWS Lambda function that will update the desired count of tasks in the ECS service using an ECS capacity provider. Create another EventBridge rule to detect S3 DELETE operations and use the Lambda function to scale down the ECS tasks by updating the capacity provider settings </strong>is incorrect because ECS capacity providers are primarily designed to manage the underlying infrastructure for ECS tasks, such as EC2 instances or Fargate Spot, rather than to scale task counts based on event triggers directly.</p><p>The option that says: <strong>Use Amazon CloudWatch Alarms on AWS CloudTrail since the S3 object-level operations are recorded on CloudTrail. Create two Lambda functions for increasing/decreasing the ECS task count. Set these as respective targets for the CloudWatch Alarm depending on the S3 event </strong>is incorrect because using CloudTrail, CloudWatch Alarm, and two Lambda functions will only create unnecessary complexity to what you want to achieve. Amazon EventBridge can directly target an ECS task on the Targets section when you create a new rule.</p><p>The option that says: <strong>Use CloudWatch Alarms on CloudTrail since this S3 object-level operations are recorded on CloudTrail. Set two alarm actions to update ECS task count to scale-out/scale-in depending on the S3 event</strong> is incorrect because you can’t directly set CloudWatch Alarms to update the ECS task count. You simply have to use Amazon EventBridge instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-tutorial-ECS.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-tutorial-ECS.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>',
        answers: [
          "<p>Create an Amazon EventBridge rule to detect S3 object PUT operations and set the target to an AWS Lambda function that will update the desired count of tasks in the ECS service using an ECS capacity provider. Create another EventBridge rule to detect S3 DELETE operations and use the Lambda function to scale down the ECS tasks by updating the capacity provider settings.</p>",
          "<p>Create an EventBridge rule to detect S3 object PUT operations and set the target to the ECS cluster to run a new ECS task. Create another EventBridge rule to detect S3 DELETE operations. Set the target to a Lambda function that will stop all ECS tasks.</p>",
          "<p>Use Amazon CloudWatch Alarms on AWS CloudTrail since the S3 object-level operations are recorded on CloudTrail. Create two Lambda functions for increasing/decreasing the ECS task count. Set these as respective targets for the CloudWatch Alarm depending on the S3 event.</p>",
          "<p>Use CloudWatch Alarms on CloudTrail since this S3 object-level operations are recorded on CloudTrail. Set two alarm actions to update ECS task count to scale-out/scale-in depending on the S3 event.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A company is running a batch job hosted on AWS Fargate to process large ZIP files. The job is triggered whenever files are uploaded on an Amazon S3 bucket. To save costs, the company wants to set the minimum number of ECS Tasks to 1, and only increase the task count when objects are uploaded to the S3 bucket again. Once processing is complete, the S3 bucket should be emptied out and all ECS tasks should be stopped. The object-level logging has already been enabled in the bucket.Which of the following options is the EASIEST way to implement this?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588509,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An Amazon S3 bucket named &lt;code&gt;team-cebu-devops&lt;/code&gt; is shared by all the DevOps administrators in the team. It is used to store artifact files for several CI/CD pipelines. A junior developer accidentally modified the S3 bucket policy, denying all pipelines from downloading the required artifact files and causing all deployments to halt.</p><p>To prevent similar issues in the future, the team wants to be notified of any S3 bucket policy changes to identify and take action if any problem occurs quickly.</p><p>Which of the following options will help achieve this?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can configure alarms for several scenarios in CloudTrail events. In this case, you can create an Amazon CloudWatch alarm that is triggered when an Amazon S3 API call is made to PUT or DELETE bucket policy, bucket lifecycle, bucket replication, or to PUT a bucket ACL. A CloudTrail trail is required since it will send its logs to a CloudWatch Log group. To create an alarm, you must first create a metric filter and then configure an alarm based on the filter.</p><p><img src="https://media.tutorialsdojo.com/AWS-CloudTrail-to-CWLogs.jpg"></p><p>For this scenario, since all CloudTrail events will be sent to the Log group, you will need to create a Metric to filter specific S3 events that change the bucket policy of your <code>team-cebu-devops</code> bucket. For notification, you will then create a CloudWatch Alarm for this Metric with a threshold of &gt;=1 and set your email as a notification recipient. Even a single S3 bucket event on the log group will trigger this alarm and should send you a notification.</p><p>Hence, the correct answer is: <strong>Create an AWS CloudTrail trail that sends logs to an Amazon CloudWatch log group. Create a CloudWatch Metric Filter on the log group for S3 bucket policy events. Create an Alarm that will notify you whenever this metric threshold is reached.</strong></p><p>The option that says: <strong>Enable S3 server access logging on your bucket. Create an Amazon CloudWatch Metric Filter for bucket policy events. Create an Alarm for this metric to notify you whenever an event is matched<em> </em></strong>is incorrect because S3 Server access logging is primarily used to provide detailed records for the requests that are made to a bucket. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant. It is more appropriate to use CloudWatch or CloudTrail to track the S3 bucket policy changes.</p><p>The option that says: <strong>Enable S3 server access logging on your bucket. Send the access logs to an Amazon CloudWatch log group. Create an Amazon CloudWatch Metric Filter for bucket policy events on the log group. Create an Alarm for this metric to notify you whenever the threshold is reached </strong>is incorrect because you can’t directly send the S3 server access logs to CloudWatch logs. You need to use CloudTrail to send the events to a log group before you can create a metric and alarm for those events.</p><p>The option that says: <strong>Create an AWS CloudTrail trail that sends logs to an Amazon CloudWatch log group. Create an Amazon EventBridge rule for S3 bucket policy events on the log group. Create an Alarm based on the event that will send you a notification whenever an event is matched<em> </em></strong>is incorrect because you can’t simply use an EventBridge rule to filter your log groups directly.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ConsoleAlarms.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ConsoleAlarms.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source-console.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source-console.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudtrail/?src=udemy">https://tutorialsdojo.com/aws-cloudtrail/</a></p>',
        answers: [
          "<p>Enable S3 server access logging on your bucket. Create an Amazon CloudWatch Metric Filter for bucket policy events. Create an Alarm for this metric to notify you whenever an event is matched.</p>",
          "<p>Create an AWS CloudTrail trail that sends logs to an Amazon CloudWatch log group. Create a CloudWatch Metric Filter on the log group for S3 bucket policy events. Create an Alarm that will notify you whenever this metric threshold is reached.</p>",
          "<p>Enable S3 server access logging on your bucket. Send the access logs to an Amazon CloudWatch log group. Create an Amazon CloudWatch Metric Filter for bucket policy events on the log group. Create an Alarm for this metric to notify you whenever the threshold is reached.</p>",
          "<p>Create an AWS CloudTrail trail that sends logs to an Amazon CloudWatch log group. Create an Amazon EventBridge rule for S3 bucket policy events on the log group. Create an Alarm based on the event that will send you a notification whenever an event is matched.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Monitoring and Logging",
      question_plain:
        "An Amazon S3 bucket named &lt;code&gt;team-cebu-devops&lt;/code&gt; is shared by all the DevOps administrators in the team. It is used to store artifact files for several CI/CD pipelines. A junior developer accidentally modified the S3 bucket policy, denying all pipelines from downloading the required artifact files and causing all deployments to halt.To prevent similar issues in the future, the team wants to be notified of any S3 bucket policy changes to identify and take action if any problem occurs quickly.Which of the following options will help achieve this?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588511,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A Data Analytics team uses a large Hadoop cluster with 100 EC2 instance nodes for gathering trends about user behavior on the company’s mobile app. Currently, the DevOps team forwards any email notification they receive from the AWS Health service to the Data Analytics team whenever there is a particular EC2 instance that needs maintenance or any old EC2 instance types that will be retired by Amazon soon. The Data Analytics team needs to determine how the maintenance will affect their cluster and ensure that their Hadoop Distributed File System (HDFS) component can recover from any failure.</p><p>Which of the following is the FASTEST way to automate this notification process?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can use AWS EventBridge (Amazon CloudWatch Events) to detect and react to changes in the status of AWS Personal Health Dashboard (AWS Health) events. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when an event matches the values that you specify in a rule.</p><p>Depending on the type of event, you can send notifications, capture event information, take corrective action, initiate events, or take other actions. In this scenario, you first need to create an Events rule for AWS Health and choose the "EC2 service” as well as specific categories of events or scheduled changes that are planned to your account.</p><p><img src="https://media.tutorialsdojo.com/aws-health-event-patterns.png"></p><p>Then, you can then set the Target of this rule to an SNS topic on which the Data Analytics team will subscribe. For every event that matches this event rule, a notification will be sent to the subscribers of your SNS topic.</p><p>Hence, the correct answer is: <strong>Create an AWS EventBridge (Amazon CloudWatch Events) rule for AWS Health. Select EC2 Service and select the Events you want to get notified. Set the target to an Amazon SNS topic that the Data Analytics team is subscribed to.</strong></p><p>The option that says: <strong>Create a CloudWatch Metric Filter for AWS Health Events of your related EC2 instances. Create a CloudWatch Alarm for this metric to notify you whenever the threshold is reached</strong> is incorrect because you can only create a Metric filter for CloudWatch log groups. Take note that the AWS Health events are not automatically sent to CloudWatch Log groups; thus, you have to manually set up a data stream for AWS Health.</p><p>The option that says: <strong>Use AWS Systems Manager (SSM) Resource Data Sync to track the AWS Health Events of your related EC2 instances. Configure the SSM Resource Data Sync to automatically poll the AWS Health API for Amazon EC2 updates. Send a notification to the Data Analytics team whenever an EC2 event is matched </strong>is incorrect. Keep in mind that the AWS Systems Manager resource data sync feature is primarily used in AWS Systems Manager Inventory to send inventory data collected from all of your managed nodes to a single Amazon Simple Storage Service (Amazon S3) bucket. The SSM resource data sync will automatically update the centralized data in Amazon S3 when new inventory data is collected. This resource data sync feature is not applicable in this scenario.</p><p>The option that says: <strong>Add your Data Analytics team email as an alternate contact for your AWS account. They will receive these EC2 maintenance emails directly, so you won’t have to forward it to them </strong>is incorrect because the Data Analytics team will also receive other financial or account/billing-related emails using this setup. This is a security risk since the Data Analytics team email could potentially be compromised and be used to access your AWS account.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><a href="https://aws.amazon.com/blogs/aws/new-cloudwatch-events-track-and-respond-to-changes-to-your-aws-resources/ ">https://aws.amazon.com/blogs/aws/new-cloudwatch-events-track-and-respond-to-changes-to-your-aws-resources/</a></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide//automating_with_cloudwatch_events.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide//automating_with_cloudwatch_events.html</a></p>',
        answers: [
          "<p>Create an AWS EventBridge (Amazon CloudWatch Events) rule for AWS Health. Select EC2 Service and select the Events you want to get notified. Set the target to an Amazon SNS topic that the Data Analytics team is subscribed to.</p>",
          "<p>Create a CloudWatch Metric Filter for AWS Health Events of your related EC2 instances. Create a CloudWatch Alarm for this metric to notify you whenever the threshold is reached.</p>",
          "<p>Use AWS Systems Manager (SSM) Resource Data Sync to track the AWS Health Events of your related EC2 instances. Configure the SSM Resource Data Sync to automatically poll the AWS Health API for Amazon EC2 updates. Send a notification to the Data Analytics team whenever an EC2 event is matched</p>",
          "<p>Add your Data Analytics team email as an alternate contact for your AWS account. They will receive these EC2 maintenance emails directly so you won’t have to forward it to them.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Monitoring and Logging",
      question_plain:
        "A Data Analytics team uses a large Hadoop cluster with 100 EC2 instance nodes for gathering trends about user behavior on the company’s mobile app. Currently, the DevOps team forwards any email notification they receive from the AWS Health service to the Data Analytics team whenever there is a particular EC2 instance that needs maintenance or any old EC2 instance types that will be retired by Amazon soon. The Data Analytics team needs to determine how the maintenance will affect their cluster and ensure that their Hadoop Distributed File System (HDFS) component can recover from any failure.Which of the following is the FASTEST way to automate this notification process?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588513,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>An organization has a fleet of Amazon EC2 instances and uses SSH for remote access. Whenever a DevOps Engineer leaves the organization, the SSH keys are rotated as a security measure. The Chief Information Officer (CIO) required to stop the use of EC2 key pairs for operational efficiency and instead utilize AWS Systems Manager Session Manager. To strengthen security, access to Session Manager should be limited solely through a private network.</p><p>Which set of actions should the new DevOps Engineer take to meet the CIO requirements? (Select TWO.)</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>With <strong>Session Manager</strong>, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs).</p><p>By default, AWS Systems Manager doesn\'t have permission to perform actions on your instances. You must grant access by using AWS Identity and Access Management (IAM).</p><p>An <strong>instance profile</strong> that contains the AWS managed policy <strong>AmazonSSMManagedInstanceCore </strong>is needed to be attached to the EC2 instance for the <strong>Session Manager</strong> to work.</p><p><img src=" https://media.tutorialsdojo.com/public/dop-c02-ssm-session-manager.png"></p><p>The security posture of managed instances (including those in a hybrid environment) can be improved by configuring AWS Systems Manager to use an interface <strong>VPC endpoint</strong> in Amazon Virtual Private Cloud (Amazon VPC). An interface VPC endpoint (interface endpoint) can be used to connect to services powered by AWS PrivateLink, which is a technology that enables <strong>private access to Amazon Elastic Compute Cloud (Amazon EC2) and Systems Manager</strong> APIs using private IP addresses.</p><p>Hence, the correct answers are the option that says:</p><p>- <strong>Create an IAM instance profile to be associated with the fleet of EC2 instances and attach an IAM policy with the required Systems Manager permissions</strong></p><p><strong>- Provision a VPC endpoint for Systems Manager in the designated region</strong></p><p>The option that says: <strong>Enable incoming traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances </strong>is incorrect because Session Manager will work without the need to open inbound ports.</p><p>The option that says: <strong>Launch a new EC2 instance that will function as a bastion host for the other EC2 instances in the fleet </strong>is incorrect because a bastion host will not meet the requirement as it will still use SSH key pairs to remote access.</p><p>The option that says: <strong>Enable outbound traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances </strong>is incorrect because Session Manager will work without the need to open outbound ports.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-instance-profile.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-instance-profile.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Create an IAM instance profile to be associated with the fleet of EC2 instances and attach an IAM policy with the required Systems Manager permissions.</p>",
          "<p>Enable incoming traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances.</p>",
          "<p>Launch a new EC2 instance that will function as a bastion host for the other EC2 instances in the fleet.</p>",
          "<p>Enable outbound traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances.</p>",
          "<p>Provision a VPC endpoint for Systems Manager in the designated region.</p>",
        ],
      },
      correct_response: ["a", "e"],
      section: "Security and Compliance",
      question_plain:
        "An organization has a fleet of Amazon EC2 instances and uses SSH for remote access. Whenever a DevOps Engineer leaves the organization, the SSH keys are rotated as a security measure. The Chief Information Officer (CIO) required to stop the use of EC2 key pairs for operational efficiency and instead utilize AWS Systems Manager Session Manager. To strengthen security, access to Session Manager should be limited solely through a private network.Which set of actions should the new DevOps Engineer take to meet the CIO requirements? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 134588515,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company currently has an internal web application hosted on EC2 but plans to rearchitect it using serverless architecture to optimize costs and take full advantage of the cloud's capabilities. However, the Chief Information Officer (CIO) is concerned about how to deploy changes to its application during the migration. The current deployment method involves creating a new Auto Scaling group of EC2 instances, a new Application Load Balancer, and using a Route 53 weighted routing policy to shift traffic.</p><p>The CIO is required to retain the ability to try out new features on a limited number of users before releasing the traffic to all users. The new serverless application will include Amazon API Gateway and AWS Lambda.</p><p>Which of the following deployment options should the DevOps Engineer implement to fulfill the requirements with the LEAST amount of effort?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>AWS Serverless Application Model (AWS SAM) comes built-in with CodeDeploy to provide gradual AWS Lambda deployments. With just a few lines of configuration, AWS SAM does the following for you:</p><p>-Deploys new versions of your Lambda function and automatically creates aliases that point to the new version.</p><p>-Gradually shifts customer traffic to the new version until you\'re satisfied that it\'s working as expected. If an update doesn\'t work correctly, you can roll back the changes.</p><p>-Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and that your application operates as expected.</p><p>Automatically rolls back the deployment if CloudWatch alarms are triggered.</p><p>If gradual deployments are enabled through AWS SAM template, a CodeDeploy resource is automatically created and can view the CodeDeploy resource directly through the AWS Management Console.</p><p>The following example demonstrates the use of CodeDeploy to gradually shift traffic to newly deployed version of Lambda function:</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-aws-sam-gradual-deployment.png"></p><p>Hence, the correct answer is the option that says: <strong>Deploy the API Gateway resource and Lambda functions by utilizing AWS Serverless Application Model (AWS SAM). For code changes, update the AWS CloudFormation stack and deploy the updated versions of APIs and Lambda functions. Implement a canary release strategy by setting the </strong><code><strong>DeploymentPreference</strong></code><strong> type to </strong><code><strong>Canary10Percent10Minutes</strong></code><strong>.</strong></p><p>The option that says: <strong>Deploy the API Gateway resource and Lambda functions by utilizing AWS Cloud Development Kit (AWS CDK). For code changes, update the AWS CloudFormation stack and deploy the updated versions of APIs and Lambda functions. Shift traffic gradually by setting a Route 53 failover routing policy for the canary release strategy</strong> is incorrect because API Gateway and Lambda function already has a built-in capability for canary release deployment. In addition, a failover routing policy is not primarily used for gradually shifting traffic to test new features on a limited number of users before releasing the traffic to all users. It is more for a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable.</p><p>The option that says:<strong> Deploy the API Gateway resource and Lambda functions using AWS CloudFormation. For code changes, create a new CloudFormation stack with the new version of the API Gateway and Lambda function then perform a blue/green deployment to gradually shift traffic</strong> is incorrect because it doesn’t meet the CIO’s requirement to test new features on a limited number of users. While AWS CloudFormation and blue/green deployments are useful for managing and deploying infrastructure changes, they don’t inherently provide a way to expose new features to a subset of users in a production environment. Blue/green deployments typically shift all traffic from one environment to another, which doesn’t align with the CIO’s requirement.</p><p>The option that says: <strong>Deploy the API Gateway resource and Lambda functions in AWS Elastic Beanstalk. Use blue/green deployment in Elastic Beanstalk when deploying new versions of API Gateway and Lambda functions</strong> is incorrect because Elastic Beanstalk is used to automatically handle the deployment from capacity provisioning, load balancing, and auto scaling to application health monitoring. It cannot be used to deploy Lambda functions and API Gateway.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html</a></p><p><a href="https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><br></p><p><strong>Check out this AWS SAM Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-serverless-application-model-sam/?src=udemy">https://tutorialsdojo.com/aws-serverless-application-model-sam/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Deploy API Gateway resource and Lambda functions by utilizing AWS Cloud Development Kit (AWS CDK). For code changes, update the AWS CloudFormation stack and deploy the updated versions of APIs and Lambda functions. Shift traffic gradually by setting a Route 53 failover routing policy for the canary release strategy.</p>",
          "<p>Deploy the API Gateway resource and Lambda functions using AWS CloudFormation. For code changes, create a new CloudFormation stack with the new version of the API Gateway and Lambda function then perform a blue/green deployment to gradually shift traffic.</p>",
          "<p>Deploy API Gateway resource and Lambda functions by utilizing AWS Serverless Application Model (AWS SAM). For code changes, update the AWS CloudFormation stack and deploy the updated versions of APIs and Lambda functions. Implement a canary release strategy by setting the <code>DeploymentPreference</code> type to <code>Canary10Percent10Minutes</code>.</p>",
          "<p>Deploy the API Gateway resource and Lambda functions in AWS Elastic Beanstalk. Use blue/green deployment in Elastic Beanstalk when deploying new versions of API Gateway and Lambda functions.</p>",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "A company currently has an internal web application hosted on EC2 but plans to rearchitect it using serverless architecture to optimize costs and take full advantage of the cloud's capabilities. However, the Chief Information Officer (CIO) is concerned about how to deploy changes to its application during the migration. The current deployment method involves creating a new Auto Scaling group of EC2 instances, a new Application Load Balancer, and using a Route 53 weighted routing policy to shift traffic.The CIO is required to retain the ability to try out new features on a limited number of users before releasing the traffic to all users. The new serverless application will include Amazon API Gateway and AWS Lambda.Which of the following deployment options should the DevOps Engineer implement to fulfill the requirements with the LEAST amount of effort?",
      related_lectures: [],
    },
  ],
};
