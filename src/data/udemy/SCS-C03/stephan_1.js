export const stephan_1 = {
  count: 65,
  next: null,
  previous: null,
  results: [
    {
      _class: "assessment",
      id: 83960102,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>At XYZ Corporation, the IT team had recently discovered a security loophole that could potentially allow unauthorized access to sensitive data. To fix the issue and ensure the protection of their company's information, the team wants to establish the ability to delete an AWS KMS Customer Master Key (CMK) within a 24-hour timeframe. This would prevent the key from being used for encrypt or decrypt operations and keep their data secure.</p>\n\n<p>Which of the following solutions will address the given use case?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Implement the KMS import key function to perform an immediate delete operation</strong></p>\n\n<p>An AWS KMS key is a logical representation of a cryptographic key. A KMS key contains metadata, such as the key ID, key spec, key usage, creation date, description, and key state. Most importantly, it contains a reference to the key material that is used when you perform cryptographic operations with the KMS key.</p>\n\n<p>You can use the KMS import key function to set an expiration time for the key material in AWS and to manually delete it, but to also make it available again in the future. You can delete the imported key material from a KMS key at any time. Also, when imported key material with an expiration date expires, AWS KMS deletes the key material. In either case, AWS KMS deletes the key material immediately, the key state of the KMS key changes to pending import, and the KMS key can\'t be used in any cryptographic operations. In contrast, scheduling key deletion requires a waiting period of 7 to 30 days, after which you cannot recover the deleted KMS key. So, this function can be used to delete a key within a 24-hour window, as per the requirement.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q2-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the manual key rotation feature within KMS to instantly create a new CMK</strong> - The manual key rotation feature within KMS is used to regularly rotate the key material associated with a CMK, but it does not delete the key immediately. It is a best practice to rotate keys periodically, but it would not address the requirement of being able to delete the CMK within a 24-hour window. So, this option is incorrect.</p>\n\n<p><strong>Utilize the scheduled key deletion feature in KMS to set the minimum wait time for deletion</strong> - The scheduled key deletion feature allows you to schedule the deletion of a CMK with a waiting period of 7 to 30 days, but it does not provide a way to delete the key immediately. So it does not allow the key to be deleted within a 24-hour window. Therefore, this option is incorrect.</p>\n\n<p><strong>Alter the KMS CMK alias to immediately stop any services from utilizing the CMK</strong> - An alias is a friendly name for a AWS KMS key. For example, an alias lets you refer to a KMS key as test-key instead of 1234abcd-12ab-34cd-56ef-1234567890ab. Because an alias is an independent resource, you can change the KMS key associated with an alias. For example, if the test-key alias is associated with one KMS key, you can use the UpdateAlias operation to associate it with a different KMS key. This is one of several ways to manually rotate a KMS key without changing its key material. You cannot update an alias in the AWS KMS console. Also, you cannot use UpdateAlias (or any other operation) to change an alias name. To alter an alias name, delete the current alias and then create a new alias for the KMS key. Even if you alter an alias, the underlying key still remains operational, so it does not address the requirement of preventing the key from being used for encrypt or decrypt operations.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/alias-manage.html#alias-update">https://docs.aws.amazon.com/kms/latest/developerguide/alias-manage.html#alias-update</a></p>\n',
        answers: [
          "<p>Use the manual key rotation feature within KMS to instantly create a new CMK</p>",
          "<p>Utilize the scheduled key deletion feature in KMS to set the minimum wait time for deletion</p>",
          "<p>Alter the KMS CMK alias to immediately stop any services from utilizing the CMK</p>",
          "<p>Implement the KMS import key function to perform an immediate delete operation</p>",
        ],
      },
      correct_response: ["d"],
      section: "Infrastructure Security",
      question_plain:
        "At XYZ Corporation, the IT team had recently discovered a security loophole that could potentially allow unauthorized access to sensitive data. To fix the issue and ensure the protection of their company's information, the team wants to establish the ability to delete an AWS KMS Customer Master Key (CMK) within a 24-hour timeframe. This would prevent the key from being used for encrypt or decrypt operations and keep their data secure.\n\nWhich of the following solutions will address the given use case?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960100,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company wants to secure the objects in S3 using server-side encryption, subject to the constraint that the key material must be generated and stored in a certified FIPS 140-2 Level 3 hardware service modules (HSM) that the company manages itself. In addition, the key material must be available in multiple Regions. The size of objects in S3 ranges from 15 KB to 5 MB.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Leverage an AWS KMS custom key store backed by AWS CloudHSM clusters. Copy backups across Regions</strong></p>\n\n<p>You can use AWS Key Management Service (KMS) to create and control the cryptographic keys that are used to protect your data on AWS. An AWS KMS key is a logical representation of a cryptographic key. A KMS key contains metadata, such as the key ID, key spec, key usage, creation date, description, and key state. Most importantly, it contains a reference to the key material that is used when you perform cryptographic operations with the KMS key.</p>\n\n<p>By default, AWS KMS creates the key material for a KMS key. However, you can import your own key material into a KMS key, or use a custom key store to create KMS keys that use key material in your AWS CloudHSM cluster or key material in an external key manager that you own and manage outside of AWS.</p>\n\n<p>A key store is a secure location for storing cryptographic keys. The default key store in AWS KMS also supports methods for generating and managing the keys that its stores. By default, the cryptographic key material for the AWS KMS keys that you create in AWS KMS is generated in and protected by hardware security modules (HSMs) that are FIPS 140-2 validated cryptographic modules. A custom key store is a logical key store within AWS KMS that is backed by a key manager outside of AWS KMS that you own and manage.</p>\n\n<p>The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a tamper-resistant hardware device. HSMs are designed to securely store cryptographic key material and use the key material without exposing it outside the cryptographic boundary of the hardware.</p>\n\n<p>AWS KMS supports two types of custom key stores:</p>\n\n<p>An AWS CloudHSM key store is an AWS KMS custom key store backed by an AWS CloudHSM cluster. AWS CloudHSM allows you to copy backups of your CloudHSM Cluster from one region to another for cross-region resilience, global workloads, and disaster recovery purposes. You can use the copied backup to create a clone of the original cluster in the new region. This simplifies the development of globally distributed or cross-region redundant workloads.</p>\n\n<p>An external key store is an AWS KMS custom key store backed by an external key manager outside of AWS that you own and control.</p>\n\n<p>AWS CloudHSM key stores:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q1-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/developerguide/keystore-cloudhsm.html">https://docs.aws.amazon.com/kms/latest/developerguide/keystore-cloudhsm.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage an AWS KMS customer managed key and store the key material in AWS with key replication enabled across Regions</strong> - Customer managed keys are KMS keys in your AWS account that you create, own, and manage. AWS KMS supports multi-Region keys, which let you encrypt data in one AWS Region and decrypt it in a different AWS Region. The cryptographic key material for the AWS KMS keys that you create in AWS KMS is generated in and protected by hardware security modules (HSMs) that are FIPS 140 - Level 3 compliant. However, these HSMs are managed by AWS. Therefore, this option is incorrect.</p>\n\n<p><strong>Leverage an AWS KMS customer managed key backed by AWS CloudHSM clusters. Store the key material securely in Amazon S3 with cross-Region replication enabled</strong> - Customer managed keys are KMS keys in your AWS account that you create, own, and manage. You cannot store KMS key material in Amazon S3. In addition, AWS KMS customer managed key cannot be backed by an AWS CloudHSM cluster. This option has been added as a distractor.</p>\n\n<p><strong>Leverage AWS CloudHSM to generate the key material. Copy backups across Regions. Use AWS Encryption SDK to encrypt and decrypt the data</strong> - The use case states that the objects in S3 must be secured using server-side encryption. However, AWS Encryption SDK can only be used for client-side encryption, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html">https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/keystore-cloudhsm.html">https://docs.aws.amazon.com/kms/latest/developerguide/keystore-cloudhsm.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/keystore-external.html">https://docs.aws.amazon.com/kms/latest/developerguide/keystore-external.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html">https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/cloudhsm/latest/userguide/copy-backup-to-region.html">https://docs.aws.amazon.com/cloudhsm/latest/userguide/copy-backup-to-region.html</a></p>\n',
        answers: [
          "<p>Leverage an AWS KMS custom key store backed by AWS CloudHSM clusters. Copy backups across Regions</p>",
          "<p>Leverage an AWS KMS customer managed key and store the key material in AWS with key replication enabled across Regions</p>",
          "<p>Leverage an AWS KMS customer managed key backed by AWS CloudHSM clusters. Store the key material securely in Amazon S3 with cross-Region replication enabled</p>",
          "<p>Leverage AWS CloudHSM to generate the key material. Copy backups across Regions. Use AWS Encryption SDK to encrypt and decrypt the data</p>",
        ],
      },
      correct_response: ["a"],
      section: "Data Protection",
      question_plain:
        "A company wants to secure the objects in S3 using server-side encryption, subject to the constraint that the key material must be generated and stored in a certified FIPS 140-2 Level 3 hardware service modules (HSM) that the company manages itself. In addition, the key material must be available in multiple Regions. The size of objects in S3 ranges from 15 KB to 5 MB.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960148,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>To improve the security of private APIs, a Security Engineer has been tasked to configure API Gateway to use an interface VPC endpoint. The VPC endpoint policy should only allow full access to two specific private APIs through the endpoint.</p>\n\n<p>Which policy should be attached to the VPC endpoint to meet the given requirements?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<pre><code>{\n    "Statement": [\n        {\n            "Principal": "*",\n            "Action": [\n                "execute-api:Invoke"\n            ],\n            "Effect": "Allow",\n            "Resource": [\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/*",\n                "arn:aws:execute-api:us-east-1:123412341234:aaaaa11111/*"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>A VPC endpoint policy is an IAM resource policy that you can attach to an interface VPC endpoint to control access to the endpoint. You can use an endpoint policy to restrict the traffic going from your internal network to access your private APIs. You can choose to allow or disallow access to specific private APIs that can be accessed through the VPC endpoint. You can create policies for Amazon Virtual Private Cloud endpoints for Amazon API Gateway in which you can specify:\n1. The principal that can perform actions.\n2. The actions that can be performed.\n3. The resources that can have actions performed on them.</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>{\n    "Version": "2012-10-17",\n    "Statement":[\n        {\n            "Effect": "Allow",\n            "Action":"ec2:*VpcEndpoint*",\n            "Resource":[\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/*",\n                "arn:aws:execute-api:us-east-1:123412341234:aaaaa11111/*"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<pre><code>{\n    "Version": "2012-10-17",\n    "Statement":[\n        {\n            "Effect": "Allow",\n            "Action":"ec2:*VpcEndpoint*",\n            "Resource":[\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*",\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>By default, users do not have permission to work with endpoints. An identity-based policy can grant users permission to create, modify, describe, and delete endpoints via the action <code>ec2:*VpcEndpoint*</code>. However, this action is not relevant to the given use case, so both of these options are incorrect.</p>\n\n<pre><code>{\n    "Statement": [\n        {\n            "Principal": "*",\n            "Action": [\n                "execute-api:Invoke"\n            ],\n            "Effect": "Allow",\n            "Resource": [\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*",\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy grants users access only to the GET methods for the specific API via the VPC endpoint that the policy is attached to. So, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-vpc-endpoint-policies.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-vpc-endpoint-policies.html</a></p>\n',
        answers: [
          '<pre><code>{\n    "Statement": [\n        {\n            "Principal": "*",\n            "Action": [\n                "execute-api:Invoke"\n            ],\n            "Effect": "Allow",\n            "Resource": [\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*",\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*"\n            ]\n        }\n    ]\n}\n</code></pre>',
          '<pre><code>{\n    "Statement": [\n        {\n            "Principal": "*",\n            "Action": [\n                "execute-api:Invoke"\n            ],\n            "Effect": "Allow",\n            "Resource": [\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/*",\n                "arn:aws:execute-api:us-east-1:123412341234:aaaaa11111/*"\n            ]\n        }\n    ]\n}\n</code></pre>',
          '<pre><code>{\n    "Version": "2012-10-17",\n    "Statement":[\n        {\n            "Effect": "Allow",\n            "Action":"ec2:*VpcEndpoint*",\n            "Resource":[\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/*",\n                "arn:aws:execute-api:us-east-1:123412341234:aaaaa11111/*"\n            ]\n        }\n    ]\n}\n</code></pre>',
          '<pre><code>{\n    "Version": "2012-10-17",\n    "Statement":[\n        {\n            "Effect": "Allow",\n            "Action":"ec2:*VpcEndpoint*",\n            "Resource":[\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*",\n                "arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*"\n            ]\n        }\n    ]\n}\n</code></pre>',
        ],
      },
      correct_response: ["b"],
      section: "Identity and Access Management",
      question_plain:
        "To improve the security of private APIs, a Security Engineer has been tasked to configure API Gateway to use an interface VPC endpoint. The VPC endpoint policy should only allow full access to two specific private APIs through the endpoint.\n\nWhich policy should be attached to the VPC endpoint to meet the given requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960146,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is planning to launch a mobile application for its business critical functions. Mobile users should have access to AWS resources without having to define an AWS identity for each of them. Guest user access is a necessity for the application.</p>\n\n<p>As a Security Engineer, which of the following would you suggest as the most optimal way of configuring the security credentials for mobile users?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use Amazon Cognito with AWS SDKs for mobile development to create unique identities for the users</strong></p>\n\n<p>For mobile applications, AWS recommends using Amazon Cognito. You can use this service with AWS SDKs for mobile development to create unique identities for users and authenticate them for secure access to your AWS resources. Amazon Cognito supports the same identity providers as AWS STS and also supports unauthenticated (guest) access and lets you migrate user data when a user signs in. Amazon Cognito also provides API operations for synchronizing user data so that it is preserved as users move between devices.</p>\n\n<p>With Amazon Cognito, you can add user sign-up and sign-in features and control access to your web and mobile applications. Amazon Cognito provides an identity store that scales to millions of users, supports social and enterprise identity federation, and offers advanced security features to protect your consumers and business. Built on open identity standards, Amazon Cognito supports various compliance regulations and integrates with frontend and backend development resources.</p>\n\n<p>Amazon Cognito:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q23-i1.jpg">\nvia - <a href="https://aws.amazon.com/cognito/">https://aws.amazon.com/cognito/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create access keys for any IAM user belonging to the AWS account that holds the resources needed for the mobile application. Use these access keys to sign programmatic requests with AWS SDKs for mobile development</strong> - Access keys are long-term credentials for an IAM user or the AWS account root user. As a security best practice, AWS recommends the use of temporary security credentials instead of creating long-term credentials like access keys. Temporary credentials are possible for mobile application scenarios and hence long-term credentials that pose security and managing risks are not preferred.</p>\n\n<p><strong>Use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources</strong> - Amazon Cognito supports the same identity providers as AWS STS and also supports unauthenticated (guest) access and lets you migrate user data when a user signs in. Amazon Cognito has a better feature set for mobile application scenarios and hence is the right choice here.</p>\n\n<p><strong>Use Access Control Lists (ACLs) with AWS SDKs for mobile development to create unique identities for the users</strong> - This option has been added as a distractor. You use Access Control Lists (ACLs) in Amazon S3 to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html#sts-introduction">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html#sts-introduction</a></p>\n',
        answers: [
          "<p>Create access keys for any IAM user belonging to the AWS account that holds the resources needed for the mobile application. Use these access keys to sign programmatic requests with AWS SDKs for mobile development</p>",
          "<p>Use Access Control Lists (ACLs) with AWS SDKs for mobile development to create unique identities for the users</p>",
          "<p>Use Amazon Cognito with AWS SDKs for mobile development to create unique identities for the users</p>",
          "<p>Use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources</p>",
        ],
      },
      correct_response: ["c"],
      section: "Identity and Access Management",
      question_plain:
        "A company is planning to launch a mobile application for its business critical functions. Mobile users should have access to AWS resources without having to define an AWS identity for each of them. Guest user access is a necessity for the application.\n\nAs a Security Engineer, which of the following would you suggest as the most optimal way of configuring the security credentials for mobile users?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960144,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A Security Engineer has been asked to create an identity-based policy that allows access to add objects to an Amazon S3 bucket. But, the access should be given from April 1, 2023, through April 30, 2023 (UTC) inclusive.</p>\n\n<p>How will you define this identity-based policy?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": "s3:PutObject",\n            "Resource": "*",\n            "Condition": {\n                "DateGreaterThan": {"aws:CurrentTime": "2023-04-01T00:00:00Z"},\n                "DateLessThan": {"aws:CurrentTime": "2023-04-30T23:59:59Z"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This identity-based policy allows access to actions based on date and time. This policy restricts access to actions that occur between April 1, 2023, and April 30, 2023 (UTC), inclusive. This policy grants the permissions necessary to complete this action programmatically from the AWS API or AWS CLI.</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": "s3:PutObject",\n            "Resource": "*",\n            "Condition": {\n                "DateGreaterThan": {"{aws:logintime}": "2023-04-01T00:00:00Z"},\n                "DateLessThan": {"{aws:logintime}": "2023-04-30T23:59:59Z"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy definition is incorrect. You cannot use a policy variable with the Date condition operator. In addition, using the <code>{aws:logintime}</code> policy variable is incorrect for the given use case.</p>\n\n<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Deny",\n            "Action": "s3:PutObject",\n            "Resource": "*",\n            "Condition": {\n                "DateGreaterThan": {"{aws:logintime}": "2023-04-01T00:00:00Z"},\n                "DateLessThan": {"{aws:logintime}": "2023-04-30T23:59:59Z"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy definition is incorrect. You cannot use a policy variable with the Date condition operator. In addition, using the <code>{aws:logintime}</code> policy variable is incorrect for the given use case. You should also note that the Deny Effect is logically opposite of what the use case requires.</p>\n\n<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": "s3:PutObject",\n            "Principal": "*",\n            "Condition": {\n                "DateGreaterThan": {"aws:CurrentTime": "2023-04-01T00:00:00Z"},\n                "DateLessThan": {"aws:CurrentTime": "2023-04-30T23:59:59Z"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy definition is incorrect. You cannot use the Principal element in an identity-based policy. Identity-based policies are permissions policies that you attach to IAM identities (users, groups, or roles). In those cases, the principal is implicitly the identity where the policy is attached.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html</a></p>\n',
        answers: [
          '<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": "s3:PutObject",\n            "Resource": "*",\n            "Condition": {\n                "DateGreaterThan": {"{aws:logintime}": "2023-04-01T00:00:00Z"},\n                "DateLessThan": {"{aws:logintime}": "2023-04-30T23:59:59Z"}\n            }\n        }\n    ]\n}\n</code></pre>',
          '<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Deny",\n            "Action": "s3:PutObject",\n            "Resource": "*",\n            "Condition": {\n                "DateGreaterThan": {"{aws:logintime}": "2023-04-01T00:00:00Z"},\n                "DateLessThan": {"{aws:logintime}": "2023-04-30T23:59:59Z"}\n            }\n        }\n    ]\n}\n</code></pre>',
          '<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": "s3:PutObject",\n            "Principal": "*",\n            "Condition": {\n                "DateGreaterThan": {"aws:CurrentTime": "2023-04-01T00:00:00Z"},\n                "DateLessThan": {"aws:CurrentTime": "2023-04-30T23:59:59Z"}\n            }\n        }\n    ]\n}\n</code></pre>',
          '<pre><code>{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": "s3:PutObject",\n            "Resource": "*",\n            "Condition": {\n                "DateGreaterThan": {"aws:CurrentTime": "2023-04-01T00:00:00Z"},\n                "DateLessThan": {"aws:CurrentTime": "2023-04-30T23:59:59Z"}\n            }\n        }\n    ]\n}\n</code></pre>',
        ],
      },
      correct_response: ["d"],
      section: "Identity and Access Management",
      question_plain:
        "A Security Engineer has been asked to create an identity-based policy that allows access to add objects to an Amazon S3 bucket. But, the access should be given from April 1, 2023, through April 30, 2023 (UTC) inclusive.\n\nHow will you define this identity-based policy?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960142,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company has decided to revamp the security for its IT infrastructure and tighten rules for access to AWS resources across the organization. In this context, a Security Engineer has been tasked with creating optimal access credentials/permissions for the company's applications to access the required resources. Some of these applications will run on EC2 instances and need cross-account access privileges for resources present in another AWS account. The company also maintains a few mobile applications that need to access AWS resources.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend as the best practices to configure access credentials/permissions for these applications? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          "<p>Correct options:</p>\n\n<p><strong>Use an IAM role to establish trust between accounts, and then grant users in one account limited permissions to access the trusted account</strong></p>\n\n<p>Use an IAM role to establish trust between accounts, and then grant users in one account limited permissions to access the trusted account. You share resources in one account with users in a different account. By setting up cross-account access in this way, you don't have to create individual IAM users in each account. In addition, users don't have to sign out of one account and sign into another to access resources in different AWS accounts.</p>\n\n<p><strong>Use Amazon Cognito to manage user identities in your mobile application. You can then use the Amazon Cognito credentials provider to manage credentials that your application uses to make requests to access AWS resources</strong></p>\n\n<p>Don't embed access keys with the app, even in encrypted storage. Instead, use Amazon Cognito to manage user identities in your app. This service lets you authenticate users using Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)–compatible identity provider. You can then use the Amazon Cognito credentials provider to manage the credentials that your app uses to make requests to access AWS resources.</p>\n\n<p><strong>Define an IAM role that has appropriate permissions for the application and launch the Amazon EC2 instance with this role associated with the instance</strong></p>\n\n<p>Don't use access keys directly in your application. Don't pass access keys to the application, embed them in the application, or let the application read access keys from any source. Instead, define an IAM role that has appropriate permissions for your application and launch the Amazon Elastic Compute Cloud (Amazon EC2) instance with this role associated with the instance. This practice also enables the application to get temporary security credentials that it can, in turn, use to make programmatic calls to AWS. The AWS SDKs and the AWS Command Line Interface (AWS CLI) can get temporary credentials from the role automatically.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create long-term access keys associated with the AWS account IAM user and use them to provide access to an application running on an EC2 instance. Since the instances run on safe private subnets on AWS Cloud, the long-term credentials are a perfect fit for this scenario with no overhead of creating and maintaining short-term credentials. It is to be noted that long-term access keys should not be associated with the AWS root user account</strong></p>\n\n<p><strong>Use access keys to provide long-term credentials to AWS for an application running on an Amazon EC2 instance. Encrypt the keys to avoid exposure to the internet. Rotate access keys periodically</strong></p>\n\n<p>These two options are incorrect from a security standpoint. Long-term access keys, such as those associated with IAM users and AWS account root users, remain valid until you manually revoke them. However, temporary security credentials obtained through IAM roles and other features of the AWS Security Token Service expire after a short period of time. AWS suggests using temporary security credentials to help reduce the risk in case credentials are accidentally exposed.</p>\n\n<p><strong>Embed access keys with the mobile application and store them in encrypted storage to avoid exposure. As an added layer of security, you can add envelope encryption to the encrypted access keys</strong> - Don't embed access keys with the app, even in encrypted storage. This is considered a security bad practice.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/accounts/latest/reference/credentials-access-keys-best-practices.html\">https://docs.aws.amazon.com/accounts/latest/reference/credentials-access-keys-best-practices.html</a></p>\n",
        answers: [
          "<p>Use access keys to provide long-term credentials to AWS for an application running on Amazon EC2 instance. Encrypt the keys to avoid exposure to the internet. Rotate access keys periodically</p>",
          "<p>Use an IAM role to establish trust between accounts, and then grant users in one account limited permissions to access the trusted account</p>",
          "<p>Embed access keys with the mobile application and store them in encrypted storage to avoid exposure. As an added layer of security, you can add envelope encryption to the encrypted access keys</p>",
          "<p>Use Amazon Cognito to manage user identities in your mobile application. You can then use the Amazon Cognito credentials provider to manage credentials that your application uses to make requests to access AWS resources</p>",
          "<p>Create long-term access keys associated with AWS account IAM user and use them to provide access to an application running on EC2 instance. Since the instances run on safe private subnets on AWS Cloud, the long-term credentials are a perfect fit for this scenario with no overhead of creating and maintaining short-term credentials. It is to be noted that long-term access keys should not be associated with the AWS root user account</p>",
          "<p>Define an IAM role that has appropriate permissions for the application and launch the Amazon EC2 instance with this role associated with the instance</p>",
        ],
      },
      correct_response: ["b", "d", "f"],
      section: "Identity and Access Management",
      question_plain:
        "A company has decided to revamp the security for its IT infrastructure and tighten rules for access to AWS resources across the organization. In this context, a Security Engineer has been tasked with creating optimal access credentials/permissions for the company's applications to access the required resources. Some of these applications will run on EC2 instances and need cross-account access privileges for resources present in another AWS account. The company also maintains a few mobile applications that need to access AWS resources.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend as the best practices to configure access credentials/permissions for these applications? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960140,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>An AWS root user has logged in to the AWS account and realized that there is no access to an Amazon S3 bucket under the given AWS account.</p>\n\n<p>What is the reason for this behavior and how will you fix the issue? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>If there is a bucket policy on the Amazon S3 bucket that doesn\'t specify the AWS account root user as a principal, the root user is denied access to that bucket</strong></p>\n\n<p><strong>Modify the bucket policy to allow root user access from the Amazon S3 console or the AWS CLI</strong></p>\n\n<p>In some cases, you might have an IAM user with full access to IAM and Amazon S3. If the IAM user assigns a bucket policy to an Amazon S3 bucket and doesn\'t specify the AWS account root user as a principal, the root user is denied access to that bucket. However, as the root user, you can still access the bucket. To do that, modify the bucket policy to allow root user access from the Amazon S3 console or the AWS CLI. Use the following principal and replace 123456789012 with the ID of the AWS account.</p>\n\n<p><code>"Principal": { "AWS": "arn:aws:iam::123456789012:root" }</code></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Only An IAM user with full access to IAM and the S3 bucket will be able to add the root user as principal to the bucket policy</strong> - As discussed above, the root user can make the changes to the bucket policy to grant the necessary permissions.</p>\n\n<p><strong>The access key of the root user account could be expired and hence needs to be recreated before accessing the S3 bucket</strong> - You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. This is irrelevant to the given use case.</p>\n\n<p><strong>A root user has full access permissions on all the AWS resources in his user account. Contact the AWS support team to sort the access issue</strong> - This statement is incorrect and given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html</a></p>\n',
        answers: [
          "<p>Modify the bucket policy to allow root user access from the Amazon S3 console or the AWS CLI</p>",
          "<p>Only An IAM user with full access to IAM and the S3 bucket will be able to add the root user as principal to the bucket policy</p>",
          "<p>The access key of the root user account could be expired and hence needs to be recreated before accessing the S3 bucket</p>",
          "<p>If there is a bucket policy on the Amazon S3 bucket that doesn't specify the AWS account root user as a principal, the root user is denied access to that bucket</p>",
          "<p>A root user has full access permissions on all the AWS resources in his user account. Contact the AWS support team to sort out the access issue</p>",
        ],
      },
      correct_response: ["a", "d"],
      section: "Identity and Access Management",
      question_plain:
        "An AWS root user has logged in to the AWS account and realized that there is no access to an Amazon S3 bucket under the given AWS account.\n\nWhat is the reason for this behavior and how will you fix the issue? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960138,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An open banking system enables secure open API integrations for financial institutions. The banking system needs mutual TLS (mTLS) authentication as part of its security standards. The application will be hosted on an Amazon EC2 server. The system has specific security compliance rules that need the server to terminate the client’s TLS connection.</p>\n\n<p>As a Security Engineer, how will you configure this requirement to support mTLS if a load balancing service is needed for the instances?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create a TCP listener using a Network Load Balancer and implement mTLS on the target</strong></p>\n\n<p>By default, the TLS protocol only requires a server to authenticate itself to the client. The authentication of the client to the server is managed by the application layer. The TLS protocol also offers the ability for the server to request that the client send an X.509 certificate to prove its identity. This is called mutual TLS (mTLS) as both parties are authenticated via certificates with TLS.</p>\n\n<p>For mTLS support, you need to create a TCP listener using a Network Load Balancer or a Classic Load Balancer and implement mTLS on the target. The load balancer passes the request through as is, so you can implement mTLS on the target.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an mTLS listener on an Application Load Balancer and enable mutual TLS authentication for better security of the application</strong> - Application Load Balancer supports mTLS. If you have an HTTPS application, AWS recommends you consider ALB if you’d like to perform application level routing. For example, performing weighted round robin load balancing for HTTPS requests, which will allow you to create blue/green style deployments. ALB will also allow you to offload the TLS/mTLS operations. Since the ALB terminates client’s TLS session, you’ll need to upload certificates for the ALB. NLB, on the other hand, operates at the transport layer (layer 4 of the OSI model), and provides low latency load balancing of TCP/UDP connections. For an HTTPS application, AWS recommends using Network Load Balancer (NLB) if you have specific security compliance rules that need the server to terminate the client’s TLS connection.</p>\n\n<p>For the given use case, you cannot configure ALB such that the server terminates the client’s TLS connection, therefore this option is incorrect.</p>\n\n<p><strong>Network Load Balancers support TLS renegotiation and mutual TLS authentication (mTLS). Configure a TLS listener for a Network Load Balancer to use mutual TLS authentication</strong> - Network Load Balancers do not support TLS renegotiation or mutual TLS authentication (mTLS). For mTLS support, you need to create a TCP listener instead of a TLS listener.</p>\n\n<p><strong>Configure a TCP listener on an Application Load Balancer and enable mutual TLS authentication on it</strong> - Application Load Balancer only supports HTTP and HTTPS protocols. Network Load Balancer is used for TCP traffic.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/mutual-authentication.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/mutual-authentication.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-mtls-for-application-load-balancer/">https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-mtls-for-application-load-balancer/</a></p>\n',
        answers: [
          "<p>Configure an mTLS listener on an Application Load Balancer and enable mutual TLS authentication for better security of the application</p>",
          "<p>Network Load Balancers support TLS renegotiation and mutual TLS authentication (mTLS). Configure a TLS listener for a Network Load Balancer to use mutual TLS authentication</p>",
          "<p>Create a TCP listener using a Network Load Balancer and implement mTLS on the target</p>",
          "<p>Configure a TCP listener on an Application Load Balancer and enable mutual TLS authentication on it</p>",
        ],
      },
      correct_response: ["c"],
      section: "Infrastructure Security",
      question_plain:
        "An open banking system enables secure open API integrations for financial institutions. The banking system needs mutual TLS (mTLS) authentication as part of its security standards. The application will be hosted on an Amazon EC2 server. The system has specific security compliance rules that need the server to terminate the client’s TLS connection.\n\nAs a Security Engineer, how will you configure this requirement to support mTLS if a load balancing service is needed for the instances?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960136,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The security team at a company has been assigned the responsibility of configuring outgoing email using Simple Email Service (SES) that leverages the Amazon SES API with mandatory TLS for the secure transfer of data.</p>\n\n<p>Which configuration should the engineer choose to make TLS mandatory for SES API?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Change the behavior of SES by using configuration sets. Set the <code>TlsPolicy</code> property for a configuration set to <code>Require</code></strong></p>\n\n<p>By default, Amazon SES uses opportunistic TLS.</p>\n\n<p>To address the given requirement, you can use the <code>PutConfigurationSetDeliveryOptions</code> API operation to set the <code>TlsPolicy</code> property for a configuration set to <code>Require</code>. You can use the AWS CLI to make this change.</p>\n\n<p>Configuration for mandatory TLS:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q18-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/ses/latest/dg/security-protocols.html">https://docs.aws.amazon.com/ses/latest/dg/security-protocols.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure STARTTLS mechanism on SES to establish a TLS-encrypted connection with the client</strong></p>\n\n<p><strong>Configure TLS Wrapper mechanism on SES to establish a secure TLS-encrypted connection with the client</strong></p>\n\n<p>If you are accessing Amazon SES through the SMTP interface, you\'re required to encrypt your connection using Transport Layer Security (TLS). Amazon SES supports two mechanisms for establishing a TLS-encrypted connection: STARTTLS and TLS Wrapper. Since the use case mentions using Amazon SES API, these two options are irrelevant to the given use case.</p>\n\n<p><strong>By default, Amazon SES configuration mandates TLS. Custom configurations are not needed to achieve secure communication</strong> - This statement is incorrect. By default, Amazon SES uses opportunistic TLS. This means that Amazon SES always attempts to make a secure connection to the receiving mail server. If Amazon SES can\'t establish a secure connection, it sends the message unencrypted.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/ses/latest/dg/security-protocols.html">https://docs.aws.amazon.com/ses/latest/dg/security-protocols.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/ses/latest/dg/data-protection.html">https://docs.aws.amazon.com/ses/latest/dg/data-protection.html</a></p>\n',
        answers: [
          "<p>By default, Amazon SES configuration mandates TLS. Custom configurations are not needed to achieve secure communication</p>",
          "<p>Configure STARTTLS mechanism on SES to establish a TLS-encrypted connection with the client</p>",
          "<p>Configure TLS Wrapper mechanism on SES to establish a secure TLS-encrypted connection with the client</p>",
          "<p>Change the behavior of SES by using configuration sets. Set the <code>TlsPolicy</code> property for a configuration set to <code>Require</code></p>",
        ],
      },
      correct_response: ["d"],
      section: "Data Protection",
      question_plain:
        "The security team at a company has been assigned the responsibility of configuring outgoing email using Simple Email Service (SES) that leverages the Amazon SES API with mandatory TLS for the secure transfer of data.\n\nWhich configuration should the engineer choose to make TLS mandatory for SES API?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960134,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company stores its critical business data on Amazon S3 buckets. A customer does not use TLS versions 1.2 or higher and hence is unable to access content stored in Amazon Simple Storage Service (Amazon S3) buckets.</p>\n\n<p>As a Security Engineer, how will you set up a solution to allow the customers to access content in the Amazon S3 buckets using TLS 1.0 or 1.1 while keeping the communication channel secure?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Make your S3 bucket private and configure access through Amazon CloudFront only by using signed requests to access the S3 bucket</strong></p>\n\n<p>AWS is enforcing the use of TLS 1.2 or higher on all AWS API endpoints. To continue to connect to AWS services, you must update all software that uses TLS 1.0 or 1.1.</p>\n\n<p>Amazon CloudFront allows the use of older TLS versions by abstracting customers from the TLS protocol that\'s used between your CloudFront distribution and Amazon S3.</p>\n\n<p>CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI). Create a CloudFront distribution with Origin Access Control (OAC). OAC is a feature that enables CloudFront customers to easily secure their S3 origins by permitting only designated CloudFront distributions to access their S3 buckets. Customers can enable AWS Signature Version 4 (SigV4) on CloudFront requests to S3 buckets with the ability to set when and if CloudFront should sign requests.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a CloudFront distribution with an Amazon S3 bucket as the custom origin. CloudFront supports anonymous and public requests to S3 buckets for undisrupted user access</strong> - If you set up an S3 bucket as a custom origin then the Amazon S3 bucket should be configured as a website endpoint. This means you cannot use origin access control (OAC) or origin access identity (OAI) to secure your bucket access. Hence, this option is incorrect for the current use case.</p>\n\n<p><strong>The wildcard domain name feature of AWS Certificate Manager (ACM) can be used to work around the TLS version limitations</strong> - In an ACM certificate, a wildcard domain name matches any first-level subdomain or hostname in a domain. A first-level subdomain is a single domain name label that does not contain a period (dot). This is irrelevant to the given use case, so it is an invalid option.</p>\n\n<p><strong>Create a bucket policy that allows secure data access via TLS 1.0 or 1.1</strong> - This option has been added as a distractor. You cannot allow secure data access via TLS 1.0 or 1.1 using a bucket policy.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-cloudfront-origin-access-control/">https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-cloudfront-origin-access-control/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n\n<p><a href="https://repost.aws/knowledge-center/s3-access-old-tls">https://repost.aws/knowledge-center/s3-access-old-tls</a></p>\n',
        answers: [
          "<p>Configure a CloudFront distribution with an Amazon S3 bucket as the custom origin. CloudFront supports anonymous and public requests to S3 buckets for undisrupted user access</p>",
          "<p>Create a CloudFront distribution with Origin Access Control (OAC). Make your S3 bucket private and configure access through Amazon CloudFront only by using signed requests to access the S3 bucket</p>",
          "<p>The wildcard domain name feature of AWS Certificate Manager (ACM) can be used to work around the TLS version limitations</p>",
          "<p>Create a bucket policy that allows secure data access via TLS 1.0 or 1.1</p>",
        ],
      },
      correct_response: ["b"],
      section: "Data Protection",
      question_plain:
        "A company stores its critical business data on Amazon S3 buckets. A customer does not use TLS versions 1.2 or higher and hence is unable to access content stored in Amazon Simple Storage Service (Amazon S3) buckets.\n\nAs a Security Engineer, how will you set up a solution to allow the customers to access content in the Amazon S3 buckets using TLS 1.0 or 1.1 while keeping the communication channel secure?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960132,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A user is trying to upload a large file to an Amazon S3 bucket present in a given AWS account. In the upload request, the user is passing the encryption information using an AWS Key Management Service (AWS KMS) key, also present in the same account. However, the user is getting an Access Denied error. Meanwhile, when the user uploads a smaller file with encryption information, the upload succeeds.</p>\n\n<p>As a Security Engineer, how will you fix this issue?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Verify that <code>kms:Decrypt</code> permissions are specified in the key policy, otherwise they need to be added to the policy</strong></p>\n\n<p>The AWS CLI (aws s3 commands), AWS SDKs, and many third-party programs automatically perform a multipart upload when the file is large. To perform a multipart upload with encryption using an AWS KMS key, the requester must have <code>kms:GenerateDataKey</code> and <code>kms:Decrypt</code> permissions. The <code>kms:GenerateDataKey</code> permissions allow the requester to initiate the upload. With <code>kms:Decrypt</code> permissions, newly uploaded parts can be encrypted with the same key used for previous parts of the same object.</p>\n\n<p>After all the parts are uploaded successfully, the uploaded parts must be assembled to complete the multipart upload operation. Because the uploaded parts are server-side encrypted using a KMS key, object parts must be decrypted before they can be assembled. For this reason, the requester must have <code>kms:Decrypt</code> permissions for multipart upload requests using server-side encryption with KMS CMKs (SSE-KMS).</p>\n\n<p>Since the user can successfully upload smaller files, it is clear that the user already has <code>kms:GenerateDataKey</code> permissions. Hence, only the <code>kms:Decrypt</code> permission needs to be added to the policy.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Verify that the requester has <code>kms:GenerateDataKey</code> permissions. This permission is needed for multipart upload to work successfully</strong> - <code>kms:GenerateDataKey</code> is needed to upload encrypted objects to the S3 bucket. Since the user can upload smaller files, the user already has this permission.</p>\n\n<p><strong>Verify that <code>kms:Decrypt</code> permissions are specified in both the key policy as well as the IAM policy of the user</strong> - If your AWS Identity and Access Management (IAM) role and key are in the same account, then <code>kms:Decrypt</code> permissions must be specified in the key policy. If your IAM role belongs to a different account than the key, <code>kms:Decrypt</code> permissions must be specified in both the key and IAM policy. Since the question clearly states that the role and key are from the same account, this option stands incorrect.</p>\n\n<p><strong>Verify that <code>kms:Encrypt</code> permissions are specified in the key policy, otherwise, they need to be added to the policy</strong> - As mentioned in the explanation above, the requester must have <code>kms:GenerateDataKey</code> and <code>kms:Decrypt</code> permissions. <code>kms:Encrypt</code> can only be used to encrypt plaintext of up to 4,096 bytes using a KMS key. So, this option acts as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://repost.aws/knowledge-center/s3-large-file-encryption-kms-key">https://repost.aws/knowledge-center/s3-large-file-encryption-kms-key</a></p>\n',
        answers: [
          "<p>Verify that the requester has <code>kms:GenerateDataKey</code> permissions. This permission is needed for multipart upload to work successfully</p>",
          "<p>Verify that <code>kms:Encrypt</code> permissions are specified in the key policy, otherwise, they need to be added to the policy</p>",
          "<p>Verify that <code>kms:Decrypt</code> permissions are specified in the key policy, otherwise, they need to be added to the policy</p>",
          "<p>Verify that <code>kms:Decrypt</code> permissions are specified in both the key policy as well as the IAM policy of the user</p>",
        ],
      },
      correct_response: ["c"],
      section: "Data Protection",
      question_plain:
        "A user is trying to upload a large file to an Amazon S3 bucket present in a given AWS account. In the upload request, the user is passing the encryption information using an AWS Key Management Service (AWS KMS) key, also present in the same account. However, the user is getting an Access Denied error. Meanwhile, when the user uploads a smaller file with encryption information, the upload succeeds.\n\nAs a Security Engineer, how will you fix this issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960130,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>The security team at a company is working to create VPC endpoints so that the AWS Systems Manager can be used to manage private EC2 instances without internet access.</p>\n\n<p>As an AWS Certified Security Specialist, which options will you combine to build a solution to meet the given requirements? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Verify that SSM Agent is installed on the instance</strong></p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) instance profile for the Systems Manager. Attach the IAM role to your private EC2 instance</strong></p>\n\n<p><strong>Create three virtual private cloud (VPC) endpoints for the Systems Manager with service names: <code>com.amazonaws.[region].ssm</code>, <code>com.amazonaws.[region].ec2messages</code>, <code>com.amazonaws.[region].ssmmessages</code></strong></p>\n\n<p>You need to create a virtual private cloud (VPC) endpoint for Systems Manager for three services: <code>com.amazonaws.[region].ssm</code>, <code>com.amazonaws.[region].ec2messages</code> and <code>com.amazonaws.[region].ssmmessages</code>. After the three endpoints are created, your instance appears in Managed Instances and can be managed using Systems Manager.</p>\n\n<p>Complete steps to be followed for above expected configuration:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q15-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) instance profile for the Systems Manager. Attach the IAM role to the VPC endpoint policy</strong></p>\n\n<p><strong>Create a virtual private cloud (VPC) endpoint for Systems Manager with service name <code>com.amazonaws.[region].ssm</code></strong></p>\n\n<p>These two statements are incorrect and given only as distractors.</p>\n\n<p><strong>Allow outbound internet access on your managed instances. The managed instances must be configured to also allow HTTPS (port 443) outbound traffic to the following endpoints: <code>ssm.[region].amazonaws.com</code></strong> - The question mentions that the instance is in a private subnet with no internet access, hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p>\n',
        answers: [
          "<p>Verify that SSM Agent is installed on the instance</p>",
          "<p>Allow outbound internet access on your managed instances. The managed instances must be configured to also allow HTTPS (port 443) outbound traffic to the following endpoints: <code>ssm.[region].amazonaws.com</code>, <code>ssmmessages.[region].amazonaws.com</code>, <code>ec2messages.[region].amazonaws.com</code></p>",
          "<p>Create an AWS Identity and Access Management (IAM) instance profile for Systems Manager. Attach the IAM role to your private EC2 instance</p>",
          "<p>Create three virtual private cloud (VPC) endpoints for Systems Manager with service names: <code>com.amazonaws.[region].ssm</code>, <code>com.amazonaws.[region].ec2messages</code>, <code>com.amazonaws.[region].ssmmessages</code></p>",
          "<p>Create an AWS Identity and Access Management (IAM) instance profile for Systems Manager. Attach the IAM role to the VPC endpoint policy</p>",
          "<p>Create a virtual private cloud (VPC) endpoint for Systems Manager with service name <code>com.amazonaws.[region].ssm</code></p>",
        ],
      },
      correct_response: ["a", "c", "d"],
      section: "Data Protection",
      question_plain:
        "The security team at a company is working to create VPC endpoints so that the AWS Systems Manager can be used to manage private EC2 instances without internet access.\n\nAs an AWS Certified Security Specialist, which options will you combine to build a solution to meet the given requirements? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960128,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A Security Engineer has been asked to configure an interface VPC endpoint to access an Amazon API Gateway private REST API that is in another AWS account.</p>\n\n<p>What are the key points of consideration while creating an interface endpoint in the Amazon VPC account for the given requirement? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>When you activate private DNS for an interface VPC endpoint, you can no longer access API Gateway public APIs from your Amazon VPC</strong> - When you activate private DNS for an interface VPC endpoint, you can no longer access API Gateway public APIs from your Amazon VPC.</p>\n\n<p>When a private DNS is enabled on a VPC endpoint, the API\'s invoke URL is covered by the private DNS name <code>*.execute-api.us-east-1.amazonaws.com</code> where * is a placeholder for the API ID. When a DNS query is resolved for a public API from inside a VPC, the resolved DNS points to the private IP of the associated VPC endpoint instead of the public IP of the public API. The API call is then routed to the public API through the VPC endpoint instead of routing it through the internet. Because VPC endpoints can route traffic only to private APIs, the result is an HTTP 403 error.</p>\n\n<p><strong>The security groups that you choose must have a rule that allows TCP Port 443 inbound HTTPS traffic from an IP address range in your Amazon VPC</strong> - The security groups that you choose must have a rule that allows TCP Port 443 inbound HTTPS traffic from either of the following: An IP address range in your Amazon VPC or another security group in your Amazon VPC.</p>\n\n<p>Key points to remember when creating an interface endpoint:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q14-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-private-cross-account-vpce/">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-private-cross-account-vpce/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For better resilience, it is mandatory to select multiple subnets across multiple Availability Zones when creating an interface endpoint</strong> - This statement is incorrect. As best practice AWS suggests configuring subnets across multiple Availability Zones to make your interface endpoint resilient to possible Availability Zone failures. However, it is not mandatory. Another best practice is to use a VPC endpoint policy to restrict endpoint access by API ID. It\'s also a best practice to use the API Gateway resource policy to restrict endpoint access by the principal.</p>\n\n<p><strong>To connect to public APIs using a VPC endpoint, enable private DNS on your VPC</strong> - This statement is incorrect. It is not possible to connect to public APIs using a VPC endpoint.</p>\n\n<p><strong>You cannot access your private API endpoint from an on-premises network using public DNS names</strong> - This is incorrect. You can use AWS Direct Connect to establish a dedicated private connection from an on-premises network to Amazon VPC and access your private API endpoint over that connection by using public DNS names.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-api-test-invoke-url.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-api-test-invoke-url.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-vpc-connections/">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-vpc-connections/</a></p>\n',
        answers: [
          "<p>For better resilience, it is mandatory to select multiple subnets across multiple Availability Zones when creating an interface endpoint</p>",
          "<p>To connect to public APIs using a VPC endpoint, enable private DNS on your VPC</p>",
          "<p>When you activate private DNS for an interface VPC endpoint, you can no longer access API Gateway public APIs from your Amazon VPC</p>",
          "<p>The security groups that you choose must have a rule that allows TCP Port 443 inbound HTTPS traffic from an IP address range in your Amazon VPC</p>",
          "<p>You cannot access your private API endpoint from an on-premises network using public DNS names</p>",
        ],
      },
      correct_response: ["c", "d"],
      section: "Data Protection",
      question_plain:
        "A Security Engineer has been asked to configure an interface VPC endpoint to access an Amazon API Gateway private REST API that is in another AWS account.\n\nWhat are the key points of consideration while creating an interface endpoint in the Amazon VPC account for the given requirement? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960124,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A Security Engineer has been tasked with the job of configuring access control and authentication for the AWS KMS keys of a particular AWS account.</p>\n\n<p>Which of the following would you identify as valid points of consideration for configuring the requirement correctly? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          "<p>Correct options:</p>\n\n<p><strong>The IAM identity that creates a KMS key is not considered to be the key owner. Like any other identity, the key creator needs to get permission through a key policy, IAM policy, or grant</strong></p>\n\n<p>KMS keys belong to the AWS account in which they were created. However, no identity or principal, including the AWS account root user, has permission to use or manage a KMS key unless that permission is explicitly provided in a key policy, IAM policy, or grant. The IAM identity that creates a KMS key is not considered to be the key owner and they don't automatically have permission to use or manage the KMS key that they created. Like any other identity, the key creator needs to get permission through a key policy, IAM policy, or grant.</p>\n\n<p><strong>AWS identities that have the <code>kms:CreateKey</code> permission can set the initial key policy and give themselves permission to use or manage the key</strong></p>\n\n<p>The AWS identities that have the <code>kms:CreateKey</code> permission can set the initial key policy and give themselves permission to use or manage the key.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>KMS keys belong to the AWS account in which they were created. The AWS account root user alone has full permission on the keys until other identities are given permission through a key policy, IAM policy, or grant</strong> - As discussed above, this statement is incorrect.</p>\n\n<p><strong>Authorization to use KMS keys is given through federated identity or user sign-in access. Resource-based policies and Access control lists (ACLs) are other forms of authorization that KMS accepts</strong> - While resource-based policies and Access control lists (ACLs) are forms of authorization that KMS accepts, federated identity or user sign-in access are modes of authentication and not authorization.</p>\n\n<p>Authentication is the process of verifying your identity. To send a request to AWS KMS, you must or sign into AWS using your AWS credentials. Authorization provides permission to send requests to create, manage, or use AWS KMS resources. For example, you must be authorized to use a KMS key in a cryptographic operation.</p>\n\n<p><strong>Permissions to resources are given through Resource-based policies that are JSON policy documents that contain details about the resource, the principal, and the conditions under which the policy can be used</strong> - Resource-based policies are JSON policy documents that you attach to a resource, such as a KMS key, to control access to the specific resource. The resource-based policy defines the actions that a specified principal can perform on that resource and under what conditions. You don't specify the resource in a resource-based policy, but you must specify a principal, such as accounts, users, roles, federated users, or AWS services. Resource-based policies are inline policies that are located in the service that manages the resource.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/control-access.html\">https://docs.aws.amazon.com/kms/latest/developerguide/control-access.html</a></p>\n",
        answers: [
          "<p>The IAM identity that creates a KMS key is not considered to be the key owner. Like any other identity, the key creator needs to get permission through a key policy, IAM policy, or grant</p>",
          "<p>KMS keys belong to the AWS account in which they were created. The AWS account root user alone has full permission on the keys until other identities are given permission through a key policy, IAM policy, or grant</p>",
          "<p>Authorization to use KMS keys is given through federated identity or user sign-in access. Resource-based policies and Access control lists (ACLs) are other forms of authorization that KMS accepts</p>",
          "<p>Permissions to resources are given through Resource-based policies that are JSON policy documents which contain details about the resource, the principal, and the conditions under which the policy can be used</p>",
          "<p>AWS identities that have the kms:CreateKey permission can set the initial key policy and give themselves permission to use or manage the key</p>",
        ],
      },
      correct_response: ["a", "e"],
      section: "Identity and Access Management",
      question_plain:
        "A Security Engineer has been tasked with the job of configuring access control and authentication for the AWS KMS keys of a particular AWS account.\n\nWhich of the following would you identify as valid points of consideration for configuring the requirement correctly? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960126,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A Security Engineer has been tasked to evaluate the outcome of different policies, including but not limited to identity-based policies, resource-based policies, IAM permissions boundaries, session policies, and AWS Organizations service control policies (SCPs) of an AWS account.</p>\n\n<p>Which of the following are valid statements regarding the aforementioned policy evaluations? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy does not impact the final decision</strong></p>\n\n<p>Resource-based policy logic differs from other policy types if the specified principal is an IAM user, an IAM role, or a session principal. If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary or a session policy does not impact the final decision.</p>\n\n<p><strong>Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy</strong></p>\n\n<p>This statement is correct. Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy.</p>\n\n<p>Impact of resource-based policies for different principal types:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q13-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n\n<p><strong>If no applicable <code>Allow</code> statement is found in the SCPs, the request is explicitly denied, even if the denial is implicit</strong></p>\n\n<p>SCPs apply to principals of the account where the SCPs are attached. If the enforcement code does not find any applicable <code>Allow</code> statements in the SCPs, the request is explicitly denied, even if the denial is implicit. The enforcement code returns a final decision of Deny. If there is no SCP, or if the SCP allows the requested action, the enforcement code evaluation continues.</p>\n\n<p>Determining whether a request is allowed or denied within an account:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q13-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy results in a final <code>Deny</code></strong> - As discussed above, this statement is incorrect.</p>\n\n<p><strong>If there is no explicit <code>Allow</code> in the SCP, the request is implicitly given an <code>Allow</code> after the SCP is evaluated</strong> - As discussed above, this statement is incorrect.</p>\n\n<p><strong>If a resource-based policy grants permission to the ARN of the federated IAM user, then requests made by the federated user during the session are not limited by an implicit deny in a permission boundary or session policy</strong> - This statement is incorrect. If a resource-based policy grants permission to the ARN of the federated IAM user, then requests made by the federated user during the session are limited by an implicit deny in a permission boundary or session policy.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n',
        answers: [
          "<p>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy does not impact the final decision</p>",
          "<p>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy results in a final <code>Deny</code></p>",
          "<p>If a resource-based policy grants permission to the ARN of the federated IAM user, then requests made by the federated user during the session are not limited by an implicit deny in a permission boundary or session policy</p>",
          "<p>Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy</p>",
          "<p>If there is no explicit <code>Allow</code> in the SCP, the request is implicitly given an <code>Allow</code> after the SCP is evaluated</p>",
          "<p>If no applicable <code>Allow</code> statement is found in the SCPs, the request is explicitly denied, even if the denial is implicit</p>",
        ],
      },
      correct_response: ["a", "d", "f"],
      section: "Identity and Access Management",
      question_plain:
        "A Security Engineer has been tasked to evaluate the outcome of different policies, including but not limited to identity-based policies, resource-based policies, IAM permissions boundaries, session policies, and AWS Organizations service control policies (SCPs) of an AWS account.\n\nWhich of the following are valid statements regarding the aforementioned policy evaluations? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960104,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The security team at an e-commerce company wants to ensure that none of the AWS accounts for its multiple IT teams can delete the AWS KMS keys.</p>\n\n<p>Which of the following represents the most operationally efficient solution?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use AWS Organizations to set a service control policy that denies the <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> actions for all accounts within the organization</strong></p>\n\n<p>Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled.</p>\n\n<p>SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account\'s administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.</p>\n\n<p>You can use SCPs to allow or deny access to AWS services for individual AWS accounts with AWS Organizations member accounts, or for groups of accounts within an organizational unit (OU). The specified actions from an attached SCP affect all IAM identities including the root user of the member account.</p>\n\n<p>AWS services that aren\'t explicitly allowed by the SCPs associated with an AWS account or its parent OUs are denied access to the AWS accounts or OUs associated with the SCP. SCPs associated with an OU are inherited by all AWS accounts in that OU.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q3-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n\n<p>For the given use case, you can create the following sample SCP which is then associated with all AWS Organizations member accounts of the multiple IT teams.</p>\n\n<pre><code>{\n"Version": "2012-10-17",\n"Statement": [\n    {\n        "Sid": "DenyKMSDelete",\n        "Effect": "Deny",\n        "Resource": "*",\n        "Action": [\n            "kms:ScheduleKeyDeletion",\n            "kms:Delete*"\n        ]\n    }\n]\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Organizations to set a permissions boundary that denies the <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> actions for all accounts within the organization</strong> - AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity\'s permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. When you use a policy to set the permissions boundary for a user, it limits the user\'s permissions but does not provide permissions on its own. You cannot use AWS Organizations to set a permissions boundary, so this option is incorrect.</p>\n\n<p><strong>Use Amazon Inspector to monitor <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls and preempt these actions</strong> - Amazon Inspector automatically discovers workloads, such as Amazon EC2 instances, containers, and Lambda functions, and scans them for software vulnerabilities and unintended network exposure. It cannot monitor <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls and preempt these actions. This option has been added as a distractor.</p>\n\n<p><strong>Use AWS Config to set a rule that sends an alert when a <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls are made and automatically remediate these actions</strong> - While AWS Config can be used to set rules and send alerts, it cannot prevent specific API actions from being executed. AWS Config can be used to monitor the configuration of resources and alert when changes are made, but it cannot stop those changes from happening. You can apply remediation using AWS Systems Manager Automation documents which can be defined to meet your custom requirements. However, this option does not prevent the initial deletion of the KMS keys. This option provides remediation, not prevention.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/what-is-config.html">https://docs.aws.amazon.com/config/latest/developerguide/what-is-config.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/monitor-and-remediate-scheduled-deletion-of-aws-kms-keys.html">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/monitor-and-remediate-scheduled-deletion-of-aws-kms-keys.html</a></p>\n\n<p><a href="https://aws.amazon.com/inspector/">https://aws.amazon.com/inspector/</a></p>\n',
        answers: [
          "<p>Use AWS Organizations to set a permissions boundary that denies the <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> actions for all accounts within the organization</p>",
          "<p>Use Amazon Inspector to monitor <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls and preempt these actions</p>",
          "<p>Use AWS Organizations to set a service control policy that denies the <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> actions for all accounts within the organization</p>",
          "<p>Use AWS Config to set a rule that sends an alert when a <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls are made and automatically remediate these actions</p>",
        ],
      },
      correct_response: ["c"],
      section: "Identity and Access Management",
      question_plain:
        "The security team at an e-commerce company wants to ensure that none of the AWS accounts for its multiple IT teams can delete the AWS KMS keys.\n\nWhich of the following represents the most operationally efficient solution?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960106,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A rapidly growing e-commerce company stores all of its sensitive customer data in an Amazon S3 bucket. To ensure the safety and security of this data, the company has chosen to encrypt it using an AWS Key Management Service (AWS KMS) customer managed key. The company also uses AWS Lambda functions to perform various tasks within the same account as the S3 bucket. The Lambda functions need to access the data in the S3 bucket but the company must ensure that each Lambda function has its own programmatic access control permissions to use the KMS key.</p>\n\n<p>Which of the following options would you recommend?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Establish a Lambda execution role that provides access to the KMS key for each Lambda function</strong></p>\n\n<p>A Lambda function\'s execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources. You provide an execution role when you create a function. When you invoke your function, Lambda automatically provides your function with temporary credentials by assuming this role. You don\'t have to call sts:AssumeRole in your function code. For Lambda to properly assume your execution role, the role\'s trust policy must specify the Lambda service principal (lambda.amazonaws.com) as a trusted service.</p>\n\n<p>For the given use case, you need to create a Lambda execution role that provides specific access permissions to use the KMS key for each Lambda function. This is a more efficient solution as it allows for easier management of access permissions for multiple functions. This allows the company to define the permissions for each Lambda function, ensuring that each function only has the necessary access to the KMS key to perform its intended task.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Assign an IAM policy to each Lambda function that grants access to the KMS key</strong> - You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines its permissions. You cannot assign an IAM policy to a Lambda function, so this option is incorrect.</p>\n\n<p><strong>Create a key grant for the Lambda service principal, and adjust the permissions as needed</strong> - A grant is a policy instrument that allows AWS principals to use KMS keys in cryptographic operations. It also can let them view a KMS key (DescribeKey) and create and manage grants. When authorizing access to a KMS key, grants are considered along with key policies and IAM policies. Grants are often used for temporary permissions because you can create one, use its permissions, and delete it without changing your key policies or IAM policies.</p>\n\n<p>Grants are a very flexible and useful access control mechanism. When you create a grant for a KMS key, the grant allows the grantee principal to call the specified grant operations on the KMS key provided that all conditions specified in the grant are met.</p>\n\n<p>The grantee principal can be any AWS principal, including an AWS account (root), an IAM user, an IAM role, a federated role or user, or an assumed role user. The grantee principal can be in the same account as the KMS key or a different account. However, the grantee principal cannot be a service principal, an IAM group, or an AWS organization. Therefore, this option is incorrect.</p>\n\n<p><strong>Set up each Lambda function to assume an IAM role that grants access to the AWS-managed KMS key for Amazon S3</strong> - This option is a distractor as it refers to an AWS-managed KMS key whereas the use case refers to the customer managed key. In addition, you should note that typically you would assume an IAM role within a Lambda function to access resources in another AWS account.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html">https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/grants.html">https://docs.aws.amazon.com/kms/latest/developerguide/grants.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a></p>\n',
        answers: [
          "<p>Assign an IAM policy to each Lambda function that grants access to the KMS key</p>",
          "<p>Establish a Lambda execution role that provides access to the KMS key for each Lambda function</p>",
          "<p>Create a key grant for the Lambda service principal, and adjust the permissions as needed</p>",
          "<p>Set up each Lambda function to assume an IAM role that grants access to the AWS-managed KMS key for Amazon S3</p>",
        ],
      },
      correct_response: ["b"],
      section: "Identity and Access Management",
      question_plain:
        "A rapidly growing e-commerce company stores all of its sensitive customer data in an Amazon S3 bucket. To ensure the safety and security of this data, the company has chosen to encrypt it using an AWS Key Management Service (AWS KMS) customer managed key. The company also uses AWS Lambda functions to perform various tasks within the same account as the S3 bucket. The Lambda functions need to access the data in the S3 bucket but the company must ensure that each Lambda function has its own programmatic access control permissions to use the KMS key.\n\nWhich of the following options would you recommend?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960108,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A financial services company is running an Amazon RDS for MySQL DB instance in a virtual private cloud (VPC) to store sensitive customer data. Due to strict security policies, the company has implemented a VPC that does not allow any network traffic to or from the internet. A security engineer at the company wants to use AWS Secrets Manager to automatically rotate the DB instance credentials for increased security. However, due to the company's security policy, the engineer is not allowed to use the standard AWS Lambda function provided by Secrets Manager to rotate the credentials.</p>\n\n<p>To address this issue, the security engineer deploys a custom Lambda function within the VPC. This function is responsible for rotating the secret in Secrets Manager. The security engineer also edits the DB instance's security group to allow connections from this custom Lambda function. However, when the function is invoked, it is unable to communicate with Secrets Manager and cannot rotate the secret.</p>\n\n<p>Which of the following options will address the given scenario?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Add a VPC Interface Endpoint for Secrets Manager and configure the Lambda function\'s subnet to use it</strong></p>\n\n<p>A VPC interface endpoint is a VPC component that enables the communication between resources in a VPC and services powered by AWS PrivateLink, without the need for an internet gateway, NAT device, VPN connection or AWS Direct Connect link. It allows for communication between the VPC and the service over an Amazon-provided private IP address, eliminating exposure to the public internet.</p>\n\n<p>AWS PrivateLink enables you to access services over an Amazon-provided IP address from within your VPC, without using public IPs or an internet gateway. With VPC interface endpoint, you can create a private connection between your VPC and supported services powered by AWS PrivateLink, using VPC endpoint services powered by AWS PrivateLink.</p>\n\n<p>A service provider creates an endpoint service to make their service available in a Region. A service consumer creates a VPC endpoint to connect their VPC to an endpoint service. A service consumer must specify the service name of the endpoint service when creating a VPC endpoint.</p>\n\n<p>How AWS PrivateLink works:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q5-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html">https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html</a></p>\n\n<p>Secrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can\'t be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q5-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p>\n\n<p>This is the correct option as it allows the custom Lambda function in the VPC to communicate with Secrets Manager without going through the internet. A VPC endpoint for Secrets Manager is a VPC component that enables the communication between the VPC and Secrets Manager without going through the internet or a VPN connection.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a NAT Gateway in the VPC. Configure the Lambda function to use the NAT Gateway for connecting to the Secrets Manager</strong> - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your AWS account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access private resources while the function is running. As mentioned in the explanation above, you can leverage the VPC endpoint to connect to Secrets Manager from within the VPC and thereby avoid the internet.</p>\n\n<p>Internet access from a private subnet requires network address translation (NAT). To give your Lambda function access to the internet, you need to route outbound traffic to a NAT gateway in a public subnet. If you configure the Lambda function to use the NAT Gateway to connect to the Secrets Manager, you will end up using the internet. Therefore, this option serves as a distractor.</p>\n\n<p><strong>Create a Direct Connect connection between the VPC and Secrets Manager and configure the Lambda function\'s subnet to use it</strong> - AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. One end of the cable is connected to your router, the other to an AWS Direct Connect router. This option is incorrect because Direct Connect is used to establish a dedicated network connection between an on-premises data center and a VPC, not between a VPC and a service like Secrets Manager.</p>\n\n<p><strong>Create a VPC Peering connection between the VPC and Secrets Manager and configure the Lambda function\'s subnet to use it</strong> - This option is incorrect because VPC Peering is used to connect two VPCs together, not a VPC and a service. Secrets Manager is not a VPC, it\'s a service, therefore it cannot be connected via VPC Peering.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html">https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html">https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_vpc-endpoint.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_vpc-endpoint.html</a></p>\n',
        answers: [
          "<p>Add a VPC Interface Endpoint for Secrets Manager and configure the Lambda function's subnet to use it</p>",
          "<p>Create a NAT Gateway in the VPC. Configure the Lambda function to use the NAT Gateway for connecting to the Secrets Manager</p>",
          "<p>Create a VPC Peering connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</p>",
          "<p>Create a Direct Connect connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</p>",
        ],
      },
      correct_response: ["a"],
      section: "Infrastructure Security",
      question_plain:
        "A financial services company is running an Amazon RDS for MySQL DB instance in a virtual private cloud (VPC) to store sensitive customer data. Due to strict security policies, the company has implemented a VPC that does not allow any network traffic to or from the internet. A security engineer at the company wants to use AWS Secrets Manager to automatically rotate the DB instance credentials for increased security. However, due to the company's security policy, the engineer is not allowed to use the standard AWS Lambda function provided by Secrets Manager to rotate the credentials.\n\nTo address this issue, the security engineer deploys a custom Lambda function within the VPC. This function is responsible for rotating the secret in Secrets Manager. The security engineer also edits the DB instance's security group to allow connections from this custom Lambda function. However, when the function is invoked, it is unable to communicate with Secrets Manager and cannot rotate the secret.\n\nWhich of the following options will address the given scenario?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960110,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A large company that uses AWS recently received an email from the AWS Abuse team. The email informed them that an IAM user associated with the company's AWS account had their access key and secret access key pair published in public code repositories, although there are no signs yet of any compromise within the company's AWS account. The IAM user in question is designated as a service account and is used in a critical customer-facing production application with hard-coded credentials. To address this situation and minimize application downtime, you have been tasked as an AWS Certified Security Specialist for implementing a solution that protects the AWS account from any unauthorized access.</p>\n\n<p>Which of the following steps would you suggest?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><em>*\n1. Inactivate the publicly exposed IAM access key\n2. Create a new access key and secret access key pair for the IAM user\n3. Update the application to use the new credentials\n4. Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user\n5. Delete AWS Management Console credentials associated with the IAM user\n*</em></p>\n\n<p>This option is correct because it prioritizes securing the AWS account by first inactivating the publicly exposed IAM access key, which would prevent any unauthorized access to the AWS resources. Then, it creates a new access key and secret access key pair for the IAM user and updates the application to use the new credentials to minimize application downtime. Finally, it revokes any temporary AWS STS credentials associated with the IAM user and deletes any AWS Management Console credentials, further reducing the risk of unauthorized access. This sequence of actions would ensure that the AWS account is secure and minimize the risk of any disruption to the customer-facing production application.</p>\n\n<p>Incorrect options:</p>\n\n<p><em>*\n1. Delete AWS Management Console credentials associated with the IAM user\n2. Create a new access key and secret access key pair for the IAM user\n3. Update the application to use the new credentials\n4. Inactivate the publicly exposed IAM access key\n5. Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user\n*</em></p>\n\n<p><em>*\n1. Delete AWS Management Console credentials associated with the IAM user\n2. Create a new access key and secret access key pair for the IAM user\n3. Inactivate the publicly exposed IAM access key\n4. Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user\n5. Update the application to use the new credentials\n*</em></p>\n\n<p>These two options entail a lower priority for inactivating the publicly exposed IAM access key, which should be the first step to be accomplished for preventing any unauthorized access to the AWS resources. So, both these options are incorrect.</p>\n\n<p><em>*\n1. Revoke temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user\n2. Inactivate the publicly exposed IAM access key\n3. Create a new access key and secret access key pair for the IAM user\n4. Update the application to use the new credentials\n5. Delete AWS Management Console credentials associated with the IAM user\n*</em></p>\n\n<p>Revoking the temporary AWS Security Token Service (AWS STS) credentials upfront would cause immediate application downtime. Therefore, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/">https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/</a></p>\n',
        answers: [
          "<ol>\n<li>Delete AWS Management Console credentials associated with the IAM user</li>\n<li>Create a new access key and secret access key pair for the IAM user</li>\n<li>Update the application to use the new credentials</li>\n<li>Inactivate the publicly exposed IAM access key</li>\n<li>Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user</li>\n</ol>",
          "<ol>\n<li>Inactivate the publicly exposed IAM access key</li>\n<li>Create a new access key and secret access key pair for the IAM user</li>\n<li>Update the application to use the new credentials</li>\n<li>Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user</li>\n<li>Delete AWS Management Console credentials associated with the IAM user</li>\n</ol>",
          "<ol>\n<li>Revoke temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user</li>\n<li>Inactivate the publicly exposed IAM access key</li>\n<li>Create a new access key and secret access key pair for the IAM user</li>\n<li>Update the application to use the new credentials</li>\n<li>Delete AWS Management Console credentials associated with the IAM user</li>\n</ol>",
          "<ol>\n<li>Delete AWS Management Console credentials associated with the IAM user</li>\n<li>Create a new access key and secret access key pair for the IAM user</li>\n<li>Inactivate the publicly exposed IAM access key</li>\n<li>Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user</li>\n<li>Update the application to use the new credentials</li>\n</ol>",
        ],
      },
      correct_response: ["b"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "A large company that uses AWS recently received an email from the AWS Abuse team. The email informed them that an IAM user associated with the company's AWS account had their access key and secret access key pair published in public code repositories, although there are no signs yet of any compromise within the company's AWS account. The IAM user in question is designated as a service account and is used in a critical customer-facing production application with hard-coded credentials. To address this situation and minimize application downtime, you have been tasked as an AWS Certified Security Specialist for implementing a solution that protects the AWS account from any unauthorized access.\n\nWhich of the following steps would you suggest?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960114,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company operates a global data analytics website hosted on AWS. The website relies on Amazon CloudFront to deliver content to its customers. Recently, the company is facing new data regulation policies and is required to block inbound traffic from a specific set of countries. The company needs to find a solution to comply with the new data regulation policies while maintaining the cost-effectiveness of its infrastructure.</p>\n\n<p>What do you recommend?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Leverage geographic restrictions in CloudFront to deny traffic from a specific set of countries</strong></p>\n\n<p>You can use geographic restrictions in CloudFront, sometimes known as geo-blocking, to prevent users in specific geographic locations from accessing content that you\'re distributing through a CloudFront distribution.</p>\n\n<p>When a user requests your content, CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geographic restrictions feature to do one of the following:</p>\n\n<p>Allow your users to access your content only if they’re in one of the approved countries on your allow list.</p>\n\n<p>Prevent your users from accessing your content if they’re in one of the banned countries on your block list.</p>\n\n<p>For example, if a request comes from a country where you are not authorized to distribute your content, you can use CloudFront geographic restrictions to block the request.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q7-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP(S) requests that are forwarded to your protected web application resources. You can protect the following resource types:</p>\n\n<p>Amazon CloudFront distribution</p>\n\n<p>Amazon API Gateway REST API</p>\n\n<p>Application Load Balancer</p>\n\n<p>AWS AppSync GraphQL API</p>\n\n<p>Amazon Cognito user pool</p>\n\n<p>AWS WAF also lets you control access to your content. Based on criteria that you specify, such as the IP addresses that requests originate from or the values of query strings, the service associated with your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response.</p>\n\n<p><strong>Leverage an AWS WAF web ACL with an IP match condition to deny traffic from a specific set of countries. Configure the CloudFront distribution to use the web ACL</strong> - This option is a distractor. If you want to allow or block web requests based on the IP addresses that the requests originate from, you need to create one or more IP match conditions. You cannot use an IP match condition to deny traffic from a specific set of countries.</p>\n\n<p><strong>Leverage an AWS WAF web ACL with a geo match condition to deny traffic from a specific set of countries. Configure the CloudFront distribution to use the web ACL</strong> - You can use the geo match statement to manage requests from specific countries or regions. Although you can use the geo match condition in a WAF to deny traffic from a specific set of countries, this option is costlier than using the built-in geographic restriction feature of CloudFront. Therefore, this option is incorrect for the given use case.</p>\n\n<p><strong>Leverage geolocation routing policies in CloudFront to deny traffic from a specific set of countries</strong> - This option has been added as a distractor. There is no such thing as geolocation routing policies in CloudFront. You use the geolocation routing policies in Route 53 to localize your content and present some or all of your website in the language of your users. You can also use the geolocation routing policies of Route 53 to restrict the distribution of content to only certain locations.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-geo-restriction/">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-geo-restriction/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html</a></p>\n',
        answers: [
          "<p>Leverage geolocation routing policies in CloudFront to deny traffic from a specific set of countries</p>",
          "<p>Leverage an AWS WAF web ACL with an IP match condition to deny traffic from a specific set of countries. Configure the CloudFront distribution to use the web ACL</p>",
          "<p>Leverage an AWS WAF web ACL with a geo match condition to deny traffic from a specific set of countries. Configure the CloudFront distribution to use the web ACL</p>",
          "<p>Leverage geographic restrictions in CloudFront to deny traffic from a specific set of countries</p>",
        ],
      },
      correct_response: ["d"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A company operates a global data analytics website hosted on AWS. The website relies on Amazon CloudFront to deliver content to its customers. Recently, the company is facing new data regulation policies and is required to block inbound traffic from a specific set of countries. The company needs to find a solution to comply with the new data regulation policies while maintaining the cost-effectiveness of its infrastructure.\n\nWhat do you recommend?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960116,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A Network Load Balancer (NLB) was recently set up in a company's AWS infrastructure, but the target instances are not entering the InService state. The security engineer was called upon to investigate the issue. After conducting a thorough investigation, the engineer determined that the health checks were failing.</p>\n\n<p>Which of the following could cause the health checks to fail? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          "<p>Correct options:</p>\n\n<p>A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.</p>\n\n<p>When you create an internet-facing load balancer, you can optionally specify one Elastic IP address per subnet. If you do not choose one of your own Elastic IP addresses, Elastic Load Balancing provides one Elastic IP address per subnet for you. These Elastic IP addresses provide your load balancer with static IP addresses that will not change during the life of the load balancer. You can't change these Elastic IP addresses after you create the load balancer.</p>\n\n<p>When you create an internal load balancer, you can optionally specify one private IP address per subnet. If you do not specify an IP address from the subnet, Elastic Load Balancing chooses one for you. These private IP addresses provide your load balancer with static IP addresses that will not change during the life of the load balancer. You can't change these private IP addresses after you create the load balancer.</p>\n\n<p>You can associate a security group with your Network Load Balancer to control the traffic that is allowed to reach and leave the load balancer. You specify the ports, protocols, and sources to allow for inbound traffic and the ports, protocols, and destinations to allow for outbound traffic. If you don't assign a security group to your load balancer, all client traffic can reach the load balancer listeners and all traffic can leave the load balancer.</p>\n\n<p><strong>The target instance’s subnet network ACL does not allow traffic from the NLB's IP Addresses</strong></p>\n\n<p>A network access control list (ACL) allows or denies specific inbound or outbound traffic at the subnet level. If the network ACL does not allow traffic from the NLB's IP Addresses, then the NLB's health checks will fail.</p>\n\n<p><strong>The target instance’s security group has no rules that allow traffic from the NLB's IP Addresses</strong></p>\n\n<p>A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. If the target instance’s security group has no rules to allow traffic from the NLB's IP Addresses, then the NLB's health checks will fail.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html</a></p>\n\n<p><strong>The target instance’s security group has rules that are not using the correct IP addresses to allow traffic from the NLB</strong></p>\n\n<p>If the target instance’s security group has rules that are not using the correct IP addresses from the list of the NLB's IP Addresses, then the NLB's health checks will fail, since the traffic from NLB will not be able to reach the instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target instance’s subnet network ACL does not allow traffic from the NLB's security group</strong> - Network ACL can only permit using CIDR range in the source or destination. So, specifying the NLB's security group as a source for the network ACL rule is not possible.</p>\n\n<p><strong>The target instance’s security group does not allow traffic from the NLB's network ACL</strong> - A security group can only use a single IPv4/IPv6 address, a range of IPv4/IPv6 addresses, a prefix list, or another security group as a source or destination in the rules. Therefore, you cannot use a network ACL name as a source or destination, hence this option is incorrect.</p>\n\n<p><strong>The target instance’s security group is not using the DNS name of the NLB to allow traffic from the NLB</strong></p>\n\n<p>A security group can only use a single IPv4/IPv6 address, a range of IPv4/IPv6 addresses, a prefix list, or another security group as a source or destination in the rules. Therefore, you cannot use a DNS name as a source or destination, hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/network-load-balancers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/network-load-balancers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html</a></p>\n",
        answers: [
          "<p>The target instance’s subnet network ACL does not allow traffic from the NLB's security group</p>",
          "<p>The target instance’s subnet network ACL does not allow traffic from the NLB's IP Addresses</p>",
          "<p>The target instance’s security group has no rules that allow traffic from the NLB's IP Addresses</p>",
          "<p>The target instance’s security group does not allow traffic from the NLB's network ACL</p>",
          "<p>The target instance’s security group is not using the DNS name of the NLB to allow traffic from the NLB</p>",
          "<p>The target instance’s security group has rules that are not using the correct IP addresses to allow traffic from the NLB</p>",
        ],
      },
      correct_response: ["b", "c", "f"],
      section: "Infrastructure Security",
      question_plain:
        "A Network Load Balancer (NLB) was recently set up in a company's AWS infrastructure, but the target instances are not entering the InService state. The security engineer was called upon to investigate the issue. After conducting a thorough investigation, the engineer determined that the health checks were failing.\n\nWhich of the following could cause the health checks to fail? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960118,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A mid-sized company stores sensitive data on an Amazon Elastic Block Store (EBS) volume attached to an Amazon Elastic Compute Cloud (EC2) instance. To ensure data durability, the company also replicates this sensitive data to an Amazon Simple Storage Service (S3) bucket. Both the EBS volume and S3 bucket are encrypted using the same AWS Key Management Service (KMS) Customer Master Key (CMK). The security team at the company has noticed that the CMK has been deleted as a former employee had set the key for deletion before leaving the company.</p>\n\n<p>As a Security Specialist, what do you suggest to access the data?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Copy the data directly from the EBS encrypted volume before the volume is detached from the EC2 instance</strong></p>\n\n<p>You can use AWS Key Management Service (KMS) to create and control the cryptographic keys that are used to protect your data on AWS. An AWS KMS key is a logical representation of a cryptographic key. A KMS key contains metadata, such as the key ID, key spec, key usage, creation date, description, and key state. Most importantly, it contains a reference to the key material that is used when you perform cryptographic operations with the KMS key.</p>\n\n<p>By default, AWS KMS creates the key material for a KMS key. However, you can import your own key material into a KMS key, or use a custom key store to create KMS keys that use key material in your AWS CloudHSM cluster, or key material in an external key manager that you own and manage outside of AWS.</p>\n\n<p>Deleting a KMS key deletes the key material and all metadata associated with the KMS key and is irreversible. After a KMS key is deleted, you can no longer decrypt the data that was encrypted under that KMS key, which means that data becomes unrecoverable. Because it is destructive and potentially dangerous to delete a KMS key, AWS KMS requires you to set a waiting period of 7 – 30 days. The default waiting period is 30 days.</p>\n\n<p>For the given use case, the EBS volume has been encrypted with a KMS key. When you attach the EBS volume to an EC2 instance, Amazon EC2 uses your KMS key to decrypt the EBS volume\'s encrypted data key. Amazon EC2 stores the plaintext data key in hypervisor memory and uses it to encrypt disk I/O to the EBS volume. The data key persists in memory as long as the EBS volume is attached to the EC2 instance. Even if someone has scheduled the key for deletion and the key is past the waiting period for deletion, this has no immediate effect on the EC2 instance or the EBS volume. Amazon EC2 is using the plaintext data key—not the KMS key—to encrypt all disk I/O while the volume is attached to the instance.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q9-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Login as the AWS account root user to restore the deleted key and then use it to recover the data from the EBS volume</strong> - You cannot use the AWS account root user privileges to restore the deleted key and then use it to recover the data from the EBS volume. Once deleted, a KMS key can never be accessed again.</p>\n\n<p><strong>Raise a ticket with AWS Support to restore the deleted key and then use it to recover the data from the EBS volume</strong> - AWS Support cannot restore a deleted KMS key. Once deleted, a KMS key can never be accessed again.</p>\n\n<p><strong>Take a snapshot of the EBS encrypted volume before the volume is detached from the EC2 instance</strong> - You cannot take a snapshot of an EBS encrypted volume for which the KMS key has been deleted as the snapshot itself needs to be encrypted using the same KMS key.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q9-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p>\n',
        answers: [
          "<p>Login as the AWS account root user to restore the deleted key and then use it to recover the data from the EBS volume</p>",
          "<p>Copy the data directly from the EBS encrypted volume before the volume is detached from the EC2 instance</p>",
          "<p>Take a snapshot of the EBS encrypted volume before the volume is detached from the EC2 instance</p>",
          "<p>Raise a ticket with AWS Support to restore the deleted key and then use it to recover the data from the EBS volume</p>",
        ],
      },
      correct_response: ["b"],
      section: "Infrastructure Security",
      question_plain:
        "A mid-sized company stores sensitive data on an Amazon Elastic Block Store (EBS) volume attached to an Amazon Elastic Compute Cloud (EC2) instance. To ensure data durability, the company also replicates this sensitive data to an Amazon Simple Storage Service (S3) bucket. Both the EBS volume and S3 bucket are encrypted using the same AWS Key Management Service (KMS) Customer Master Key (CMK). The security team at the company has noticed that the CMK has been deleted as a former employee had set the key for deletion before leaving the company.\n\nAs a Security Specialist, what do you suggest to access the data?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960120,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>During regular maintenance tasks, an application support team noticed an abnormal activity on an Amazon EC2 instance that is configured with an EBS volume. The team immediately informed a Security Engineer of the anomaly. The instance is part of an Auto Scaling Group fronted by an Elastic Load Balancer.</p>\n\n<p>What immediate steps should the Security Engineer take for preventing any further attacks to secure the connecting systems and understand the root cause?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group and snapshot the Amazon EBS data volumes that are attached to the EC2 instance. Launch an EC2 instance with a forensic toolkit and attach an EBS volume created from the snapshot of the suspicious EBS volume</strong></p>\n\n<p>AWS recommends the following actions when a potential security anomaly is detected on your Amazon EC2 instance:</p>\n\n<ol>\n<li><p>Capture the metadata from the Amazon EC2 instance, before you make any changes to your environment.</p></li>\n<li><p>Protect the Amazon EC2 instance from accidental termination by enabling termination protection for the instance.</p></li>\n<li><p>Isolate the Amazon EC2 instance by switching the VPC Security Group. However, be aware of VPC connection tracking and other containment techniques.</p></li>\n<li><p>Detach the Amazon EC2 instance from any AWS Auto Scaling groups.</p></li>\n<li><p>Deregister the Amazon EC2 instance from any related Elastic Load Balancing service.</p></li>\n<li><p>Snapshot the Amazon EBS data volumes that are attached to the EC2 instance for preservation and follow-up investigations.</p></li>\n<li><p>Tag the Amazon EC2 instance as quarantined for investigation, and add any pertinent metadata, such as the trouble ticket associated with the investigation.</p></li>\n</ol>\n\n<p>You can perform all of the preceding steps using the AWS APIs, AWS SDKs, AWS CLI, and AWS Management Console. To interact with AWS using these methods, the IAM service helps you securely control access to AWS resources. You use IAM to control who is authenticated and authorized to use resources at the Account Level. The IAM service provides the authentication and authorization for you to perform these actions and interact with the service domain.</p>\n\n<p>A snapshot of an Amazon EBS volume is a point-in-time, block-level copy of an EBS data volume, which occurs asynchronously and might take time to complete, but it is a delta of that data going forward. You can create new EBS volumes from these copies and mount them to the forensic EC2 instance for deep analysis offline by forensic investigators.</p>\n\n<p>EC2 Instance Isolation and Snapshots:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q10-i1.jpg">\nvia - <a href="https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf">https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group. Launch a new EC2 instance with a forensic toolkit, and allow the forensic toolkit image to connect to the suspicious instance to perform the investigation</strong> - You cannot launch a new EC2 instance with a forensic toolkit and then connect it to the suspicious instance for investigation as this goes against the best practice of quarantining and isolating the suspicious instance. You must also note that placing the suspicious instance within an isolation security group would not allow any new connections from the forensic EC2 instance. Therefore, this option is incorrect.</p>\n\n<p><strong>Remove the instance from the Auto Scaling group. Place the instance within an isolation security group. Launch an EC2 instance with a forensic toolkit, and use the forensic toolkit image to deploy another ENI to inspect all traffic coming from the suspicious instance</strong> - You must quarantine and isolate the suspicious instance immediately. You must not inspect live traffic coming from the suspicious instance. You must also note that placing the suspicious instance within an isolation security group would not allow any new connections from the forensic EC2 instance. Hence this option is incorrect.</p>\n\n<p><strong>Detach the instance from the Auto Scaling group and place it within an isolation security group. Detach the suspicious EBS volume. Launch an EC2 instance with a forensic toolkit and attach the detached EBS volume to investigate</strong> - Connecting the suspicious EBS volume directly to another EC2 instance does not help contain the malicious activity. Also, the EBS volume connected to the EC2 instance is of great importance in investigating the root cause of the attack. A snapshot of EBS volume is necessary for data preservation and follow-up investigations. Hence, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf">https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf</a></p>\n',
        answers: [
          "<p>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group. Launch a new EC2 instance with a forensic toolkit, and allow the forensic toolkit image to connect to the suspicious instance to perform the investigation</p>",
          "<p>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group and snapshot the Amazon EBS data volumes that are attached to the EC2 instance. Launch an EC2 instance with a forensic toolkit and attach an EBS volume created from the snapshot of the suspicious EBS volume</p>",
          "<p>Remove the instance from the Auto Scaling group. Place the instance within an isolation security group. Launch an EC2 instance with a forensic toolkit, and use the forensic toolkit image to deploy another ENI to inspect all traffic coming from the suspicious instance</p>",
          "<p>Detach the instance from the Auto Scaling group and place it within an isolation security group. Detach the suspicious EBS volume. Launch an EC2 instance with a forensic toolkit and attach the detached EBS volume to investigate</p>",
        ],
      },
      correct_response: ["b"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "During regular maintenance tasks, an application support team noticed an abnormal activity on an Amazon EC2 instance that is configured with an EBS volume. The team immediately informed a Security Engineer of the anomaly. The instance is part of an Auto Scaling Group fronted by an Elastic Load Balancer.\n\nWhat immediate steps should the Security Engineer take for preventing any further attacks to secure the connecting systems and understand the root cause?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960122,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A security engineer must ensure that all certificates imported into AWS Certificate Manager (ACM) in all AWS Regions, must be notified of expiry, 30 days before their actual expiry via a single notification to the security administrator. The notification along with the certificate information should be sent to the security administrator and the Security Hub for centralized management.</p>\n\n<p>Which steps must be taken to perform these tasks optimally?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Configure the <code>DaysToExpiry</code> CloudWatch metric to schedule a batch search of expiring ACM certificates and trigger an AWS Lambda function to send the certificates-to-be-expired notification to an SNS topic. This Lambda function can also be configured to log all the expiring certificates as findings in Security Hub</strong></p>\n\n<p>ACM provides managed renewals that automatically renew certificates in most cases, there are exceptions, such as imported certs, where an automatic renewal isn’t possible.</p>\n\n<p>This option provides a scheduled solution to examine all expiring certificates in ACM, log all the findings in Security Hub, and generate a single notification through SNS for all certificates that are found. The option workflow is as follows:\n1. CloudWatch runs the rule on a timer and invokes a Lambda function.\n2. The function finds all certificates that have a DaysToExpiry metric in CloudWatch.\n3. The function logs all the expiring certificates as findings in Security Hub.\n4. The function publishes a notification to an SNS topic with the expiration details.\n5. SNS creates a notification (most commonly, through email) to any subscribers of the topic.</p>\n\n<p>Monitor expirations of imported certificates in AWS ACM using the scheduled solution:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q11-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/">https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Security Hub has a built-in feature to monitor certificate expirations of ACM certificates. Configure the security Hub to trigger SNS notifications 30 days before the actual expiry date of the certificate</strong> - Although Security Hub can be used to monitor certificate expirations without the solution described above, Security Hub is a Regional service, therefore the monitoring of certificate expirations across Regions can be time-consuming for the initial configuration and difficult to maintain. So this option is not the best fit. The correct solution described above consolidates all certificate notifications from all Regions in which the solution is deployed into the findings of a single Region.</p>\n\n<p><strong>ACM built-in Certificate Expiration event raised through Amazon EventBridge, can be used to invoke a Lambda function. This event-based function raised from a specific certificate, can be configured to publish the result as a finding to Security Hub, and further to an SNS topic used for email subscriptions. An IT service management system can be configured to automatically open a case or incident through SNS and remediate the issue</strong> - This option uses the ACM built-in Certificate Expiration event, which is raised through Amazon EventBridge, to invoke a Lambda function. In this option, the function is configured to publish the result as a finding in Security Hub, and also as an SNS topic used for email subscriptions. As a result, an administrator can be notified of a specific expiring certificate, or an IT service management (ITSM) system can automatically open a case or incident through email or SNS.</p>\n\n<p>This solution provides a Lambda function that makes use of CloudWatch rules to report back those certificates that are due to expire within a pre-defined amount of time. Since the event is based on an event that is raised from a specific certificate, the function examines the single certificate and then generates a separate notification for each certificate that is marked for expiry. This option is not optimal for the given use case since the ask is to receive alerts via a single notification for all certificates marked for expiry.</p>\n\n<p>Monitor expirations of imported certificates in AWS ACM using an event-based solution:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q11-i2.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/">https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/</a></p>\n\n<p><strong>Leverage the ACM_CERTIFICATE_EXPIRATION_CHECK managed rule provided by AWS Config to automatically renew the certificates imported into ACM. Forward the rule invocation to trigger SNS notifications to the security administrator</strong> - You can use the ACM_CERTIFICATE_EXPIRATION_CHECK managed rule to check if AWS Certificate Manager certificates in your account are marked for expiration within the specified number of days. Certificates provided by ACM are automatically renewed. ACM does not automatically renew the certificates that you import. You cannot use this managed rule to automatically renew the certificates imported into ACM, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/">https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/acm/latest/userguide/cloudwatch-metrics.html">https://docs.aws.amazon.com/acm/latest/userguide/cloudwatch-metrics.html</a></p>\n',
        answers: [
          "<p>Leverage the ACM_CERTIFICATE_EXPIRATION_CHECK managed rule provided by AWS Config to automatically renew the certificates imported into ACM. Forward the rule invocation to trigger SNS notifications to the security administrator</p>",
          "<p>Security Hub has a built-in feature to monitor certificate expirations of ACM certificates. Configure the security Hub to trigger SNS notifications 30 days before the actual expiry date of the certificate</p>",
          "<p>Configure the <code>DaysToExpiry</code> CloudWatch metric to schedule a batch search of expiring ACM certificates and trigger an AWS Lambda function to send the certificates-to-be-expired notification to an SNS topic. This Lambda function can also be configured to log all the expiring certificates as findings in Security Hub</p>",
          "<p>ACM built-in Certificate Expiration event raised through Amazon EventBridge, can be used to invoke a Lambda function. This event-based function raised from a specific certificate, can be configured to publish the result as a finding to Security Hub, and further to an SNS topic used for email subscriptions. An IT service management system can be configured to automatically open a case or incident through SNS and remediate the issue</p>",
        ],
      },
      correct_response: ["c"],
      section: "Data Protection",
      question_plain:
        "A security engineer must ensure that all certificates imported into AWS Certificate Manager (ACM) in all AWS Regions, must be notified of expiry, 30 days before their actual expiry via a single notification to the security administrator. The notification along with the certificate information should be sent to the security administrator and the Security Hub for centralized management.\n\nWhich steps must be taken to perform these tasks optimally?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960192,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A data analytics company uses Amazon GuardDuty to identify unexpected, potentially unauthorized, and malicious activity within its AWS environment. The security team at the company wants all Medium/High Severity findings to automatically generate a ticket in a third-party ticketing system through email integration.</p>\n\n<p>As an AWS Certified Security Specialist, what would you suggest as the most optimal solution?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create an Amazon EventBridge rule that includes an event pattern that matches Medium/High severity GuardDuty findings. Set up an Amazon Simple Notification Service (Amazon SNS) topic. Configure the third-party ticketing email system as a subscriber to the SNS topic. Set the SNS topic as the target for the EventBridge rule</strong></p>\n\n<p>Amazon GuardDuty is a security monitoring service that analyzes and processes Foundational data sources, such as AWS CloudTrail management events, AWS CloudTrail event logs, VPC flow logs (from Amazon EC2 instances), and DNS logs.</p>\n\n<p>GuardDuty informs you of the status of your AWS environment by producing security findings that you can view in the GuardDuty console or through Amazon EventBridge.</p>\n\n<p>EventBridge is a serverless service that uses events to connect application components together, making it easier for you to build scalable event-driven applications. Event-driven architecture is a style of building loosely coupled software systems that work together by emitting and responding to events. Event-driven architecture can help you boost agility and build reliable, scalable applications. An Eventbridge target is a resource or endpoint that EventBridge sends an event to when the event matches the event pattern defined for a rule. The rule processes the event data and sends the pertinent information to the target. To deliver event data to a target, EventBridge needs permission to access the target resource. You can define up to five targets for each rule.</p>\n\n<p>For the given use case, you can use a custom event pattern with the EventBridge rule to match Medium/High severity GuardDuty findings. Then, route the response to an Amazon Simple Notification Service (Amazon SNS) topic. You also need to set the third-party ticketing email system as a subscriber to the given SNS topic.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage the GuardDuty CreateFilter API operation to set up a filter in GuardDuty to monitor for Medium/High severity findings. Set up an SES endpoint as the target for the GuardDuty CreateFilter API so that SES can send out an email to the third-party ticketing email system</strong> -</p>\n\n<p><strong>Leverage the GuardDuty CreateFilter API operation to set up a filter in GuardDuty to monitor for Medium/High severity findings. Set up an Amazon Simple Notification Service (Amazon SNS) topic. Configure the third-party ticketing email system as a subscriber to the SNS topic. Set the SNS topic as the target for the GuardDuty CreateFilter API</strong></p>\n\n<p>GuardDuty CreateFilter API creates a filter using the specified finding criteria. If the action is successful, the service sends back an HTTP 200 response along with the name of the successfully created filter. There is no such thing as setting a target for the GuardDuty CreateFilter API. So both these options are incorrect.</p>\n\n<p><strong>Create an Amazon EventBridge rule that includes an event pattern that matches Medium/High severity GuardDuty findings. Set up an SES endpoint as the target for the EventBridge rule so that SES can send out an email to the third-party ticketing email system</strong> - Eventbridge does not support Amazon SES endpoint as a target, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html">https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings_cloudwatch.html">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings_cloudwatch.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/APIReference/API_CreateFilter.html">https://docs.aws.amazon.com/guardduty/latest/APIReference/API_CreateFilter.html</a></p>\n\n<p><a href="https://repost.aws/knowledge-center/guardduty-eventbridge-sns-rule">https://repost.aws/knowledge-center/guardduty-eventbridge-sns-rule</a></p>\n',
        answers: [
          "<p>Leverage the GuardDuty CreateFilter API operation to set up a filter in GuardDuty to monitor for Medium/High severity findings. Set up an SES endpoint as the target for the GuardDuty CreateFilter API so that SES can send out an email to the third-party ticketing email system</p>",
          "<p>Create an Amazon EventBridge rule that includes an event pattern that matches Medium/High severity GuardDuty findings. Set up an SES endpoint as the target for the EventBridge rule so that SES can send out an email to the third-party ticketing email system</p>",
          "<p>Create an Amazon EventBridge rule that includes an event pattern that matches Medium/High severity GuardDuty findings. Set up an Amazon Simple Notification Service (Amazon SNS) topic. Configure the third-party ticketing email system as a subscriber to the SNS topic. Set the SNS topic as the target for the EventBridge rule</p>",
          "<p>Leverage the GuardDuty CreateFilter API operation to set up a filter in GuardDuty to monitor for Medium/High severity findings. Set up an Amazon Simple Notification Service (Amazon SNS) topic. Configure the third-party ticketing email system as a subscriber to the SNS topic. Set the SNS topic as the target for the GuardDuty CreateFilter API</p>",
        ],
      },
      correct_response: ["c"],
      section: "Management and Security Governance",
      question_plain:
        "A data analytics company uses Amazon GuardDuty to identify unexpected, potentially unauthorized, and malicious activity within its AWS environment. The security team at the company wants all Medium/High Severity findings to automatically generate a ticket in a third-party ticketing system through email integration.\n\nAs an AWS Certified Security Specialist, what would you suggest as the most optimal solution?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960194,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The origin of an Amazon CloudFront distribution requires that all requests must include the Authorization header. This mandates the CloudFront distribution to forward the Authorization headers to the origin.</p>\n\n<p>As a Security Engineer, how will you configure a solution to address this use case?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create a cache policy. Then, associate the cache policy with the cache behavior that must forward the Authorization header</strong></p>\n\n<p>You can configure your distribution to forward the Authorization header to the origin in one of these ways:</p>\n\n<ol>\n<li><p>Create a cache policy. Then, associate the cache policy with the cache behavior that must forward the Authorization header.</p></li>\n<li><p>Edit an existing cache behavior with legacy cache settings.</p></li>\n</ol>\n\n<p>Steps to be followed:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q47-i1.jpg">\nvia - <a href="https://repost.aws/knowledge-center/cloudfront-authorization-header">https://repost.aws/knowledge-center/cloudfront-authorization-header</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the CloudFront origin request policy to forward the Authorization header to the origins</strong> - You can\'t use an origin request policy to forward the Authorization header. The header must be a part of the cache key to prevent the cache from satisfying unauthorized requests. CloudFront returns an HTTP 400 error if you create an origin request policy that forwards the Authorization header.</p>\n\n<p><strong>For Amazon Simple Storage Service (Amazon S3) origins, caching based on the Authorization header is supported only when the OPTIONS responses are cached. Hence, choose the options for default cache behavior settings that enable caching for OPTIONS responses</strong> - This statement is incorrect. For Amazon Simple Storage Service (Amazon S3) origins, caching based on the Authorization header isn\'t supported.</p>\n\n<p><strong>Configure the CloudFront viewer request policy to forward the Authorization header to the origin</strong> - Amazon CloudFront offers three different kinds of policies that you can use to customize CloudFront: CloudFront cache policy, CloudFront origin request policy, and CloudFront response headers policy. There is no CloudFront viewer request policy, this is a made-up option.</p>\n\n<p>References:</p>\n\n<p><a href="https://repost.aws/knowledge-center/cloudfront-authorization-header">https://repost.aws/knowledge-center/cloudfront-authorization-header</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html#header-caching-web-selecting">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html#header-caching-web-selecting</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/working-with-policies.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/working-with-policies.html</a></p>\n',
        answers: [
          "<p>Configure the CloudFront origin request policy to forward the Authorization header to the origin</p>",
          "<p>For Amazon Simple Storage Service (Amazon S3) origins, caching based on the Authorization header is supported only when the OPTIONS responses are cached. Hence, choose the options for default cache behavior settings that enable caching for OPTIONS responses</p>",
          "<p>Create a cache policy. Then, associate the cache policy with the cache behavior that must forward the Authorization header</p>",
          "<p>Configure the CloudFront viewer request policy to forward the Authorization header to the origin</p>",
        ],
      },
      correct_response: ["c"],
      section: "Infrastructure Security",
      question_plain:
        "The origin of an Amazon CloudFront distribution requires that all requests must include the Authorization header. This mandates the CloudFront distribution to forward the Authorization headers to the origin.\n\nAs a Security Engineer, how will you configure a solution to address this use case?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960196,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>An application deployed on an Amazon Elastic Compute Cloud (Amazon EC2) instance needs to read from and write files to an S3 bucket in the same AWS account (Account A1). The application also reads (but doesn’t write) files from an S3 bucket in another AWS Account (Account A2). The company uses a multi-account strategy and each application has its own AWS account.</p>\n\n<p>Three teams access the company's data: the Central Cloud Team, the Application Team, and the Data Lake Team. The Central Cloud Team is responsible for the overall security and governance of the AWS environment across all AWS accounts. The Application Team is responsible for building, deploying, and running their application within the application account (Account A1) that they own and manage. Likewise, the Data Lake Team owns and manages the Data Lake account (Account A2). The Central Cloud Team has two security requirements that they want to apply:</p>\n\n<p>a) All AWS API calls across all accounts must be encrypted in transit and accounts can’t leave the organization on their own.</p>\n\n<p>b) Least privilege policy/permissions should be configured for the application in Account A1 to access files from the S3 bucket in Account A2.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following options would you combine to implement a solution for the given security and access requirements? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Create a Service Control Policy (SCP) that denies all requests that are not sent using SSL (TLS) and also prevents an account from leaving the organization. Apply the SCP to the root of the organization</strong></p>\n\n<p>SCPs are meant to be used as coarse-grained guardrails, and they don’t directly grant access. The primary function of SCPs is to enforce security invariants across AWS accounts and OUs in an organization. Security invariants are control objectives or configurations that you apply to multiple accounts, OUs, or the whole AWS organization. For example, you can use an SCP to prevent member accounts from leaving your organization or to enforce that AWS resources can only be deployed to certain Regions.</p>\n\n<p>For the current use case, SCP can be used to enforce all AWS API calls to be encrypted in transit and Accounts can’t leave the organization on their own.</p>\n\n<p><strong>The application team has to create an IAM role in Account A1, that the application running on the EC2 instance will use to get objects from the S3 bucket in Account A2. A resource-based policy has to be attached to the bucket in the data lake account (Account A2) that grants read access to the role in the application account (Account A1)</strong></p>\n\n<p>The only resource-based policy needed in this example is attached to the bucket in the data lake account (Account A2) which is external to the application account (Account A1). Both the identity-based policy and resource-based policy must grant access to an action on the S3 bucket for access to be allowed in a cross-account scenario. The resource-based policy configured for the S3 bucket in Account A2 grants read access to the IAM role in the application account (Account A1). Then, give the IAM role in Account A1 the necessary permissions to read (GetObject) objects from the S3 bucket in Account A2. This read-only access configuration also adheres to the least privilege principle recommended by the Central Cloud team.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a permission boundary that denies all requests that are not sent using SSL (TLS) and also prevents an account from leaving the organization. Apply the permission boundary to the root of the organization</strong> - A permissions boundary is a type of identity-based policy that doesn’t directly grant access. Instead, like an SCP, a permissions boundary acts as a guardrail for your IAM principals that allows you to set coarse-grained access controls. A permissions boundary is typically used to delegate the creation of IAM principals. Delegation enables other individuals in your accounts to create new IAM principals but limits the permissions that can be granted to the new IAM principals.</p>\n\n<p><strong>The application team has to create an IAM role in Account A1, that the application running on the EC2 instance will use to get objects from the S3 bucket in Account A2. A resource-based policy has to be attached to the bucket in the data lake account (Account A2) that grants full access to the role in the application account (Account A1)</strong> - The given use case requires the least privilege access to read the files stored in the S3 bucket in Account A2. Since this option grants full access to the role in the application account (Account A1), so this option is incorrect.</p>\n\n<p><strong>The application team has to create an IAM role in Account A1, that the application running on the EC2 instance will use to get objects from the S3 bucket in Account A2. An identity-based role must then be created in the data lake account (Account A2) that grants access to the application in Account A1</strong> - The identity-based role needs to be created in Account A1 and not in Account A2, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/">https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/</a></p>\n\n<p><a href="https://repost.aws/knowledge-center/cross-account-access-s3">https://repost.aws/knowledge-center/cross-account-access-s3</a></p>\n',
        answers: [
          "<p>Create a Service Control Policy (SCP) that denies all requests that are not sent using SSL (TLS) and also prevents an account from leaving the organization. Apply the SCP to the root of the organization</p>",
          "<p>Create a permission boundary that denies all requests that are not sent using SSL (TLS) and also prevents an account from leaving the organization. Apply the permission boundary to the root of the organization</p>",
          "<p>The application team has to create an IAM role in Account A1, that the application running on the EC2 instance will use to get objects from the S3 bucket in Account A2. A resource-based policy has to be attached to the bucket in the data lake account (Account A2) that grants full access to the role in the application account (Account A1)</p>",
          "<p>The application team has to create an IAM role in Account A1, that the application running on the EC2 instance will use to get objects from the S3 bucket in Account A2. A resource-based policy has to be attached to the bucket in the data lake account (Account A2) that grants read access to the role in the application account (Account A1)</p>",
          "<p>The application team has to create an IAM role in Account A1, that the application running on the EC2 instance will use to get objects from the S3 bucket in Account A2. An identity-based role must then be created in the data lake account (Account A2) that grants access to the application in Account A1</p>",
        ],
      },
      correct_response: ["a", "d"],
      section: "Management and Security Governance",
      question_plain:
        "An application deployed on an Amazon Elastic Compute Cloud (Amazon EC2) instance needs to read from and write files to an S3 bucket in the same AWS account (Account A1). The application also reads (but doesn’t write) files from an S3 bucket in another AWS Account (Account A2). The company uses a multi-account strategy and each application has its own AWS account.\n\nThree teams access the company's data: the Central Cloud Team, the Application Team, and the Data Lake Team. The Central Cloud Team is responsible for the overall security and governance of the AWS environment across all AWS accounts. The Application Team is responsible for building, deploying, and running their application within the application account (Account A1) that they own and manage. Likewise, the Data Lake Team owns and manages the Data Lake account (Account A2). The Central Cloud Team has two security requirements that they want to apply:\n\na) All AWS API calls across all accounts must be encrypted in transit and accounts can’t leave the organization on their own.\n\nb) Least privilege policy/permissions should be configured for the application in Account A1 to access files from the S3 bucket in Account A2.\n\nAs an AWS Certified Security Specialist, which of the following options would you combine to implement a solution for the given security and access requirements? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960198,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>The development team at a company deploys to their AWS production environment through a continuous integration/continuous deployment (CI/CD) pipeline. The pipeline itself has broad access to create AWS resources needed to run the application. The company's security team wants to allow the development team to deploy their own IAM principals and policies for their application. However, the security team also needs a control mechanism that requires all resources created by the pipeline to have minimum privileges that comply with the security guidelines. All teams at the company are only allowed to modify the AWS production environment through their CI/CD pipeline.</p>\n\n<p>Which options will you combine to address this use case? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Create an IAM role for the CI/CD pipeline to be used for deploying application resources</strong></p>\n\n<p>The CI/CD pipeline role has broad access to the account to create resources. Access for deployment through the CI/CD pipeline should be tightly controlled and monitored. The CI/CD pipeline is allowed to create new IAM roles for use with the application, but those roles are limited to only the actions allowed by the permissions boundary.</p>\n\n<p><strong>The security team should create a permissions boundary policy and attach it to the IAM role used by the CI/CD pipeline</strong></p>\n\n<p>A permissions boundary is a type of identity-based policy that doesn’t directly grant access. Instead, like an SCP, a permissions boundary acts as a guardrail for your IAM principals that allows you to set coarse-grained access controls. A permissions boundary is typically used to delegate the creation of IAM principals. Delegation enables other individuals in your accounts to create new IAM principals but limits the permissions that can be granted to the new IAM principals.</p>\n\n<p>An example of the permissions boundary policy that the security team should attach to IAM roles created by the CI/CD pipeline is shown below. This same permissions boundary policy can be centrally managed and attached to IAM roles created by other pipelines at Financial Corp. The policy describes the maximum possible permissions that additional roles created by the development team are allowed to have, and it limits those permissions to some Amazon S3 and Amazon SQS data access actions. It’s common for a permissions boundary policy to include data access actions when used to delegate role creation. This is because most applications only need permission to read and write data and only sometimes need permission to modify infrastructure.</p>\n\n<p>The roles, policies, and EC2 instance profiles that the pipeline creates should also be restricted to specific role paths. This enables you to enforce that the pipeline can only modify roles and policies or pass roles that it has created. This helps prevent the pipeline, and roles created by the pipeline, from elevating privileges by modifying or passing a more privileged role.</p>\n\n<p>Example permissions boundary policy attached to IAM roles created by the CI/CD pipeline:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q49-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/">https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM role for the CI/CD pipeline to be used for deploying application resources Also, create resource-based policies for all the AWS resources created by the CI/CD pipeline</strong> - Access can be granted by either an identity-based policy or a resource-based policy when access is within the same AWS account. Therefore, using resource-based policies for the given use case is unnecessary. You should also note that all AWS resources do not support resource-based policies. This option acts as a distractor.</p>\n\n<p><strong>The development team should create a permissions boundary policy and attach it to all the IAM roles created by the CI/CD pipeline</strong> - Permission boundary policy should be created by the security team for central access and exercising control over the permissions configured by other teams.</p>\n\n<p><strong>Create a Service Control Policy (SCP) and attach it to all the member accounts to monitor and control the access privileges given to the IAM roles in the AWS accounts</strong> - Service control policies (SCPs) are a feature of AWS Organizations. AWS Organizations is a service for grouping and centrally managing the AWS accounts that your business owns. SCPs are policies that specify the maximum permissions for an organization, organizational unit (OU), or individual account. An SCP can limit permissions for principals in member accounts, including the AWS account root user. This option has been added as a distractor since the use case does not mention anything about using AWS Organizations.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/">https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/</a></p>\n',
        answers: [
          "<p>Create an IAM role for the CI/CD pipeline to be used for deploying application resources</p>",
          "<p>Create an IAM role for the CI/CD pipeline to be used for deploying application resources. Also, create resource-based policies for all the AWS resources created by the CI/CD pipeline</p>",
          "<p>The development team should create a permissions boundary policy and attach it to the IAM role used by the CI/CD pipeline</p>",
          "<p>The security team should create a permissions boundary policy and attach it to the IAM role used by the CI/CD pipeline</p>",
          "<p>Create a Service Control Policy (SCP) and attach it to all the member accounts to monitor and control the access privileges given to the IAM roles in the AWS accounts</p>",
        ],
      },
      correct_response: ["a", "d"],
      section: "Management and Security Governance",
      question_plain:
        "The development team at a company deploys to their AWS production environment through a continuous integration/continuous deployment (CI/CD) pipeline. The pipeline itself has broad access to create AWS resources needed to run the application. The company's security team wants to allow the development team to deploy their own IAM principals and policies for their application. However, the security team also needs a control mechanism that requires all resources created by the pipeline to have minimum privileges that comply with the security guidelines. All teams at the company are only allowed to modify the AWS production environment through their CI/CD pipeline.\n\nWhich options will you combine to address this use case? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960200,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has two VPCs (VPC1 and VPC2) configured in two different AWS Regions that are part of the same AWS account. There is an active VPC peering connection between the VPCs that has been configured in the route tables for both VPCs.</p>\n\n<p>The database is present in VPC1 and the access to the database instance is controlled through a security group defined in VPC1. VPC2 consists of an Auto Scaling group that scales in/out any Amazon EC2 instances based on the CPU usage. Each instance launched as part of the Auto Scaling group belongs to a security group defined specifically for the Auto Scaling group. The launched instances need seamless access to the database instance present in VPC1.</p>\n\n<p>Which additional step is needed for the solution to work if the route tables are already configured for VPC peering?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Add an inbound rule to the security group of the database instance in VPC1, with the source as the CIDR block of VPC2 (VPC for the instances launched by the Auto Scaling Group)</strong></p>\n\n<p>You cannot reference the security group of a peer VPC that\'s in a different AWS Region. Instead, use the CIDR block of the peer VPC.</p>\n\n<p>Rules to update your security groups to reference peer security groups:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q50-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html">https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an outbound rule on the security group of the instances launched in the Auto Scaling Group in VPC2, with the destination as the ID of the security group of the database instance</strong></p>\n\n<p><strong>Configure an outbound rule on the security group of the instances launched in the Auto Scaling Group in VPC2, with the destination as the CIDR block of VPC1 (VPC for the database instance)</strong></p>\n\n<p>By default, security groups contain outbound rules that allow all outbound traffic. So, both these options just act as distractors.</p>\n\n<p><strong>Add an inbound rule to the security group of the database instance in VPC1, with the source as the ID of the security group of the instances launched in the Auto Scaling Group in VPC2</strong> - You cannot reference the security group of a peer VPC that\'s in a different AWS Region. This option would be correct if both the VPCs belonged to the same AWS region.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html">https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/devicefarm/latest/developerguide/amazon-vpc-cross-region.html">https://docs.aws.amazon.com/devicefarm/latest/developerguide/amazon-vpc-cross-region.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html</a></p>\n',
        answers: [
          "<p>Add an inbound rule to the security group of the database instance in VPC1, with the source as the CIDR block of VPC2 (VPC for the instances launched by the Auto Scaling Group)</p>",
          "<p>Add an inbound rule to the security group of the database instance in VPC1, with the source as the ID of the security group of the instances launched in the Auto Scaling Group in VPC2</p>",
          "<p>Configure an outbound rule on the security group of the instances launched in the Auto Scaling Group in VPC2, with the destination as the ID of the security group of the database instance</p>",
          "<p>Configure an outbound rule on the security group of the instances launched in the Auto Scaling Group in VPC2, with the destination as the CIDR block of VPC1 (VPC for the database instance)</p>",
        ],
      },
      correct_response: ["a"],
      section: "Data Protection",
      question_plain:
        "A company has two VPCs (VPC1 and VPC2) configured in two different AWS Regions that are part of the same AWS account. There is an active VPC peering connection between the VPCs that has been configured in the route tables for both VPCs.\n\nThe database is present in VPC1 and the access to the database instance is controlled through a security group defined in VPC1. VPC2 consists of an Auto Scaling group that scales in/out any Amazon EC2 instances based on the CPU usage. Each instance launched as part of the Auto Scaling group belongs to a security group defined specifically for the Auto Scaling group. The launched instances need seamless access to the database instance present in VPC1.\n\nWhich additional step is needed for the solution to work if the route tables are already configured for VPC peering?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960202,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A hybrid AWS network is configured to route internet traffic such that it egresses from an on-premises gateway rather than from a VPC Internet Gateway (IGW). Since enabling Amazon GuardDuty, an error has been repeatedly seen in the GuardDuty findings: <code>UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS</code>. This finding informs you that a host outside of AWS has attempted to run AWS API operations using temporary AWS credentials that were created on an EC2 instance in your AWS environment. The listed EC2 instance might be compromised, and the temporary credentials from this instance might have been exfiltrated to a remote host outside of AWS.</p>\n\n<p>As a Security engineer, what steps would you take to address this issue, so that the VPC's internet traffic that egresses from an on-premises gateway does not trigger the given error? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Use suppression rules and create a rule that consists of two filter criteria. The first criterion is finding type, which should be <code>UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration</code></strong></p>\n\n<p><strong>The second filter criterion is <code>API caller IPv4 address</code> with the IP address or CIDR range of the on-premises internet gateway</strong></p>\n\n<p>A suppression rule is a set of criteria, consisting of a filter attribute paired with a value, used to filter findings by automatically archiving new findings that match the specified criteria. Suppression rules can be used to filter low-value findings, false positive findings, or threats you do not intend to act on, to make it easier to recognize the security threats with the most impact on your environment.</p>\n\n<p>After you create a suppression rule, new findings that match the criteria defined in the rule are automatically archived as long as the suppression rule is in place. You can use an existing filter to create a suppression rule or create a suppression rule from a new filter you define. You can configure suppression rules to suppress entire finding types or define more granular filter criteria to suppress only specific instances of a particular finding type. Your suppression rules can be edited at any time.</p>\n\n<p>GuardDuty continues to generate findings even when they match your suppression rules, however, those findings are automatically marked as archived. The archived finding is stored in GuardDuty for 90 days and can be viewed at any time during that period. You can view suppressed findings in the GuardDuty console by selecting Archived from the findings table, or through the GuardDuty API using the ListFindings API with a findingCriteria criterion of service.archived equal to true.</p>\n\n<p><code>UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS</code> finding informs you that a host outside of AWS has attempted to run AWS API operations using temporary AWS credentials that were created on an EC2 instance in your AWS environment. The listed EC2 instance might be compromised, and the temporary credentials from this instance might have been exfiltrated to a remote host outside of AWS.</p>\n\n<p>However, authorized users can export credentials from their EC2 instances to make legitimate API calls. To rule out a potential attack and verify the legitimacy of the activity, validate if the use of instance credentials from the remote IP in the finding is expected.</p>\n\n<p>Suppression rule for <code>UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS</code>:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q51-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/guardduty/latest/ug/findings_suppression-rule.html">https://docs.aws.amazon.com/guardduty/latest/ug/findings_suppression-rule.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use suppression rules and create a rule that consists of two filter criteria. The first criterion is finding type, which should be <code>UnauthorizedAccess:EC2/SSHBruteForce</code></strong> - <code>UnauthorizedAccess:EC2/SSHBruteForce</code> is incorrect as it is not relevant to the given use case.</p>\n\n<p><strong>Create a finding filter from the GuardDuty console for two different criteria. The first criterion is finding type, which should be <code>UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration</code></strong> - Creating a finding filter only narrows down the search from the findings created by GuardDuty. A suppression rule is required for this use case, so the GuardDuty findings are immediately archived and not sent for further analysis.</p>\n\n<p><strong>The second filter criterion is <code>Trusted IP list</code> to which you add the IP address or CIDR range of the on-premises internet gateway</strong> - Trusted IP lists prevent non-DNS findings from being generated from IPs you consider trusted. This is another important feature (like suppression rules) of GuardDuty to help sort, store, and manage GuardDuty findings. However, this option is not relevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-iam.html#unauthorizedaccess-iam-instancecredentialexfiltrationoutsideaws">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-iam.html#unauthorizedaccess-iam-instancecredentialexfiltrationoutsideaws</a></p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/findings_suppression-rule.html">https://docs.aws.amazon.com/guardduty/latest/ug/findings_suppression-rule.html</a></p>\n',
        answers: [
          "<p>Use suppression rules and create a rule that consists of two filter criteria. The first criterion is finding type, which should be <code>UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS</code></p>",
          "<p>Use suppression rules and create a rule that consists of two filter criteria. The first criterion is finding type, which should be <code>UnauthorizedAccess:EC2/SSHBruteForce</code></p>",
          "<p>Create a finding filter from the GuardDuty console for two different criteria. The first criterion is finding type, which should be <code>UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration</code></p>",
          "<p>The second filter criterion is <code>Trusted IP list</code> to which you add the IP address or CIDR range of the on-premises internet gateway</p>",
          "<p>The second filter criterion is <code>API caller IPv4 address</code> with the IP address or CIDR range of the on-premises internet gateway</p>",
        ],
      },
      correct_response: ["a", "e"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "A hybrid AWS network is configured to route internet traffic such that it egresses from an on-premises gateway rather than from a VPC Internet Gateway (IGW). Since enabling Amazon GuardDuty, an error has been repeatedly seen in the GuardDuty findings: UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS. This finding informs you that a host outside of AWS has attempted to run AWS API operations using temporary AWS credentials that were created on an EC2 instance in your AWS environment. The listed EC2 instance might be compromised, and the temporary credentials from this instance might have been exfiltrated to a remote host outside of AWS.\n\nAs a Security engineer, what steps would you take to address this issue, so that the VPC's internet traffic that egresses from an on-premises gateway does not trigger the given error? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960204,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The development team at a company accesses resources across all AWS Regions. The management wants only the security team to have access to resources from all AWS Regions. Any access to members of the development team needs to be restricted to the resources in a single AWS Region (<code>us-west-2</code>) except for the global AWS services. The development team is sized at 40 members, with all members being part of the <code>developers</code> IAM group. The company needs to implement this access restriction immediately.</p>\n\n<p>What is the optimal way to meet this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create an identity-based policy with the IAM <code>aws:RequestedRegion</code> condition key that denies access to all actions outside the specified Region, except for actions related to the global AWS services specified using <code>NotAction</code>. Attach the policy to the <code>developers</code> IAM group</strong></p>\n\n<p>You can create an identity-based policy that denies access to any actions outside the Regions specified using the <code>aws:RequestedRegion</code> condition key, except for actions in the services specified using <code>NotAction</code>. The policy uses the <code>NotAction</code> element with the Deny effect, which explicitly denies access to all of the actions not listed in the statement. Actions in the CloudFront, IAM, Route 53, and AWS Support services should not be denied because these are popular AWS global services with a single endpoint that is physically located in the us-east-1 Region. Because all requests to these services are made to the us-east-1 Region, the requests would be denied without the NotAction element.</p>\n\n<p>Example identity-based policy with <code>aws:RequestedRegion</code> condition key:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q52-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an identity-based policy with the IAM <code>aws:SourceIp</code> that denies access to all actions outside the specified IP address range of the given AWS Region, except for actions related to the global AWS services. Attach the policy to the <code>developers</code> IAM group</strong> - You can use <code>aws:SourceIp</code> to create an identity-based policy that denies access to all AWS actions in the account when the request comes from principals outside the specified IP range. Using sourceIp represents an unnecessarily complicated solution when you could directly reference the <code>aws:RequestedRegion</code> condition.</p>\n\n<p><strong>Create an identity-based policy that allows adding and removing the IAM tag with the tag key Region from IAM entities. Restrict the access using the defined tags. Attach the policy to the <code>developers</code> IAM group</strong> - You can create identity-based policies that allow adding and removing the IAM tag, restricting tag values, and restricting access using tags. However, for the given use case, leveraging an existing <code>aws:RequestedRegion</code> condition is the optimal way of implementing the requirement.</p>\n\n<p><strong>Create a Service control policy (SCP) that denies access to any operations outside of the specified AWS Regions. Apart from the Condition and Resource elements, configure the NotAction element to allow access to the needed AWS services only. Attach the policy to the <code>developers</code> IAM group</strong> - When AWS Organizations is defined for any organization, using SCP is an optimal way to restrict access based on AWS regions. Since the use case does not reference AWS Organizations, SCP is not the right choice.</p>\n\n<p>References:</p>\n\n<p><a href="https://repost.aws/knowledge-center/iam-restrict-access-policy">https://repost.aws/knowledge-center/iam-restrict-access-policy</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html</a></p>\n',
        answers: [
          "<p>Create an identity-based policy with the IAM <code>aws:RequestedRegion</code> condition key that denies access to all actions outside the specified Region, except for actions related to the global AWS services specified using <code>NotAction</code>. Attach the policy to the <code>developers</code> IAM group</p>",
          "<p>Create an identity-based policy with the IAM <code>aws:SourceIp</code> that denies access to all actions outside the specified IP address range of the given AWS Region, except for actions related to the global AWS services. Attach the policy to the <code>developers</code> IAM group</p>",
          "<p>Create a Service control policy (SCP) that denies access to any operations outside of the specified AWS Regions. Apart from the Condition and Resource elements, configure the NotAction element to allow access to the needed AWS services only. Attach the policy to the <code>developers</code> IAM group</p>",
          "<p>Create an identity-based policy that allows adding and removing the IAM tag with the tag key Region from IAM entities. Restrict the access using the defined tags. Attach the policy to the <code>developers</code> IAM group</p>",
        ],
      },
      correct_response: ["a"],
      section: "Data Protection",
      question_plain:
        "The development team at a company accesses resources across all AWS Regions. The management wants only the security team to have access to resources from all AWS Regions. Any access to members of the development team needs to be restricted to the resources in a single AWS Region (us-west-2) except for the global AWS services. The development team is sized at 40 members, with all members being part of the developers IAM group. The company needs to implement this access restriction immediately.\n\nWhat is the optimal way to meet this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960206,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A procurement application connects to Amazon API Gateway REST API for its core functionality needs. The development team at the company wants to restrict the access to allow only specific public IP address ranges of the company's selected vendor systems to access this public API Gateway REST API.</p>\n\n<p>What configuration is needed to restrict access to the API Gateway REST API?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create a resource policy for your REST API that denies access to any IP address that isn\'t specifically allowed. In the resource policy, for <code>aws:SourceIp</code>, give the value of the specific public IP address ranges that you want to grant access to</strong></p>\n\n<p>Create a resource policy for your REST API that denies access to any IP address that isn\'t specifically allowed.</p>\n\n<p>After the resource policy is attached to your REST API, users who call the API from specified IP addresses (allowed users) can access the API. Calls from any other IP address are denied access and receive an HTTP 403 Forbidden error.</p>\n\n<p>The <code>aws:SourceIp</code> condition value works only for public IP address ranges.</p>\n\n<p>Example resource policy that allows only specific IP addresses access to your API Gateway REST API:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q53-i1.jpg">\nvia - <a href="https://repost.aws/knowledge-center/api-gateway-resource-policy-access">https://repost.aws/knowledge-center/api-gateway-resource-policy-access</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a resource policy for your REST API that denies access to any IP address that isn\'t specifically allowed. In the resource policy, for the <code>aws:VpcSourceIp</code> value, enter the specific public IP address ranges that you want to grant access to</strong> - To allow access to private IP address ranges, use the condition value <code>aws:VpcSourceIp</code> and enter the private IP address of your HTTP client that\'s invoking your private API endpoint through the interface VPC endpoint. This configuration is not correct for public IPs.</p>\n\n<p><strong>Amazon API Gateway REST API does not support resource policies. Offer the REST API as HTTP API and create a resource policy for your HTTP API that denies access to any IP address that isn\'t specifically allowed. In the resource policy, for <code>aws:SourceIp</code>, give the value of the specific public IP addresses that you want to grant access to</strong> - This statement is incorrect and given only as a distractor. API Gateway HTTP APIs currently don\'t support resource policies.</p>\n\n<p><strong>Use a VPC endpoint policy along with an API Gateway resource policy. The resource policy is used to specify which principals can access the API. The endpoint policy specifies which private APIs can be called via the VPC endpoint</strong> - This configuration is used to improve the security of your private APIs. Since the REST API mentioned in the use case is a public API, hence this configuration is irrelevant.</p>\n\n<p>References:</p>\n\n<p><a href="https://repost.aws/knowledge-center/api-gateway-resource-policy-access">https://repost.aws/knowledge-center/api-gateway-resource-policy-access</a></p>\n\n<p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-vpc-endpoint-policies.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-vpc-endpoint-policies.html</a></p>\n',
        answers: [
          "<p>Create a resource policy for your REST API that denies access to any IP address that isn't specifically allowed. In the resource policy, for the <code>aws:VpcSourceIp</code> value, enter the specific public IP address ranges that you want to grant access to</p>",
          "<p>Amazon API Gateway REST API does not support resource policies. Offer the REST API as HTTP API and create a resource policy for your HTTP API that denies access to any IP address that isn't specifically allowed. In the resource policy, for <code>aws:SourceIp</code>, give the value of the specific public IP addresses that you want to grant access to</p>",
          "<p>Use a VPC endpoint policy along with an API Gateway resource policy. The resource policy is used to specify which principals can access the API. The endpoint policy specifies which private APIs can be called via the VPC endpoint</p>",
          "<p>Create a resource policy for your REST API that denies access to any IP address that isn't specifically allowed. In the resource policy, for <code>aws:SourceIp</code>, give the value of the specific public IP address ranges that you want to grant access to</p>",
        ],
      },
      correct_response: ["d"],
      section: "Data Protection",
      question_plain:
        "A procurement application connects to Amazon API Gateway REST API for its core functionality needs. The development team at the company wants to restrict the access to allow only specific public IP address ranges of the company's selected vendor systems to access this public API Gateway REST API.\n\nWhat configuration is needed to restrict access to the API Gateway REST API?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960208,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The security team at a retail company utilizes Amazon EventBridge to monitor Amazon S3 objects, aiming to detect public access and any other changes in S3 bucket policies/settings that result in public access. They configure EventBridge to watch specific CloudTrail API calls (s3:PutObjectAcl, s3:DeleteBucketPolicy, and s3:PutBucketPolicy) and use Amazon SNS for immediate email notifications.</p>\n\n<p>However, during development, the team finds that s3:PutObjectAcl doesn't trigger an EventBridge event, while the other two do. CloudTrail for AWS management events is enabled with a basic configuration in the relevant region, and EventBridge pattern verification is correct.</p>\n\n<p>The team needs a solution to ensure s3:PutObjectAcl triggers an EventBridge event without generating false notifications. What is the appropriate solution for this scenario?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Enable CloudTrail to monitor data events for read and write operations for S3 buckets</strong></p>\n\n<p>By default, CloudTrail logs S3 bucket-level API calls that were made in the last 90 days, but not log requests made to objects. Bucket-level calls include events such as CreateBucket, DeleteBucket, PutBucketLifecycle, PutBucketPolicy, DeleteBucketPolicy, and so on. You can see bucket-level events on the CloudTrail console. However, you can\'t view data events (Amazon S3 object-level calls) there—you must parse or query CloudTrail logs for them.</p>\n\n<p>You can also get CloudTrail logs for object-level Amazon S3 actions. To do this, enable data events for your S3 bucket or all buckets in your account. When an object-level action occurs in your account, CloudTrail evaluates your trail settings. If the event matches the object that you specified in a trail, the event is logged. This would take care of the missing PutObjectAcl API call-specific event in EventBridge.</p>\n\n<p>Therefore, for the given use case, you need to enable CloudTrail to monitor data events for read and write operations for S3 buckets.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the EventBridge event pattern by selecting Amazon S3. Select All Events as the event type</strong></p>\n\n<p><strong>Change the EventBridge event pattern by selecting Amazon S3. Select Data Events as the event type</strong></p>\n\n<p>Amazon S3 can send events to Amazon EventBridge whenever certain events happen in your bucket. Unlike other destinations, you don\'t need to select which event types you want to deliver. You can use EventBridge rules to route events to additional targets.</p>\n\n<p>Using EventBridge with Amazon S3:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q54-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html</a></p>\n\n<p>So, you can see that the relevant bucket-related events (s3:DeleteBucketPolicy, and s3:PutBucketPolicy) are not captured in Eventbridge. So both these options are incorrect.</p>\n\n<p><strong>Enable CloudTrail to monitor insights events for S3 buckets</strong> - CloudTrail Insights events capture unusual API call rate or error rate activity in your AWS account by analyzing CloudTrail management activity. Insights events provide relevant information, such as the associated API, error code, incident time, and statistics, that help you understand and act on unusual activity. This option acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/ev-mapping-troubleshooting.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ev-mapping-troubleshooting.html</a></p>\n',
        answers: [
          "<p>Change the EventBridge event pattern by selecting Amazon S3. Select All Events as the event type</p>",
          "<p>Change the EventBridge event pattern by selecting Amazon S3. Select Data Events as the event type</p>",
          "<p>Enable CloudTrail to monitor data events for read and write operations for S3 buckets</p>",
          "<p>Enable CloudTrail to monitor insights events for S3 buckets</p>",
        ],
      },
      correct_response: ["c"],
      section: "Security Logging and Monitoring",
      question_plain:
        "The security team at a retail company utilizes Amazon EventBridge to monitor Amazon S3 objects, aiming to detect public access and any other changes in S3 bucket policies/settings that result in public access. They configure EventBridge to watch specific CloudTrail API calls (s3:PutObjectAcl, s3:DeleteBucketPolicy, and s3:PutBucketPolicy) and use Amazon SNS for immediate email notifications.\n\nHowever, during development, the team finds that s3:PutObjectAcl doesn't trigger an EventBridge event, while the other two do. CloudTrail for AWS management events is enabled with a basic configuration in the relevant region, and EventBridge pattern verification is correct.\n\nThe team needs a solution to ensure s3:PutObjectAcl triggers an EventBridge event without generating false notifications. What is the appropriate solution for this scenario?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960210,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company maintains a robust security posture by use of AWS services like AWS Config, AWS Firewall Manager, Amazon GuardDuty, Amazon Inspector, Amazon Detective, and AWS Trusted Advisor. Earlier, the company was using a custom dashboard to aggregate information about its security footprint, the company has now decided to use AWS Security Hub to help assess its AWS environment vis-a-vis the security best practices.</p>\n\n<p>Which of the following statements are correct about Security Hub integration with other AWS services? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>AWS Firewall Manager sends findings to AWS Security Hub when AWS Shield Advanced is not protecting the resources</strong></p>\n\n<p>This statement is true. AWS Firewall Manager sends findings to AWS Security Hub when AWS Shield Advanced is not protecting resources, or when an attack is identified. AWS Firewall Manager also sends findings when a web application firewall (WAF) policy for resources or a web access control list (web ACL) rule is not in compliance.</p>\n\n<p>After you enable Amazon Security Hub, this integration is automatically activated. AWS Firewall Manager immediately begins to send findings to AWS Security Hub.</p>\n\n<p><strong>AWS Security Hub sends the Amazon GuardDuty findings to Amazon Detective to visualize and investigate the findings</strong></p>\n\n<p>Amazon Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to help you visualize and conduct faster and more efficient security investigations.</p>\n\n<p>AWS Security Hub integration with Amazon Detective allows you to pivot from Amazon GuardDuty findings in Security Hub into Amazon Detective. You can then use the Detective tools and visualizations to investigate them. The integration does not require any additional configuration in AWS Security Hub or Amazon Detective.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Trusted Advisor inspects your AWS environment and sends recommendations as findings to AWS Security Hub</strong> - Trusted Advisor draws upon best practices learned from serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment and then makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps. Security Hub sends the results of its AWS Foundational Security Best Practices checks to Trusted Advisor. In short, Trusted Advisor receives findings from Security Hub.</p>\n\n<p><strong>Amazon Detective automatically collects log data from the integrated AWS resources and uses machine learning and graph theory to conduct faster and more efficient security investigations. These investigations are sent to AWS Security Hub as findings in JSON format</strong> - Amazon Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to help you visualize and conduct faster and more efficient security investigations. Amazon Detective receives findings from AWS Security Hub, rather than sending the findings to Security Hub.</p>\n\n<p><strong>AWS Security Hub doesn\'t retroactively detect and consolidate security findings that were generated before you enabled Security Hub. However, as a global service, Security Hub can consolidate findings from all AWS regions to a single S3 bucket of your choice</strong> - Indeed, Security Hub doesn\'t retroactively detect and consolidate security findings that were generated before you enabled Security Hub. However, Security Hub is a regional service and not a global service.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-internal-providers.html">https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-internal-providers.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html">https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html</a></p>\n',
        answers: [
          "<p>AWS Trusted Advisor inspects your AWS environment and sends recommendations as findings to AWS Security Hub</p>",
          "<p>AWS Firewall Manager sends findings to Security Hub when AWS Shield Advanced is not protecting the resources</p>",
          "<p>AWS Security Hub sends the Amazon GuardDuty findings to Amazon Detective to visualize and investigate the findings</p>",
          "<p>Amazon Detective automatically collects log data from the integrated AWS resources and uses machine learning and graph theory to conduct faster and more efficient security investigations. These investigations are sent to AWS Security Hub as findings in JSON format</p>",
          "<p>AWS Security Hub doesn't retroactively detect and consolidate security findings that were generated before you enabled AWS Security Hub. However, as a global service, AWS Security Hub can consolidate findings from all AWS regions to a single S3 bucket of your choice</p>",
        ],
      },
      correct_response: ["b", "c"],
      section: "Management and Security Governance",
      question_plain:
        "A company maintains a robust security posture by use of AWS services like AWS Config, AWS Firewall Manager, Amazon GuardDuty, Amazon Inspector, Amazon Detective, and AWS Trusted Advisor. Earlier, the company was using a custom dashboard to aggregate information about its security footprint, the company has now decided to use AWS Security Hub to help assess its AWS environment vis-a-vis the security best practices.\n\nWhich of the following statements are correct about Security Hub integration with other AWS services? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960212,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company wants to allow its developers to create temporary environments to test their code using the latest Amazon Linux distribution. To control costs, the company wants the teams to create Amazon EC2 instances using only small instance types while also restricting the size of the attached EBS volumes. To comply with security requirements, the developers are expected to create only encrypted volumes and use a non-standard port for secure shell access to the instances.</p>\n\n<p>What is the most optimal way to proactively evaluate resource configurations in CloudFormation templates without writing custom code in Python or other languages?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use AWS CloudFormation Guard (cfn-guard), an open-source tool that helps you write compliance rules and validate the CloudFormation templates against those rules</strong></p>\n\n<p>CloudFormation Guard (cfn-guard) is an open-source tool that helps you write compliance rules using a simple, policy as code language. It will help you validate CloudFormation templates against those rules to keep your AWS resources in compliance with your company policy guidelines. You can use cfn-guard to evaluate templates locally as you write them and after you submit them to be deployed in your CI/CD pipelines.</p>\n\n<p>Although similar tools exist to create custom compliance rules, such as cfn-nag, cfripper, and checkov, cfn-guard uses a domain-specific language (DSL) to write rules. Learning the rule language is easier than learning a programming language like Python or Ruby, which is required to make custom rules in similar tools. Because cfn-guard is written in Rust, it can be compiled to a native binary to evaluate thousands of rules across templates.</p>\n\n<p>CloudFormation Guard Rule Format:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q56-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/mt/write-preventive-compliance-rules-for-aws-cloudformation-templates-the-cfn-guard-way/">https://aws.amazon.com/blogs/mt/write-preventive-compliance-rules-for-aws-cloudformation-templates-the-cfn-guard-way/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS CloudFormation Linter (cfn-lint), an open-source tool that you can use to perform detailed validation on your AWS CloudFormation templates</strong> - AWS CloudFormation Guard doesn\'t validate CloudFormation templates for valid syntax or allowed property values. You use the cfn-lint tool to perform a thorough inspection of the template structure. Validating syntax is necessary but not sufficient for the given requirement, hence this is an incorrect option.</p>\n\n<p><strong>Use AWS CloudFormation Drift Detection to understand the difference between the expected configuration values of stack resources defined in CloudFormation templates and the actual configuration values of these resources in the corresponding CloudFormation stacks</strong> - Drift is the difference between the expected configuration values of stack resources defined in CloudFormation templates and the actual configuration values of these resources in the corresponding CloudFormation stacks. This allows you to better manage your CloudFormation stacks and ensure consistency in your resource configurations.</p>\n\n<p><strong>Use AWS Cloud Development Kit (AWS CDK), an open-source tool that helps you write compliance rules and validate the CloudFormation templates against those rules</strong> - AWS Cloud Development Kit (AWS CDK) is a framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/write-preventive-compliance-rules-for-aws-cloudformation-templates-the-cfn-guard-way/">https://aws.amazon.com/blogs/mt/write-preventive-compliance-rules-for-aws-cloudformation-templates-the-cfn-guard-way/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/validate-cfn-lint.html">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/validate-cfn-lint.html</a></p>\n',
        answers: [
          "<p>Use AWS CloudFormation Drift Detection to understand the difference between the expected configuration values of stack resources defined in CloudFormation templates and the actual configuration values of these resources in the corresponding CloudFormation stacks</p>",
          "<p>Use AWS Cloud Development Kit (AWS CDK), an open-source tool that helps you write compliance rules and validate the CloudFormation templates against those rules</p>",
          "<p>Use AWS CloudFormation Guard (cfn-guard), an open-source tool that helps you write compliance rules and validate the CloudFormation templates against those rules</p>",
          "<p>Use AWS CloudFormation Linter (cfn-lint), an open-source tool that you can use to perform detailed validation on your AWS CloudFormation templates</p>",
        ],
      },
      correct_response: ["c"],
      section: "Management and Security Governance",
      question_plain:
        "A company wants to allow its developers to create temporary environments to test their code using the latest Amazon Linux distribution. To control costs, the company wants the teams to create Amazon EC2 instances using only small instance types while also restricting the size of the attached EBS volumes. To comply with security requirements, the developers are expected to create only encrypted volumes and use a non-standard port for secure shell access to the instances.\n\nWhat is the most optimal way to proactively evaluate resource configurations in CloudFormation templates without writing custom code in Python or other languages?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960214,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company uses AWS CloudFormation templates to provision all of its AWS infrastructure resources. One such CloudFormation template needs to provide the username and password as credentials to the newly created Amazon Redshift database.</p>\n\n<p>Which is the optimal way to configure the database credentials during the stack creation without compromising these credentials?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>First create a secret with a password generated by Secrets Manager. Then use a dynamic reference in the CloudFormation template to retrieve the username and password from the secret to use as credentials for the new database created</strong></p>\n\n<p>With AWS CloudFormation, you can retrieve a secret to use in another AWS CloudFormation resource. A common scenario is to first create a secret with a password generated by Secrets Manager, and then retrieve the username and password from the secret to use as credentials for a new database. To retrieve a secret in an AWS CloudFormation template, you use a dynamic reference. When you create the stack, the dynamic reference pulls the secret value into the AWS CloudFormation resource, so you don\'t have to hardcode the secret information. Instead, you refer to the secret by name or ARN. You can use a dynamic reference for a secret in any resource property.</p>\n\n<p>For Amazon Redshift and Amazon DocumentDB credentials, first create a secret with a password generated by Secrets Manager, and then use a dynamic reference to retrieve the username and password from the secret to use as credentials for a new database. Next, use the <code>AWS::SecretsManager::SecretTargetAttachment</code> resource to add details about the database to the secret that Secrets Manager needs to rotate the secret. Finally, to turn on automatic rotation, use the AWS::SecretsManager::RotationSchedule resource and provide a rotation function and a schedule.</p>\n\n<p>Example of a dynamic reference for a secret:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q57-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/cfn-example_reference-secret.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/cfn-example_reference-secret.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>First create a secret with a password generated by Secrets Manager. Use a dynamic reference for the secret in resource metadata such as <code>AWS::CloudFormation::Init</code> for the new database created</strong> - You can\'t use a dynamic reference for a secret in resource metadata such as AWS::CloudFormation::Init because that would make the secret value visible in the console.</p>\n\n<p><strong>Create a separate CloudFormation template for the database resources. The database template should be created with the credentials of the database encrypted into the template. Use this template to create database resources</strong> - This is a made-up option given only as a distractor.</p>\n\n<p><strong>First create a secret with a password generated by Secrets Manager. Then use a dynamic reference in the CloudFormation template using a backslash as the final value. The backslash will be replaced with the credentials during the new database creation</strong> - You should not create a dynamic reference using a backslash as the final value. AWS CloudFormation can\'t resolve those references, which causes a resource failure.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/cfn-example_reference-secret.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/cfn-example_reference-secret.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/cloudformation.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/cloudformation.html</a></p>\n',
        answers: [
          "<p>Create a separate CloudFormation template for the database resources. The database template should be created with the credentials of the database encrypted into the template. Use this template to create database resources</p>",
          "<p>First create a secret with a password generated by Secrets Manager. Use a dynamic reference for the secret in resource metadata such as <code>AWS::CloudFormation::Init</code> for the new database created</p>",
          "<p>First create a secret with a password generated by Secrets Manager. Then use a dynamic reference in the CloudFormation template using a backslash as the final value. The backslash will be replaced with the credentials during the new database creation</p>",
          "<p>First create a secret with a password generated by Secrets Manager. Then use a dynamic reference in the CloudFormation template to retrieve the username and password from the secret to use as credentials for the new database created</p>",
        ],
      },
      correct_response: ["d"],
      section: "Data Protection",
      question_plain:
        "A company uses AWS CloudFormation templates to provision all of its AWS infrastructure resources. One such CloudFormation template needs to provide the username and password as credentials to the newly created Amazon Redshift database.\n\nWhich is the optimal way to configure the database credentials during the stack creation without compromising these credentials?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960216,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An e-commerce company's security team needs to receive a notification whenever an AWS access key has not been rotated in 30 or more days. You have been hired as an AWS Certified Security Specialist to develop a solution that provides these notifications automatically.</p>\n\n<p>Which solution will you recommend to address these requirements with the LEAST effort?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Enable the AWS Config <code>access-keys-rotated</code> managed rule and configure the <code>maxAccessKeyAge</code> parameter to 30 days. Have AWS Config apply remediation using the AWS Systems Manager Automation document for every non-compliant resource. The Automation document, in turn, publishes a customized message to an SNS topic</strong></p>\n\n<p>AWS Config provides AWS-managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. You can customize the behavior of a managed rule to suit your needs. For example, you can define the rule\'s scope to constrain which resources trigger an evaluation for the rule, such as EC2 instances or volumes. You can customize the rule\'s parameters to define attributes that your resources must have to comply with the rule. For example, you can customize a parameter to specify that your security group should block incoming traffic to a specific port number.</p>\n\n<p>The <code>access-keys-rotated</code> managed rule checks if active IAM access keys are rotated (changed) within the number of days specified in <code>maxAccessKeyAge</code>. The rule is NON_COMPLIANT if access keys are not rotated within the specified time period. The default value is 90 days. For the given use case, you need to set the <code>maxAccessKeyAge</code> value to 30.</p>\n\n<p>For the given use case, the optimal solution is to use AWS Config’s automatic remediation feature. The remediation is declared in an AWS Systems Manager automation document, which is invoked by AWS Config when a resource is found to be non-compliant. As its name implies, these steps are meant to remediate non-compliant resources, but in our case, we use them as part of a notification system.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the AWS Config <code>access-keys-rotated</code> managed rule and configure the <code>maxAccessKeyAge</code> parameter to 30 days. Have AWS Config apply remediation using an AWS Lambda function for every non-compliant resource. The Lambda function, in turn, publishes a customized message to an SNS topic</strong> - AWS Config cannot apply remediation using an AWS Lambda function, so this option is incorrect.</p>\n\n<p><strong>Enable the AWS Config <code>access-keys-rotated</code> managed rule and configure the <code>maxAccessKeyAge</code> parameter to 30 days. Create an Amazon EventBridge rule that runs on a daily schedule to trigger an AWS Lambda function that executes the AWS Config managed rule. Publish a customized message to an SNS topic when the Lambda function detects non-compliance</strong> - There is no need to configure an AWS Lambda function to execute an AWS Config managed rule, as AWS Config itself automatically executes the managed rules. So this option is incorrect.</p>\n\n<p><strong>Configure AWS Trusted Advisor to apply remediation using an AWS Lambda function for every non-compliant AWS access key. The Lambda function, in turn, publishes a customized message to an SNS topic</strong> - You cannot use AWS Trusted Advisor to apply remediation using an AWS Lambda function for any non-compliant AWS access key for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/managing-aged-access-keys-through-aws-config-remediations/">https://aws.amazon.com/blogs/mt/managing-aged-access-keys-through-aws-config-remediations/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/access-keys-rotated.html">https://docs.aws.amazon.com/config/latest/developerguide/access-keys-rotated.html</a></p>\n',
        answers: [
          "<p>Enable the AWS Config <code>access-keys-rotated</code> managed rule and configure the <code>maxAccessKeyAge</code> parameter to 30 days. Have AWS Config apply remediation using an AWS Lambda function for every non-compliant resource. The Lambda function, in turn, publishes a customized message to an SNS topic</p>",
          "<p>Enable the AWS Config <code>access-keys-rotated</code> managed rule and configure the <code>maxAccessKeyAge</code> parameter to 30 days. Create an Amazon EventBridge rule that runs on a daily schedule to trigger an AWS Lambda function that evaluates the AWS Config managed rule. Publish a customized message to an SNS topic when the Lambda function detects non-compliance</p>",
          "<p>Configure AWS Trusted Advisor to apply remediation using AWS Lambda function for every non-compliant AWS access key. The Lambda function, in turn, publishes a customized message to an SNS topic</p>",
          "<p>Enable the AWS Config <code>access-keys-rotated</code> managed rule and configure the <code>maxAccessKeyAge</code> parameter to 30 days. Have AWS Config apply remediation using AWS Systems Manager Automation document for every non-compliant resource. The Automation document, in turn, publishes a customized message to an SNS topic</p>",
        ],
      },
      correct_response: ["d"],
      section: "Security Logging and Monitoring",
      question_plain:
        "An e-commerce company's security team needs to receive a notification whenever an AWS access key has not been rotated in 30 or more days. You have been hired as an AWS Certified Security Specialist to develop a solution that provides these notifications automatically.\n\nWhich solution will you recommend to address these requirements with the LEAST effort?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960218,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The security team at a company needs to follow the security requirements:</p>\n\n<ul>\n<li>Monitor all traffic leaving a particular VPC</li>\n<li>Monitor all traffic whose source is outside of the VPC</li>\n</ul>\n\n<p>The purpose of this traffic monitoring is to put in place a proper content inspection, troubleshooting, and threat monitoring solution.</p>\n\n<p>Which of the following options represents the best solution for the given requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Configure a traffic mirror target for the monitoring appliance. Create a traffic mirror filter with a rule for outbound traffic to reject all packets that have a destination IP in the VPC CIDR block and accept all other outbound packets. Also, create another rule for inbound traffic to reject all packets that have a source IP in the VPC CIDR block and accept all other inbound packets</strong></p>\n\n<p>Consider the scenario where you want to monitor traffic leaving your VPC or traffic whose source is outside your VPC. In this case, you will mirror all traffic except traffic passing within your VPC and send it to a single monitoring appliance. You need the following traffic mirror resources:</p>\n\n<ol>\n<li><p>A traffic mirror target for the appliance (Target A)</p></li>\n<li><p>A traffic mirror filter that has two sets of rules for outbound and inbound traffic. For outbound traffic, it will reject all packets that have a destination IP in the VPC CIDR block and accept all other outbound packets. For inbound traffic, it will reject all packets that have a source IP in the VPC CIDR block and accept all other inbound packets.</p></li>\n<li><p>A traffic mirror session that has the following:</p>\n\n<ol>\n<li><p>A traffic mirror source</p></li>\n<li><p>A traffic mirror target for the appliance (Target A)</p></li>\n<li><p>A traffic mirror filter with a traffic mirror rule for the TCP inbound traffic (Filter F)</p></li>\n</ol></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces in the VPC. Publish the flow log data directly to Amazon CloudWatch for further analysis and alert generation</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Whereas, traffic mirroring is a feature used to copy network traffic from a network interface of an Amazon EC2 instance and send it to out-of-band security and monitoring appliances for deep packet inspection. You can detect network and security anomalies, gain operational insights, implement compliance and security controls, and troubleshoot issues.</p>\n\n<p><strong>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces in the VPC. Configure a traffic mirror with VPC Flow Logs as the source and the target as the appliance. Create a traffic mirroring filter with a traffic mirroring rule for the TCP inbound traffic</strong> - This is not possible and is given only as a distractor.</p>\n\n<p><strong>Configure a traffic mirror target for the monitoring appliance. Create a traffic mirror filter with a traffic mirror rule for the TCP inbound traffic. Also, create another traffic mirror filter with a traffic mirror rule for the UDP inbound traffic</strong> - This configuration is used if you want to mirror inbound TCP and UDP traffic on an instance.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/monitoring.html">https://docs.aws.amazon.com/vpc/latest/userguide/monitoring.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/mirroring/tm-example-non-vpc.html">https://docs.aws.amazon.com/vpc/latest/mirroring/tm-example-non-vpc.html</a></p>\n',
        answers: [
          "<p>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces in the VPC. Configure a traffic mirror with VPC Flow Logs as the source and the target as the appliance. Create a traffic mirroring filter with a traffic mirroring rule for the TCP inbound traffic</p>",
          "<p>Configure a traffic mirror target for the monitoring appliance. Create a traffic mirror filter with a traffic mirror rule for the TCP inbound traffic. Also, create another traffic mirror filter with a traffic mirror rule for the UDP inbound traffic</p>",
          "<p>Configure a traffic mirror target for the monitoring appliance. Create a traffic mirror filter with a rule for outbound traffic to reject all packets that have a destination IP in the VPC CIDR block and accept all other outbound packets. Also, create another rule for inbound traffic to reject all packets that have a source IP in the VPC CIDR block and accept all other inbound packets</p>",
          "<p>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces in the VPC. Publish the flow log data directly to Amazon CloudWatch for further analysis and alert generation</p>",
        ],
      },
      correct_response: ["c"],
      section: "Security Logging and Monitoring",
      question_plain:
        "The security team at a company needs to follow the security requirements:\n\n\nMonitor all traffic leaving a particular VPC\nMonitor all traffic whose source is outside of the VPC\n\n\nThe purpose of this traffic monitoring is to put in place a proper content inspection, troubleshooting, and threat monitoring solution.\n\nWhich of the following options represents the best solution for the given requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960220,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An AWS service present in AWS Account 1 is exposed to AWS Account 2 using VPC private link. The Network Load Balancer (NLB) in Account 1 is configured and has accepted the connection. While data is seen leaving from the NLB, the client side is not getting the transmitted data.</p>\n\n<p>What steps should be undertaken to troubleshoot this issue?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Ensure that the Security Groups and Network Access Control Lists (NACLs) in both VPCs allow traffic</strong></p>\n\n<p>You can create an interface VPC endpoint to connect to services powered by AWS PrivateLink, including many AWS services. For each subnet that you specify from your VPC, AWS creates an endpoint network interface in the subnet and assigns it a private IP address from the subnet address range. An endpoint network interface is a requester-managed network interface; you can view it in your AWS account, but you can\'t manage it yourself.</p>\n\n<p>An improper configuration of the involved security groups or the Network Access Control Lists (NACLs) can result in communication not getting established as expected. This is the right point to start the troubleshooting process. VPC flow logs are not available for this configuration because traffic between an endpoint network interface and a Network Load Balancer network interface isn\'t logged.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces to the NLB</strong> - Flow logs do not capture all IP traffic. One such exception is - Traffic between an endpoint network interface and a Network Load Balancer network interface.</p>\n\n<p><strong>Configure Gateway Endpoint instead of VPC private link to access the AWS service across AWS accounts</strong> - Gateway VPC endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. This is irrelevant to the given use case.</p>\n\n<p><strong>Use AWS CloudTrail to capture detailed information about the calls made to the Amazon VPC API. You can use the generated CloudTrail logs to determine which calls were made and the source IP address where the call came from</strong> - AWS CloudTrail will log all data about calls made to the Amazon VPC API, and not about the network traffic data.</p>\n\n<p>References:</p>\n\n<p><a href="https://repost.aws/questions/QUKYHCVsB3Swmf9mO6cp-6TQ/capture-flow-logs-for-vpc-privatelink">https://repost.aws/questions/QUKYHCVsB3Swmf9mO6cp-6TQ/capture-flow-logs-for-vpc-privatelink</a></p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p>\n',
        answers: [
          "<p>Ensure that the Security Groups and Network Access Control Lists (NACLs) in both VPCs allow traffic</p>",
          "<p>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces to the NLB</p>",
          "<p>Configure Gateway VPC Endpoint instead of VPC private link to access the AWS service across AWS accounts</p>",
          "<p>Use AWS CloudTrail to capture detailed information about the calls made to the Amazon VPC API. You can use the generated CloudTrail logs to determine which calls were made and the source IP address where the call came from</p>",
        ],
      },
      correct_response: ["a"],
      section: "Infrastructure Security",
      question_plain:
        "An AWS service present in AWS Account 1 is exposed to AWS Account 2 using VPC private link. The Network Load Balancer (NLB) in Account 1 is configured and has accepted the connection. While data is seen leaving from the NLB, the client side is not getting the transmitted data.\n\nWhat steps should be undertaken to troubleshoot this issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960222,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>As a Security Specialist, you have received an alert from a log analyzer about the suspicious increase in traffic to your application’s login page probably indicating a potential brute force or credential-stuffing attack against the application. The application under attack is configured behind a Web Application Firewall (WAF) using Amazon CloudFront and Amazon S3.</p>\n\n<p>What should be the initial response to this probable attack?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create a URI-specific rate-based WAF rule to prevent a single source IP address from connecting to the login page more than a defined threshold number of times, over a given period</strong></p>\n\n<p>A blanket rate-based rule is designed to prevent any single source IP address from negatively impacting the availability of a website. For example, if the threshold for the rate-based rule is set to 2,000, the rule will block all IPs that are making more than 2,000 requests in a rolling 5-minute period.</p>\n\n<p>A URI-specific rule can prevent a single source IP address from connecting to the login page as few as 100 times per 5-minute period, while still allowing a much higher request volume to the rest of the application.</p>\n\n<p>Some applications naturally have computationally expensive URIs that, when called, require considerably more resources to process the request. An example of this could be a database query or search function. If a bad actor targets these computationally expensive URIs, this can quickly lead to application performance or availability issues. If you assign a URI-specific rate-based rule to these portions of your site, you can configure a much lower threshold than the blanket rate-based rule.</p>\n\n<p>While URI-specific rate-based WAF rule should be the initial response to the probable attack, it is not sufficient to strengthen the security posture completely.</p>\n\n<p>It’s important to note that the more specific AWS WAF rules should have a higher priority because you want these rules to limit the request volume first. In our example, the rules strategy is first based on a specific URI, and then on a blanket rule that limits requests across the whole application. The rate-based rules provide a solid foundation to help you protect your internet-facing applications from common basic HTTP request floods. However, the WAF rate-based rule solution shouldn’t be seen as a one-time setup but rather as an iterative activity.</p>\n\n<p>The example below showcases a workflow to collect and query logs and apply rate-based rules automatically:</p>\n\n<p>Implementing rate-based rules:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q61-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/three-most-important-aws-waf-rate-based-rules/">https://aws.amazon.com/blogs/security/three-most-important-aws-waf-rate-based-rules/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Blanket rate-based WAF rule to prevent a single source IP address from connecting to the application more than a defined threshold number of times, over a given period</strong> - A blanket rate-based rule is designed to prevent any single source IP address from negatively impacting the availability of a website. For example, if the threshold for the rate-based rule is set to 2,000, the rule will block all IPs that are making more than 2,000 requests in a rolling 5-minute period. This is the most basic rate-based rule and one of the most valuable for AWS WAF customers to implement. The URI-specific rate-based WAF rule is better suited for this use case since the logs show a clear change of activity on a specific login page than on the entire website/application.</p>\n\n<p><strong>Create an IP reputation rate-based WAF rule and completely block all web requests from the four open-source threat intelligence lists that AWS WAF Security Automations solution provides</strong> - The AWS WAF Security Automations solution provides AWS WAF customers with a subscription to four open-source threat intelligence lists. Rate-based rules with low thresholds can be applied to requests coming from these suspect sources. Some customers feel comfortable completely blocking web requests from these IPs, but at the very least, requests from these IPs should be rate-limited to protect the application from these well-known malicious sources.</p>\n\n<p>This configuration should be a proactive activity rather than a reactive one taken during an emergency.</p>\n\n<p><strong>Create a TCP-packet rate-based WAF rule to prevent a single source IP address from connecting to the login page more than a defined threshold number of times, over a given period</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/three-most-important-aws-waf-rate-based-rules/">https://aws.amazon.com/blogs/security/three-most-important-aws-waf-rate-based-rules/</a></p>\n',
        answers: [
          "<p>Create a Blanket rate-based WAF rule to prevent a single source IP address from connecting to the application more than a defined threshold number of times, over a given period</p>",
          "<p>Create a TCP-packet rate-based WAF rule to prevent a single source IP address from connecting to the login page more than a defined threshold number of times, over a given period</p>",
          "<p>Create an IP reputation rate-based WAF rule and completely block all web requests from the four open-source threat intelligence lists that AWS WAF Security Automations solution provides</p>",
          "<p>Create a URI-specific rate-based WAF rule to prevent a single source IP address from connecting to the login page more than a defined threshold number of times, over a given period</p>",
        ],
      },
      correct_response: ["d"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "As a Security Specialist, you have received an alert from a log analyzer about the suspicious increase in traffic to your application’s login page probably indicating a potential brute force or credential-stuffing attack against the application. The application under attack is configured behind a Web Application Firewall (WAF) using Amazon CloudFront and Amazon S3.\n\nWhat should be the initial response to this probable attack?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960224,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>For a threat alert raised by the security team, a company needs content inspection of the traffic passing through an Amazon Route 53 resolver outbound endpoint.</p>\n\n<p>As A Security Specialist, how will you implement a solution for this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p>Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of type <code>interface</code>. You can then send the traffic to out-of-band security and monitoring appliances for content inspection, threat monitoring and troubleshooting.</p>\n\n<p>The security and monitoring appliances can be deployed as individual instances, or as a fleet of instances behind either a Network Load Balancer or a Gateway Load Balancer with a UDP listener. Traffic Mirroring supports filters and packet truncation so that you can extract only the traffic of interest, using the monitoring tools of your choice.</p>\n\n<p>How Traffic Mirroring works:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q62-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html">https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable VPC Flow Logs to capture all network traffic information passing through Route 53 resolver endpoints. Use the Athena integration feature in the Amazon VPC Console to create Athena tables for direct querying</strong> - VPC Flow Logs capture metadata about the traffic, not the actual traffic itself. VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC, whereas, traffic mirroring streams a copy of the network traffic say to an EC2 or Appliance for packet inspection.</p>\n\n<p><strong>Turn on Route 53 public query logging in each public-hosted zone. Amazon Route 53 publishes the logs to Amazon CloudWatch Logs for further analysis and troubleshooting</strong> - Once you configure Amazon Route 53 to log information about the public DNS queries that Route 53 receives, the following information is captured: Domain or subdomain that was requested, Date and time of the request, DNS record type, Route 53 edge location that responded to the DNS query, DNS response code. This option is irrelevant to the given use case.</p>\n\n<p><strong>Create a trail in AWS CloudTrail for continuous tracking of all Route 53 events. Deliver the log files to an Amazon S3 bucket and use Athena to query the log data for user patterns and troubleshooting</strong> - Route 53 is integrated with AWS CloudTrail that captures all API calls for Route 53 as events, including calls from the Route 53 console and code calls to the Route 53 APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Route 53.</p>\n\n<p>While the logs above are extremely useful in troubleshooting and monitoring the security infrastructure of AWS architecture, content inspection of a packet can only be done through Traffic Mirroring.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html">https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html</a></p>\n\n<p><a href="https://repost.aws/knowledge-center/route53-view-endpoint-traffic">https://repost.aws/knowledge-center/route53-view-endpoint-traffic</a></p>\n',
        answers: [
          "<p>Enable VPC Flow Logs to capture all network traffic information passing through Route 53 resolver endpoints. Use the Athena integration feature in the Amazon VPC Console to create Athena tables for direct querying</p>",
          "<p>To view traffic passing through Route 53 resolver endpoints, configure Amazon Virtual Private Cloud (Amazon VPC) Traffic Mirroring</p>",
          "<p>Turn on Route 53 public query logging in each public-hosted zone. Amazon Route 53 publishes the logs to Amazon CloudWatch Logs for further analysis and troubleshooting</p>",
          "<p>Create a trail in AWS CloudTrail for continuous tracking of all Route 53 events. Deliver the log files to an Amazon S3 bucket and use Athena to query the log data for user patterns and troubleshooting</p>",
        ],
      },
      correct_response: ["b"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "For a threat alert raised by the security team, a company needs content inspection of the traffic passing through an Amazon Route 53 resolver outbound endpoint.\n\nAs A Security Specialist, how will you implement a solution for this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960226,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company exposes most of its business functions as container applications and utilizes Amazon Elastic Container Registry (Amazon ECR) service for managing the container images. To strengthen the security backbone of its AWS architecture, the company is looking for a solution that provides automatic scanning of operating systems and programming language package vulnerabilities. All the images pushed to Amazon ECR should be continuously scanned and the updates of the scan should be notified to specified teams.</p>\n\n<p>Which solution is the right fit for this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Turn on enhanced scanning for your Amazon ECR registry. By default, the duration of the scans is set to <code>Lifetime</code>. When enhanced scanning is turned on, Amazon ECR sends scan events to EventBridge which can be configured for further notifications to specified teams</strong></p>\n\n<p>Amazon ECR enhanced scanning is an integration with Amazon Inspector which provides vulnerability scanning for your container images. Your container images are scanned for both operating systems and programming language package vulnerabilities. You can view the scan findings with both Amazon ECR and Amazon Inspector directly.</p>\n\n<p>Amazon Inspector supports configuring the duration that your private repositories are continuously monitored for. By default, when enhanced scanning is turned on for your Amazon ECR private registry, the Amazon Inspector service continually monitors your repositories until either the image is deleted or enhanced scanning is disabled. The duration that Amazon Inspector scans your images can be changed using the Amazon Inspector settings. The available scan durations are Lifetime (default), 180 days, and 30 days. When the scan duration for a repository elapses, the scan status of SCAN_ELIGIBILITY_EXPIRED is displayed when listing your scan vulnerabilities.</p>\n\n<p>If you want to know more about ECR-enhanced scanning, some key considerations are given below:</p>\n\n<p>Considerations for enhanced scanning:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q63-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on basic scanning for your Amazon ECR registry. Configure the repository to scan on push, so every new image pushed is scanned immediately when uploaded to the registry. Amazon ECR is integrated with Amazon EventBridge and sends events to EventBridge about image scan updates. Configure EventBridge to send notifications to specified teams</strong> - Amazon ECR provides a basic scanning type that uses the Common Vulnerabilities and Exposures (CVEs) database from the open-source Clair project. Programing language package vulnerabilities are not scanned in basic type.</p>\n\n<p><strong>Turn on Amazon GuardDuty advanced findings to register for vulnerability checks of the Amazon ECR registry. GuardDuty\'s integration with AWS Security Hub can be used to send notifications to the concerned teams when findings are raised in GuardDuty against the ECR registry</strong> - Amazon GuardDuty is a security monitoring service that analyzes and processes Foundational data sources, such as AWS CloudTrail management events, AWS CloudTrail event logs, VPC flow logs (from Amazon EC2 instances), and DNS logs. This option is given only as a distractor. GuardDuty has no provision for checking against the Amazon ECR registry for vulnerabilities.</p>\n\n<p><strong>Turn on AWS Trusted Advisor checks for the Amazon ECR registry. When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your results to CloudWatch. Create CloudWatch alarms to be notified of these changes</strong> - This option is given only as a distractor. AWS Trusted Advisor has no checks for the Amazon ECR registry.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html</a></p>\n',
        answers: [
          "<p>Turn on enhanced scanning for your Amazon ECR registry. By default, the duration of the scans is set to <code>Lifetime</code>. When enhanced scanning is turned on, Amazon ECR sends scan events to EventBridge which can be configured for further notifications to specified teams</p>",
          "<p>Turn on basic scanning for your Amazon ECR registry. Configure the repository to scan on push, so every new image pushed is scanned immediately when uploaded to the registry. Amazon ECR is integrated with Amazon EventBridge and sends events to EventBridge about image scan updates. Configure EventBridge to send notifications to specified teams</p>",
          "<p>Turn on Amazon GuardDuty advanced findings to register for vulnerability checks of Amazon ECR registry. GuardDuty's integration with AWS Security Hub can be used to send notifications to the concerned teams when findings are raised in GuardDuty against the ECR registry</p>",
          "<p>Turn on AWS Trusted Advisor checks for Amazon ECR registry. When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your results to CloudWatch. Create CloudWatch alarms to be notified of these changes</p>",
        ],
      },
      correct_response: ["a"],
      section: "Management and Security Governance",
      question_plain:
        "A company exposes most of its business functions as container applications and utilizes Amazon Elastic Container Registry (Amazon ECR) service for managing the container images. To strengthen the security backbone of its AWS architecture, the company is looking for a solution that provides automatic scanning of operating systems and programming language package vulnerabilities. All the images pushed to Amazon ECR should be continuously scanned and the updates of the scan should be notified to specified teams.\n\nWhich solution is the right fit for this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960228,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company has decided to enable AWS Security Hub to help assess its growing AWS environment against security industry standards and best practices.</p>\n\n<p>Which of the following represents true statements for the AWS Security Hub service? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>AWS Config must be enabled as a pre-requisite for using Security Hub</strong></p>\n\n<p>Security Hub is a security and compliance service that provides security and compliance posture management, as a service. It uses AWS Config and AWS Config rules as its primary mechanism to evaluate the configuration of AWS resources. Hence, AWS Config is mandatory while configuring Security Hub.</p>\n\n<p><strong>When you enable both GuardDuty and Security Hub, the mutual integration is enabled automatically. GuardDuty immediately begins to send findings to Security Hub</strong></p>\n\n<p>This statement is true - when you enable both GuardDuty and Security Hub, the integration is enabled automatically. GuardDuty immediately begins to send findings to AWS Security Hub. The Amazon GuardDuty integration with Security Hub enables you to send findings from GuardDuty to Security Hub. Security Hub can then include those findings in its analysis of your security posture.</p>\n\n<p>Once the integration is enabled, GuardDuty sends all of the findings it generates to AWS Security Hub. The findings are sent to AWS Security Hub using the AWS Security Finding Format (ASFF).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon GuardDuty must be enabled as a pre-requisite for using Security Hub. GuardDuty immediately begins to send findings to Security Hub</strong> - This statement is incorrect. GuardDuty is not a mandatory service for AWS Security Hub.</p>\n\n<p><strong>Tracking an underutilized Amazon Redshift instance or an over-utilized Amazon EC2 instance is a classic example of AWS Security Hub use cases</strong> - Security issues (e.g., Amazon S3 buckets that are publicly accessible or detecting crypto-mining on Amazon EC2 instances) and operational issues (e.g., underutilized Amazon Redshift instances or over-utilized Amazon EC2 instances) differ from each other because security issues are sensitive and typically have different response requirements. As a result, use Security Hub to understand, manage, and remediate the security issues, and use Systems Manager to understand, manage, and remediate operational issues.</p>\n\n<p><strong>Security Hub uses resource-linked roles to perform security checks for most controls from AWS Config</strong> - This statement is incorrect and given as a distractor. AWS Security Hub uses service-linked AWS Config rules to perform security checks for most controls.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-setup-prereqs.html">https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-setup-prereqs.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/securityhub-integration.html">https://docs.aws.amazon.com/guardduty/latest/ug/securityhub-integration.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-standards-enable-disable.html">https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-standards-enable-disable.html</a></p>\n\n<p><a href="https://aws.amazon.com/security-hub/faqs">https://aws.amazon.com/security-hub/faqs</a></p>\n',
        answers: [
          "<p>Amazon GuardDuty must be enabled as a pre-requisite for using Security Hub. GuardDuty immediately begins to send findings to Security Hub</p>",
          "<p>Tracking an underutilized Amazon Redshift instance or an over-utilized Amazon EC2 instance is a classic example of AWS Security Hub use cases</p>",
          "<p>Security Hub uses resource-linked roles to perform security checks for most controls from AWS Config</p>",
          "<p>AWS Config must be enabled as a pre-requisite for using Security Hub</p>",
          "<p>When you enable both GuardDuty and Security Hub, the mutual integration is enabled automatically. GuardDuty immediately begins to send findings to Security Hub</p>",
        ],
      },
      correct_response: ["d", "e"],
      section: "Management and Security Governance",
      question_plain:
        "A company has decided to enable AWS Security Hub to help assess its growing AWS environment against security industry standards and best practices.\n\nWhich of the following represents true statements for the AWS Security Hub service? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960230,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A corporation X is looking for a solution that provides automatic scanning of operating system and programming language package vulnerabilities for all its container images stored on Amazon Elastic Container Registry (Amazon ECR). The images should only be scanned once when they are pushed onto the repository.</p>\n\n<p>Which of the following options is the right fit for the given requirements?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Opt for enhanced scanning and specify a filter for a scan on push</strong></p>\n\n<p>With enhanced scanning, Amazon ECR integrates with Amazon Inspector to provide automated, continuous scanning of your repositories. Your container images are scanned for both operating systems and programming language package vulnerabilities. As new vulnerabilities appear, the scan results are updated and Amazon Inspector emits an event to EventBridge to notify you.</p>\n\n<p>Since both the OS and programming language vulnerabilities have to be scanned, enhanced scanning has to be selected. Let\'s look at the filter options available for enhanced scanning.</p>\n\n<p>When enhanced scanning is used, you may specify separate filters for scan on push and continuous scanning. Any repositories that do not match an enhanced scanning filter will have scanning disabled. If you are using enhanced scanning and specify separate filters for scan on push and continuous scanning where multiple filters match the same repository, then Amazon ECR enforces the continuous scanning filter over the scan on push filter for that repository.</p>\n\n<p>Key considerations while enabling Amazon ECR enhanced scanning:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q65-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for enhanced scanning and specify a filter for continuous scanning</strong> - Continuous scanning will continue to scan the images event after the scan at the initial push. This does not meet the given requirements.</p>\n\n<p><strong>Opt for enhanced scanning and set to manual scan frequency</strong> - This option is incorrect. Manual scans that use enhanced scanning aren\'t supported.</p>\n\n<p><strong>Opt for basic scanning on push filter</strong> - With basic scanning, Amazon ECR uses the Common Vulnerabilities and Exposures (CVEs) database from the open-source Clair project. Programming language package vulnerabilities are not covered in this scan plan.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html</a></p>\n',
        answers: [
          "<p>Opt for basic scanning on push filter</p>",
          "<p>Opt for enhanced scanning and set to manual scan frequency</p>",
          "<p>Opt for enhanced scanning and specify a filter for continuous scanning</p>",
          "<p>Opt for enhanced scanning and specify a filter for a scan on push</p>",
        ],
      },
      correct_response: ["d"],
      section: "Infrastructure Security",
      question_plain:
        "A corporation X is looking for a solution that provides automatic scanning of operating system and programming language package vulnerabilities for all its container images stored on Amazon Elastic Container Registry (Amazon ECR). The images should only be scanned once when they are pushed onto the repository.\n\nWhich of the following options is the right fit for the given requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960170,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>As per the latest security guidelines of a company, root user login access should be intimated to the security team every time it is used.</p>\n\n<p>How will you create a solution for this requirement in the most efficient way?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Create an Amazon EventBridge event rule to monitor <code>userIdentity</code> root logins from the AWS Management Console and trigger notifications to the SNS topic when root user login activity is detected</strong></p>\n\n<p>This option is the most efficient way of configuring the given requirement. You can even launch an AWS CloudFormation stack to create an Amazon Simple Notification Service (Amazon SNS) topic. Then, create an Amazon EventBridge event rule to monitor userIdentity root logins from the AWS Management Console.</p>\n\n<p>Before you begin, confirm that the AWS CloudTrail Management read/write events are set to <code>All</code> or <code>Write-only</code> for EventBridge events to trigger the log-in event notification.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Create an Amazon CloudWatch Events rule that detects any AWS account root user API events. This rule triggers an AWS Lambda function which publishes the message to the created SNS topic</strong> - This option will be the right choice if all root user activities have to be notified. Since the use case only asks for root user login access notifications, this option is not the most efficient way of configuring the requirement.</p>\n\n<p>AWS suggests using Amazon EventBridge to manage your events. CloudWatch Events and EventBridge are the same underlying service and API, but EventBridge provides more features.</p>\n\n<p><strong>Save the AWS CloudTrail logs to an Amazon S3 bucket in the AWS account used by the security team. Analyze the logs using AWS Athena. Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Configure a Lambda function to run an Athena query and trigger notifications to this SNS topic when root user login is detected</strong> - This option is an unnecessarily complicated solution since Amazon CloudWatch Events or Amazon EventBridge events offer better features to easily implement the asked requirement.</p>\n\n<p><strong>Send the data of VPC Flow logs to Amazon Simple Queue Service (SQS). Use the AWS Lambda function to process these messages and send notifications to the SNS topic in case root user login activity is detected</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. It does not deal with user access information and hence is an incorrect choice for the given requirement.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/root-user-account-eventbridge-rule/">https://aws.amazon.com/premiumsupport/knowledge-center/root-user-account-eventbridge-rule/</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/">https://aws.amazon.com/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n',
        answers: [
          "<p>Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Create an Amazon CloudWatch Events rule that detects any AWS account root user API events. This rule triggers an AWS Lambda function which publishes the message to the created SNS topic</p>",
          "<p>Save the AWS CloudTrail logs to an Amazon S3 bucket in the AWS account used by the security team. Analyze the logs using AWS Athena. Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Configure a Lambda function to run an Athena query and trigger notifications to this SNS topic when root user login is detected</p>",
          "<p>Send the data of VPC Flow logs to Amazon Simple Queue Service (SQS). Use AWS Lambda function to process these messages and send notifications to SNS topic in case root user login activity is detected</p>",
          "<p>Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Create an Amazon EventBridge event rule to monitor <code>userIdentity</code> root logins from the AWS Management Console and trigger notifications to the SNS topic when root user login activity is detected</p>",
        ],
      },
      correct_response: ["d"],
      section: "Security Logging and Monitoring",
      question_plain:
        "As per the latest security guidelines of a company, root user login access should be intimated to the security team every time it is used.\n\nHow will you create a solution for this requirement in the most efficient way?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960150,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A media company uses S3 to store artifacts that may only be accessible to EC2 instances running in a private VPC. The security team at the company is apprehensive about an attack vector wherein any team member with access to this instance could also set up an EC2 instance in another VPC to access these artifacts.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following solutions will you recommend to prevent such unauthorized access to the artifacts in S3?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Configure an S3 VPC endpoint and create an S3 bucket policy to allow access only from this VPC endpoint</strong></p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. One of the ways of letting EC2 instances running in private subnets of a VPC access S3-based resources is by setting up NAT instances in a public subnet and then access those S3-based resources. However, there is a more efficient and secure way. The EC2 instances running in private subnets of a VPC can control access to S3 buckets, objects, and API functions that are in the same Region as the VPC by using the S3 gateway endpoints.</p>\n\n<p>Here are the steps to set up a gateway endpoint:</p>\n\n<p><img src="https://media.amazonwebservices.com/blog/2015/vpc_config_endpoint_5.png"></p>\n\n<p><img src="https://media.amazonwebservices.com/blog/2015/vpc_config_endpoint_routes_2.png">\nvia - <a href="https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/">https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/</a></p>\n\n<p>Important Characteristics for S3 Gateway Endpoints:</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q25-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html</a></p>\n\n<p>You can further use an S3 bucket policy to indicate which VPCs and VPC Endpoints have access to your S3 buckets.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q25-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an IAM role that allows access to the artifacts in S3 and then create an S3 bucket policy to allow access only from this role attached to the instance profile</strong> - This allows the possibility to attach the given role to multiple EC2 instance profiles and therefore opens up doors for unauthorized access from different EC2 instances. Hence this option is incorrect.</p>\n\n<p><strong>Attach an Elastic IP to the EC2 instance and create an S3 bucket policy to allow access only from this Elastic IP</strong> - As described in the explanation above, you cannot use the aws:SourceIp condition in your IAM policies for requests to Amazon S3 through a VPC endpoint. This applies to IAM policies for users and roles, and any bucket policies. Hence this option is incorrect.</p>\n\n<p><strong>Set up a highly restricted Security Group for the EC2 instance and create an S3 bucket policy to allow access only from this Security Group</strong> - This option has been added as a distractor as a Security Group is not a valid Principal to be used in an S3 bucket policy. Security Group also cannot be used in a valid Condition statement in the bucket policy.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/">https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p>\n',
        answers: [
          "<p>Set up an IAM role that allows access to the artifacts in S3 and then create an S3 bucket policy to allow access only from this role attached to the instance profile</p>",
          "<p>Attach an Elastic IP to the EC2 instance and create an S3 bucket policy to allow access only from this Elastic IP</p>",
          "<p>Configure an S3 VPC endpoint and create an S3 bucket policy to allow access only from this VPC endpoint</p>",
          "<p>Set up a highly restricted Security Group for the EC2 instance and create an S3 bucket policy to allow access only from this Security Group</p>",
        ],
      },
      correct_response: ["c"],
      section: "Identity and Access Management",
      question_plain:
        "A media company uses S3 to store artifacts that may only be accessible to EC2 instances running in a private VPC. The security team at the company is apprehensive about an attack vector wherein any team member with access to this instance could also set up an EC2 instance in another VPC to access these artifacts.\n\nAs an AWS Certified Security Specialist, which of the following solutions will you recommend to prevent such unauthorized access to the artifacts in S3?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960152,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An e-commerce company recently saw a huge spike in its monthly AWS spend. Upon further investigation, it was found that some developers had accidentally launched Amazon RDS instances in unexpected Regions. The company has hired you as an AWS Certified Security Specialist to establish best practices around the least privileges for developers and control access to on-premises as well as AWS Cloud resources using Active Directory. The company has mandated that you institute a mechanism to control costs by restricting the level of access that developers have to the AWS Management Console without impacting their productivity. The company would also like to allow developers to launch RDS instances only in <code>us-east-1</code> Region without limiting access to other services in any Region.</p>\n\n<p>How can you help the company achieve the new security mandate while minimizing the operational burden on the systems administration team?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Configure SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code></strong></p>\n\n<p>Security Assertion Markup Language 2.0 (SAML) is an open federation standard that allows an identity provider (IdP) to authenticate users and pass identity and security information about them to a service provider which is an AWS application or service for the current use case. With SAML, you can enable a single sign-on experience for your users across many SAML-enabled applications and services. Users authenticate with the IdP once using a single set of credentials and then get access to multiple applications and services without additional sign-ins.</p>\n\n<p>For the given scenario, the company wants to control access to on-premises as well as AWS Cloud resources (specifically via the AWS Management Console) using Active Directory, so it should use SAML 2.0 federated users to access the AWS Management Console. You also create an IAM role with a trust policy that sets the SAML provider as the principal, which establishes a trust relationship between your organization and AWS. The role\'s permission policy establishes what users from your organization are allowed to do in AWS. In this case, the role will have a PowerUserAccess managed policy attached. As the PowerUserAccess managed policy will allow the developers to create RDS instances in any Region, therefore, you also need to attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code>.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q26-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html</a></p>\n\n<p>At a high level, it is useful to think of these access privileges in the form of this equation:</p>\n\n<p>PowerUserAccess = AdministrativeAccess - IAM</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure SAML-based authentication tied to an IAM role that has the AdministrativeAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code></strong> - Using an IAM role with an AdministrativeAccess managed policy attache to it would violate the key requirement of providing the least privileges for developers. PowerUserAccess provides full access to AWS services and resources but does not allow management of users and groups.</p>\n\n<p>At a high level, it is useful to think of these access privileges in the form of this equation:</p>\n\n<p>PowerUserAccess = AdministrativeAccess - IAM</p>\n\n<p>So, PowerUserAccess provides just the right access privileges required for the given use case.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q26-i2.jpg"></p>\n\n<p><strong>Set up an IAM user for each developer and add them to the developer IAM group that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that allows the developers access to RDS only in <code>us-east-1</code> Region</strong> - Setting up an IAM user for each developer and adding them to the developer IAM group goes against the requirement of minimizing the operational burden on the DevOps team because this solution does not take advantage of the existing Active Directory that supports SAML-based authentication.</p>\n\n<p><strong>Configure SAML-based authentication tied to an IAM role that has a PowerUserAccess managed policy and a customer-managed policy that denies all the developers access to any AWS services except AWS Service Catalog. Within AWS Service Catalog, create a product containing only RDS service in <code>us-east-1</code> region</strong> - This option is a distractor as it\'s too restrictive. As the customer-managed policy denies the developers access to any AWS services except AWS Service Catalog, therefore it would limit access to all other services in any Region, hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html</a></p>\n',
        answers: [
          "<p>Configure SAML-based authentication tied to an IAM role that has the AdministrativeAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code></p>",
          "<p>Set up an IAM user for each developer and add them to the developer IAM group that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that allows the developers access to RDS only in <code>us-east-1</code> Region</p>",
          "<p>Configure SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code></p>",
          "<p>Configure SAML-based authentication tied to an IAM role that has a PowerUserAccess managed policy and a customer-managed policy that denies all the developers access to any AWS services except AWS Service Catalog. Within AWS Service Catalog, create a product containing only RDS service in <code>us-east-1</code> region</p>",
        ],
      },
      correct_response: ["c"],
      section: "Identity and Access Management",
      question_plain:
        "An e-commerce company recently saw a huge spike in its monthly AWS spend. Upon further investigation, it was found that some developers had accidentally launched Amazon RDS instances in unexpected Regions. The company has hired you as an AWS Certified Security Specialist to establish best practices around the least privileges for developers and control access to on-premises as well as AWS Cloud resources using Active Directory. The company has mandated that you institute a mechanism to control costs by restricting the level of access that developers have to the AWS Management Console without impacting their productivity. The company would also like to allow developers to launch RDS instances only in us-east-1 Region without limiting access to other services in any Region.\n\nHow can you help the company achieve the new security mandate while minimizing the operational burden on the systems administration team?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960154,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A financial services company manages its IT infrastructure on AWS. The security team at the company has been tasked to monitor and report all the root user activities of the AWS account.</p>\n\n<p>Which options should be combined as a solution so that the security team can meet these requirements? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Set up a CloudWatch Events rule that is triggered on any API call from the root user</strong></p>\n\n<p><strong>Using Amazon SNS as a target of the trigger that further notifies the security team</strong></p>\n\n<p><img src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/07/06/flow_diagram-1.jpg">\nvia - <a href="https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/">https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/</a></p>\n\n<p>For the given use case, you can set up a CloudWatch rule that catches a console login event and all other API events by a root user, and triggers the Lambda function (set as a target) when such events are detected. This is accomplished by leveraging the CloudTrail API that captures the AWS API call for the root user sign-in.</p>\n\n<p><img src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/07/06/CWE_snapshot.png">\nvia - <a href="https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/">https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/</a></p>\n\n<p>The function collects the necessary information about the root API event and publishes it to the SNS topic. The function parses the name of the event and the AWS account alias where this root API event occurred and puts them in the subject field for the message that it publishes to the SNS topic.</p>\n\n<p>Finally, the SNS topic sends the email notification published by the Lambda function.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Trusted Advisor to monitor root user API calls on the company\'s AWS account and trigger a downstream event to SNS</strong> - AWS Trusted Advisor is a service that continuously analyzes your AWS accounts and provides recommendations to help you to follow AWS best practices and AWS Well-Architected guidelines. Trusted Advisor cannot monitor root user API calls on the company\'s AWS account and trigger a downstream event to SNS.</p>\n\n<p><strong>Set up an AWS Config rule that triggers a downstream event to SNS on all API calls from the root user</strong> - AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. AWS Config has a managed rule - <code>iam-root-access-key-check</code> that checks if the root user access key is available. The rule is COMPLIANT if the user access key does not exist. Otherwise, NON_COMPLIANT. AWS Config cannot monitor all root user API calls on the company\'s AWS account and trigger a downstream event to SNS.</p>\n\n<p><strong>Set up AWS Inspector to monitor root user API calls on the company\'s AWS account and trigger a downstream event to SNS</strong> - Amazon Inspector is a vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions for known software vulnerabilities and unintended network exposure. Inspector cannot monitor root user API calls on the company\'s AWS account and trigger a downstream event to SNS.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/">https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/root-user-account-eventbridge-rule/">https://aws.amazon.com/premiumsupport/knowledge-center/root-user-account-eventbridge-rule/</a></p>\n',
        answers: [
          "<p>Set up AWS Trusted Advisor to monitor root user API calls on the company's AWS account and trigger a downstream event to SNS</p>",
          "<p>Set up an AWS Config rule that triggers a downstream event to SNS on all API calls from the root user</p>",
          "<p>Set up AWS Inspector to monitor root user API calls on the company's AWS account and trigger a downstream event to SNS</p>",
          "<p>Set up a CloudWatch Events rule that is triggered on any API call from the root user</p>",
          "<p>Using Amazon SNS as a target of the trigger that further notifies the security team</p>",
        ],
      },
      correct_response: ["d", "e"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A financial services company manages its IT infrastructure on AWS. The security team at the company has been tasked to monitor and report all the root user activities of the AWS account.\n\nWhich options should be combined as a solution so that the security team can meet these requirements? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960156,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A financial services company recently faced a security event resulting in an S3 bucket with sensitive data containing Personally Identifiable Information (PII) for customers being made public. The company policy mandates never to have public S3 objects so the Governance and Compliance team must be notified immediately as soon as any public objects are identified. The company has hired you as an AWS Certified Security Specialist to help build a solution that detects the presence of a public S3 object, which in turn sets off an alarm to trigger notifications and then automatically remediate the said object.</p>\n\n<p>Which of the following solutions would you combine to address the requirements of the given use case? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Configure a Lambda function as one of the SNS topic subscribers, which is invoked to secure the objects in the S3 bucket</strong></p>\n\n<p><strong>Enable object-level logging for S3. Set up an EventBridge event pattern when a PutObject API call with public-read permission is detected in the AWS CloudTrail logs and set the target as an SNS topic for downstream notifications</strong></p>\n\n<p>You can enable object-level logging for an S3 bucket to send logs to CloudTrail for object-level API operations such as GetObject, DeleteObject, and PutObject. These events are called data events. By default, CloudTrail trails don\'t log data events, but you can configure trails to log data events for S3 buckets that you specify, or to log data events for all the Amazon S3 buckets in your AWS account.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html</a></p>\n\n<p>You need to further configure an EventBridge event-pattern based rule to analyze the CloudTrail logs for S3 PutObject API call with public-read permissions. The target for this rule can be set as an SNS topic. The SNS would send the notification via an email or SMS as soon as a public object is detected. Moreover, the SNS topic is also subscribed by a Lambda function which runs custom code to secure the objects in the S3 bucket.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/sns/latest/dg/welcome.html">https://docs.aws.amazon.com/sns/latest/dg/welcome.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable object-level logging for S3. When a PutObject API call is made with public-read permission, use S3 event notifications to trigger a Lambda that sends a notification via SNS</strong> - S3 event notification allows you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. S3 can publish notifications for the new create object events.</p>\n\n<p>You can request notification when only a specific API is used (for example, s3:ObjectCreated:Put), or you can use a wildcard (for example, s3:ObjectCreated:*), however, you cannot check if the API call was made with public-read permission. So, this option is incorrect.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n\n<p><strong>Leverage AWS Trusted Advisor to check for S3 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded</strong> - Trusted Advisor is an application that inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. The Trusted Advisor notification feature helps you stay up-to-date with your AWS resource deployment. However, you will only be notified by weekly email when you opt-in for this service, so this does not meet the key requirement for the use case wherein the notification should be sent as soon as a public object is uploaded. Also, Trusted Advisor just checks buckets in Amazon Simple Storage Service (Amazon S3) that have open access permissions. It cannot be used for near real-time detection of a new public object uploaded on S3.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i4.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/faqs/">https://aws.amazon.com/premiumsupport/faqs/</a></p>\n\n<p><strong>Leverage AWS Access Analyzer to check for S3 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded</strong> - You can use AWS Access Analyzer to receive findings into the source and level of public or shared access for each public or shared bucket. For example, Access Analyzer for S3 might show that a bucket has read or write access provided through a bucket access control list (ACL), a bucket policy, or an access point policy. It cannot be used for near real-time detection of a new public object uploaded on S3. Additionally, you cannot invoke a Lambda function from Access Analyzer. The findings for Access Analyzer are available within the AWS Console or they can be downloaded in a CSV report.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i5.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html</a></p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/sns/latest/dg/welcome.html">https://docs.aws.amazon.com/sns/latest/dg/welcome.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/faqs/">https://aws.amazon.com/premiumsupport/faqs/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html</a></p>\n',
        answers: [
          "<p>Enable object-level logging for S3. When a PutObject API call is made with public-read permission, use S3 event notifications to trigger a Lambda that sends a notification via SNS</p>",
          "<p>Leverage AWS Trusted Advisor to check for S3 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded</p>",
          "<p>Leverage AWS Access Analyzer to check for S3 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded</p>",
          "<p>Configure a Lambda function as one of the SNS topic subscribers, which is invoked to secure the objects in the S3 bucket</p>",
          "<p>Enable object-level logging for S3. Set up an EventBridge event pattern when a PutObject API call with public-read permission is detected in the AWS CloudTrail logs and set the target as an SNS topic for downstream notifications</p>",
        ],
      },
      correct_response: ["d", "e"],
      section: "Management and Security Governance",
      question_plain:
        "A financial services company recently faced a security event resulting in an S3 bucket with sensitive data containing Personally Identifiable Information (PII) for customers being made public. The company policy mandates never to have public S3 objects so the Governance and Compliance team must be notified immediately as soon as any public objects are identified. The company has hired you as an AWS Certified Security Specialist to help build a solution that detects the presence of a public S3 object, which in turn sets off an alarm to trigger notifications and then automatically remediate the said object.\n\nWhich of the following solutions would you combine to address the requirements of the given use case? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960158,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A data analytics company wants to move all its clients belonging to the regulated and security-sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The Security team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources. You have been hired as an AWS Certified Security Specialist to spearhead this strategic initiative.</p>\n\n<p>Which of the following strategies would you adopt to address these business requirements for continuously assessing, auditing, and monitoring the configurations of AWS resources? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Leverage Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule\'s scope changes in configuration</strong></p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”.</p>\n\n<p>How AWS Config Works:\n<img src="https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png">\nvia - <a href="https://aws.amazon.com/config/">https://aws.amazon.com/config/</a></p>\n\n<p>For the given use case, you can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. You can also create your own custom rules. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes violate any of the conditions in your rules. If a resource violates a rule, AWS Config flags the resource and marks the rule as non-compliant.</p>\n\n<p>There are two types of evaluation trigger types for Config rules:</p>\n\n<p>Configuration changes – AWS Config triggers the evaluation when any resource that matches the rule\'s scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p>\n\n<p>Periodic – AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p>\n\n<p><strong>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</strong></p>\n\n<p>CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or AWS service are recorded as events in CloudTrail. An event in CloudTrail is the record of activity in an AWS account. This activity can be an action taken by a user, role, or service that is monitorable by CloudTrail. CloudTrail events provide a history of both API and non-API account activity made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.</p>\n\n<p>CloudTrail data events are disabled by default. You can enable logging at an additional cost. Data events are also known as data plane operations and are often high-volume activities. Data events aren\'t viewable in CloudTrail event history and are charged for all copies at a reduced rate compared to management events.</p>\n\n<p>CloudTrail records management events for the last 90 days free of charge, and are viewable in the Event History with the CloudTrail console. For Amazon S3 delivery of CloudTrail events, the first copy delivered is free. Additional copies of management events are charged.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q29-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on using CloudWatch Logs agent to collect all the AWS SDK logs. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. So this option is not the best-fit solution.</p>\n\n<p><strong>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on capturing unauthorized API activities. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. In addition, the use case just talks about assessing, auditing, and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p><strong>Leverage EventBridge events near-real-time capabilities to monitor system event patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</strong> - The use-case just talks about assessing, auditing, and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/config/">https://aws.amazon.com/config/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a></p>\n',
        answers: [
          "<p>Leverage Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule's scope changes in configuration</p>",
          "<p>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</p>",
          "<p>Leverage CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</p>",
          "<p>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</p>",
          "<p>Leverage EventBridge events near-real-time capabilities to monitor system event patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</p>",
        ],
      },
      correct_response: ["a", "b"],
      section: "Management and Security Governance",
      question_plain:
        "A data analytics company wants to move all its clients belonging to the regulated and security-sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The Security team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources. You have been hired as an AWS Certified Security Specialist to spearhead this strategic initiative.\n\nWhich of the following strategies would you adopt to address these business requirements for continuously assessing, auditing, and monitoring the configurations of AWS resources? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960160,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A retail company has a three-tier web application with separate subnets for Web, Application, and Database tiers. The CTO at the company wants to monitor any malicious activity targeting the web application running on EC2 instances. You have been tasked with developing a solution to notify the security team in case the network exposure of EC2 instances on specific ports violates the security policies of the company.</p>\n\n<p>Which AWS Services would you use to build an automated notification system to meet these requirements with the least development effort? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Amazon SNS</strong></p>\n\n<p><strong>Amazon Inspector</strong></p>\n\n<p>Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.</p>\n\n<p>You can perform network security assessments via your own custom solutions, however, that entails significant time and effort. You might need to run network port-scanning tools to test routing and firewall configurations, then validate what processes are listening on your instance network ports, before finally mapping the IPs identified in the port scan back to the host’s owner.</p>\n\n<p>To make this process simpler for its customers, AWS offers the Network Reachability rules package in Amazon Inspector, which is an automated security assessment service that enables you to understand and improve the security and compliance of applications deployed on AWS. The existing Amazon Inspector host assessment rules packages check the software and configurations on your Amazon Elastic Compute Cloud (Amazon EC2) instances for vulnerabilities and deviations from best practices.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q30-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/">https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/</a></p>\n\n<p>You can use these rules packages to analyze the accessibility of critical ports, as well as all other network ports. For critical ports, Amazon Inspector will show the exposure of each and will offer findings per port. When critical, well-known ports (based on Amazon’s standard guidance) are reachable, findings will be created with higher severities.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q30-i2.jpg">\nvia <a href="https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/">https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/</a></p>\n\n<p>The findings also have recommendations that include information about exactly which Security Group you can edit to remove the access. And like all Amazon Inspector findings, these can be published to an SNS topic for additional processing or you could use a Lambda to automatically remove ingress rules in the Security Group to address a network reachability finding. For the given use case, the network engineer can use the SNS topic to send notifications to the team.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Shield</strong> - AWS Shield is a managed service that protects against Distributed Denial of Service (DDoS) attacks for applications running on AWS. AWS Shield Standard is automatically enabled for all AWS customers at no additional cost. AWS Shield Advanced is an optional paid service. AWS Shield Advanced provides additional protections against more sophisticated and larger attacks for your applications running on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53. AWS Shield cannot be used to assess network exposure of EC2 instances on specific ports.</p>\n\n<p><strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. CloudWatch cannot be used to assess network exposure of EC2 instances on specific ports.</p>\n\n<p><strong>VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. You can use VPC Flow Logs to assess network exposure of EC2 instances on specific ports but the solution would entail significant development effort to parse through the logs and identify the exposed ports. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/">https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/</a></p>\n',
        answers: [
          "<p>Amazon SNS</p>",
          "<p>Amazon Inspector</p>",
          "<p>AWS Shield</p>",
          "<p>Amazon CloudWatch</p>",
          "<p>VPC Flow Logs</p>",
        ],
      },
      correct_response: ["a", "b"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A retail company has a three-tier web application with separate subnets for Web, Application, and Database tiers. The CTO at the company wants to monitor any malicious activity targeting the web application running on EC2 instances. You have been tasked with developing a solution to notify the security team in case the network exposure of EC2 instances on specific ports violates the security policies of the company.\n\nWhich AWS Services would you use to build an automated notification system to meet these requirements with the least development effort? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960162,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A mid-sized company recently deployed Amazon GuardDuty to monitor their AWS environment for potential security threats. The security team noticed a high number of RDP brute force attacks originating from an Amazon EC2 instance and decided to take action to prevent any issues. The company's security engineer was tasked with implementing an automated solution that could block the suspicious instance until the issue could be investigated and remediated.</p>\n\n<p>Which of the following solutions should the security engineer implement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Have Security Hub ingest GuardDuty findings and send events to Kinesis Data Streams via EventBridge. Configure a Lambda function to process the data stream and block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules</strong></p>\n\n<p>AWS Security Hub provides you with a comprehensive view of your security state in AWS and helps you check your environment against security industry standards and best practices.</p>\n\n<p>Security Hub collects security data from across AWS accounts, services (such as GuardDuty), and supported third-party partner products and helps you analyze your security trends and identify the highest priority security issues.</p>\n\n<p>How Security Hub works:\n<img src="https://d1.awsstatic.com/Digital%20Marketing/House/Hero/products/Security%20Hub/Product-Page-Diagram_AWS-Security-Hub%402x.7e7c0483e9ce1507af2e9214247a1825a27d6bde.png">\nvia - <a href="https://aws.amazon.com/security-hub/">https://aws.amazon.com/security-hub/</a></p>\n\n<p>Leveraging Amazon EventBridge\'s integration with Security Hub, you can automate your AWS services to respond automatically to system events such as application availability issues or resource changes. Events from AWS services are delivered to EventBridge in near-real time and on a guaranteed basis. You can write simple rules to indicate which events you are interested in and what automated actions to take when an event matches a rule. The actions that can be automatically triggered include the following:</p>\n\n<p>Invoking an AWS Lambda function</p>\n\n<p>Invoking the Amazon EC2 run command</p>\n\n<p>Relaying the event to Amazon Kinesis Data Streams</p>\n\n<p>Activating an AWS Step Functions state machine</p>\n\n<p>Notifying an Amazon SNS topic or an Amazon SQS queue</p>\n\n<p>Sending a finding to a third-party ticketing, chat, SIEM, or incident response and management tool</p>\n\n<p>For the given use case, you can process the Security Hub events in Kinesis Data Streams by using a Lambda function that monitors any <code>UnauthorizedAccess:EC2/RDPBruteForce\n</code> finding from GuardDuty that is relayed via Security Hub. This finding informs you that an EC2 instance in your AWS environment was involved in a brute force attack aimed at obtaining passwords to RDP services on Windows-based systems. This can indicate unauthorized access to your AWS resources. When the Lambda function sees a matching finding, it can block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Have Security Hub ingest GuardDuty findings and send events to EventBridge that triggers a Lambda function to block traffic to/from the suspicious instance by updating the WAF web ACL</strong> - WAF web ACL can only be applied to the following resource types: CloudFront distribution, Amazon API Gateway REST API, Application Load Balancer, AWS AppSync GraphQL API and Amazon Cognito user pool. You can use AWS WAF to control how your protected resources respond to HTTP(S) web requests. The given use case is about RDP brute force attacks originating from an EC2 instance, so using WAF web ACL is not relevant, as it cannot monitor traffic originating from an EC2 instance.</p>\n\n<p><strong>Have Security Hub ingest GuardDuty findings and send events to EventBridge that triggers a Lambda function to block traffic to/from the suspicious instance by updating the network ACL rules</strong> - Using Network ACL rules would impact all instances in a subnet. It will not isolate the traffic only for the suspicious instance. Hence this option is incorrect.</p>\n\n<p><strong>Have Security Hub ingest GuardDuty findings and send events to Kinesis Data Streams via EventBridge. Configure Kinesis Data Analytics to process the data stream and block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules</strong> - Amazon Kinesis Data Analytics can be used to transform and analyze streaming data in real-time with Apache Flink. Apache Flink is an open-source framework and engine for processing data streams. Kinesis Data Analytics reduces the complexity of building, managing, and integrating Apache Flink applications with other AWS services. This option has been added as a distractor as Kinesis Data Analytics cannot be used to update the security groups for an instance.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/security-hub/">https://aws.amazon.com/security-hub/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-cloudwatch-events.html">https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-cloudwatch-events.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#unauthorizedaccess-ec2-rdpbruteforce">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#unauthorizedaccess-ec2-rdpbruteforce</a></p>\n',
        answers: [
          "<p>Have Security Hub ingest GuardDuty findings and send events to Kinesis Data Streams via EventBridge. Configure Kinesis Data Analytics to process the data stream and block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules</p>",
          "<p>Have Security Hub ingest GuardDuty findings and send events to EventBridge that triggers a Lambda function to block traffic to/from the suspicious instance by updating the WAF web ACL</p>",
          "<p>Have Security Hub ingest GuardDuty findings and send events to EventBridge that triggers a Lambda function to block traffic to/from the suspicious instance by updating the network ACL rules</p>",
          "<p>Have Security Hub ingest GuardDuty findings and send events to Kinesis Data Streams via EventBridge. Configure a Lambda function to process the data stream and block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules</p>",
        ],
      },
      correct_response: ["d"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A mid-sized company recently deployed Amazon GuardDuty to monitor their AWS environment for potential security threats. The security team noticed a high number of RDP brute force attacks originating from an Amazon EC2 instance and decided to take action to prevent any issues. The company's security engineer was tasked with implementing an automated solution that could block the suspicious instance until the issue could be investigated and remediated.\n\nWhich of the following solutions should the security engineer implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960164,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A cybersecurity company is using AWS Systems Manager Session Manager to manage Amazon EC2 instances in the us-east-1 AWS Region. A user is unable to connect to a new EC2 instance that runs Amazon Linux 2 in a private subnet in a newly created VPC. The systems administrator has confirmed that the new EC2 instance has the correct IAM instance profile attached.</p>\n\n<p>As an AWS Certified Security Specialist, what would you attribute as the root cause behind this issue?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>The EC2 instance is in a private subnet and it does not have the <code>com.amazonaws.us-east-1.ssmmessages</code> VPC endpoint for Session Manager</strong></p>\n\n<p>Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs). You can use either an interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI).</p>\n\n<p>If you want to use Systems Manager to manage private EC2 instances without internet access, you need to create VPC endpoint for Session Manager that uses <code>com.amazonaws.us-east-1.ssmmessages</code> as the service name. Systems Manager uses the ssmmessages endpoint for API operations from SSM Agent to Session Manager, a capability of AWS Systems Manager, in the cloud. This endpoint is required to create and delete session channels with the Session Manager service in the cloud.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q32-i1.jpg">\nvia - <a href="https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints">https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance security group has no rule to allow inbound SSH traffic on port 22</strong></p>\n\n<p><strong>The EC2 key pair associated with the EC2 instance is invalid for the given user</strong></p>\n\n<p><strong>There is no bastion host to facilitate connection from the AWS Systems Manager Session Manager</strong></p>\n\n<p>Session Manager provides secure and auditable node management without the need to open inbound SSH ports, maintain bastion hosts, or manage SSH keys. So these three options are incorrect.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q32-i2.jpg">\nvia - <a href="https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints">https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints</a></p>\n\n<p>References:</p>\n\n<p><a href="https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints">https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-setting-up-messageAPIs.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-setting-up-messageAPIs.html</a></p>\n',
        answers: [
          "<p>The EC2 instance security group has no rule to allow inbound SSH traffic on port 22</p>",
          "<p>There is no bastion host to facilitate connection from the AWS Systems Manager Session Manager</p>",
          "<p>The EC2 key pair associated with the EC2 instance is invalid for the given user</p>",
          "<p>The EC2 instance is in a private subnet and it does not have the <code>com.amazonaws.us-east-1.ssmmessages</code> VPC endpoint for Session Manager</p>",
        ],
      },
      correct_response: ["d"],
      section: "Infrastructure Security",
      question_plain:
        "A cybersecurity company is using AWS Systems Manager Session Manager to manage Amazon EC2 instances in the us-east-1 AWS Region. A user is unable to connect to a new EC2 instance that runs Amazon Linux 2 in a private subnet in a newly created VPC. The systems administrator has confirmed that the new EC2 instance has the correct IAM instance profile attached.\n\nAs an AWS Certified Security Specialist, what would you attribute as the root cause behind this issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960166,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company has moved its business-critical data to an Amazon EFS file system which will be accessed by multiple EC2 instances.</p>\n\n<p>Which of the following would you recommend to exercise access control such that only the permitted EC2 instances can read from the EFS file system? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Use VPC security groups to control the network traffic to and from your file system</strong></p>\n\n<p><strong>Use an IAM policy to control access for clients who can mount your file system with the required permissions</strong></p>\n\n<p>You control which EC2 instances can access your EFS file system by using VPC security group rules and AWS Identity and Access Management (IAM) policies. Use VPC security groups to control the network traffic to and from your file system. Attach an IAM policy to your file system to control which clients can mount your file system and with what permissions, and you may use EFS Access Points to manage application access. Control access to files and directories with POSIX-compliant user and group-level permissions.</p>\n\n<p>Files and directories in an Amazon EFS file system support standard Unix-style read, write, and execute permissions based on the user ID and group IDs. When an NFS client mounts an EFS file system without using an access point, the user ID and group ID provided by the client is trusted. You can also use EFS access points to override user ID and group IDs used by the NFS client. When users attempt to access files and directories, Amazon EFS checks their user IDs and group IDs to verify that each user has permission to access the objects</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Network ACLs to control the network traffic to and from your Amazon EC2 instance</strong> - Network ACLs operate at the subnet level and not at the instance level.</p>\n\n<p><strong>Set up the IAM policy root credentials to control and configure the clients accessing the EFS file system</strong> - There is no such thing as an IAM policy root credentials and this statement has been added as a distractor.</p>\n\n<p><strong>Use Amazon GuardDuty to curb unwanted access to the EFS file system</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. It cannot be used for access control to the EFS file system.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison</a></p>\n\n<p><a href="https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html">https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html">https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html</a></p>\n',
        answers: [
          "<p>Use VPC security groups to control the network traffic to and from your file system</p>",
          "<p>Use an IAM policy to control access for clients who can mount your file system with the required permissions</p>",
          "<p>Use Network ACLs to control the network traffic to and from your Amazon EC2 instance</p>",
          "<p>Use Amazon GuardDuty to curb unwanted access to the EFS file system</p>",
          "<p>Set up the IAM policy root credentials to control and configure the clients accessing the EFS file system</p>",
        ],
      },
      correct_response: ["a", "b"],
      section: "Infrastructure Security",
      question_plain:
        "A company has moved its business-critical data to an Amazon EFS file system which will be accessed by multiple EC2 instances.\n\nWhich of the following would you recommend to exercise access control such that only the permitted EC2 instances can read from the EFS file system? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960168,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A business maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the business has moved from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all data to the Amazon S3 bucket, the business is looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data.</p>\n\n<p>How will you implement this requirement without adding the overhead of splitting the data into logical groups?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong></p>\n\n<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates.</p>\n\n<p>Note: Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong> - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement.</p>\n\n<p><strong>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect.</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</strong> - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n',
        answers: [
          "<p>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>",
          "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>",
          "<p>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</p>",
          "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</p>",
        ],
      },
      correct_response: ["b"],
      section: "Data Protection",
      question_plain:
        "A business maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the business has moved from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all data to the Amazon S3 bucket, the business is looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data.\n\nHow will you implement this requirement without adding the overhead of splitting the data into logical groups?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960190,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A social media company runs all its workloads on AWS and it uses AWS Organizations to implement a multi-account strategy. The company currently has multiple AWS member accounts for its departments. The company anticipates that it will not have more than a total of 15 AWS accounts at any time in the future.</p>\n\n<p>The company wants to enforce a new security policy with the following requirements:</p>\n\n<p><strong>The company should use a centrally managed VPC that all departmental AWS accounts can access to launch workloads in subnets</strong>\n<strong>The centrally managed VPC should reside in an existing AWS account (Account X) within the organization</strong>\n<strong>No departmental AWS account should use a VPC within its own account for workloads</strong>\n<strong>No departmental AWS account should be able to modify another department's AWS account-specific application resources within the centrally managed VPC</strong></p>\n\n<p>Which solution will facilitate the security setup to address these requirements?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Use AWS Resource Access Manager (AWS RAM) to share the subnets in Account X\'s centrally managed VPC with the other member accounts. Configure the member accounts to use the shared subnets to launch workloads</strong></p>\n\n<p>AWS Resource Access Manager (AWS RAM) helps you securely share your resources across AWS accounts, within your organization or organizational units (OUs), and with IAM roles and users for supported resource types.</p>\n\n<p>Overview of AWS Resource Access Manager (AWS RAM):\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q45-i1.jpg">\nvia - <a href="https://aws.amazon.com/ram/">https://aws.amazon.com/ram/</a></p>\n\n<p>VPC sharing allows multiple AWS accounts to create their application resources, such as Amazon EC2 instances, Amazon Relational Database Service (RDS) databases, Amazon Redshift clusters, and AWS Lambda functions, into shared, centrally-managed virtual private clouds (VPCs). In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner.</p>\n\n<p>For the given use case, you can set up AWS Resource Access Manager (AWS RAM) to share the subnets in Account X\'s centrally managed VPC with the other member accounts. You can share non-default subnets with other accounts within your organization. To share subnets, you must first create a Resource Share with the subnets to be shared and the AWS accounts, organizational units, or an entire organization that you want to share the subnets with. Then, you can configure the member accounts to use the shared subnets to launch workloads.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Systems Manager to share the subnets in Account X\'s centrally managed VPC with the other member accounts. Configure the member accounts to use the shared subnets to launch workloads</strong> - You cannot use AWS Systems Manager to share the subnets in Account X\'s centrally managed VPC with the other member accounts.</p>\n\n<p><strong>Set up VPC Peering among Account X\'s centrally managed VPC and the VPC\'s in all other member accounts. Configure the member accounts to use the shared subnets in Account X to launch workloads</strong> - You cannot use VPC Peering to share the subnets in Account X\'s centrally managed VPC with the other member accounts to launch resources in the shared subnets.</p>\n\n<p><strong>Configure a transit gateway in Account X\'s centrally managed VPC. Configure the member accounts to leverage the transit gateway to access the shared subnets in Account X to launch workloads</strong> - You cannot use a transit gateway to access the shared subnets to launch workloads.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/ram/">https://aws.amazon.com/ram/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/security/configure-fine-grained-access-to-your-resources-shared-using-aws-resource-access-manager/">https://aws.amazon.com/blogs/security/configure-fine-grained-access-to-your-resources-shared-using-aws-resource-access-manager/</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/">https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/</a></p>\n',
        answers: [
          "<p>Use AWS Resource Access Manager (AWS RAM) to share the subnets in Account X's centrally managed VPC with the other member accounts. Configure the member accounts to use the shared subnets to launch workloads</p>",
          "<p>Use AWS Systems Manager to share the subnets in Account X's centrally managed VPC with the other member accounts. Configure the member accounts to use the shared subnets to launch workloads</p>",
          "<p>Set up VPC Peering among Account X's centrally managed VPC and the VPC's in all other member accounts. Configure the member accounts to use the shared subnets in Account X to launch workloads</p>",
          "<p>Configure a transit gateway in Account X's centrally managed VPC. Configure the member accounts to leverage the transit gateway to access the shared subnets in Account X to launch workloads</p>",
        ],
      },
      correct_response: ["a"],
      section: "Management and Security Governance",
      question_plain:
        "A social media company runs all its workloads on AWS and it uses AWS Organizations to implement a multi-account strategy. The company currently has multiple AWS member accounts for its departments. The company anticipates that it will not have more than a total of 15 AWS accounts at any time in the future.\n\nThe company wants to enforce a new security policy with the following requirements:\n\nThe company should use a centrally managed VPC that all departmental AWS accounts can access to launch workloads in subnets\nThe centrally managed VPC should reside in an existing AWS account (Account X) within the organization\nNo departmental AWS account should use a VPC within its own account for workloads\nNo departmental AWS account should be able to modify another department's AWS account-specific application resources within the centrally managed VPC\n\nWhich solution will facilitate the security setup to address these requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960172,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A video processing application uses an AWS Lambda function to create image thumbnails from larger images. The AWS Lambda function needs read and write access to an Amazon S3 bucket configured in the same AWS account.</p>\n\n<p>Which is the most efficient solution to provide the necessary access permissions to the AWS Lambda function? </p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          "<p>Correct option:</p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) role for the Lambda function that also grants access to the S3 bucket. Configure the IAM role as the Lambda functions execution role. Verify that the S3 bucket policy doesn't explicitly deny access to your Lambda function or its execution role</strong></p>\n\n<p>To give your Lambda function access to an Amazon S3 bucket in the same AWS account, do the following:</p>\n\n<ol>\n<li><p>Create an AWS Identity and Access Management (IAM) role for the Lambda function that also grants access to the S3 bucket.</p></li>\n<li><p>Configure the IAM role as the Lambda functions execution role.</p></li>\n<li><p>Verify that the S3 bucket policy doesn't explicitly deny access to your Lambda function or its execution role.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) role for the Lambda function that also grants access to the S3 bucket. Grant the required permissions on the S3 bucket policy. Verify that the S3 bucket policy doesn't explicitly deny access to your Lambda function or its execution role</strong> - If your S3 bucket and the functions IAM role are in different accounts, then you must also grant the required permissions on the S3 bucket policy. Since both resources are part of the same AWS account, this step is unnecessary and inefficient.</p>\n\n<p><strong>Create a security group. Attach the security group to the Lambda function. Attach a bucket policy that allows access to the Amazon S3 bucket through the security group ID</strong> - You cannot attach a security group ID to an S3 bucket policy.</p>\n\n<p><strong>Generate an Amazon EC2 key pair. Store the private key in AWS Secrets Manager. Modify the AWS Lambda function to retrieve the private key from Secrets Manager and to use the private key during any communication with the S3 bucket</strong> - An Amazon EC2 key pair, consisting of a public key and a private key, is a set of security credentials that you use to prove your identity when connecting to an Amazon EC2 instance. This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/lambda-execution-role-s3-bucket\">https://repost.aws/knowledge-center/lambda-execution-role-s3-bucket</a></p>\n",
        answers: [
          "<p>Create an AWS Identity and Access Management (IAM) role for the Lambda function that also grants access to the S3 bucket. Grant the required permissions on the S3 bucket policy. Verify that the S3 bucket policy doesn't explicitly deny access to your Lambda function or its execution role</p>",
          "<p>Create an AWS Identity and Access Management (IAM) role for the Lambda function that also grants access to the S3 bucket. Configure the IAM role as the Lambda functions execution role. Verify that the S3 bucket policy doesn't explicitly deny access to your Lambda function or its execution role</p>",
          "<p>Create a security group. Attach the security group to the Lambda function. Attach a bucket policy that allows access to the Amazon S3 bucket through the security group ID</p>",
          "<p>Generate an Amazon EC2 key pair. Store the private key in AWS Secrets Manager. Modify the AWS Lambda function to retrieve the private key from Secrets Manager and to use the private key during any communication with the S3 bucket</p>",
        ],
      },
      correct_response: ["b"],
      section: "Identity and Access Management",
      question_plain:
        "A video processing application uses an AWS Lambda function to create image thumbnails from larger images. The AWS Lambda function needs read and write access to an Amazon S3 bucket configured in the same AWS account.\n\nWhich is the most efficient solution to provide the necessary access permissions to the AWS Lambda function?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960174,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An e-commerce company is designing a multi-account structure for its Finance and Operations teams using AWS Organizations and AWS Single Sign-On (AWS SSO). The teams should only be able to access specific AWS services in the designated AWS Regions.</p>\n\n<p>Which solution will implement these requirements with the LEAST operational overhead?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create Service control policies (SCPs) that deny access to any operations outside of the designated AWS Regions. Apart from the Condition and  Resource elements, configure the NotAction element to allow access to the required AWS services</strong></p>\n\n<p>The SCP can be defined to deny access to AWS based on the designated AWS Region and exempt necessary AWS resources from this exclusion. The following example SCP denies access to any operations outside of the designated Regions. Replace eu-central-1 and eu-west-1 with the AWS Regions you want to use. It provides exemptions for operations in approved global services. This example also shows how to exempt requests made by either of two specified administrator roles.</p>\n\n<p>This policy uses the Deny effect to deny access to all requests for operations that don\'t target one of the two approved regions (eu-central-1 and eu-west-1). The <code>NotAction</code> element enables you to list services whose operations (or individual operations) are exempted from this restriction. Because global services have endpoints that are physically hosted by the us-east-1 Region , they must be exempted in this way. With an SCP structured this way, requests made to global services in the us-east-1 Region are allowed if the requested service is included in the NotAction element. Any other requests to services in the us-east-1 Region are denied by this example policy.</p>\n\n<p>Example SCP to deny access to AWS based on the requested AWS Region:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q37-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-deny-region">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-deny-region</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Service control policy (SCP) that mandates multi-factor authentication (MFA) for access to the required services in the designated AWS Regions. Share the MFA credentials with only the AWS users that need access</strong> - This still does not solve the problem of allowing access to only specific AWS services in the designated AWS Regions.</p>\n\n<p><strong>Create a Service control policy (SCP) with a deny list policy strategy to deny access to users for certain AWS services and AWS Regions. Exclude administrators of the member accounts from this SCP</strong> - This is given only as a distractor. SCPs generally use a deny list policy strategy. Deny list policies must be attached along with other policies that allow the approved actions in the affected accounts. For example, the default FullAWSAccess policy permits the use of all services in an account. This policy is attached by default to the root, all organizational units (OUs), and all accounts. It doesn\'t actually grant the permissions; no SCP does. SCP applies to all users of the member accounts. You cannot exclude administrators of the member accounts from an SCP. You can delegate access to specific actions on selected resources by attaching standard AWS Identity and Access Management (IAM) permissions policies to users, roles, or groups in the member accounts.</p>\n\n<p><strong>Create a Service control policy (SCP) that allows access to users for certain AWS services in the designated AWS Regions</strong> - SCPs alone are not sufficient in granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail or sets limits on the actions that the account\'s administrator can delegate to the IAM users and roles in the affected accounts. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-deny-region">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-deny-region</a></p>\n\n<p><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html</a></p>\n',
        answers: [
          "<p>Create Service control policies (SCPs) that deny access to any operations outside of the designated AWS Regions. Apart from the Condition and Resource elements, configure the NotAction element to allow access to the required AWS services</p>",
          "<p>Create a Service control policy (SCP) that mandates multi-factor authentication (MFA) for access to the required services in the designated AWS Regions. Share the MFA credentials with only the AWS users that need access</p>",
          "<p>Create a Service control policy (SCP) with a deny list policy strategy to deny access to users for certain AWS services and AWS Regions. Exclude administrators of the member accounts from this SCP</p>",
          "<p>Create a Service control policy (SCP) that allows access to users for certain AWS services in the designated AWS Regions</p>",
        ],
      },
      correct_response: ["a"],
      section: "Data Protection",
      question_plain:
        "An e-commerce company is designing a multi-account structure for its Finance and Operations teams using AWS Organizations and AWS Single Sign-On (AWS SSO). The teams should only be able to access specific AWS services in the designated AWS Regions.\n\nWhich solution will implement these requirements with the LEAST operational overhead?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960176,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company uses Amazon EC2 instances (fronted by an Application Load Balancer) with Amazon RDS MySQL as the database. Now, the company wants to store sensitive client data and needs to follow strict security and compliance guidelines. Data must be end-to-end secured while in-transit, as well as, at-rest. The company needs a solution that can implement strict security guidelines while keeping the cost and operational overhead to a minimum.</p>\n\n<p>Which combination of steps will meet all the requirements? (Select three)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Use TLS certificates from AWS Certificate Manager (ACM) with an Application Load Balancer. Deploy self-signed certificates on the EC2 instances</strong></p>\n\n<p>Public ACM certificates can be installed on Amazon EC2 instances that are connected to a Nitro Enclave, but not to other Amazon EC2 instances. In general, to serve secure content over SSL/TLS, load balancers require that SSL/TLS certificates be installed on either the load balancer or the back-end Amazon EC2 instance. ACM is integrated with Elastic Load Balancing to deploy ACM certificates on the load balancer.</p>\n\n<p><strong>Ensure that the database client software uses a TLS connection to Amazon RDS. Enable encryption of the Amazon RDS DB instance</strong></p>\n\n<p>You can use Secure Socket Layer (SSL) or Transport Layer Security (TLS) from your application to encrypt a connection to a DB instance running MariaDB, Microsoft SQL Server, MySQL, Oracle, or PostgreSQL.</p>\n\n<p>SSL/TLS connections provide a layer of security by encrypting data that moves between your client and DB instance.</p>\n\n<p><strong>Enable encryption on the Amazon Elastic Block Store (Amazon EBS) volumes that support the Amazon EC2 instances</strong></p>\n\n<p>Use Amazon EBS encryption as a straightforward encryption solution for your EBS resources associated with your EC2 instances. With Amazon EBS encryption, you aren\'t required to build, maintain, and secure your own key management infrastructure. Amazon EBS encryption uses AWS KMS keys when creating encrypted volumes and snapshots.</p>\n\n<p>Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS CloudHSM to generate TLS certificates for the Amazon EC2 instances. Install the TLS certificates on the Amazon EC2 instances</strong> - Use AWS CloudHSM when you need to manage the HSMs that generate and store your encryption keys. In AWS CloudHSM, you create and manage HSMs, including creating users and setting their permissions. You also create the symmetric keys and asymmetric key pairs that the HSM stores. CloudHSM is neither cost-efficient nor operationally efficient for this use case.</p>\n\n<p><strong>Use Amazon CloudFront with AWS Web Application Firewall (AWS WAF). Send HTTP connections to the origin Amazon EC2 instances</strong> - You need to configure Amazon CloudFront to use HTTPS with your origin so that connections are encrypted when CloudFront communicates with your origin. HTTP option is incorrect since data has to be encrypted end-to-end while in-transit.</p>\n\n<p><strong>Use TLS certificates from a third-party vendor with an Application Load Balancer. Configure the same certificates on the Amazon EC2 instances</strong> - Application Load Balancers do not support mutual TLS authentication (mTLS). Using a third-party vendor adds to the operational overhead as opposed to using AWS ACM. TLS certificate from ACM should be configured on the ALB and a self-signed certificate should be set up on the Amazon EC2 instances.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html">https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n',
        answers: [
          "<p>Use TLS certificates from AWS Certificate Manager (ACM) with an Application Load Balancer. Deploy self-signed certificates on the EC2 instances</p>",
          "<p>Use AWS CloudHSM to generate TLS certificates for the Amazon EC2 instances. Install the TLS certificates on the Amazon EC2 instances</p>",
          "<p>Use Amazon CloudFront with AWS Web Application Firewall (AWS WAF). Send HTTP connections to the origin Amazon EC2 instances</p>",
          "<p>Use TLS certificates from a third-party vendor with an Application Load Balancer. Configure the same certificates on the Amazon EC2 instances</p>",
          "<p>Ensure that the database client software uses a TLS connection to Amazon RDS. Enable encryption of the Amazon RDS DB instance</p>",
          "<p>Enable encryption on the Amazon Elastic Block Store (Amazon EBS) volumes that support the Amazon EC2 instances</p>",
        ],
      },
      correct_response: ["a", "e", "f"],
      section: "Management and Security Governance",
      question_plain:
        "A company uses Amazon EC2 instances (fronted by an Application Load Balancer) with Amazon RDS MySQL as the database. Now, the company wants to store sensitive client data and needs to follow strict security and compliance guidelines. Data must be end-to-end secured while in-transit, as well as, at-rest. The company needs a solution that can implement strict security guidelines while keeping the cost and operational overhead to a minimum.\n\nWhich combination of steps will meet all the requirements? (Select three)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960178,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has meticulously strengthened its AWS Cloud security solution to detect and respond to the organization’s security requirements by using AWS Firewall Manager, Amazon Inspector, and AWS Shield Advanced services in its AWS accounts. The company has recently added the Amazon Macie data security service to discover and help protect sensitive data. The company wants to implement a solution (using data from these security services) that can initiate alerts if a DDoS attack happens on the company's AWS resources.</p>\n\n<p>Which solution will implement this requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create an Amazon CloudWatch alarm that monitors AWS Shield Advanced CloudWatch metrics for an active DDoS event</strong></p>\n\n<p>AWS Shield Advanced reports metrics to Amazon CloudWatch on an AWS resource more frequently during DDoS events than when no events are underway. Shield Advanced reports metrics once a minute during an event, and then once right after the event ends. While no events are underway, Shield Advanced reports metrics once a day, at a time assigned to the resource. This periodic report keeps the metrics active and available for use in custom CloudWatch alarms.</p>\n\n<p>AWS Shield Advanced provides a few detection metrics and dimensions in the <code>AWS/DDoSProtection</code> namespace. <code>DDoSDetected</code> metric indicates whether a DDoS event is underway for a particular Amazon Resource Name (ARN).\nThis metric has a non-zero value during an event.</p>\n\n<p>Detection metrics provided by AWS Shield Advanced:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q39-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/waf/latest/developerguide/monitoring-cloudwatch.html">https://docs.aws.amazon.com/waf/latest/developerguide/monitoring-cloudwatch.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon CloudWatch alarm that monitors AWS Firewall Manager CloudWatch metrics for an active DDoS event</strong> - AWS Firewall Manager doesn\'t record metrics, so you can\'t create Amazon CloudWatch alarms specifically for Firewall Manager.</p>\n\n<p><strong>Create an Amazon CloudWatch alarm that monitors Amazon Inspector logs for vulnerabilities related to an active DDoS event</strong> - Amazon Inspector automatically discovers workloads, such as Amazon EC2 instances, containers, and AWS Lambda functions, and scans them for software vulnerabilities and unintended network exposure. Amazon Inspector cannot be used for tracking an active DDoS event.</p>\n\n<p><strong>Create an Amazon CloudWatch alarm that monitors AWS Web Application Firewall (AWS WAF) for an active DDoS event</strong> -  This option acts as a distractor. AWS WAF is not being used as a security service in the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/monitoring-cloudwatch.html">https://docs.aws.amazon.com/waf/latest/developerguide/monitoring-cloudwatch.html</a></p>\n',
        answers: [
          "<p>Create an Amazon CloudWatch alarm that monitors Amazon Inspector logs for vulnerabilities related to an active DDoS event</p>",
          "<p>Create an Amazon CloudWatch alarm that monitors AWS Shield Advanced CloudWatch metrics for an active DDoS event</p>",
          "<p>Create an Amazon CloudWatch alarm that monitors AWS Firewall Manager CloudWatch metrics for an active DDoS event</p>",
          "<p>Create an Amazon CloudWatch alarm that monitors AWS Web Application Firewall (AWS WAF) for an active DDoS event</p>",
        ],
      },
      correct_response: ["b"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A company has meticulously strengthened its AWS Cloud security solution to detect and respond to the organization’s security requirements by using AWS Firewall Manager, Amazon Inspector, and AWS Shield Advanced services in its AWS accounts. The company has recently added the Amazon Macie data security service to discover and help protect sensitive data. The company wants to implement a solution (using data from these security services) that can initiate alerts if a DDoS attack happens on the company's AWS resources.\n\nWhich solution will implement this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960180,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has migrated most of its legacy applications to AWS Cloud. Compliance guidelines mandate that the company must keep its data center on-premises and it must implement IPsec encryption for all network communications outside the premises. The workloads are sensitive to network latency vis-a-vis the data center.</p>\n\n<p>Which of the following would you suggest to implement a solution for AWS applications to connect to the data center?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Combine AWS Direct Connect with AWS Site-to-Site VPN</strong></p>\n\n<p>AWS Direct Connect does not encrypt your traffic that is in transit by default. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service.</p>\n\n<p>With AWS Direct Connect and AWS Site-to-Site VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.</p>\n\n<p>AWS Direct Connect + AWS Site-to-Site VPN:\n<img src="https://docs.aws.amazon.com/images/whitepapers/latest/aws-vpc-connectivity-options/images/aws-direct-connect-and-aws-site-to-site-vpn.png">\nvia - <a href="https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Direct Connect, as it encrypts in-transit traffic, by default</strong> - AWS Direct Connect does not encrypt your traffic that is in transit by default. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service.</p>\n\n<p><strong>Use AWS PrivateLink to establish a private connection between the AWS cloud and the on-premises data center</strong> - AWS PrivateLink provides private connectivity between virtual private clouds (VPCs), supported AWS services, and your on-premises networks without exposing your traffic to the public internet. Interface VPC endpoints, powered by PrivateLink, connect you to services hosted by AWS Partners and supported solutions available in AWS Marketplace.</p>\n\n<p><strong>Use VPC peering that supports MACsec to encrypt data from the on-premises data center to the AWS cloud</strong> - VPC peering cannot be used to connect the AWS cloud to the on-premises network.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html">https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html</a></p>\n',
        answers: [
          "<p>Use AWS Direct Connect, as it encrypts in-transit traffic, by default</p>",
          "<p>Use AWS PrivateLink to establish a private connection between the AWS cloud and the on-premises data center</p>",
          "<p>Combine AWS Direct Connect with AWS Site-to-Site VPN</p>",
          "<p>Use VPC peering that supports MACsec to encrypt data from the on-premises data center to the AWS cloud</p>",
        ],
      },
      correct_response: ["c"],
      section: "Management and Security Governance",
      question_plain:
        "A company has migrated most of its legacy applications to AWS Cloud. Compliance guidelines mandate that the company must keep its data center on-premises and it must implement IPsec encryption for all network communications outside the premises. The workloads are sensitive to network latency vis-a-vis the data center.\n\nWhich of the following would you suggest to implement a solution for AWS applications to connect to the data center?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960182,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company maintains independent AWS accounts for its departments. For a specific requirement, a user in the Finance account needs full access to an Amazon S3 bucket in the Audit account. The security administrator has attached the necessary IAM permissions to the user of the Finance account. But, the user still has no access to the S3 bucket.</p>\n\n<p>Which additional configuration is needed for the given requirement?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Create an S3 bucket policy in the Audit account that allows access to the S3 bucket for the user from the Finance account</strong></p>\n\n<p>Depending on the type of access that you want to provide, use one of the following solutions to grant cross-account access to objects:</p>\n\n<ol>\n<li><p>AWS Identity and Access Management (IAM) policies and resource-based bucket policies for programmatic-only access to S3 bucket objects.</p></li>\n<li><p>IAM policies and resource-based Access Control Lists (ACLs) for programmatic-only access to S3 bucket objects.</p></li>\n<li><p>Cross-account IAM roles for programmatic and console access to S3 bucket objects.</p></li>\n</ol>\n\n<p>While necessary permissions are needed for an IAM user or an IAM role to connect to the Amazon S3 bucket, it is not sufficient. The bucket policy of the Amazon S3 bucket should also allow access to the user or role for successful access to the data present in the S3 buckets.</p>\n\n<p>Following is an example bucket policy for Account A to grant permissions to the IAM role or user that you created in Account B. Use this bucket policy to grant a user the permissions to GetObject and PutObject for objects in a bucket owned by Account A.</p>\n\n<p>Example bucket policy for cross-account access:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q41-i1.jpg">\nvia - <a href="https://repost.aws/knowledge-center/cross-account-access-s3">https://repost.aws/knowledge-center/cross-account-access-s3</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the <code>bucket owner enforced</code> setting in the Audit account. Use Access Control Lists (ACLs) to grant cross-account access</strong> - This statement is incorrect. When the <code>bucket owner enforced</code> setting is enabled, all bucket and object ACLs are disabled. Therefore, you can\'t use ACLs to grant cross-account access. By default, all newly created buckets have the <code>bucket owner enforced</code> setting enabled.</p>\n\n<p><strong>Create an S3 bucket policy in the Finance account that allows access to the S3 bucket for the user from the Finance account</strong> - The S3 bucket is in the Audit account and hence creating the bucket policy in the Finance account does not make sense. So this option is incorrect.</p>\n\n<p><strong>Configure S3 bucket ARN as Principal for the IAM trust policy for the user</strong> - You cannot add S3 bucket ARN as a Principal in an IAM trust policy. You can specify any of the following principals in a policy: AWS account and root user, IAM roles, Role sessions, IAM users, Federated user sessions, AWS services, or All principals.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html</a></p>\n\n<p><a href="https://repost.aws/knowledge-center/cross-account-access-s3">https://repost.aws/knowledge-center/cross-account-access-s3</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html</a></p>\n',
        answers: [
          "<p>Create an S3 bucket policy in the Finance account that allows access to the S3 bucket for the user from the Finance account</p>",
          "<p>Enable the <code>bucket owner enforced</code> setting in the Audit account. Use Access Control Lists (ACLs) to grant cross-account access</p>",
          "<p>Configure S3 bucket ARN as Principal for the IAM trust policy for the user</p>",
          "<p>Create an S3 bucket policy in the Audit account that allows access to the S3 bucket for the user from the Finance account</p>",
        ],
      },
      correct_response: ["d"],
      section: "Management and Security Governance",
      question_plain:
        "A company maintains independent AWS accounts for its departments. For a specific requirement, a user in the Finance account needs full access to an Amazon S3 bucket in the Audit account. The security administrator has attached the necessary IAM permissions to the user of the Finance account. But, the user still has no access to the S3 bucket.\n\nWhich additional configuration is needed for the given requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960184,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company has recently set up AWS Organizations to get all its AWS accounts under one organization to standardize the monitoring and compliance needs of the company. The company has the following requirements:</p>\n\n<p>a) All user actions have to be logged.\nb) Based on the company's security needs, define alarms that respond to specific user actions.\nc) Send real-time alerts for the alarms raised.</p>\n\n<p>Which of the following options can be combined to create an optimal solution for the given requirements? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Implement an AWS CloudTrail trail as an organizational trail. Configure the trail to forward the trail data to an Amazon CloudWatch Logs log group</strong></p>\n\n<p><strong>In CloudWatch Logs, set a metric filter for any user action event the company needs to track. Create an Amazon CloudWatch alarm against the metric. When triggered, the alarm sends notifications to the subscribed users through an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>You can configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when specific activity occurs.</p>\n\n<ol>\n<li><p>Configure your trail to send log events to CloudWatch Logs.</p></li>\n<li><p>Define CloudWatch Logs metric filters to evaluate log events for matches in terms, phrases, or values. For example, you can monitor for ConsoleLogin events.</p></li>\n<li><p>Assign CloudWatch metrics to the metric filters.</p></li>\n<li><p>Create CloudWatch alarms that are triggered according to thresholds and time periods that you specify. You can configure alarms to send notifications when alarms are triggered so that you can take action.</p></li>\n</ol>\n\n<p>Note: Only the management account for an AWS Organizations organization can configure a CloudWatch Logs log group for an organization trail.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define an AWS Lambda function to process the logs and send messages to an Amazon Simple Queue Service (Amazon SQS) queue</strong> - Amazon SQS is ruled out since we need a near real-time notification solution and not a queue-based solution.</p>\n\n<p><strong>Implement an AWS CloudTrail trail as an organizational trail. Configure the trail to store logs in an Amazon S3 bucket</strong></p>\n\n<p><strong>Use Amazon Athena to analyze the logs and trigger a notification to an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>While it is possible to configure AWS CloudTrail to send trail logs to an Amazon S3 bucket and then use Amazon Athena to analyze the logs and trigger SNS notification, this is not an optimal solution for the given use case. As mentioned earlier, CloudWatch metrics and alarms can get the job done more optimally.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/monitor-cloudtrail-log-files-with-cloudwatch-logs.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/monitor-cloudtrail-log-files-with-cloudwatch-logs.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html#cloudtrail-aws-service-specific-topics-integrations">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html#cloudtrail-aws-service-specific-topics-integrations</a></p>\n\n<p><a href="https://docs.aws.amazon.com/sns/latest/dg/sns-event-sources.html">https://docs.aws.amazon.com/sns/latest/dg/sns-event-sources.html</a></p>\n',
        answers: [
          "<p>Define an AWS Lambda function to process the logs and send messages to an Amazon Simple Queue Service (Amazon SQS) queue</p>",
          "<p>Implement an AWS CloudTrail trail as an organizational trail. Configure the trail to forward the trail data to an Amazon CloudWatch Logs log group</p>",
          "<p>In CloudWatch Logs, set a metric filter for any user action event the company needs to track. Create an Amazon CloudWatch alarm against the metric. When triggered, the alarm sends notifications to the subscribed users through an Amazon Simple Notification Service (Amazon SNS) topic</p>",
          "<p>Implement an AWS CloudTrail trail as an organizational trail. Configure the trail to store logs in an Amazon S3 bucket</p>",
          "<p>Use Amazon Athena to analyze the logs and trigger notification to an Amazon Simple Notification Service (Amazon SNS) topic</p>",
        ],
      },
      correct_response: ["b", "c"],
      section: "Security Logging and Monitoring",
      question_plain:
        "A company has recently set up AWS Organizations to get all its AWS accounts under one organization to standardize the monitoring and compliance needs of the company. The company has the following requirements:\n\na) All user actions have to be logged.\nb) Based on the company's security needs, define alarms that respond to specific user actions.\nc) Send real-time alerts for the alarms raised.\n\nWhich of the following options can be combined to create an optimal solution for the given requirements? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960186,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company has created trails in CloudTrail for all its AWS accounts as a security best practice. Recently, the company's security team has highlighted increased user login failures for a particular AWS account and asked for an immediate fix. The solution should send notifications to the concerned manager if a user login fails for three consecutive attempts within a span of five minutes.</p>\n\n<p>As an AWS Certified Security Specialist, how will you implement a solution for this requirement? (Select two)</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Correct options:</p>\n\n<p><strong>Configure AWS CloudTrail to send trail events to Amazon CloudWatch Logs. Create a metric filter for the relevant log group with a filter pattern with having eventName as <code>ConsoleLogin</code> and errorMessage as <code>Failed authentication</code></strong></p>\n\n<p>CloudTrail logs any attempts to sign in to the AWS Management Console, the AWS Discussion Forums, and the AWS Support Center. All IAM user and root user sign-in events, as well as all federated user sign-in events, generate records in CloudTrail log files. You can check the AWS Management Console sign-in events from the example link below.</p>\n\n<p>Example record that shows an unsuccessful sign-in attempt:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q43-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-aws-console-sign-in-events.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-aws-console-sign-in-events.html</a></p>\n\n<p>Check the example below that showcases how a metric value can be incremented using values from log events.</p>\n\n<p>Using values in log events to increment a metric\'s value:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q43-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntaxForMetricFilters.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntaxForMetricFilters.html</a></p>\n\n<p><strong>Create a CloudWatch alarm with the <code>threshold</code> parameter set to 3 and the <code>period</code> parameter set to 5 minutes. The alarm action is a notification sent to an Amazon Simple Notification Service (Amazon SNS) topic subscribed by the concerned manager(s)</strong></p>\n\n<p>You can use an alarm to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time period and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy.</p>\n\n<p>Since three failed attempts have to be monitored, set the threshold value to 3 and time period of 5 minutes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS CloudTrail to send trail events to an Amazon CloudWatch Alarm. Create a metric filter for the relevant log group with a filter pattern having eventName as <code>ConsoleSignin</code> and errorMessage as <code>Failed authentication</code></strong> - You need to send the trail events to CloudWatch Logs and not to CloudWatch Alarm. In addition, the trail log eventName that you need to track is <code>ConsoleLogin</code> and not <code>ConsoleSignin</code>.</p>\n\n<p><strong>Create an Amazon Athena table by specifying the location of log files for querying CloudTrail logs stored on Amazon S3. Run a query via a Lambda function for eventName matching <code>ConsoleLogin</code> and for errorMessage matching <code>Failed authentication</code></strong></p>\n\n<p><strong>Create a notification action from the Lambda function to send an Amazon Simple Notification Service (Amazon SNS) notification when the query result shows up with a count of 3 or more within a span of 5 minutes</strong></p>\n\n<p>AWS CloudTrail and Amazon Athena help make it easier by combining the detailed CloudTrail log files with the power of the Athena SQL engine to easily find, analyze, and respond to changes and activities in an AWS account. AWS CloudTrail records API calls and account activities and publishes the log files to Amazon S3. Account activity is tracked as an event in the CloudTrail log file. Each event carries information such as who performed the action, when the action was done, which resources were impacted, and many more details. You can use an AWS Lambda function to initiate the query and then analyze the query results to send the notification if required.</p>\n\n<p>While these two options offer a solution for the use case, it\'s not an optimal solution, as it adds unnecessary complexity.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-aws-console-sign-in-events.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-aws-console-sign-in-events.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>\n',
        answers: [
          "<p>Configure AWS CloudTrail to send trail events to Amazon CloudWatch Alarm. Create a metric filter for the relevant log group with a filter pattern having eventName as <code>ConsoleSignin</code> and errorMessage as <code>Failed authentication</code></p>",
          "<p>Configure AWS CloudTrail to send trail events to Amazon CloudWatch Logs. Create a metric filter for the relevant log group with a filter pattern having eventName as <code>ConsoleLogin</code> and errorMessage as <code>Failed authentication</code></p>",
          "<p>Create an Amazon Athena table by specifying the location of log files for querying CloudTrail logs stored on Amazon S3. Run a query via a Lambda function for eventName matching <code>ConsoleLogin</code> and for errorMessage matching <code>Failed authentication</code></p>",
          "<p>Create a CloudWatch alarm with the <code>threshold</code> parameter set to 3 and the <code>period</code> parameter set to 5 minutes. The alarm action is a notification sent to an Amazon Simple Notification Service (Amazon SNS) topic subscribed by the concerned manager(s)</p>",
          "<p>Create a notification action from the Lambda function to send an Amazon Simple Notification Service (Amazon SNS) notification when the query result shows up with a count of 3 or more within a span of 5 minutes</p>",
        ],
      },
      correct_response: ["b", "d"],
      section: "Threat Detection and Incident Response",
      question_plain:
        "A company has created trails in CloudTrail for all its AWS accounts as a security best practice. Recently, the company's security team has highlighted increased user login failures for a particular AWS account and asked for an immediate fix. The solution should send notifications to the concerned manager if a user login fails for three consecutive attempts within a span of five minutes.\n\nAs an AWS Certified Security Specialist, how will you implement a solution for this requirement? (Select two)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 83960188,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A security engineer has deployed an AWS Config rule that detects changes to a security group and sends notifications when the rule is non-compliant. However, recent changes to the security group, which were compliant with the AWS Config rule, went unnoticed till connectivity issues were noticed by the users. Now, the company needs a solution that can initiate an alert to a specified email address when ANY changes are made to the security groups.</p>\n\n<p>What do you recommend?</p>\n",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Correct option:</p>\n\n<p><strong>Enable AWS CloudTrail and configure the trail to send the logs to Amazon CloudWatch Logs. Configure a CloudWatch metric filter for the log group with a filter pattern on all security group changes. Create a CloudWatch alarm based on the log group-metric filter to publish notifications to an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>AWS Config views a change to a security group as a compliance risk. Use AWS Config when you want to bolster your company’s compliance management. Using AWS CloudTrail and Amazon CloudWatch Logs to identify AWS API calls that could change the configurations of security groups is the right approach since changing the configuration of security groups is a potential security incident that should be identified in near real-time.</p>\n\n<p>Refer below for a similar example that uses CloudWatch Metric Filters to search and match terms, phrases, or values in CloudTrail log events and create CloudWatch Alarms for notification or remediation action:</p>\n\n<p>Analyzing CloudTrail logs in CloudWatch:\n<img src="https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q44-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/mt/analyzing-cloudtrail-in-cloudwatch/">https://aws.amazon.com/blogs/mt/analyzing-cloudtrail-in-cloudwatch/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Delete the AWS Config rule and recreate a new rule with the same name. Configure the AWS Config managed rule to detect changes to the security groups and fire notifications using Amazon Simple Notification Service (Amazon SNS) as automatic remediation to non-compliance. Grant sufficient permissions to the AWS Config rule to be able to send notifications using Amazon SNS</strong> - AWS Config will only trigger notifications if the configuration changes to the security groups result in non-compliance. However, the company needs to track all changes to the security groups, hence AWS Config is not the right service for this requirement.</p>\n\n<p><strong>Enable AWS CloudTrail and configure the trail to send the logs to an Amazon S3 bucket. Create a non-partitioned Athena table for querying CloudTrail logs directly from the CloudTrail console. Configure Amazon Athena to trigger notifications to Amazon Simple Notification Service (Amazon SNS) whenever an activity is detected on security groups</strong> - You cannot configure Athena to trigger notifications to Amazon SNS, so this option is incorrect.</p>\n\n<p><strong>Create CloudTrail Lake to aggregate information coming from all security groups into a single source. You can perform SQL queries on CloudTrail event information in the CloudTrail Lake using Amazon Athena. Configure Athena to trigger notifications to the specified emails using Amazon Simple Notification Service (Amazon SNS)</strong> - If you want to perform SQL queries on CloudTrail event information across accounts, regions, and dates, you should consider using CloudTrail Lake. You cannot configure Athena to trigger notifications to Amazon SNS, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/analyzing-cloudtrail-in-cloudwatch/">https://aws.amazon.com/blogs/mt/analyzing-cloudtrail-in-cloudwatch/</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/security/how-to-monitor-aws-account-configuration-changes-and-api-calls-to-amazon-ec2-security-groups/">https://aws.amazon.com/blogs/security/how-to-monitor-aws-account-configuration-changes-and-api-calls-to-amazon-ec2-security-groups/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html">https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html</a></p>\n',
        answers: [
          "<p>Enable AWS CloudTrail and configure the trail to send the logs to Amazon CloudWatch Logs. Configure a CloudWatch metric filter for the log group with a filter pattern on all security group changes. Create a CloudWatch alarm based on the log group-metric filter to publish notifications to an Amazon SNS topic</p>",
          "<p>Delete the AWS Config rule and recreate a new rule with the same name. Configure the AWS Config managed rule to detect changes to the security groups and fire notifications using Amazon Simple Notification Service (Amazon SNS) as automatic remediation to non-compliance. Grant sufficient permissions to the AWS Config rule to be able to send notifications using Amazon SNS</p>",
          "<p>Enable AWS CloudTrail and configure the trail to send the logs to an Amazon S3 bucket. Create a non-partitioned Athena table for querying CloudTrail logs directly from the CloudTrail console. Configure Amazon Athena to trigger notifications to Amazon Simple Notification Service (Amazon SNS) whenever an activity is detected on security groups</p>",
          "<p>Create CloudTrail Lake to aggregate information coming from all security groups into a single source. You can perform SQL queries on CloudTrail event information in the CloudTrail Lake using Amazon Athena. Configure Athena to trigger notifications to the specified emails using Amazon Simple Notification Service (Amazon SNS)</p>",
        ],
      },
      correct_response: ["a"],
      section: "Management and Security Governance",
      question_plain:
        "A security engineer has deployed an AWS Config rule that detects changes to a security group and sends notifications when the rule is non-compliant. However, recent changes to the security group, which were compliant with the AWS Config rule, went unnoticed till connectivity issues were noticed by the users. Now, the company needs a solution that can initiate an alert to a specified email address when ANY changes are made to the security groups.\n\nWhat do you recommend?",
      related_lectures: [],
    },
  ],
};
