const dojo_1 = {
  count: 75,
  next: null,
  previous: null,
  results: [
    {
      _class: "assessment",
      id: 138248099,
      assessment_type: "multiple-choice",
      prompt: {
        relatedLectureIds: "",
        explanation:
          '<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an Internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p><p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components that allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p><p>There are two types of VPC endpoints: <em>interface endpoints</em> and <em>gateway endpoints</em>. You can create the type of VPC endpoint required by the supported service. S3 and DynamoDB are using Gateway endpoints while most of the services are using Interface endpoints.</p><p>You can use an S3 bucket to store the required dependencies and then set up a VPC Endpoint to allow your EC2 instances to access the data without having to traverse the public Internet.</p><p>Hence, the correct answer is the option that says: <strong>Download all of the external application dependencies from the public Internet and then store them to an S3 bucket. Set up a VPC endpoint for the S3 bucket and then assign an IAM instance profile to the instances in order to allow them to fetch the required dependencies from the bucket.</strong></p><p>The option that says: <strong>Deploy the Amazon EC2 instances in a private subnet and associate Elastic IP addresses on each of them. Run a custom shell script to disassociate the Elastic IP addresses after the application has been successfully installed and is running properly</strong> is incorrect because it is possible that the custom shell script may fail and the disassociation of the Elastic IP addresses might not be fully implemented which will allow the EC2 instances to access the Internet.</p><p>The option that says:<strong> Use a NAT gateway to disallow any traffic to the VPC which originated from the public Internet. Deploy the Amazon EC2 instances to a private subnet then set the subnet\'s route table to use the NAT gateway as its default route</strong><em> </em>is incorrect because although a NAT Gateway can safeguard the instances from any incoming traffic that were initiated from the Internet, it still permits them to send outgoing requests externally.</p><p>The option that says: <strong>Set up a brand new security group for the Amazon EC2 instances. Use a whitelist configuration to only allow outbound traffic to the site where all of the application dependencies are hosted. Delete the security group rule once the installation is complete. Use AWS Config to monitor the compliance</strong> is incorrect because this solution has a high operational overhead since the actions are done manually. This is susceptible to human error such as in the event that the DevOps team forgets to delete the security group. The use of AWS Config will just monitor and inform you about the security violation but it won\'t do anything to remediate the issue.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-vpc/">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        question:
          "<p>An application is hosted in an Auto Scaling group of Amazon EC2 instances with public IP addresses in a public subnet. The instances are configured with a user data script which fetch and install the required system dependencies of the application from the Internet upon launch. A change was recently introduced to prohibit any Internet access from these instances to improve the security but after its implementation, the instances could not get the external dependencies anymore. Upon investigation, all instances are properly running but the hosted application is not starting up completely due to the incomplete installation. </p><p>Which of the following is the MOST secure solution to solve this issue and also ensure that the instances do not have public Internet access? </p>",
        feedbacks: ["", "", "", ""],
        answers: [
          "Download all of the external application dependencies from the public Internet and then store them to an S3 bucket. Set up a VPC endpoint for the S3 bucket and then assign an IAM instance profile to the instances in order to allow them to fetch the required dependencies from the bucket.",
          "Deploy the Amazon EC2 instances in a private subnet and associate Elastic IP addresses on each of them. Run a custom shell script to disassociate the Elastic IP addresses after the application has been successfully installed and is running properly.",
          "Use a NAT gateway to disallow any traffic to the VPC which originated from the public Internet. Deploy the Amazon EC2 instances to a private subnet then set the subnet&#39;s route table to use the NAT gateway as its default route.",
          "Set up a brand new security group for the Amazon EC2 instances. Use a whitelist configuration to only allow outbound traffic to the site where all of the application dependencies are hosted. Delete the security group rule once the installation is complete. Use AWS Config to monitor the compliance.",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "An application is hosted in an Auto Scaling group of Amazon EC2 instances with public IP addresses in a public subnet. The instances are configured with a user data script which fetch and install the required system dependencies of the application from the Internet upon launch. A change was recently introduced to prohibit any Internet access from these instances to improve the security but after its implementation, the instances could not get the external dependencies anymore. Upon investigation, all instances are properly running but the hosted application is not starting up completely due to the incomplete installation. Which of the following is the MOST secure solution to solve this issue and also ensure that the instances do not have public Internet access?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248133,
      assessment_type: "multi-select",
      prompt: {
        question:
          '<p>A development company is currently using AWS CodeBuild for automated building and testing of their application. They recently hired a DevOps engineer to review their current process as well as to provide recommendations for optimization and security. It is of utmost importance that the engineer identifies security issues and ensure that the company complies with AWS security best practices. One of their buildspec.yaml files is shown below:</p><img src="https://img-c.udemycdn.com/redactor/raw/quiz_question/2022-04-20_06-00-38-205d0552e6afa583f51d8589923e8150.png"><p>Which of the following changes should the DevOps engineer recommend? (Select TWO.)</p>',
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p><strong>AWS Systems Manager Run Command</strong> lets you remotely and securely manage the configuration of your managed instances. A <em>managed instance</em> is any Amazon EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.</p><p><img src="https://media.tutorialsdojo.com/aws_run_command_system_manager_tutorialsdojo.png"></p><p>Administrators use Run Command to perform the following types of tasks on their managed instances: install or bootstrap applications, build a deployment pipeline, capture log files when an instance is terminated from an Auto Scaling group, and join instances to a Windows domain, to name a few.</p><p>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter.</p><p>Using an IAM Role is better than storing and using your AWS access keys since this is a security risk. The same goes for your database passwords and any other credentials.</p><p><strong>Hence, the correct answers are:</strong></p><p><strong>- Configure the CodeBuild project to use an IAM Role with the required permissions and remove the AWS credentials from the buildspec.yaml file. Run scp and ssh commands using the AWS Systems Manager Run Command.</strong></p><p><strong>- Using the AWS Systems Manager Parameter Store, create a DATABASE_PASSWORD secure string parameter then remove the DATABASE_PASSWORD from the environment variables.</strong></p><p>The option that says: <strong>In the </strong><code><strong>post-build</strong></code><strong> phase of the buildspec.yaml file, add a configuration that will remove all temporary files which contain the environment variables and passwords from the container</strong> is incorrect because this solution still exposes the sensitive credentials in the buildspec.yaml file. You should use an IAM Role and store the database password in the Systems Manager Parameter Store instead.</p><p>The option that says: <strong>Store the environment variables to the </strong><code><strong>tutorialsdojo-db</strong></code><strong> S3 bucket and then enable Server Side Encryption. In the </strong><code><strong>pre_build</strong></code><strong> phase of the buildspec.yaml file, add the configuration that will download and export the environment variables </strong>is incorrect because storing sensitive passwords in Amazon S3 is a security risk, especially if the bucket was accidentally set to public.</p><p>The option that says: <strong>Hash the environment variables and passwords using a Base64 encoder to prevent other developers to see the credentials in plaintext</strong> is incorrect because hashed credentials could be reversed to their original plaintext form. Using the Systems Manager Parameter Store feature is still the best way to store your database credentials.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html</a></p><p><a href="https://aws.amazon.com/systems-manager/">https://aws.amazon.com/systems-manager/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>In the <code>post-build</code> phase of the buildspec.yaml file, add a configuration that will remove all temporary files which contain the environment variables and passwords from the container.</p>",
          "<p>Configure the CodeBuild project to use an IAM Role with the required permissions and remove the AWS credentials from the buildspec.yaml file. Run scp and ssh commands using the AWS Systems Manager Run Command.</p>",
          "Using the AWS Systems Manager Parameter Store, create a DATABASE_PASSWORD secure string parameter then remove the DATABASE_PASSWORD from the environment variables.",
          "<p>Store the environment variables to the <code>tutorialsdojo-db</code> S3 bucket and then enable Server Side Encryption. In the <code>pre_build</code> phase of the buildspec.yaml file, add the configuration that will download and export the environment variables.</p>",
          "Hash the environment variables and passwords using a Base64 encoder to prevent other developers from seeing the credentials in plaintext.",
        ],
      },
      correct_response: ["b", "c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A development company is currently using AWS CodeBuild for automated building and testing of their application. They recently hired a DevOps engineer to review their current process as well as to provide recommendations for optimization and security. It is of utmost importance that the engineer identifies security issues and ensure that the company complies with AWS security best practices. One of their buildspec.yaml files is shown below:Which of the following changes should the DevOps engineer recommend? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248135,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          '<p>A startup recently hired a replacement for the DevOps engineer who abruptly resigned from the position. Due to the lack of time for a proper handover, the new hire has to start from scratch and understand the current configurations made in the client\'s AWS account. Upon review, there is an existing EventBridge rule with a custom event pattern, as shown below:</p>\n<img src="https://img-c.udemycdn.com/redactor/raw/practice_test_question/2025-01-30_03-54-01-bcda99bfefdc5a1907776c8a41e6222f.png">What does this EventBridge rule do?',
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Monitoring is an important part of maintaining the reliability, availability, and performance of AWS CodePipeline. You should collect monitoring data from all parts of your AWS solution so that you can more easily debug a multi-point failure if one occurs.</p><p>You can use the following tools to monitor your CodePipeline pipelines and their resources:</p><p><strong>Amazon EventBridge (Amazon CloudWatch Events)</strong> — Use Amazon EventBridge to detect and react to pipeline execution state changes (for example, send an Amazon SNS notification or invoke a Lambda function).</p><p><strong>AWS CloudTrail</strong> — Use CloudTrail to capture API calls made by or on behalf of CodePipeline in your AWS account and deliver the log files to an Amazon S3 bucket. You can choose to have CloudWatch publish Amazon SNS notifications when new log files are delivered so you can take quick action.</p><p><strong>Console and CLI</strong> — You can use the CodePipeline console and CLI to view details about the status of a pipeline or a particular pipeline execution.</p><p><strong><img src="https://media.tutorialsdojo.com/public/td-awscodepipeline-7Jan2025.png"></strong></p><p><strong>Amazon EventBridge</strong> is a web service that monitors your AWS resources and the applications you run on AWS. You can use Amazon EventBridge to detect and react to changes in the state of a pipeline, stage, or action. Then, based on the rules you create, the EventBridge rules invoke one or more target actions when a pipeline, stage, or action enters the state you specify in a rule. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions.</p><p>You can configure notifications to be sent when the state changes for:</p><p>- Specified pipelines or all your pipelines. You control this by using <code>"detail-type":</code> <code>"CodePipeline <strong>Pipeline</strong> Execution State Change"</code>.</p><p>- Specified stages or all your stages, within a specified pipeline or all your pipelines. You control this by using <code>"detail-type":</code> <code>"CodePipeline <strong>Stage</strong> Execution State Change"</code>.</p><p>- Specified actions or all actions, within a specified stage or all stages, within a specified pipeline or all your pipelines. You control this by using <code>"detail-type":</code> <code>"CodePipeline <strong>Action</strong> Execution State Change"</code>.</p><p><br></p><p>Hence, the correct answer is: <strong>It will capture all rejected or failed approval actions across all the pipelines in AWS CodePipeline and send a notification.</strong></p><p>The option that says: <strong>It will capture all manual approval actions across all the pipelines in AWS CodePipeline and send a notification</strong> is incorrect because this will just capture all rejected or failed approval actions, excluding the successful ones.</p><p>The option that says: <strong>It will capture all pipelines with a </strong><code><strong>FAILED</strong></code><strong> state in AWS CodePipeline and send a notification</strong> is incorrect because the custom event pattern only tracks the changes of the specific "Actions" and not the entire "State" of the pipeline.</p><p>The option that says: <strong>It will capture all rejected or failed build actions across all the pipelines in AWS CodePipeline and send a notification</strong> is incorrect because the indicated source is <em>aws.codepipeline</em> and not <em>aws.codebuild</em> which means that this rule simply tracks the failed or rejected approval actions across all the pipelines, and not the status of the build actions.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring.html</a></p><p><br></p><p><strong>Check out these AWS CodePipeline and Amazon EventBridge Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-eventbridge/?src=udemy">https://tutorialsdojo.com/amazon-eventbridge/</a></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p>',
        answers: [
          "It will capture all rejected or failed approval actions across all the pipelines in AWS CodePipeline and send a notification.",
          "It will capture all manual approval actions across all the pipelines in AWS CodePipeline and send a notification.",
          "<p>It will capture all pipelines with a <code>FAILED</code> state in AWS CodePipeline and send a notification.</p>",
          "It will capture all rejected or failed build actions across all the pipelines in AWS CodePipeline and send a notification.",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "A startup recently hired a replacement for the DevOps engineer who abruptly resigned from the position. Due to the lack of time for a proper handover, the new hire has to start from scratch and understand the current configurations made in the client's AWS account. Upon review, there is an existing EventBridge rule with a custom event pattern, as shown below:\nWhat does this EventBridge rule do?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248137,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading software development company has various web applications hosted in an Auto Scaling group of Amazon EC2 instances which are designed for high availability and fault tolerance. The company is using AWS CloudFormation to easily manage its cloud infrastructure as code as well as for deployment. Currently, it has to manually update its CloudFormation templates for every new available AMI of its application. This procedure is prone to human errors and entails a high management overhead on its deployment process.</p><p>Which of the following is the MOST suitable and cost-effective solution that the DevOps engineer should implement to automate this process?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. For example, you might want to include resources that aren\'t available as AWS CloudFormation resource types. You can include those resources by using custom resources. That way, you can still manage all your related resources in a single stack.</p><p>Use the <strong><em>AWS::CloudFormation::CustomResource</em></strong> or, alternatively, the <strong><em>Custom::&lt;User-Defined Resource Name&gt;</em></strong><em> </em>resource type to define custom resources in your templates. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic.</p><p><img src="https://media.tutorialsdojo.com/public/CloudFormation%20AMIManager%20Flow%20-%20Create.png">When you associate a Lambda function with a custom resource, the function is invoked whenever the custom resource is created, updated, or deleted. AWS CloudFormation calls a Lambda API to invoke the function and to pass all the request data (such as the request type and resource properties) to the function. The power and customizability of Lambda functions, in combination with AWS CloudFormation, enable a wide range of scenarios, such as dynamically looking up AMI IDs during stack creation or implementing and using utility functions, such as string reversal functions.</p><p>AWS CloudFormation templates that declare an Amazon Elastic Compute Cloud (Amazon EC2) instance must also specify an Amazon Machine Image (AMI) ID, which includes an operating system and other software and configuration information used to launch the instance. The correct AMI ID depends on the instance type and region in which you\'re launching your stack. And IDs can change regularly, such as when an AMI is updated with software updates.</p><p>Normally, you might map AMI IDs to specific instance types and regions. To update the IDs, you manually change them in each of your templates. By using custom resources and AWS Lambda (Lambda), you can create a function that gets the IDs of the latest AMIs for the region and instance type that you\'re using so that you don\'t have to maintain mappings.</p><p>Hence, the correct answer is:<strong> Pull the new AMI IDs using an AWS Lambda-backed custom resource in the CloudFormation template. Reference the AMI ID that the custom resource fetched in the launch template resource block.</strong></p><p>The option that says: <strong>Configure the CloudFormation template to use AMI mappings. Integrate AWS Lambda and Amazon EventBridge to create a function that regularly runs every hour to detect new AMIs as well as update the mapping in the template. Reference the AMI mappings in the launch template resource block</strong> is incorrect. Although this solution may work, it is not economical to set up a scheduled job that runs every 1 hour just to detect new AMIs and update your CloudFormation templates. This is an inefficient solution since the AMIs are not updated that often to begin with, which means that most of the hourly processing done by the Lambda function will yield no result. A better design would be to use AWS Lambda-backed custom resources instead in CloudFormation, which will fetch the new AMI IDs upon deployment.</p><p>The option that says: <strong>Configure the CloudFormation template to use conditional statements to check if new AMIs are available. Fetch the new AMI ID using the </strong><code><strong>cfn-init</strong></code><strong> helper script and reference it in the launch template resource block<em> </em></strong>is incorrect because a cfn-init helper script is primarily used to fetch metadata, install packages and start/stop services to your EC2 instances that are already running. A better solution to implement here is to use AWS Lambda-backed custom resource in the CloudFormation template to pull the new AMI IDs.</p><p>The option that says: <strong>Launch an EC2 instance to run a custom shell script every hour to check for new AMIs. The script should update the launch template resource block of the CloudFormation template with the new AMI ID if there are new ones available<em> </em></strong>is incorrect. Although this solution may work, it includes the unnecessary cost of running an EC2 instance which is charged 24/7 but only does the actual processing every hour. This can simply be replaced by using an AWS Lambda-backed custom resource in CloudFormation.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/">https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p>',
        answers: [
          "<p>Configure the CloudFormation template to use AMI mappings. Integrate AWS Lambda and Amazon EventBridge to create a function that regularly runs every hour to detect new AMIs as well as update the mapping in the template. Reference the AMI mappings in the launch template resource block.</p>",
          "<p>Configure the CloudFormation template to use conditional statements to check if new AMIs are available. Fetch the new AMI ID using the <code>cfn-init</code> helper script and reference it in the launch template resource block.</p>",
          "<p>Pull the new AMI IDs using an AWS Lambda-backed custom resource in the CloudFormation template. Reference the AMI ID that the custom resource fetched in the launch template resource block.</p>",
          "<p>Launch an EC2 instance to run a custom shell script every hour to check for new AMIs. The script should update the launch template resource block of the CloudFormation template with the new AMI ID if there are new ones available.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A leading software development company has various web applications hosted in an Auto Scaling group of Amazon EC2 instances which are designed for high availability and fault tolerance. The company is using AWS CloudFormation to easily manage its cloud infrastructure as code as well as for deployment. Currently, it has to manually update its CloudFormation templates for every new available AMI of its application. This procedure is prone to human errors and entails a high management overhead on its deployment process.Which of the following is the MOST suitable and cost-effective solution that the DevOps engineer should implement to automate this process?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248139,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A global IT consulting company has a multi-tier enterprise resource planning application which is hosted in AWS. It runs on an Auto Scaling group of Amazon EC2 instances across multiple Availability Zones behind an Application Load Balancer. For its database tier, all of its data is persisted in an Amazon RDS MySQL database running in a Multi-AZ deployments configuration. All of the static content of the application is durably stored in Amazon S3. The company is already using AWS CloudFormation templates for managing and deploying its AWS resources. Few weeks ago, the company failed an IT audit due to its application’s long recovery time and excessive data loss in a simulated disaster recovery scenario drill.</p><p>How should the DevOps Engineer implement a multi-region disaster recovery plan which has the LOWEST recovery time and the LEAST data loss?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon RDS Read Replicas</strong> provide enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle as well as Amazon Aurora.</p><p>Read replicas in Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle provide a complementary availability mechanism to Amazon RDS Multi-AZ Deployments. You can promote a read replica if the source DB instance fails. You can also replicate DB instances across AWS Regions as part of your disaster recovery strategy, which is not available with Multi-AZ Deployments since this is only applicable in a single AWS Region. This functionality complements the synchronous replication, automatic failure detection, and failover provided with Multi-AZ deployments.</p><p><img src="https://media.tutorialsdojo.com/public/2019-11-07_22-42-29-e6f64cabd382181b6d81c7d67fead25c.png"></p><p>When you copy a snapshot to an AWS Region that is different from the source snapshot\'s AWS Region, the first copy is a full snapshot copy, even if you copy an incremental snapshot. A full snapshot copy contains all of the data and metadata required to restore the DB instance. After the first snapshot copy, you can copy incremental snapshots of the same DB instance to the same destination region within the same AWS account.</p><p>Depending on the AWS Regions involved and the amount of data to be copied, <em>a cross-region snapshot copy can take hours to complete.</em> In some cases, there might be a large number of cross-region snapshot copy requests from a given source AWS Region. In these cases, Amazon RDS might put new cross-region copy requests from that source AWS Region into a queue until some in-progress copies complete. No progress information is displayed about copy requests while they are in the queue. Progress information is displayed when the copy starts.</p><p>This means that a cross-region snapshot doesn\'t provide a high RPO compared with a Read Replica since the snapshot takes significant time to complete. Although this is better than Multi-AZ deployments since you can replicate your database across AWS Regions, using a Read Replica is still the best choice for providing a high RTO and RPO for disaster recovery.</p><p>Hence, the correct answer is: <strong>Launch the application stack in another AWS region using the CloudFormation template. Create an RDS Read Replica in the other region then enable cross-region replication between the original Amazon S3 bucket and a new S3 bucket. Promote the RDS Read Replica as the master in the event of application failover. Increase the capacity of the Auto Scaling group using the CloudFormation stack template to improve the scalability of the application.</strong></p><p>The option that says: <strong>Launch the application stack in another AWS region using the CloudFormation template. Take a daily RDS cross-region snapshot to the other region using a scheduled job running in AWS Lambda and Amazon EventBridge. Enable cross-region replication between the original S3 bucket and Amazon Glacier. In the event of application outages, launch a new application stack in the other AWS region and restore the database from the most recent snapshot</strong> is incorrect. Although this solution may work, the use of cross-region snapshot doesn\'t provide the LOWEST recovery time and the LEAST data loss because a snapshot can take several hours to complete. The best solution to use here is to simply launch a Read Replica to another AWS Region, which asynchronously replicates the data from the source database to the other AWS Region.</p><p>The option that says:<strong> Launch the application stack in another AWS region using the CloudFormation template. Create another RDS standby DB instance in the other region then enable cross-region replication between the original S3 bucket and a new S3 bucket. The Standby DB instance will automatically be the master DB in the event of an application fail over. Increase the capacity of the Auto Scaling group using the CloudFormation stack template to improve the scalability of the application </strong>is incorrect because the scope of the Multi-AZ deployments is bound to a single AWS Region only. You cannot host your standby DB instance to another AWS Region. You should either use Read Replicas or cross-region snapshots instead.</p><p>The option that says: <strong>Launch the application stack in another AWS region using the CloudFormation template. Enable cross-region replication between the original S3 bucket and a new S3 bucket. Set up an Application Load Balancer which will distribute the traffic to the other AWS region in the event of an outage. Maintain the Multi-AZ deployments configuration of the RDS database which can ensure the availability of your data even in the event of a regional AWS outage in the primary site<em> </em></strong>is incorrect because an ELB can\'t distribute traffic to different AWS Regions, unlike Route 53. Moreover, a Multi-AZ deployments configuration can only handle an outage of one or more Availability Zones and not the entire AWS Region.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/ ">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions</a></p><p><a href="https://aws.amazon.com/rds/details/read-replicas/">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>',
        answers: [
          "<p>Launch the application stack in another AWS region using the CloudFormation template. Create an RDS Read Replica in the other region then enable cross-region replication between the original Amazon S3 bucket and a new S3 bucket. Promote the RDS Read Replica as the master in the event of application failover. Increase the capacity of the Auto Scaling group using the CloudFormation stack template to improve the scalability of the application.</p>",
          "<p>Launch the application stack in another AWS region using the CloudFormation template. Take a daily RDS cross-region snapshot to the other region using a scheduled job running in AWS Lambda and Amazon EventBridge. Enable cross-region replication between the original S3 bucket and Amazon Glacier. In the event of application outages, launch a new application stack in the other AWS region and restore the database from the most recent snapshot.</p>",
          "<p>Launch the application stack in another AWS region using the CloudFormation template. Create another RDS standby DB instance in the other region then enable cross-region replication between the original S3 bucket and a new S3 bucket. The Standby DB instance will automatically be the master DB in the event of an application fail over. Increase the capacity of the Auto Scaling group using the CloudFormation stack template to improve the scalability of the application.</p>",
          "<p>Launch the application stack in another AWS region using the CloudFormation template. Enable cross-region replication between the original S3 bucket and a new S3 bucket. Set up an Application Load Balancer which will distribute the traffic to the other AWS region in the event of an outage. Maintain the Multi-AZ deployments configuration of the RDS database which can ensure the availability of your data even in the event of a regional AWS outage in the primary site.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A global IT consulting company has a multi-tier enterprise resource planning application which is hosted in AWS. It runs on an Auto Scaling group of Amazon EC2 instances across multiple Availability Zones behind an Application Load Balancer. For its database tier, all of its data is persisted in an Amazon RDS MySQL database running in a Multi-AZ deployments configuration. All of the static content of the application is durably stored in Amazon S3. The company is already using AWS CloudFormation templates for managing and deploying its AWS resources. Few weeks ago, the company failed an IT audit due to its application’s long recovery time and excessive data loss in a simulated disaster recovery scenario drill.How should the DevOps Engineer implement a multi-region disaster recovery plan which has the LOWEST recovery time and the LEAST data loss?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248141,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A government-sponsored health service is running its web application containing information about the clinics, hospitals, medical specialists, and other medical services in the country. The organization also has a set of public web services which enable third-party companies to search medical data for its respective applications and clients. AWS Lambda functions are used for the public APIs. For its database-tier, an Amazon DynamoDB table stores all of the data with an Amazon OpenSearch Service domain, which supports the search feature and stores the indexes. A DevOps engineer has been instructed to ensure that in the event of a failed deployment, there should be no downtime and a system should be in place to prevent subsequent deployments. The service must strictly maintain full capacity during API deployment without any reduced capacity to avoid degradation of service.</p><p>How can the engineer meet the above requirements in the MOST efficient way?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Elastic Beanstalk</strong> provides several options for how <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html">deployments</a> are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it\'s an automatically scaling environment (you didn\'t specify the <code>--single</code> option), it uses rolling deployments.</p><p>With rolling deployments, Elastic Beanstalk splits the environment\'s EC2 instances into batches and deploys the new version of the application to one batch at a time, leaving the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p><p><img src="https://media.tutorialsdojo.com/public/environments-mgmt-updates-immutable.png"></p><p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.\'\'</p><p>Immutable deployments perform an <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html">immutable update</a> to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don\'t pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.</p><p>Hence, the correct answer is: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>Immutable.</strong></code></p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>All at Once</strong></code><strong><em> </em></strong>is incorrect because this policy only deploys the new version to all instances simultaneously, which means that the instances in your environment are out of service for a short time while the deployment occurs.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to</strong> <code><strong>Rolling</strong></code><em> </em>is incorrect because this policy will just deploy the new version in batches where each batch is taken out of service during the deployment phase, reducing your environment\'s capacity by the number of instances in a batch.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication</strong> is incorrect because you can\'t host a dynamic web application in Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>',
        answers: [
          "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>All at Once</code>.</p>",
          "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Rolling</code>.</p>",
          "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Immutable</code></p>",
          "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A government-sponsored health service is running its web application containing information about the clinics, hospitals, medical specialists, and other medical services in the country. The organization also has a set of public web services which enable third-party companies to search medical data for its respective applications and clients. AWS Lambda functions are used for the public APIs. For its database-tier, an Amazon DynamoDB table stores all of the data with an Amazon OpenSearch Service domain, which supports the search feature and stores the indexes. A DevOps engineer has been instructed to ensure that in the event of a failed deployment, there should be no downtime and a system should be in place to prevent subsequent deployments. The service must strictly maintain full capacity during API deployment without any reduced capacity to avoid degradation of service.How can the engineer meet the above requirements in the MOST efficient way?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248143,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>You are working as a DevOps engineer for a company that has hundreds of Amazon EC2 instances in their AWS account which runs their enterprise web applications. The company is using Slack, which is a cloud-based proprietary instant messaging platform, for updates and system alerts. There is a new requirement to create a system that should automatically send all AWS-scheduled maintenance notifications to the Slack channel of the company. This will easily notify their IT Operations team if there are any AWS-initiated changes to their EC2 instances and other resources. </p><p>Which of the following is the EASIEST method that you should implement in order to meet the above requirement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Health</strong> provides ongoing visibility into the state of your AWS resources, services, and accounts. The service gives you awareness and remediation guidance for resource performance or availability issues that affect your applications running on AWS. AWS Health provides relevant and timely information to help you manage events in progress. AWS Health also helps to be aware of and to prepare for planned activities. The service delivers alerts and notifications triggered by changes in the health of AWS resources so that you get near-instant event visibility and guidance to help accelerate troubleshooting.</p><p>All customers can use the <a href="https://phd.aws.amazon.com/phd/home#/">Personal Health Dashboard</a> (PHD), powered by the AWS Health API. The dashboard requires no setup, and it\'s ready to use for <a href="https://docs.aws.amazon.com/health/latest/ug/controlling-access.html">authenticated AWS users</a>. Additionally, <a href="https://aws.amazon.com/premiumsupport/">AWS Support</a> customers who have a Business or Enterprise support plan can use the AWS Health API to integrate with in-house and third-party systems.</p><p>You can use Amazon CloudWatch Events to detect and react to changes in the status of AWS Personal Health Dashboard (AWS Health) events. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when an event matches the values that you specify in a rule. Depending on the type of event, you can send notifications, capture event information, take corrective action, initiate events, or take other actions.</p><p><img src="https://media.tutorialsdojo.com/public/cloudwatch-monitor-health-011624.png"></p><p>Only those AWS Health events that are specific to your AWS account and resources are published to CloudWatch Events. This includes events such as EBS volume lost, EC2 instance store drive performance degraded, and all the scheduled change events. In contrast, <a href="https://status.aws.amazon.com/">Service Health Dashboard</a> events provide information about the regional availability of service and are not specific to AWS accounts, so they are not published to CloudWatch Events. These event types have the word "operational" in the title in the Personal Health Dashboard.</p><p>Hence, the correct answer is: <strong>Use a combination of AWS Personal Health Dashboard and Amazon CloudWatch Events to track the AWS-initiated activities to your resources. Create an event using CloudWatch Events which can invoke an AWS Lambda function to send notifications to the company\'s Slack channel.</strong></p><p>The option that says: <strong>Use a combination of AWS Config and AWS Trusted Advisor to track any AWS-initiated changes. Create rules in AWS Config which can trigger an event that invokes an AWS Lambda function to send notifications to the company\'s Slack channel </strong>is incorrect because AWS Config is not capable of tracking any AWS-initiated changes or maintenance in your AWS resources.</p><p>The option that says: <strong>Use a combination of Amazon EC2 Events and Amazon CloudWatch to track and monitor the AWS-initiated activities to your resources. Create an alarm using CloudWatch Alarms which can invoke an AWS Lambda function to send notifications to the company\'s Slack channel </strong>is incorrect because although the use of CloudWatch is acceptable, especially CloudWatch Events, it is still invalid to use Amazon EC2 Events to track AWS-initiated activities. A better combination should be the AWS Personal Health Dashboard and Amazon CloudWatch Events.</p><p>The option that says: <strong>Use a combination of AWS Support APIs and AWS CloudTrail to track and monitor the AWS-initiated activities to your resources. Create a new trail using AWS CloudTrail which can invoke an AWS Lambda function to send notifications to the company\'s Slack channel </strong>is incorrect because an integration of AWS Support API and AWS CloudTrail will not be able to properly monitor the AWS-initiated activities in your resources since CloudTrail simply tracks all of the API events of your account only but not the AWS-initiated events or maintenance.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions ">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions</a></p><p><a href="https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><a href="https://docs.aws.amazon.com/health/latest/ug/what-is-aws-health.html">https://docs.aws.amazon.com/health/latest/ug/what-is-aws-health.html</a></p><p><a href="https://aws.amazon.com/blogs/security/how-to-use-amazon-cloudwatch-events-to-monitor-application-health/">https://aws.amazon.com/blogs/security/how-to-use-amazon-cloudwatch-events-to-monitor-application-health/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p><p><br></p><p><strong>Check out this AWS Health Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-health/?src=udemy">https://tutorialsdojo.com/aws-health/</a></p>',
        answers: [
          "Use a combination of AWS Config and AWS Trusted Advisor to track any AWS-initiated changes. Create rules in AWS Config which can trigger an event that invokes an AWS Lambda function to send notifications to the company&#39;s Slack channel.",
          "Use a combination of AWS Personal Health Dashboard and Amazon CloudWatch Events to track the AWS-initiated activities to your resources. Create an event using CloudWatch Events which can invoke an AWS Lambda function to send notifications to the company&#39;s Slack channel.",
          "Use a combination of Amazon EC2 Events and Amazon CloudWatch to track and monitor the AWS-initiated activities to your resources. Create an alarm using CloudWatch Alarms which can invoke an AWS Lambda function to send notifications to the company&#39;s Slack channel.",
          "Use a combination of AWS Support APIs and AWS CloudTrail to track and monitor the AWS-initiated activities to your resources. Create a new trail using AWS CloudTrail which can invoke an AWS Lambda function to send notifications to the company&#39;s Slack channel.",
        ],
      },
      correct_response: ["b"],
      section: "Monitoring and Logging",
      question_plain:
        "You are working as a DevOps engineer for a company that has hundreds of Amazon EC2 instances in their AWS account which runs their enterprise web applications. The company is using Slack, which is a cloud-based proprietary instant messaging platform, for updates and system alerts. There is a new requirement to create a system that should automatically send all AWS-scheduled maintenance notifications to the Slack channel of the company. This will easily notify their IT Operations team if there are any AWS-initiated changes to their EC2 instances and other resources. Which of the following is the EASIEST method that you should implement in order to meet the above requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248145,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A popular e-commerce website that has customers across the globe is hosted in the us-east-1 AWS region with a backup site in the us-west-1 region. Due to an unexpected regional outage in the us-east-1 region, the company initiated its disaster recovery plan and turned on the backup site. However, the company discovered that the actual failover still entails several hours of manual effort to prepare and switch over the database. The company also noticed the database missing up to three hours of data transactions during the regional outage.</p><p>Which of the following solutions should the DevOps engineer implement to improve the RTO and RPO of the website for the cross-region failover?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>When you copy a snapshot to an AWS Region that is different from the source snapshot\'s AWS Region, the first copy is a full snapshot copy, even if you copy an incremental snapshot. A full snapshot copy contains all of the data and metadata required to restore the DB instance. After the first snapshot copy, you can copy incremental snapshots of the same DB instance to the same destination region within the same AWS account.</p><p><img src="https://media.tutorialsdojo.com/public/DBSnapshotCopy1.png"></p><p>An incremental snapshot contains only the data that has changed after the most recent snapshot of the same DB instance. Incremental snapshot copying is faster and results in lower storage costs than full snapshot copying. Incremental snapshot copying across AWS Regions is supported for both unencrypted and encrypted snapshots. For shared snapshots, copying incremental snapshots is not supported. For shared snapshots, all of the copies are full snapshots, even within the same region.</p><p>Depending on the AWS Regions involved and the amount of data to be copied, a cross-region snapshot copy can take hours to complete. In some cases, there might be a large number of cross-region snapshot copy requests from a given source AWS Region. In these cases, Amazon RDS might put new cross-region copy requests from that source AWS Region into a queue until some in-progress copies complete. No progress information is displayed about copy requests while they are in the queue. Progress information is displayed when the copy starts.</p><p>Take note that when you copy a source snapshot that is a snapshot copy, the copy isn\'t incremental because the snapshot copy doesn\'t include the required metadata for incremental copies.</p><p>Hence, the correct solution is:<strong> Use Step Functions with 2 Lambda functions that call the RDS API to create a snapshot of the database, create a cross-region snapshot copy, and restore the database instance from a snapshot in the backup region. Use Amazon EventBridge to trigger the function to take a database snapshot every hour. Set up an SNS topic that will receive published messages from AWS Health API, RDS availability, and other events that will trigger the Lambda function to create a cross-region snapshot copy. During failover, configure the Lambda function to restore the database from a snapshot in the backup region.</strong></p><p>The option that says: <strong>Configure an Amazon RDS Multi-AZ Deployment configuration and place the standby instance in the us-west-1 region. Set up the RDS option group to enable multi-region availability for native automation of cross-region recovery as well as for continuous data replication. Set up a notification system using Amazon SNS which is integrated with AWS Health API to monitor RDS-related systems events and notify the Operations team. In the actual failover where the primary database instance is down, RDS will automatically make the standby instance in the backup region as the primary instance </strong>is incorrect because the standby instance of an Amazon RDS Multi-AZ database can only be placed in the same AWS region where the primary instance is hosted. Thus, you cannot failover to the standby instance as your replacement for your primary instance in another region. A better solution would be to set up a cross-region snapshot copy from the primary to the backup region. Another solution would be to use Read Replicas since these can be placed in another AWS region.</p><p>The option that says: <strong>Create a snapshot every hour using Amazon RDS scheduled instance lifecycle events which will also allow you to monitor specific RDS events. Perform a cross-region snapshot copy into the us-west-1 backup region once the </strong><code><strong>SnapshotCreateComplete</strong></code><strong> event occurred. Create an Amazon CloudWatch Alert which will trigger an action to restore the Amazon RDS database snapshot in the backup region when the CPU Utilization metric of the RDS instance in CloudWatch falls to 0% for more than 15 minutes </strong>is incorrect because you cannot create a snapshot using the Amazon RDS scheduled instance lifecycle events.</p><p>The option that says: <strong>Use an ECS cluster to host a custom python script that calls the RDS API to create a snapshot of the database, create a cross-region snapshot copy, and restore the database instance from a snapshot in the backup region. Create a scheduled rule using Amazon EventBridge that triggers the Lambda function to snapshot a database instance every hour. Set up an SNS topic that will receive published messages about AWS-initiated RDS events from Trusted Advisor that will trigger the function to create a cross-region snapshot copy. During failover, restore the database from a snapshot in the backup region </strong>is incorrect because the AWS Trusted Advisor doesn\'t provide any information regarding AWS-initiated RDS events. You should use the AWS Health API instead. Moreover, providing an ECS cluster is unnecessary just to host a custom Python program when you can simply use Lambda functions instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions</a></p><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>',
        answers: [
          "<p>Configure an Amazon RDS Multi-AZ Deployment configuration and place the standby instance in the us-west-1 region. Set up the RDS option group to enable multi-region availability for native automation of cross-region recovery as well as for continuous data replication. Set up a notification system using Amazon SNS which is integrated with AWS Health API to monitor RDS-related systems events and notify the Operations team. In the actual failover where the primary database instance is down, RDS will automatically make the standby instance in the backup region as the primary instance.</p>",
          "<p>Use Step Functions with 2 Lambda functions that call the RDS API to create a snapshot of the database, create a cross-region snapshot copy, and restore the database instance from a snapshot in the backup region. Use Amazon EventBridge to trigger the function to take a database snapshot every hour. Set up an SNS topic that will receive published messages from AWS Health API, RDS availability, and other events that will trigger the Lambda function to create a cross-region snapshot copy. During failover, configure the Lambda function to restore the database from a snapshot in the backup region.</p>",
          "<p>Create a snapshot every hour using Amazon RDS scheduled instance lifecycle events which will also allow you to monitor specific RDS events. Perform a cross-region snapshot copy into the us-west-1 backup region once the <code>SnapshotCreateComplete</code> event occurred. Create an Amazon CloudWatch Alert which will trigger an action to restore the Amazon RDS database snapshot in the backup region when the CPU Utilization metric of the RDS instance in CloudWatch falls to 0% for more than 15 minutes.</p>",
          "<p>Use an ECS cluster to host a custom python script that calls the RDS API to create a snapshot of the database, create a cross-region snapshot copy, and restore the database instance from a snapshot in the backup region. Create a rule in Amazon EventBridge that triggers the Lambda function to take an hourly snapshot of a database instance. Set up an SNS topic that will receive published messages about AWS-initiated RDS events from Trusted Advisor that will trigger the function to create a cross-region snapshot copy. During failover, restore the database from a snapshot in the backup region.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A popular e-commerce website that has customers across the globe is hosted in the us-east-1 AWS region with a backup site in the us-west-1 region. Due to an unexpected regional outage in the us-east-1 region, the company initiated its disaster recovery plan and turned on the backup site. However, the company discovered that the actual failover still entails several hours of manual effort to prepare and switch over the database. The company also noticed the database missing up to three hours of data transactions during the regional outage.Which of the following solutions should the DevOps engineer implement to improve the RTO and RPO of the website for the cross-region failover?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248147,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A multinational investment bank is using AWS Organizations to handle its multiple AWS accounts across various AWS regions around the world. To comply with the strict financial IT regulations, the bank must ensure that all of its Amazon EBS volumes in its AWS accounts are encrypted. A DevOps engineer has been requested to set up an automated solution that will provide a detailed report of all unencrypted EBS volumes of the company as well as to notify if there is a newly launched Amazon EC2 instance which uses an unencrypted volume.</p><p>Which of the following should the DevOps engineer implement to meet this requirement with the LEAST amount of operational overhead?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.</p><p><img src="https://media.tutorialsdojo.com/public/Aggregate_Data_Landing_Page_Diagram.png">An AWS <em>resource</em> is an entity you can work with in AWS, such as an Amazon Elastic Compute Cloud (EC2) instance, an Amazon Elastic Block Store (EBS) volume, a security group, or an Amazon Virtual Private Cloud (VPC).</p><p>With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p><p>- Multiple accounts and multiple regions.</p><p>- Single account and multiple regions.</p><p>- An organization in AWS Organizations and all the accounts in that organization.</p><p>You can use an aggregator to view the resource configuration and compliance data recorded in AWS Config.</p><p>Hence, the correct answer is: <strong>Set up an AWS Config rule with a corresponding AWS Lambda function on all the target accounts of the company. Collect data from multiple accounts and AWS Regions using Config’s aggregators. Export the aggregated report to an Amazon S3 bucket then deliver the notifications using Amazon SNS.</strong></p><p>The option that says: <strong>Configure AWS CloudTrail to deliver all events to an S3 bucket in a centralized AWS account. Run a Lambda function to parse CloudTrail logs whenever logs are delivered to the S3 bucket using the S3 event notification. Use the same Lambda function to publish the results to SNS </strong>is incorrect. Although this solution may work, it certainly entails a lot of operational overhead to execute and implement. Parsing thousands of API actions from all of your accounts in CloudTrail just to ensure that the EBS encryption was enabled on all volumes could take a significant amount of time compared with just using AWS Config.</p><p>The option that says: <strong>Prepare an AWS CloudFormation template which contains a Config managed rule for EBS encryption of your EBS volumes. Deploy the template across all accounts and regions of the company using the CloudFormation stack set. Store consolidated results of the Config rules evaluation in an S3 bucket. When non-compliant EBS resources are detected, send a notification to the Operations team using SNS </strong>is incorrect. Although it is right to use AWS Config here, this solution still entails a lot of management overhead to maintain all of the CloudFormation templates. A better solution is to use AWS Config aggregators instead.</p><p>The option that says: <strong>Use the AWS Systems Manager Configuration Compliance to monitor all EBS volumes across all the accounts and AWS Regions of the company. Export and store the detailed compliance report to an S3 bucket and then deliver the notifications using SNS</strong> is incorrect. Although you can collect and aggregate data from multiple AWS accounts and Regions using the AWS Systems Manager Configuration Compliance service, this solution has a lot of prerequisites and configuration needed. You have to install SSM agent to all of your EC2 instances, create Resource Data Syncs, set up a custom compliance type to check the EBS encryption and many others. Moreover, the AWS Systems Manager Configuration Compliance service is more suitable only for verifying the patch compliance of all your resources.</p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html">https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-config/?src=udemy">https://tutorialsdojo.com/aws-config/</a></p>',
        answers: [
          "<p>Set up an AWS Config rule with a corresponding AWS Lambda function on all the target accounts of the company. Collect data from multiple accounts and AWS Regions using Config’s aggregators. Export the aggregated report to an Amazon S3 bucket then deliver the notifications using Amazon SNS.</p>",
          "<p>Configure AWS CloudTrail to deliver all events to an S3 bucket in a centralized AWS account. Run a Lambda function to parse CloudTrail logs whenever logs are delivered to the S3 bucket using the S3 event notification. Use the same Lambda function to publish the results to SNS.</p>",
          "<p>Prepare an AWS CloudFormation template which contains a Config managed rule for EBS encryption of your EBS volumes. Deploy the template across all accounts and regions of the company using the CloudFormation stack set. Store consolidated results of the Config rules evaluation in an S3 bucket. When non-compliant EBS resources are detected, send a notification to the Operations team using SNS.</p>",
          "<p>Use the AWS Systems Manager Configuration Compliance to monitor all EBS volumes across all the accounts and AWS Regions of the company. Export and store the detailed compliance report to an S3 bucket and then deliver the notifications using SNS.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A multinational investment bank is using AWS Organizations to handle its multiple AWS accounts across various AWS regions around the world. To comply with the strict financial IT regulations, the bank must ensure that all of its Amazon EBS volumes in its AWS accounts are encrypted. A DevOps engineer has been requested to set up an automated solution that will provide a detailed report of all unencrypted EBS volumes of the company as well as to notify if there is a newly launched Amazon EC2 instance which uses an unencrypted volume.Which of the following should the DevOps engineer implement to meet this requirement with the LEAST amount of operational overhead?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248149,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An enterprise resource planning application is hosted in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer which uses an Amazon DynamoDB table as its data store. For the application’s reporting module, there are five AWS Lambda functions which are reading from the DynamoDB Streams of the table to count the number of products, monitor the inventory, generate reports, move new items to an Amazon Data Firehose for analytics, and many more. The operations team discovered that in peak times, the Lambda functions are getting a stream throttling error and some of the requests fail which affects the performance of the reporting module.</p><p>Which of the following is the MOST scalable and cost-effective solution with the LEAST amount of operational overhead?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Using the Amazon Kinesis Adapter is the recommended way to consume streams from Amazon DynamoDB. The DynamoDB Streams API is intentionally similar to that of Kinesis Data Streams, a service for real-time processing of streaming data at a massive scale. In both services, data streams are composed of shards, which are containers for stream records. Both services\' APIs contain <code>ListStreams</code>, <code>DescribeStream</code>, <code>GetShards</code>, and <code>GetShardIterator</code> operations. (Although these DynamoDB Streams actions are similar to their counterparts in Kinesis Data Streams, they are not 100 percent identical.)</p><p>You can write applications for Kinesis Data Streams using the Kinesis Client Library (KCL). The KCL simplifies coding by providing useful abstractions above the low-level Kinesis Data Streams API.</p><p>As a DynamoDB Streams user, you can use the design patterns found within the KCL to process DynamoDB Streams shards and stream records. To do this, you use the DynamoDB Streams Kinesis Adapter. The Kinesis Adapter implements the Kinesis Data Streams interface so that the KCL can be used for consuming and processing records from DynamoDB Streams.</p><p><img src="https://media.tutorialsdojo.com/amazon-dynamodb-streams-console-saa-c03.png"></p><p>Amazon Managed Service for Apache Flink Studio is the easiest way to transform and analyze streaming data in real time using Apache Flink, an open-source framework and engine for processing data streams. Amazon Managed Service for Apache Flink Studio simplifies building and managing Apache Flink workloads and allows you to easily integrate applications with other AWS services.</p><p>Hence, the correct answer is: <strong>Refactor your architecture to use Amazon Kinesis Adapter for real-time processing of streaming data at a massive scale instead of directly consuming the stream using Lambda. Re-architect the reporting module to use Managed Service for Apache Flink Studio.</strong></p><p>The option that says: <strong>Delete all of the Lambda functions and move all of the processing on an Amazon ECS Cluster with Auto Scaling enabled using AWS App Runner. Set up AWS Glue to consume the DynamoDB stream which will be processed by ECS. Re-factor the reporting module to use Amazon Managed Service for Apache Flink Studio</strong> is incorrect because using an ECS Cluster is more expensive than using Lambda functions. The use of AWS Glue is not warranted and is irrelevant here since this is just a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics, and not for consuming DynamoDB streams.</p><p>The option that says: <strong>Create a new local secondary index (LSI) in the DynamoDB table to improve the performance of the queries. Double the allocated RCU of the table. Refactor the Lambda functions to directly query from the table and disable the DynamoDB streams. Increase the concurrency limits of each Lambda function to avoid throttling errors and set the </strong><code><strong>ParallelizationFactor</strong></code><strong> to 10. Re-architect the reporting module to use Managed Service for Apache Flink Studio </strong>is incorrect because doubling the allocated RCU of the table will significantly increase the cost of your architecture, and technically, this will not solve the throttling issue of the Lambda functions. The same goes for the creation of an LSI since the issue here is the consumption of the streams and not the actual DynamoDB query performance. Although increasing the concurrency limits of the Lambda function may help, this is not the most scalable solution to consume the high amount of DynamoDB Streams. The recommended way is to use an Amazon Kinesis Adapter instead in order to consume the streams at a massive scale.</p><p>The option that says: <strong>Use the Amazon CodeGuru service to optimize the codebase of the reporting module. Create a new global secondary index (GSI) in the DynamoDB table to improve the performance of the queries. Increase the allocated RCU of the table. Disable the DynamoDB streams and refactor the Lambda functions to directly query from the table. Refactor the reporting module to use Managed Service for Apache Flink Studio</strong> is incorrect because, as mentioned above, adding a GSI and increasing the allocated RCU are simply not related to this issue. In fact, querying directly from the table entails a lot of operation overhead since you have to develop the queries and maintain the indices as well as the capacity units (e.g. RCU, WCU) of your table.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.KCLAdapter.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.KCLAdapter.html</a></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><a href="https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html">https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-dynamodb/?src=udemy">https://tutorialsdojo.com/amazon-dynamodb/</a></p>',
        answers: [
          "<p>Delete all of the Lambda functions and move all of the processing on an Amazon ECS Cluster with Auto Scaling enabled using AWS App Runner. Set up AWS Glue to consume the DynamoDB stream which will be processed by ECS. Re-factor the reporting module to use Amazon Managed Service for Apache Flink Studio.</p>",
          "<p>Refactor your architecture to use Amazon Kinesis Adapter for real-time processing of streaming data at a massive scale instead of directly consuming the stream using Lambda. Re-architect the reporting module to use Managed Service for Apache Flink Studio.</p>",
          "<p>Create a new local secondary index (LSI) in the DynamoDB table to improve the performance of the queries. Double the allocated RCU of the table. Refactor the Lambda functions to directly query from the table and disable the DynamoDB streams. Increase the concurrency limits of each Lambda function to avoid throttling errors and set the <code>ParallelizationFactor</code> to 10. Re-architect the reporting module to use Managed Service for Apache Flink Studio.</p>",
          "<p>Use the Amazon CodeGuru service to optimize the codebase of the reporting module. Create a new global secondary index (GSI) in the DynamoDB table to improve the performance of the queries. Increase the allocated RCU of the table. Disable the DynamoDB streams and refactor the Lambda functions to directly query from the table. Refactor the reporting module to use Managed Service for Apache Flink Studio.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Incident and Event Response",
      question_plain:
        "An enterprise resource planning application is hosted in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer which uses an Amazon DynamoDB table as its data store. For the application’s reporting module, there are five AWS Lambda functions which are reading from the DynamoDB Streams of the table to count the number of products, monitor the inventory, generate reports, move new items to an Amazon Data Firehose for analytics, and many more. The operations team discovered that in peak times, the Lambda functions are getting a stream throttling error and some of the requests fail which affects the performance of the reporting module.Which of the following is the MOST scalable and cost-effective solution with the LEAST amount of operational overhead?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248151,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A software development company is doing an all-in migration of their on-premises resources to AWS. The company has a hybrid architecture that comprises over a thousand on-premises VMware servers and a few EC2 instances in their VPC. The company is using a VMWare vCenter Server for data center management of their vSphere environments and virtual servers. A DevOps engineer is tasked to implement a solution that will collect various information from their on-premises and EC2 instances, such as operating system details, MAC address, IP address, and many others. The Operations team should also be able to analyze the collected data in a visual format.</p><p>Which of the following is the MOST appropriate solution that the engineer should implement with the LEAST amount of effort</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Application Discovery Service</strong> helps you plan your migration to the AWS cloud by collecting usage and configuration data about your on-premises servers. Application Discovery Service is integrated with AWS Migration Hub, which simplifies your migration tracking. After performing discovery, you can view the discovered servers, group them into applications, and then track the migration status of each application from the Migration Hub console. The discovered data can be exported for analysis in Microsoft Excel or AWS analysis tools such as Amazon Athena and Amazon QuickSight.</p><p>Using Application Discovery Service APIs, you can export the system performance and utilization data for your discovered servers. You can input this data into your cost model to compute the cost of running those servers in AWS. Additionally, you can export the network connections and process data to understand the network connections that exist between servers. This will help you determine the network dependencies between servers and group them into applications for migration planning.</p><p><img src="https://media.tutorialsdojo.com/aws-migration-hub-console.png"></p><p>Application Discovery Service offers two ways of performing discovery and collecting data about your on-premises servers:</p><p><strong>- Agentless discovery</strong> can be performed by deploying the AWS Agentless Discovery Connector (OVA file) through your VMware vCenter. After the Discovery Connector is configured, it identifies virtual machines (VMs) and hosts associated with vCenter. The Discovery Connector collects the following static configuration data: Server hostnames, IP addresses, MAC addresses, and disk resource allocations. Additionally, it collects the utilization data for each VM and computes average and peak utilization for metrics such as CPU, RAM, and Disk I/O. You can export a summary of the system performance information for all the VMs associated with a given VM host and perform a cost analysis of running them in AWS.</p><p><strong>- Agent-based discovery</strong> can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers. The agent installer is available for both Windows and Linux operating systems. It collects static configuration data, detailed time-series system-performance information, inbound and outbound network connections, and processes that are running. You can export this data to perform a detailed cost analysis and to identify network connections between servers for grouping servers as applications.</p><p>The Agentless discovery uses the AWS Discovery Connector, which is a VMware appliance that can collect information only about VMware virtual machines (VMs). This mode doesn\'t require you to install a connector on each host. You install the Discovery Connector as a VM in your VMware vCenter Server environment using an Open Virtualization Archive (OVA) file. Because the Discovery Connector relies on VMware metadata to gather server information regardless of the operating system, it minimizes the time required for initial on-premises infrastructure assessment.</p><p>Hence, the correct answer is: <strong>Using the AWS Application Discovery Service, deploy the Agentless Discovery Connector in an OVA file format to your VMware vCenter and then install the AWS Discovery Agents on the EC2 instances to collect the required data. Use the AWS Migration Hub Dashboard to analyze your hybrid infrastructure.</strong></p><p>The option that says:<strong> Develop a custom python script and install them on both VMware servers and EC2 instances to collect all of the required information. Push the data to a centralized S3 bucket. Use VMware vSphere to collect the data from your on-premises resources and push the results into a file gateway in order to store the data in Amazon S3. Use Amazon Athena on the S3 bucket to analyze the data</strong> is incorrect. Although this solution may work, it takes a lot of effort to develop a custom python script as well as to manually install it to over a thousand VMWare servers on the company\'s on-premises data center.</p><p>The option that says: <strong>Register all of the on-premises virtual machines as well as the EC2 instances to AWS Service Catalog where all the required information such as the operating system details, and many others will be automatically populated. Export the consolidated data from AWS Service Catalog to an Amazon S3, bucket and then use Amazon QuickSight for analytics</strong> is incorrect because the AWS Service Catalog service doesn\'t have the capability to integrate with the on-premises VMWare servers. This service only allows organizations to create and manage catalogs of IT services that are approved for use on AWS.</p><p>The option that says: <strong>Install the AWS Systems Manager Agent (SSM Agent) on all on-premises virtual machines and the EC2 instances. Utilize the AWS Systems Manager Inventory service to provide visibility into your Amazon EC2 and on-premises computing environment. Set up an AWS Systems Manager Resource Data Sync to an S3 bucket in order to analyze the data with Amazon QuickSight </strong>is incorrect. Although the solution of using the AWS Systems Manager is valid, this is definitely not the one that can be implemented with the least amount of effort. You can use the SSM Agent to fetch all of the required information about your servers, the task of installing it to each and every on-premises VMWare server is a herculean task that entails a lot of execution time. The use of AWS Systems Manager Resource Data Sync for analyzing data is irrelevant too. Moreover, the scenario mentioned that the company is doing an all-in migration of their on-premises resources to AWS, which means that installing the SSM agent is not appropriate. A better solution would be to use the Agentless Discovery Connector of the AWS Application Discovery Service to your on-premises VMware vCenter, which can easily fetch the required information from hundreds of VMware servers.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html">https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html</a></p><p><a href="https://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-connector.html">https://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-connector.html</a></p><p><a href="https://docs.aws.amazon.com/application-discovery/latest/userguide/dashboard.html">https://docs.aws.amazon.com/application-discovery/latest/userguide/dashboard.html</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "Develop a custom python script and install them on both VMware servers and EC2 instances to collect all of the required information. Push the data to a centralized S3 bucket. Use VMware vSphere to collect the data from your on-premises resources and push the results into a file gateway in order to store the data in Amazon S3. Use Amazon Athena on the S3 bucket to analyze the data.",
          "<p>Register all of the on-premises virtual machines as well as the EC2 instances to AWS Service Catalog where all the required information such as the operating system details and many others will be automatically populated. Export the consolidated data from AWS Service Catalog to an Amazon S3 bucket and then use Amazon QuickSight for analytics.</p>",
          "<p>Install the AWS Systems Manager Agent (SSM Agent) on all on-premises virtual machines and the EC2 instances. Utilize the AWS Systems Manager Inventory service to provide visibility into your Amazon EC2 and on-premises computing environment. Set up an AWS Systems Manager Resource Data Sync to an S3 bucket in order to analyze the data with Amazon QuickSight.</p>",
          "Using the AWS Application Discovery Service, deploy the Agentless Discovery Connector in an OVA file format to your VMware vCenter and then install the AWS Discovery Agents on the EC2 instances to collect the required data. Use the AWS Migration Hub Dashboard to analyze your hybrid infrastructure.",
        ],
      },
      correct_response: ["d"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A software development company is doing an all-in migration of their on-premises resources to AWS. The company has a hybrid architecture that comprises over a thousand on-premises VMware servers and a few EC2 instances in their VPC. The company is using a VMWare vCenter Server for data center management of their vSphere environments and virtual servers. A DevOps engineer is tasked to implement a solution that will collect various information from their on-premises and EC2 instances, such as operating system details, MAC address, IP address, and many others. The Operations team should also be able to analyze the collected data in a visual format.Which of the following is the MOST appropriate solution that the engineer should implement with the LEAST amount of effort",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248153,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A government agency has a VMware-based automated server build system on its on-premises network that uses virtualization software that allows the creation of server images of the application. The DevOps Engineer was tasked to set up a system that will allow to test its server images using its on-premises server pipeline to resemble the build and behavior on Amazon EC2. In this way, the agency can verify the functionality of the application, detect incompatibility issues, and determine any prerequisites on the new Amazon Linux 2 operating system that will be used in AWS.</p><p>Which of the following solutions should the DevOps Engineer implement to accomplish this task?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>The <strong>VM Import/Export</strong> enables you to easily import virtual machine images from your existing environment to Amazon EC2 instances and export them back to your on-premises environment. This offering allows you to leverage your existing investments in the virtual machines that you have built to meet your IT security, configuration management, and compliance requirements by bringing those virtual machines into Amazon EC2 as ready-to-use instances. You can also export imported instances back to your on-premises virtualization infrastructure, allowing you to deploy workloads across your IT infrastructure.</p><p>To import your images, use the AWS CLI or other developer tools to import a virtual machine (VM) image from your VMware environment. If you use the VMware vSphere virtualization platform, you can also use the AWS Management Portal for vCenter to import your VM. As part of the import process, VM Import will convert your VM into an Amazon EC2 AMI, which you can use to run Amazon EC2 instances. Once your VM has been imported, you can take advantage of Amazon’s elasticity, scalability, and monitoring via offerings like Auto Scaling, Elastic Load Balancing, and CloudWatch to support your imported images.</p><p><img src="https://media.tutorialsdojo.com/public/vmimport-export-architecture-ami-copy.png"></p><p>You can export previously imported EC2 instances using the Amazon EC2 API tools. You simply specify the target instance, virtual machine file format and a destination S3 bucket, and VM Import/Export will automatically export the instance to the S3 bucket. You can then download and launch the exported VM within your on-premises virtualization infrastructure.</p><p>You can import Windows and Linux VMs that use VMware ESX or Workstation, Microsoft Hyper-V, and Citrix Xen virtualization formats. And you can export previously imported EC2 instances to VMware ESX, Microsoft Hyper-V or Citrix Xen formats.</p><p>Hence, the correct answer is: <strong>Launch an EC2 instance with the latest Linux OS in AWS. Use the AWS VM Import/Export service to import the EC2 image, export it to a VMware ISO in an Amazon S3 bucket, and then import the ISO to an on-premises server. Once done, commence the testing activity to verify the application\'s functionalities.</strong></p><p>The option that says: <strong>Download the latest </strong><code><strong>AmazonLinux2.iso</strong></code><strong> of the Amazon Linux 2 operating system and import it to your on-premises network. Directly launch a new on-premises server based on the imported ISO, without any virtual platform. Deploy the application, and commence testing<em> </em></strong>is incorrect because there is no way to directly download the <code>AmazonLinux2.iso</code> for Amazon Linux 2. You just have to use VM Import/Export service instead or, alternatively, run the Amazon Linux 2 as a virtual machine in your on-premises data center. Again, you won\'t be able to directly download the ISO image, but you can get the Amazon Linux 2 image for the specific virtualization platform of your choice. If you are using VMware, you can download the ESX image *.ova, and for VirtualBox, you\'ll get the *.vdi image file. What you should do first is to prepare the seed.iso boot image and then connect it to the VM of your choice on the first boot.</p><p>The option that says: <strong>Configure a hybrid cloud environment using AWS Outposts, install the Linux 2 operating system on the AWS Outposts servers, and connect them to your on-premises network. Deploy the application on the Linux 2 servers in the Outposts environment for testing</strong> is incorrect. While AWS Outposts would indeed provide access to Amazon Linux 2, it introduces an unnecessary layer of complexity and cost to what should be a straightforward testing process. The agency\'s goal is to simulate EC2 environments using their current on-premises VMware-based system, not to extend their AWS infrastructure to on-premises environments.</p><p>The option that says: <strong>Launch a new on-premises server with any distribution of Linux operating system such as CentOS, Ubuntu or Fedora since these are technically the same. Deploy the application to the server for testing </strong>is incorrect because these Linux distributions are actually different from one another. There could be some incompatibility issues between the different Linux operating systems, which is why you need to test your application on a specific Amazon Linux 2 type only.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html">https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html</a></p><p><a href="https://aws.amazon.com/ec2/vm-import/">https://aws.amazon.com/ec2/vm-import/</a></p><p><a href="https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html">https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html</a></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-linux-2-virtual-machine.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-linux-2-virtual-machine.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>',
        answers: [
          "<p>Download the latest AmazonLinux2.iso of the Amazon Linux 2 operating system and import it to your on-premises network. Directly launch a new on-premises server based on the imported ISO, without any virtual platform. Deploy the application, and commence testing.</p>",
          "<p>Configure a hybrid cloud environment using AWS Outposts, install the Linux 2 operating system on the AWS Outposts servers, and connect them to your on-premises network. Deploy the application on the Linux 2 servers in the Outposts environment for testing.</p>",
          "<p>Launch an EC2 instance with the latest Linux OS in AWS. Use the AWS VM Import/Export service to import the EC2 image, export it to a VMware ISO in an Amazon S3 bucket, and then import the ISO to an on-premises server. Once done, commence the testing activity to verify the application's functionalities.</p>",
          "Launch a new on-premises server with any distribution of Linux operating system such as CentOS, Ubuntu or Fedora since these are technically the same. Deploy the application to the server for testing.",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "A government agency has a VMware-based automated server build system on its on-premises network that uses virtualization software that allows the creation of server images of the application. The DevOps Engineer was tasked to set up a system that will allow to test its server images using its on-premises server pipeline to resemble the build and behavior on Amazon EC2. In this way, the agency can verify the functionality of the application, detect incompatibility issues, and determine any prerequisites on the new Amazon Linux 2 operating system that will be used in AWS.Which of the following solutions should the DevOps Engineer implement to accomplish this task?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248155,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is building a cost-effective solution for running extensive data processing tasks. These tasks, which cannot be containerized, must run on Amazon EC2 instances. The processing tasks are prone to interruptions and should be able to resume from the latest checkpoint. To support this, there is a need for a shared, persistent file system to store both intermediate files and checkpoint data. The company is also considering using AWS Data Migration Service (AWS DMS) for data transfer during the processing.</p><p>Which of the following options will meet the given requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Amazon EFS (Elastic File System) provides a scalable, fully managed, and shared file system that can be accessed concurrently by multiple EC2 instances, making it ideal for storing intermediate files and checkpoint data.</p><p><img alt="Amazon EFS" height="463" src="https://media.tutorialsdojo.com/public/amazon-efs-103024.png" width="1000"></p><p>EC2 Spot Instances allow you to utilize spare EC2 capacity at a reduced cost. Given that these tasks can be interrupted, Spot Instances offer a cost-effective solution. In the event of an interruption, tasks can be resumed from the latest checkpoint stored in EFS, reducing any disruption.</p><p><img alt="Spot Instances" height="464" src="https://media.tutorialsdojo.com/public/spot-instances-103024.png" width="1000"></p><p>Hence, the correct answer is: <strong>Use Amazon Elastic File System (Amazon EFS) to store intermediate files and checkpoint data, and EC2 Spot Instances to run the tasks.</strong></p><p>The option that says: <strong>Use Amazon S3 to store intermediate files and EC2 On-Demand Instances to run the tasks</strong> is incorrect because Amazon S3 is primarily designed for object storage and does not provide a shared file system interface needed for this use case. While EC2 On-Demand Instances offer consistent performance, they are more expensive and not the most cost-effective choice.</p><p>The option that says: <strong>Use GlusterFS to store intermediate files and checkpoint data, and EC2 Spot Instances to run the tasks</strong> is incorrect. Although GlusterFS can typically serve as a distributed file system, it involves more management overhead compared to managed services like EFS. EC2 Spot Instances are a cost-effective choice, but the additional complexity of managing GlusterFS reduces this option\'s overall appeal.</p><p>The option that says: <strong>Using Amazon EBS to store intermediate files and EC2 Reserved Instances to run the tasks</strong> is incorrect because Amazon EBS provides block storage attached to individual instances, not a shared file system. Additionally, EC2 Reserved Instances require a long-term commitment, making them less flexible compared to Spot Instances.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p><p><a href="https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html">https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html</a></p><p><a href="https://aws.amazon.com/blogs/hpc/cost-optimization-on-spot-instances-using-checkpoints-for-ansys-ls-dyna/">https://aws.amazon.com/blogs/hpc/cost-optimization-on-spot-instances-using-checkpoints-for-ansys-ls-dyna/</a></p><p><br></p><p><strong>Check out these Amazon EFS and Amazon EC2 Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-efs/?src=udemy">https://tutorialsdojo.com/amazon-efs/</a></p><p><a href="https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>',
        answers: [
          "<p>Use Amazon Elastic File System (Amazon EFS) to store intermediate files and checkpoint data, and EC2 Spot Instances to run the tasks.</p>",
          "<p>Use Amazon S3 to store intermediate files and EC2 On-Demand Instances to run the tasks.</p>",
          "<p>Use GlusterFS to store intermediate files and checkpoint data, and EC2 Spot Instances to run the tasks.</p>",
          "<p>Using Amazon EBS to store intermediate files and EC2 Reserved Instances to run the tasks.</p>",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "A company is building a cost-effective solution for running extensive data processing tasks. These tasks, which cannot be containerized, must run on Amazon EC2 instances. The processing tasks are prone to interruptions and should be able to resume from the latest checkpoint. To support this, there is a need for a shared, persistent file system to store both intermediate files and checkpoint data. The company is also considering using AWS Data Migration Service (AWS DMS) for data transfer during the processing.Which of the following options will meet the given requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248157,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is hosting their high-frequency trading application in AWS which serves millions of investors around the globe. The application is hosted in an Auto Scaling Group of EC2 instances behind an Application Load Balancer with an Amazon DynamoDB database. The architecture was deployed using a CloudFormation template with a Route 53 record. There recently was a production deployment that had caused system degradation and outage, costing the company a significant monetary loss due to their application's unavailability. As a result, the company instructed their DevOps engineer to implement an efficient strategy for deploying updates to their web application with the ability to perform an immediate rollback of the stack. All deployments should maintain the normal number of active EC2 instances to keep the performance of the application.</p><p>Which of the following should the DevOps engineer implement to satisfy these requirements?&nbsp; </p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>If you plan to launch an Auto Scaling group of EC2 instances, you can configure the <code>AWS::AutoScaling::AutoScalingGroup</code> resource type reference in your CloudFormation template to define an Amazon EC2 Auto Scaling group with the specified name and attributes. To configure Amazon EC2 instances launched as part of the group, you can specify a launch template. It is recommended that you use a launch template to make sure that you can use the latest features of Amazon EC2, such as T2 Unlimited instances.</p><p>You can add an UpdatePolicy attribute to your Auto Scaling group to perform rolling updates (or replace the group) when a change has been made to the group.</p><p>To specify how AWS CloudFormation handles replacement updates for an Auto Scaling group, use the <code>AutoScalingReplacingUpdate</code> policy. This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group.</p><p><img src="https://media.tutorialsdojo.com/public/2019-11-22_03-48-11-a352802536c1d852293fc176ddad72a9.png">During replacement, AWS CloudFormation retains the old group until it finishes creating the new one. If the update fails, AWS CloudFormation can roll back to the old Auto Scaling group and delete the new Auto Scaling group. While AWS CloudFormation creates the new group, it doesn\'t detach or attach any instances. After successfully creating the new Auto Scaling group, AWS CloudFormation deletes the old Auto Scaling group during the cleanup process.</p><p>When you set the <code>WillReplace</code> parameter, remember to specify a matching CreationPolicy. If the minimum number of instances (specified by the MinSuccessfulInstancesPercent property) doesn\'t signal success within the Timeout period (specified in the CreationPolicy policy), the replacement update fails, and AWS CloudFormation rolls back to the old Auto Scaling group.</p><p>Hence, the correct answer is: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy. Update the required properties for the new Auto Scaling group policy. Set the </strong><code><strong>WillReplace</strong></code><strong> property to true.</strong></p><p>The option that says: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy. Set the </strong><code><strong>WillReplace</strong></code><strong> property to false. Also, specify the </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy to update instances that are in an Auto Scaling group in batches</strong> is incorrect because if both the <code>AutoScalingReplacingUpdate</code> and <code>AutoScalingRollingUpdate</code> policies are specified, setting the <code>WillReplace</code> property to <code>true</code> gives <code>AutoScalingReplacingUpdate</code> precedence. But since this property is set to false, then the <code>AutoScalingRollingUpdate</code> policy will take precedence instead.</p><p>The option that says: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy. Update the required properties for the new Auto Scaling group policy. Set the </strong><code><strong>MinSuccessfulInstancesPercent</strong></code><strong> property, you must also enable the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> and </strong><code><strong>PauseTime</strong></code><strong> properties</strong> is incorrect because this type of deployment will affect the existing compute capacity of your application. The rolling update doesn\'t maintain the total number of active EC2 instances during deployment. A better solution is to use the <em>AutoScalingReplacingUpdate</em> policy instead, which will create a separate Auto Scaling group and is able to perform an immediate rollback of the stack in the event of an update failure.</p><p>The option that says: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::DeploymentUpdates</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy. Update the required properties for the new Auto Scaling group policy. Set the </strong><code><strong>WillReplace</strong></code><strong> property to false</strong> is incorrect because there is no <code>AWS::AutoScaling::DeploymentUpdates</code> resource. You have to use the <code>AWS::AutoScaling::AutoscalingGroup</code> resource instead and set the <code>WillReplace</code> property to true.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate</a></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p>',
        answers: [
          "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template to use the <code>AutoScalingReplacingUpdate</code> policy. Update the required properties for the new Auto Scaling group policy. Set the <code>WillReplace</code> property to true.</p>",
          "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template to use the <code>AutoScalingReplacingUpdate</code> policy. Set the <code>WillReplace</code> property to false. Also, specify the <code>AutoScalingRollingUpdate</code> policy to update instances that are in an Auto Scaling group in batches.</p>",
          "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template to use the <code>AutoScalingRollingUpdate</code> policy. Update the required properties for the new Auto Scaling group policy. Set the <code>MinSuccessfulInstancesPercent</code> property, as well as its corresponding <code>WaitOnResourceSignals</code> and <code>PauseTime</code> properties.</p>",
          "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::DeploymentUpdates</code> resource in the CloudFormation template to use the <code>AutoScalingReplacingUpdate</code> policy. Update the required properties for the new Auto Scaling group policy. Set the <code>WillReplace</code> property to false.</p>",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "A company is hosting their high-frequency trading application in AWS which serves millions of investors around the globe. The application is hosted in an Auto Scaling Group of EC2 instances behind an Application Load Balancer with an Amazon DynamoDB database. The architecture was deployed using a CloudFormation template with a Route 53 record. There recently was a production deployment that had caused system degradation and outage, costing the company a significant monetary loss due to their application's unavailability. As a result, the company instructed their DevOps engineer to implement an efficient strategy for deploying updates to their web application with the ability to perform an immediate rollback of the stack. All deployments should maintain the normal number of active EC2 instances to keep the performance of the application.Which of the following should the DevOps engineer implement to satisfy these requirements?&nbsp;",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248159,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has a suite of applications that are hosted in AWS and each app has its own AMI. Currently, a new AMI must be manually created and deployed to the server if there is a new application version. A DevOps engineer was instructed to automate the process of generating the AMIs to streamline the company's CI/CD workflow. The ID of the newly created AMI must be stored in a centralized location where other build pipelines can programmatically access it.</p><p>Which of the following is the MOST cost-effective way to accomplish this requirement with the LEAST amount of overhead?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p><br></p><p>Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machine Images (AMIs) and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for various reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img alt="Custom AMI" height="771" src="https://media.tutorialsdojo.com/public/custom_ami_1.gif" width="1000"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that the Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</strong></p><p>The option that says: <strong>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store </strong>is incorrect because manually customizing the image using an interactive shell and downloading each application image in an OVF file will simply entails a lot of effort. It is also better to use the AWS Systems Manager Automation instead of creating a new pipeline in AWS CodePipeline.</p><p>The option that says: <strong>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket </strong>is incorrect. Although you can technically generate an AMI using an EBS volume snapshot, this process is still tedious and entails a lot of configuration. Using the AWS Systems Manager Automation to generate the AMIs is a more suitable solution.</p><p>The option that says: <strong>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table </strong>is incorrect. Although this may work, this solution will only costs more to maintain than other options since it uses an EC2 instance and an Amazon DynamoDB table. There is also an associated overhead in configuring and using Packer for generating the AMIs and preparing the Jenkins pipeline.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store.</p>",
          "<p>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</p>",
          "<p>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket.</p>",
          "<p>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A company has a suite of applications that are hosted in AWS and each app has its own AMI. Currently, a new AMI must be manually created and deployed to the server if there is a new application version. A DevOps engineer was instructed to automate the process of generating the AMIs to streamline the company's CI/CD workflow. The ID of the newly created AMI must be stored in a centralized location where other build pipelines can programmatically access it.Which of the following is the MOST cost-effective way to accomplish this requirement with the LEAST amount of overhead?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248161,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An insurance firm is using AWS CloudFormation for deploying its applications in AWS. The firm has a multi-tier web application that stores financial data in an Amazon RDS MySQL database in a Multi-AZ deployments configuration. The firm instructed its DevOps Engineer to upgrade the RDS instance to the latest major version of MySQL database. It is of utmost importance to ensure minimal downtime when doing the upgrade to avoid any business disruption.</p><p>Which of the following should the engineer implement to properly upgrade the database while minimizing downtime?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>If your MySQL DB instance is currently in use with a production application, you can follow a procedure to upgrade the database version for your DB instance that can reduce the amount of downtime for your application.</p><p>Periodically, Amazon RDS performs maintenance on Amazon RDS resources. Maintenance most often involves updates to the DB instance\'s underlying hardware, underlying operating system (OS), or database engine version. Updates to the operating system most often occur for security issues and should be done as soon as possible.</p><p>Some maintenance items require that Amazon RDS take your DB instance offline for a short time. Maintenance items that require a resource to be offline include the required operating system or database patching. Required patching is automatically scheduled only for patches that are related to security and instance reliability. Such patching occurs infrequently (typically once every few months) and seldom requires more than a fraction of your maintenance window.</p><p><img src="https://media.tutorialsdojo.com/public/offlinepatchavailabledetails.png"></p><p>When you modify the database engine for your DB instance in a Multi-AZ deployment, Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.</p><p>Hence, the correct answer is: <strong>In the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource type in the CloudFormation template, update the </strong><code><strong>EngineVersion</strong></code><strong> property to the latest MySQL database version. Create a second application stack and launch a new Read Replica with the same properties as the primary database instance that will be upgraded. Finally, perform an Update Stack operation in CloudFormation</strong>.</p><p>The option that says: <strong>In the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource type in the CloudFormation template, update the </strong><code><strong>DBEngineVersion</strong></code><strong> property to the latest MySQL database version. Trigger an Update Stack operation in CloudFormation. Launch a new RDS Read Replica with the same properties as the primary database instance that will be upgraded. Finally, perform a second Update Stack operation</strong> is incorrect because this solution may possibly experience downtime since you trigger the Update Stack operation first before creating a Read Replica, which you could have used as a backup instance in the event of update failures. Remember that when you modify the database engine for your RDS Multi-AZ instance, the database engine for the entire Multi-AZ deployment is shut down during the upgrade. In addition, there is no such <em>DBEngineVersion</em> property.</p><p>The option that says: <strong>In the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource type in the CloudFormation template, update the </strong><code><strong>AutoMinorVersionUpgrade</strong></code><strong> property to the latest MySQL database version. Launch a new RDS Read Replica with the same properties as the primary database instance that will be upgraded. Finally, trigger an Update Stack operation in CloudFormation</strong> is incorrect because the AutoMinorVersionUpgrade property is simply a value that indicates whether minor engine upgrades are applied automatically to the DB instance during the maintenance window. By default, minor engine upgrades are applied automatically. You have to use the <em>EngineVersion</em> property instead.</p><p>The option that says: <strong>In the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource type in the CloudFormation template, update the </strong><code><strong>AllowMajorVersionUpgrade</strong></code><strong> property to the latest MySQL database version. Afterward, directly trigger an Update Stack operation in CloudFormation </strong>is incorrect because if the database upgrade fails, your entire system will be unavailable since you have no Read Replicas that you can use as a failover. Remember that when you modify the database engine for your DB instance in a Multi-AZ deployment configuration, Amazon RDS upgrades both the primary and secondary DB instances at the same time, which means that the RDS shuts down the whole database. The <code>AllowMajorVersionUpgrade</code> property is only a value that indicates whether major version upgrades are allowed.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html ">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime</a></p><p><br></p><p><strong>Check out these AWS CloudFormation and Amazon RDS Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p>',
        answers: [
          "<p>In the <code>AWS::RDS::DBInstance</code> resource type in the CloudFormation template, update the <code>EngineVersion</code> property to the latest MySQL database version. Create a second application stack and launch a new Read Replica with the same properties as the primary database instance that will be upgraded. Finally, perform an Update Stack operation in CloudFormation.</p>",
          "<p>In the <code>AWS::RDS::DBInstance</code> resource type in the CloudFormation template, update the <code>DBEngineVersion</code> property to the latest MySQL database version. Trigger an Update Stack operation in CloudFormation. Launch a new RDS Read Replica with the same properties as the primary database instance that will be upgraded. Finally, perform a second Update Stack operation.</p>",
          "<p>In the <code>AWS::RDS::DBInstance</code> resource type in the CloudFormation template, update the <code>AutoMinorVersionUpgrade</code> property to the latest MySQL database version. Launch a new RDS Read Replica with the same properties as the primary database instance that will be upgraded. Finally, trigger an Update Stack operation in CloudFormation.</p>",
          "<p>In the <code>AWS::RDS::DBInstance</code> resource type in the CloudFormation template, update the <code>AllowMajorVersionUpgrade</code> property to the latest MySQL database version. Afterward, directly trigger an Update Stack operation in CloudFormation.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "An insurance firm is using AWS CloudFormation for deploying its applications in AWS. The firm has a multi-tier web application that stores financial data in an Amazon RDS MySQL database in a Multi-AZ deployments configuration. The firm instructed its DevOps Engineer to upgrade the RDS instance to the latest major version of MySQL database. It is of utmost importance to ensure minimal downtime when doing the upgrade to avoid any business disruption.Which of the following should the engineer implement to properly upgrade the database while minimizing downtime?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248163,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading commercial bank has its online banking application hosted in AWS. It uses an encrypted Amazon S3 bucket to store the confidential files of its customers. The DevOps team has configured federated access to a particular Active Directory user group from the bank's on-premises network to allow access to the S3 bucket. For audit purposes, there is a new requirement to automatically detect any policy changes that are related to the restricted federated access of the bucket and to have the ability to revert any accidental changes made by the administrators.</p><p>Which of the following options provides the FASTEST way to detect configuration changes?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.</p><p>When you add a rule to your account, you can specify when you want AWS Config to run the rule; this is called a <em>trigger</em>. AWS Config evaluates your resource configurations against the rule when the trigger occurs.</p><p><img src="https://media.tutorialsdojo.com/public/2019-11-22_22-41-48-8712a29506a6524f89ee82708c97854b.png"></p><p>There are two types of triggers:</p><ol><li><p>Configuration changes</p></li><li><p>Periodic</p></li></ol><p>If you choose both <em>configuration changes</em> and <em>periodic </em>triggers, AWS Config invokes your Lambda function when it detects a configuration change and also at the frequency that you specify.</p><p><strong>Configuration changes</strong></p><p>AWS Config runs evaluations for the rule when certain types of resources are created, changed, or deleted. You choose which resources trigger the evaluation by defining the rule\'s <em>scope</em>. The scope can include the following:</p><p>- One or more resource types</p><p>- A combination of a resource type and a resource ID</p><p>- A combination of a tag key and value</p><p>- When any recorded resource is created, updated, or deleted</p><p>AWS Config runs the evaluation when it detects a change to a resource that matches the rule\'s scope. You can use the scope to constrain which resources trigger evaluations. Otherwise, evaluations are triggered when any recorded resource changes.</p><p><strong>Periodic</strong></p><p>AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p><p>Hence, the correct answer is: <strong>Set up an AWS Config rule with a configuration change trigger that will detect any changes in the S3 bucket configuration and which will also invoke an AWS Systems Manager Automation document with an AWS Lambda function that will revert any changes.</strong></p><p>The option that says: <strong>Using Amazon EventBridge, integrate an Event Bus with AWS CloudTrail API in order to trigger an AWS Lambda function that will detect and revert any particular changes</strong> is incorrect. Although you can track all changes to your configuration using CloudTrail API, it would be difficult to integrate it with EventBridge in order to monitor the changes. There is no direct way of integrating these two services and you have to create a custom mapping in order for this to work.</p><p>The option that says: <strong>Integrate Amazon EventBridge and an AWS Lambda function to create a scheduled job that runs every hour to scan the IAM policy attached to the federated access role. Configure the function to detect as well as revert any recent changes made in the current configuration</strong> is incorrect. Although this solution may work, there would be a significant delay since the Lambda function is only run every one hour. So if the new S3 bucket configuration was applied at 12:05 PM, the change will only be detected at 1:00 PM. Moreover, this entails a lot of overhead since you have to develop a custom function that will scan your IAM policies.</p><p>The option that says: <strong>Set up an AWS Config rule with a periodic trigger that runs every hour which will detect any changes in the S3 bucket configuration. Associate an AWS Lambda function in the rule that will revert any recent changes made in the bucket<em> </em></strong>is incorrect. Although this may work, it is not the fastest way of detecting a change in your resource configurations in AWS. Since the rule is using a periodic trigger, the rule will run every hour and not in near real-time, unlike the <strong><em>Configuration changes</em></strong> trigger. So, say a new configuration was applied at 12:01 PM, the change will only be detected at 1:00 PM after the rule has been run.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html</a></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_getting-started.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_getting-started.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-config/?src=udemy">https://tutorialsdojo.com/aws-config/</a></p>',
        answers: [
          "<p>Using Amazon EventBridge, integrate an Event Bus with AWS CloudTrail API in order to trigger an AWS Lambda function that will detect and revert any particular changes.</p>",
          "<p>Set up an AWS Config rule with a configuration change trigger that will detect any changes in the S3 bucket configuration and which will also invoke an AWS Systems Manager Automation document with an AWS Lambda function that will revert any changes.</p>",
          "<p>Integrate Amazon EventBridge and an AWS Lambda function to create a scheduled job that runs every hour to scan the IAM policy attached to the federated access role. Configure the function to detect as well as revert any recent changes made in the current configuration.</p>",
          "<p>Set up an AWS Config rule with a periodic trigger that runs every hour which will detect any changes in the S3 bucket configuration. Associate an AWS Lambda function in the rule that will revert any recent changes made in the bucket.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A leading commercial bank has its online banking application hosted in AWS. It uses an encrypted Amazon S3 bucket to store the confidential files of its customers. The DevOps team has configured federated access to a particular Active Directory user group from the bank's on-premises network to allow access to the S3 bucket. For audit purposes, there is a new requirement to automatically detect any policy changes that are related to the restricted federated access of the bucket and to have the ability to revert any accidental changes made by the administrators.Which of the following options provides the FASTEST way to detect configuration changes?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248165,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A software development company has a microservices architecture that consists of several AWS Lambda functions with a DynamoDB table as its data store. The current workflow of the development team is to manually deploy the new version of the Lambda function right after the QA team completed the testing. There is a new requirement to improve the workflow by automating the tests as well as the code deployments. Whenever there is a new release, the application traffic to the new versions of each microservice should be incrementally shifted over time after deployment. This will provide them the option to verify the changes to a subset of users in production and easily rollback the changes if needed.&nbsp; </p><p>Which of the following solutions will improve the velocity of the company's development workflow?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS CodeBuild</strong> is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p><p><strong>AWS CodePipeline</strong> is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.</p><p>When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application.</p><p>There are three ways traffic can shift during a deployment:</p><p><strong>Canary</strong>: Traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p><p><strong>Linear</strong>: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment.</p><p><strong>All-at-once</strong>: All traffic is shifted from the original Lambda function to the updated Lambda function version all at once.</p><p><img src="https://media.tutorialsdojo.com/public/cicd-taskcat-pipeline.9f41c290e6636774f3c92a1dad55350e14d5cf52.png">Hence, the correct answer is <strong>Set up a new pipeline in AWS CodePipeline and configure a new source code step that will automatically trigger whenever a new code is pushed to a GitHub repository. Use AWS CodeBuild for the build step to run the tests automatically then set up an AWS CodeDeploy configuration to deploy the updated Lambda function. Select the predefined </strong><code><strong>CodeDeployDefault.LambdaLinear10PercentEvery3Minutes</strong></code><strong> configuration option for deployment.</strong></p><p>The option that says: <strong>Set up a new pipeline in AWS CodePipeline and configure a post-commit hook to start the pipeline after all the automated tests have passed. Configure AWS CodeDeploy to use an </strong><code><strong>All-at-once</strong></code><strong> deployment configuration for deployments</strong> is incorrect because the All-at-once configuration will only cause all traffic to be shifted from the original Lambda function to the updated Lambda function version all at once, just as what its name implies.</p><p>The option that says: <strong>Set up an AWS CodeBuild configuration which automatically starts whenever a new code is pushed. Configure CloudFormation to trigger a pipeline in AWS CodePipeline that deploys the new Lambda version. Specify the percentage of traffic that will initially be routed to your updated Lambda function as well as the interval to deploy the code over time in the CloudFormation template<em> </em></strong>is incorrect because you can just use CodeDeploy for deployments to streamline the workflow instead of using a combination of CodePipeline and CloudFormation.</p><p>The option that says:<strong><em> </em>Develop a custom shell script that utilizes a post-commit hook to upload the latest version of the Lambda function in an S3 bucket. Set up the S3 event trigger which will invoke a Lambda function that deploys the new version. Specify the percentage of traffic that will initially be routed to your updated Lambda as well as the interval to deploy the code over time</strong> is incorrect because developing a custom shell script as well as using S3 event triggers take a lot of time and administrative effort to implement. You should use a combination of GitHub, CodeBuild, CodeDeploy, and CodePipeline instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps-lambda.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codedeploy/?src=udemy">https://tutorialsdojo.com/aws-codedeploy/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Set up a new pipeline in AWS CodePipeline and configure a post-commit hook to start the pipeline after all the automated tests have passed. Configure AWS CodeDeploy to use an <code>All-at-once</code> deployment configuration for deployments.</p>",
          "Set up an AWS CodeBuild configuration which automatically starts whenever a new code is pushed. Configure CloudFormation to trigger a pipeline in AWS CodePipeline that deploys the new Lambda version. Specify the percentage of traffic that will initially be routed to your updated Lambda function as well as the interval to deploy the code over time in the CloudFormation template.",
          "<p>Set up a new pipeline in AWS CodePipeline and configure a new source code step that will automatically trigger whenever a new code is pushed to a GitHub repository. Use AWS CodeBuild for the build step to run the tests automatically then set up an AWS CodeDeploy configuration to deploy the updated Lambda function. Select the predefined <code>CodeDeployDefault.LambdaLinear10PercentEvery3Minutes</code> configuration option for deployment.</p>",
          "Develop a custom shell script that utilizes a post-commit hook to upload the latest version of the Lambda function in an S3 bucket. Set up the S3 event trigger which will invoke a Lambda function that deploys the new version. Specify the percentage of traffic that will initially be routed to your updated Lambda as well as the interval to deploy the code over time.",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "A software development company has a microservices architecture that consists of several AWS Lambda functions with a DynamoDB table as its data store. The current workflow of the development team is to manually deploy the new version of the Lambda function right after the QA team completed the testing. There is a new requirement to improve the workflow by automating the tests as well as the code deployments. Whenever there is a new release, the application traffic to the new versions of each microservice should be incrementally shifted over time after deployment. This will provide them the option to verify the changes to a subset of users in production and easily rollback the changes if needed.&nbsp; Which of the following solutions will improve the velocity of the company's development workflow?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248167,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading telecommunications company is using CloudFormation templates to deploy enterprise applications to their production, staging, and development environments in AWS. Their current process involves manual&nbsp;changes to their CloudFormation templates in order to specify the configuration variables and static attributes for each environment. The DevOps Engineer was tasked to set up automated deployments using AWS CodePipeline and ensure that the CloudFormation template is reusable across multiple pipelines. </p><p>How should the DevOps Engineer satisfy this requirement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Continuous delivery is a release practice in which code changes are automatically built, tested, and prepared for release to production. With <strong>AWS CloudFormation</strong> and <strong>CodePipeline</strong>, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. This release process lets you rapidly and reliably make changes to your AWS infrastructure.</p><p>For example, you can create a workflow that automatically builds a test stack when you submit an updated template to a code repository. After AWS CloudFormation builds the test stack, you can test it and then decide whether to push the changes to a production stack.</p><p>You can use CodePipeline to build a continuous delivery workflow by building a pipeline for AWS CloudFormation stacks. CodePipeline has built-in integration with AWS CloudFormation, so you can specify AWS CloudFormation-specific actions, such as creating, updating, or deleting a stack within a pipeline.</p><p><img src="https://media.tutorialsdojo.com/public/product-page-diagram_CodePipeLine.7b8dd19eb6478b7f6f747d936c2f0b0b66757bbf.png"></p><p>In a CodePipeline stage, you can specify parameter overrides for AWS CloudFormation actions. Parameter overrides let you specify template parameter values that override values in a template configuration file. AWS CloudFormation provides functions to help you specify dynamic values (values that are unknown until the pipeline runs).</p><p>You can set the <code>Fn::GetArtifactAtt</code> function which retrieves the value of an attribute from an input artifact, such as the S3 bucket name where the artifact is stored. You can use this function to specify attributes of an artifact, such as its filename or S3 bucket name, that can be used in the pipeline.</p><p>Hence, the correct answer is: <strong>Launch a new pipeline using AWS CodePipeline that has multiple stages for each environment and configure it to use input parameters. Switch the associated </strong><code><strong>UserData</strong></code><strong> of the EC2 instances to match the environment where the application stack is being launched using CloudFormation mappings. Specify parameter overrides for AWS CloudFormation actions.</strong></p><p>The option that says: <strong>Set up a Lambda-backed custom resource in the CloudFormation templates. Configure the custom resource to monitor the status of the pipeline in AWS CodePipeline in order to detect which environment was launched. Use the cfn-init helper script to modify the launch template of each application stack based on its environment</strong> is incorrect because monitoring the pipeline using a custom resource in CloudFormation entails a lot of administrative overhead. A better solution would be to use input parameters or parameter overrides for AWS CloudFormation actions.</p><p>The option that says: <strong>Launch a new pipeline using AWS CodePipeline for each environment with multiple stages for each application. Trigger the CloudFormation deployments using a Lambda function to dynamically modify the </strong><code><strong>UserData</strong></code><strong> of the EC2 instances that were launched in each environment</strong> is incorrect because using a Lambda function to modify the <code><strong>UserData</strong></code> of the already running EC2 instances is not a suitable solution. The parameters should have been dynamically populated and set before the resources were launched by using parameter overrides.</p><p>The option that says: <strong>Manually configure the CloudFormation templates to use input parameters. Add a configuration that whenever the CloudFormation stack is updated, it will dynamically modify the </strong><code><strong>LaunchConfiguration</strong></code><strong> and </strong><code><strong>UserData</strong></code><strong> sections of the EC2 instances</strong> is incorrect. Although using input parameters is helpful in this scenario, you should still integrate CloudFormation and CodePipeline in order to properly map the configuration files for each environment.</p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-parameter-override-functions.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-parameter-override-functions.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p><p><a href="https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/?src=udemy">https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</a></p>',
        answers: [
          "<p>Set up a Lambda-backed custom resource in the CloudFormation templates. Configure the custom resource to monitor the status of the pipeline in AWS CodePipeline in order to detect which environment was launched. Use the cfn-init helper script to modify the launch template of each application stack based on its environment.</p>",
          "<p>Launch a new pipeline using&nbsp;AWS CodePipeline that has multiple stages for each environment and configure it to use input parameters. Switch the associated <code>UserData</code> of the EC2 instances to match the environment where the application stack is being launched using CloudFormation mappings. Specify&nbsp;parameter overrides&nbsp;for AWS CloudFormation actions.&nbsp;</p>",
          "<p>Launch a new pipeline using&nbsp;AWS CodePipeline for each environment with multiple stages for each application. Trigger the CloudFormation deployments using a Lambda function to dynamically modify the <code>UserData</code> of the EC2 instances that were launched in each environment.</p>",
          "<p>Manually configure the CloudFormation templates to use input parameters. Add a configuration that whenever the CloudFormation stack is updated, it will dynamically modify the <code>LaunchConfiguration</code> and <code>UserData</code> sections of the EC2 instances.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A leading telecommunications company is using CloudFormation templates to deploy enterprise applications to their production, staging, and development environments in AWS. Their current process involves manual&nbsp;changes to their CloudFormation templates in order to specify the configuration variables and static attributes for each environment. The DevOps Engineer was tasked to set up automated deployments using AWS CodePipeline and ensure that the CloudFormation template is reusable across multiple pipelines. How should the DevOps Engineer satisfy this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248169,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has a web application that runs on an Auto Scaling group of Amazon EC2 instances distributed across multiple Availability Zones, with traffic managed by an Application Load Balancer. Amazon RDS MySQL is used for the database tier, and incoming traffic is routed to the load balancer through Amazon Route 53. The Application Load Balancer has a health check that monitors the status of the web servers and verifies that the servers can properly access the database. For compliance purposes, management instructed the Operations team to implement a geographically isolated disaster recovery site to ensure business continuity. The required RPO is 5 minutes, while the RTO should be 2 hours.</p><p>Which of the following options requires the LEAST amount of changes to the application stack?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>When you have more than one resource performing the same function — for example, more than one HTTP server or mail server — you can configure Amazon Route 53 to check the health of your resources and respond to DNS queries using only the healthy resources. Suppose you have a website with a domain name of tutorialsdojo.com, which is hosted on six servers, two each in three data centers around the world. You can configure Amazon Route 53 to check the health of those servers and to respond to DNS queries for tutorialsdojo.com using only the servers that are currently healthy.</p><p><strong>Route 53</strong> can check the health of your resources in both simple and complex configurations:</p><p>- In simple configurations, you create a group of records that all have the same name and type, such as a group of weighted records with a type of A for tutorialsdojo.com. You then configure Route 53 to check the health of the corresponding resources. Route 53 responds to DNS queries based on the health of your resources.</p><p>- In more complex configurations, you create a tree of records that route traffic based on multiple criteria. For example, if latency for your users is your most important criterion, then you might use latency alias records to route traffic to the region that provides the best latency. The latency alias records might have weighted records in each region as the alias target. The weighted records might route traffic to EC2 instances based on the instance type. As with a simple configuration, you can configure Route 53 to route traffic based on the health of your resources.</p><p><img alt="Amazon Route 53 Evaluate Target Health" height="471" src="https://media.tutorialsdojo.com/public/td-amazon-route53-evaluate-target-health.png" width="1000"></p><p>This approach provides a disaster recovery site in a different AWS region as required. Read Replicas copy data asynchronously to the new region and can be promoted to the primary database when needed. Although promotion is manual or needs automation, it supports the required recovery point objective (RPO) of 5 minutes and recovery time objective (RTO) of 2 hours. Route 53 failover routing automatically redirects traffic to the disaster recovery site during outages with minimal changes to the application stack.</p><p>Hence, the correct answer is: <strong>Clone the application stack except for RDS in a different AWS Region. Create Read Replicas in the new region and configure the new application stack to point to the local RDS database instance. Set up a failover routing policy in Route 53 that will automatically route traffic to the new application stack in the event of an outage.</strong></p><p>The option that says: <strong>Clone the entire application stack except for its RDS database in a different Availability Zone. Create Read Replicas in another Availability Zone and configure the new stack to point to the local RDS instance. Set up a failover routing policy in Route 53 that will automatically route traffic to the new stack in the event of an outage </strong>is incorrect because this is only deployed in another Availability Zone which could also be affected by an AWS Region outage. The new stack should be deployed on a totally separate AWS Region instead.</p><p>The option that says:<strong> Clone the application stack except for RDS in a different AWS Region. Enable RDS Multi-AZ deployments configuration and deploy the standby database instance in the new region. Configure the new application stack to point to the local RDS database instance. Set up a latency routing policy in Route 53 that will automatically route traffic to the new stack in the event of an outage</strong> is incorrect because a Multi-AZ RDS database spans to several Availability Zones within a single Region only, and not to an entirely new region. You cannot deploy the standby database instance in the new AWS region.</p><p>The option that says: <strong>Configure the RDS to use Multi-AZ deployments configuration and create Read Replicas. Increase the number of application servers of the stack. Set up a latency routing policy in Route 53 that will automatically route traffic to the application servers</strong> is incorrect. Although this architecture can cope with an individual AZ outage, the systems will still be unavailable in the event of an AWS Region-wide unavailability.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p><p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><br></p><p><strong>Check out these AWS Elastic Beanstalk and Amazon Route 53 Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p><p><a href="https://tutorialsdojo.com/amazon-route-53/?src=udemy">https://tutorialsdojo.com/amazon-route-53/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "Clone the entire application stack except for its RDS database in a different Availability Zone. Create Read Replicas in another Availability Zone and configure the new stack to point to the local RDS instance. Set up a failover routing policy in Route 53 that will automatically route traffic to the new stack in the event of an outage.",
          "<p>Clone the application stack except for RDS in a different AWS Region. Enable RDS Multi-AZ deployments configuration and deploy the standby database instance in the new region. Configure the new application stack to point to the local RDS database instance. Set up a latency routing policy in Route 53 that will automatically route traffic to the new stack in the event of an outage.</p>",
          "<p>Configure the RDS to use Multi-AZ deployments configuration and create Read Replicas. Increase the number of application servers of the stack. Set up a latency routing policy in Route 53 that will automatically route traffic to the application servers.</p>",
          "<p>Clone the application stack except for RDS in a different AWS Region. Create Read Replicas in the new region and configure the new application stack to point to the local RDS database instance. Set up a failover routing policy in Route 53 that will automatically route traffic to the new application stack in the event of an outage.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A company has a web application that runs on an Auto Scaling group of Amazon EC2 instances distributed across multiple Availability Zones, with traffic managed by an Application Load Balancer. Amazon RDS MySQL is used for the database tier, and incoming traffic is routed to the load balancer through Amazon Route 53. The Application Load Balancer has a health check that monitors the status of the web servers and verifies that the servers can properly access the database. For compliance purposes, management instructed the Operations team to implement a geographically isolated disaster recovery site to ensure business continuity. The required RPO is 5 minutes, while the RTO should be 2 hours.Which of the following options requires the LEAST amount of changes to the application stack?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248171,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>An analytics web portal processes data using an Amazon EMR cluster with an Auto Scaling group of EC2 instances. After three months, AWS Trusted Advisor recommended terminating underutilized EMR instances. It was found that no scale-down parameters were set, resulting in high billing costs.</p><p>To prevent this issue, frequent notifications of AWS Trusted Advisor recommendations are required for timely action.</p><p>Which of the following solutions can help achieve this? (Select THREE.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>You can use <strong>Amazon EventBridge (Amazon CloudWatch Events)</strong> to detect and react to changes in the status of Trusted Advisor checks. Then, based on the rules that you create, EventBridge invokes one or more target actions when a check status changes to the value you specify in a rule. Depending on the type of status change, you might want to send notifications, capture status information, take corrective action, initiate events, or take other actions. You can select the following types of targets when using EventBridge as a part of your Trusted Advisor workflow:</p><p>- AWS Lambda functions</p><p>- Amazon Kinesis streams</p><p>- Amazon Simple Queue Service queues</p><p>- Built-in targets (CloudWatch alarm actions)</p><p>- Amazon Simple Notification Service topics</p><p><img src="https://media.tutorialsdojo.com/public/AWS-trusted-advisor.5b9909d5f29f680eeb12ccff536e8d88d8701304.png"></p><p>The following are some use cases:</p><p>- Use a Lambda function to pass a notification to a Slack channel when check status changes.</p><p>- Push data about checks to a Kinesis stream to support comprehensive, real-time status monitoring.</p><p>You can also configure CloudWatch Logs to send a notification whenever an alarm is triggered. Doing so enables you to respond quickly based on the logs collected by CloudWatch Logs. CloudWatch uses Amazon Simple Notification Service (SNS) to send an email.</p><p>Hence, the correct answers are:</p><p><strong>- Write a Lambda function that runs daily to refresh AWS Trusted Advisor via API and then publish a message to an SNS Topic to notify the subscribers based on the results.</strong></p><p><strong>- Write a Lambda function that runs daily to refresh AWS Trusted Advisor changes via API and send results to CloudWatch Logs. Create a CloudWatch Log metric and have it send an alarm notification when it is triggered.</strong></p><p><strong>- Create an Amazon EventBridge rule that monitors changes in the Trusted Advisor check status. Set a trigger in Amazon SNS for sending email notifications.</strong></p><p>The option that says: <strong>Create an Amazon EventBridge to monitor changes in Trusted Advisor checks. Set a trigger in Amazon SES for sending email notifications </strong>is incorrect because it is just partially correct since the integration for sending email notifications should use SNS, not SES.</p><p>The option that says: <strong>Enable the built-in Amazon EventBridge notification feature that scans changes in the Trusted Advisor check result. Send notifications automatically to the account owner</strong> is incorrect because there are simply no automatic checks on Amazon EventBridge for AWS Trusted Advisor in the first place. You need to manually configure those rules and have them trigger notifications to you.</p><p>The option that says: <strong>Enable the built-in Trusted Advisor notification feature to automatically receive notification emails, which include the summary of savings estimates along with Trusted Advisor check results </strong>is incorrect. Although this is a viable solution, the notification will be sent on a weekly basis only, which can already incur a lot of costs by the time it is notified.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html ">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p><p><a href="https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html ">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a></p><p><a href="https://docs.aws.amazon.com/lambda/latest/dg/tutorial-scheduled-events-schedule-expressions.html ">https://docs.aws.amazon.com/lambda/latest/dg/tutorial-scheduled-events-schedule-expressions.html</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/">https://tutorialsdojo.com/aws-certified-devops-engineer-professional</a></p><p><br></p><p><strong>Check out these Amazon EventBridge and AWS Trusted Advisor Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-eventbridge/?src=udemy">https://tutorialsdojo.com/amazon-eventbridge/</a></p><p><a href="https://tutorialsdojo.com/aws-trusted-advisor/?src=udemy">https://tutorialsdojo.com/aws-trusted-advisor/</a></p>',
        answers: [
          "Write a Lambda function that runs daily to refresh AWS Trusted Advisor via API and then publish a message to an SNS Topic to notify the subscribers based on the results.",
          "<p>Create an Amazon EventBridge to monitor changes in Trusted Advisor checks. Set a trigger in Amazon SES for sending email notifications.</p>",
          "Write a Lambda function that runs daily to refresh AWS Trusted Advisor changes via API and send results to CloudWatch Logs. Create a CloudWatch Log metric and have it send an alarm notification when it is triggered.",
          "<p>Create an Amazon EventBridge rule that monitors changes in the Trusted Advisor check status. Set a trigger in Amazon SNS for sending email notifications.</p>",
          "<p>Enable the built-in Amazon EventBridge notification feature that scans changes in the Trusted Advisor check result. Send notifications automatically to the account owner.</p>",
          "<p>Enable the built-in Trusted Advisor notification feature to automatically receive notification emails, which include the summary of savings estimates along with Trusted Advisor check results.</p>",
        ],
      },
      correct_response: ["a", "c", "d"],
      section: "Incident and Event Response",
      question_plain:
        "An analytics web portal processes data using an Amazon EMR cluster with an Auto Scaling group of EC2 instances. After three months, AWS Trusted Advisor recommended terminating underutilized EMR instances. It was found that no scale-down parameters were set, resulting in high billing costs.To prevent this issue, frequent notifications of AWS Trusted Advisor recommendations are required for timely action.Which of the following solutions can help achieve this? (Select THREE.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248173,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A developer has been tasked with developing a mobile news homepage that curates several news sources into one page. The app mainly comprises several AWS Lambda functions configured as a deployment group on AWS CodeDeploy. The developer must test all APIs for each new app version before fully deploying it to production. The APIs use a set of Lambda validation scripts. The goal is to check the APIs during deployments, be notified of any API errors, and implement automatic rollback if the validation fails.</p><p>Which combination of the options below can help implement a solution for this scenario? (Select THREE.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>You can use <strong>CloudWatch Alarms</strong> to track metrics on your new deployment and you can set thresholds for those metrics in your Auto Scaling groups being managed by CodeDeploy. This can invoke an action if the metric you are tracking crosses the threshold for a defined period of time. You can also monitor metrics such as instance CPU utilization, Memory utilization or custom metrics you have configured. If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance. You will also have the option to automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated. CodeDeploy will redeploy the last known working version of the application when it rolls back.</p><p>With <strong>Amazon</strong> <strong>SNS</strong>, you can create triggers that send notifications to subscribers of an Amazon SNS topic when specified events, such as success or failure events, occur in deployments and instances. CloudWatch Alarms can trigger sending out notifications to your configured SNS topic.</p><p><img alt="lifecycle-event-order-lam" height="1347" src="https://media.tutorialsdojo.com/public/lifecycle-event-order-lambda.png" width="1000"></p><p>The <code>BeforeAllowTraffic</code> and <code>AfterAllowTraffic</code> lifecycle hooks of the AppSpec.yaml file allows you to use Lambda functions to validate the new version task set using the test traffic during the deployment. For example, a Lambda function can serve traffic to the test listener and track metrics from the replacement task set. If rollbacks are configured, you can configure a CloudWatch alarm that triggers a rollback when the validation test in your Lambda function fails.</p><p>Hence, the correct answers are:</p><p><strong>- Define your Lambda validation scripts on the AppSpec lifecycle hook during deployment to run the validation using test traffic and trigger a rollback if checks fail.</strong></p><p><strong>- Associate an Amazon CloudWatch Alarm with the deployment group that can send a notification to an Amazon SNS topic when the threshold for 5xx errors is reached on CloudWatch.</strong></p><p><strong>- Configure the Lambda validation scripts to run during deployment and configure an Amazon CloudWatch Alarm that will trigger a rollback when the function validation fails.</strong></p><p>The option that says: <strong>Add a step on CodeDeploy to trigger the Lambda validation scripts after deployment and invoke them after deployment to validate the new app version<em> </em></strong>is incorrect because you only want the validation script to run before production traffic is flowing on the new app version. You can use AppSpec hooks to do this, which also includes an option to rollback when validation fails.</p><p>The option that says: <strong>Have CodeDeploy run the Lambda validations after the deployment to test with production traffic. When errors are found, have another trigger to rollback the deployment</strong> is incorrect because when the new app version is deployed to production, there\'s a possibility that clients will notice these errors. Rollback will take some time as the old version will need to be re-deployed. It is better to run the validation scripts during the deployment using the test traffic.</p><p>The option that says: <strong>Have Lambda send results to Amazon CloudWatch Alarms directly and trigger a rollback when 5xx reply errors are received during deployment </strong>is incorrect because CloudWatch Alarms simply can\'t receive direct test results from AWS Lambda. Lambda should log its test results to CloudWatch Logs, and Amazon EventBridge should monitor those logs to trigger actions, such as rolling back the deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html#deployment-steps-what-happens ">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html#deployment-steps-what-happens</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#deployments-rollback-and-redeploy-automatic-rollbacks ">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#deployments-rollback-and-redeploy-automatic-rollbacks</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p><p><br></p><p><strong>Check out these Amazon CloudWatch and Amazon SNS Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><a href="https://tutorialsdojo.com/amazon-sns/?src=udemy">https://tutorialsdojo.com/amazon-sns/</a></p>',
        answers: [
          "<p>Add a step on CodeDeploy to trigger the Lambda validation scripts after deployment and invoke them after deployment to validate the new app version.</p>",
          "<p>Have CodeDeploy run the Lambda validations after the deployment to test with production traffic. When errors are found, have another trigger to rollback the deployment.</p>",
          "Define your Lambda validation scripts on the AppSpec lifecycle hook during deployment to run the validation using test traffic and trigger a rollback if checks fail.",
          "<p>Have Lambda send results to Amazon CloudWatch Alarms directly and trigger a rollback when 5xx reply errors are received during deployment.</p>",
          "<p>Associate an Amazon CloudWatch Alarm with the deployment group that can send a notification to an Amazon SNS topic when the threshold for 5xx errors is reached on CloudWatch.</p>",
          "<p>Configure the Lambda validation scripts to run during deployment and configure an Amazon CloudWatch Alarm that will trigger a rollback when the function validation fails.</p>",
        ],
      },
      correct_response: ["c", "e", "f"],
      section: "Incident and Event Response",
      question_plain:
        "A developer has been tasked with developing a mobile news homepage that curates several news sources into one page. The app mainly comprises several AWS Lambda functions configured as a deployment group on AWS CodeDeploy. The developer must test all APIs for each new app version before fully deploying it to production. The APIs use a set of Lambda validation scripts. The goal is to check the APIs during deployments, be notified of any API errors, and implement automatic rollback if the validation fails.Which combination of the options below can help implement a solution for this scenario? (Select THREE.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248175,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A developer created a mobile app that allows users to rate and review hotels where users have stayed. The app is hosted on Docker containers deployed on Amazon ECS. Validation scripts need to be run for new deployments to determine if the application is working as expected before allowing production traffic to flow to the new app version. Automatic rollback should also be configured if the validation is not successful.</p><p>Which of the following options will implement this validation?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can use a Lambda function to validate part of the deployment of an updated Amazon ECS application. During an Amazon ECS deployment with validation tests, CodeDeploy can be configured to use a load balancer with two target groups: one production traffic listener and one test traffic listener. To add a validation test, you first implement the test in a Lambda function. Next, in your deployment AppSpec file, you specify the Lambda function for the lifecycle hook you want to test. If a validation test fails, the deployment stops, it is rolled back, and marked as failed. If the test succeeds, the deployment continues to the next deployment lifecycle event or hook.</p><p>The content in the \'hooks\' section of the AppSpec file varies depending on the compute platform for your deployment. The \'hooks\' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The \'hooks\' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event.</p><p>When the deployment starts, the deployment lifecycle events start to execute one at a time. Some lifecycle events are hooks that only execute Lambda functions specified in the AppSpec file. An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. On the AfterAllowTestTraffic hook, you can specify Lambda functions that can validate the deployment using the test traffic.</p><p><img src="https://media.tutorialsdojo.com/public/lifecycle-event-order-ecs.png"></p><p>For example, a Lambda function can serve traffic to the test listener and track metrics from the replacement task set. If rollbacks are configured, you can configure a CloudWatch alarm that triggers a rollback when the validation test in your Lambda function fails. After the validation tests are complete, one of the following occurs:</p><p>- If validation fails and rollbacks are configured, the deployment status is marked Failed, and components return to their state when the deployment started.</p><p>- If validation fails and rollbacks are not configured, the deployment status is marked Failed, and components remain in their current state.</p><p>- If validation succeeds, the deployment continues to the BeforeAllowTraffic hook.</p><p>Hence, the correct answer is:<strong> Create validation scripts in AWS Lambda and define them on the </strong><code><strong>AfterAllowTestTraffic</strong></code><strong> lifecycle hook of the </strong><code><strong>AppSpec.yaml</strong></code><strong> file. The functions can validate the deployment using the test traffic and rollback if the tests fail.</strong></p><p>The option that says: <strong>Create validation scripts in AWS Lambda and define on the </strong><code><strong>AfterInstall</strong></code><strong> lifecycle hook of the </strong><code><strong>AppSpec.yaml</strong></code><strong> file. The functions can validate the deployment after installing the new version and rollback if the tests fail</strong> is incorrect because in the <code>AfterInstall</code> lifecycle hook, the new task version is not yet attached on the test listener and application load balancer, therefore, no traffic is flowing yet on this cluster.</p><p>The option that says: <strong>Create validation scripts in AWS Lambda and define them on the </strong><code><strong>BeforeAllowTraffic</strong></code><strong> lifecycle hook of the </strong><code><strong>AppSpec.yaml</strong></code><strong> file. The functions can validate the deployment before allowing production traffic and rollback if the tests fail </strong>is incorrect because in the <code>BeforeAllowTraffic</code> lifecycle hook, validation has already succeeded, which only defeats its purpose. You have to use this hook to perform additional actions <strong>before</strong> allowing production traffic to flow to the new task version.</p><p>The option that says: <strong>Create validation scripts in AWS Lambda and define them on the </strong><code><strong>AfterAllowTraffic</strong></code><strong> lifecycle hook of the </strong><code><strong>AppSpec.yaml</strong></code><strong> file. The functions can validate the deployment using production traffic and rollback if the tests fail </strong>is incorrect because in the <code>AfterAllowTraffic</code> lifecycle hook, production traffic is simply rerouted from the old task set to the new task set. You want to verify the application <strong>before</strong> opening it to production traffic and not <strong>after.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-ecs ">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-ecs</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html#deployment-steps-what-happens">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html#deployment-steps-what-happens</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorial-ecs-deployment-with-hooks.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorial-ecs-deployment-with-hooks.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codedeploy/?src=udemy">https://tutorialsdojo.com/aws-codedeploy/</a></p>',
        answers: [
          "<p>Create validation scripts in AWS Lambda and define them on the <code>AfterAllowTestTraffic</code> lifecycle hook of the <code>AppSpec.yaml</code> file. The functions can validate the deployment using the test traffic and rollback if the tests fail.</p>",
          "<p>Create validation scripts in AWS Lambda and define on the <code>AfterInstall</code> lifecycle hook of the <code>AppSpec.yaml</code> file. The functions can validate the deployment after installing the new version and rollback if the tests fail.</p>",
          "<p>Create validation scripts in AWS Lambda and define them on the <code>BeforeAllowTraffic</code> lifecycle hook of the <code>AppSpec.yaml</code> file. The functions can validate the deployment before allowing production traffic and rollback if the tests fail.</p>",
          "<p>Create validation scripts in AWS Lambda and define them on the <code>AfterAllowTraffic</code> lifecycle hook of the <code>AppSpec.yaml</code> file. The functions can validate the deployment using production traffic and rollback if the tests fail.</p>",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "A developer created a mobile app that allows users to rate and review hotels where users have stayed. The app is hosted on Docker containers deployed on Amazon ECS. Validation scripts need to be run for new deployments to determine if the application is working as expected before allowing production traffic to flow to the new app version. Automatic rollback should also be configured if the validation is not successful.Which of the following options will implement this validation?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248177,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A technology company is planning to develop its custom online forum that covers various AWS-related technologies. They are planning to use AWS Fargate to host the containerized application and Amazon DynamoDB as its data store. The DevOps team is instructed to define the schema of the DynamoDB table with the required indexes, partition key, sort key, projected attributes, and others. To minimize cost, the schema must support certain search operations using the least provisioned read capacity units. A <code>Thread</code> attribute contains the user comments in JSON format. The sample data set is shown in the diagram below:</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/LSI_01.png \"></p><p><br></p><p>The online forum should support searches within <code>ForumName</code> attribute for items where the <code>Subject</code> begins with a particular letter, such as 'a' or 'b'. It should allow fetches of items within the given <code>LastPostDateTime</code> time frame as well as the capability to return the threads that have been posted within the last quarter. </p><p>Which of the following schema configuration meets the above requirements?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon DynamoDB</strong> provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table and issue Query or Scan requests against these indexes.</p><p>A secondary index is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations. You can retrieve data from the index using a Query, in much the same way as you use Query with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns.</p><p>DynamoDB supports two types of secondary indexes:</p><p>- <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html">Global secondary index</a> — an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered "global" because queries on the index can span all of the data in the base table, across all partitions.</p><p>- <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html">Local secondary index</a> — an index that has the same partition key as the base table, but a different sort key. A local secondary index is "local" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.</p><p>A <strong>local secondary index</strong> maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.</p><p>Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to <code>Scan</code> the entire <em>Thread </em>table and discard any posts that were not within the specified time frame. With a local secondary index, a <code>Query</code> operation could use <em>LastPostDateTime</em> as a sort key and find the data quickly.</p><p>In the provided scenario, you can create a local secondary index named <em>LastPostIndex </em>to meet the requirements. Note that the partition key is the same as that of the <em>Thread</em> table, but the sort key is <em>LastPostDateTime </em>as shown in the diagram below:</p><p><img src="https://media.tutorialsdojo.com/public/LSI_02.png">With <code>LastPostIndex</code>, an application could use <code>ForumName</code> and <code>LastPostDateTime</code> as query criteria. However, to retrieve any additional attributes, DynamoDB must perform additional read operations against the <code>Thread</code> table. These extra reads are known as <em>fetches</em>, and they can increase the total amount of provisioned throughput required for a query.</p><p>Suppose that you wanted to populate a webpage with a list of all the threads in "S3" and the number of replies for each thread, sorted by the last reply date/time beginning with the most recent reply. To populate this list, you would need the following attributes:</p><p><code>-Subject</code></p><p><code>-Replies</code></p><p><code>-LastPostDateTime</code></p><p>The most efficient way to query this data and to avoid fetch operations would be to project the <code>Replies</code> attribute from the table into the local secondary index, as shown in this diagram.</p><p><img src="https://media.tutorialsdojo.com/public/LSI_03.png">DynamoDB stores all of the items with the same partition key value contiguously. In this example, given a particular ForumName, a Query operation could immediately locate all of the threads for that forum. Within a group of items with the same partition key value, the items are sorted by sort key value. If the sort key (Subject) is also provided in the query, DynamoDB can narrow down the results that are returned — for example, returning all of the threads in the "S3" forum that have a Subject beginning with the letter "a".</p><p>A <em>projection</em> is the set of attributes that is copied from a table into a secondary index. The partition key and sort key of the table are always projected into the index; you can project other attributes to support your application\'s query requirements. When you query an index, Amazon DynamoDB can access any attribute in the projection as if those attributes were in a table of their own.</p><p>Hence, the correct answer is: <strong>Set the </strong><code><strong>ForumName</strong></code><strong> attribute as the primary key and </strong><code><strong>Subject</strong></code><strong> as the sort key. Create a Local Secondary Index with </strong><code><strong>LastPostDateTime</strong></code><strong> as the sort key and the </strong><code><strong>Thread</strong></code><strong> as a projected attribute.</strong></p><p>The option that says: <strong>Set the </strong><code><strong>Subject</strong></code><strong> attribute as the primary key and </strong><code><strong>ForumName</strong></code><strong> as the sort key. Create a Local Secondary Index with </strong><code><strong>LastPostDateTime</strong></code><strong> as the sort key and the </strong><code><strong>Thread</strong></code><strong> as a projected attribute </strong>is incorrect because the scenario says that the online forum should support searches within <code>ForumName</code> attribute for items where the <code>Subject</code> begins with a particular letter. DynamoDB stores all of the items with the same partition key value contiguously. In this example, given a particular ForumName, a Query operation could immediately locate all of the threads for that forum. Within a group of items with the same partition key value, the items are sorted by sort key value. If the sort key (Subject) is also provided in the query, DynamoDB can narrow down the results that are returned—for example, returning all of the threads in the "S3" forum that have a Subject beginning with the letter "a". Hence, you should set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key instead.</p><p>The option that says: <strong>Set the </strong><code><strong>ForumName</strong></code><strong> attribute as the primary key and </strong><code><strong>Subject</strong></code><strong> as the sort key. Create a Global Secondary Index with </strong><code><strong>Thread</strong></code><strong> as the sort key and fetch operations for </strong><code><strong>LastPostDateTime<em> </em></strong></code>is incorrect because using a fetches operation can increase the total amount of provisioned throughput required for a query. Remember that the scenario mentioned that the schema must support certain search operations using the least provisioned read capacity units to minimize cost. In addition, you should create an LSI instead of GSI.</p><p>The option that says: <strong>Set the </strong><code><strong>Subject</strong></code><strong> attribute as the primary key and </strong><code><strong>ForumName</strong></code><strong> as the sort key. Create a Global Secondary Index with </strong><code><strong>Thread</strong></code><strong> as the sort key and </strong><code><strong>LastPostDateTime</strong></code><strong> as a projected attribute </strong>is incorrect because you should use a Local Secondary Index instead. You should also set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-dynamodb/?src=udemy">https://tutorialsdojo.com/amazon-dynamodb/</a></p>',
        answers: [
          "<p>Set the <code>Subject</code> attribute as the primary key and <code>ForumName</code> as the sort key. Create a Local Secondary Index with <code>LastPostDateTime</code> as the sort key and the <code>Thread</code> as a projected attribute.</p>",
          "<p>Set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key. Create a Local Secondary Index with <code>LastPostDateTime</code> as the sort key and the <code>Thread</code> as a projected attribute.</p>",
          "<p>Set the <code>Subject</code> attribute as the primary key and <code>ForumName</code> as the sort key. Create a Global Secondary Index with <code>Thread</code> as the sort key and <code>LastPostDateTime</code> as a projected attribute.</p>",
          "<p>Set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key. Create a Global Secondary Index with <code>Thread</code> as the sort key and fetch operations for <code>LastPostDateTime</code>.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A technology company is planning to develop its custom online forum that covers various AWS-related technologies. They are planning to use AWS Fargate to host the containerized application and Amazon DynamoDB as its data store. The DevOps team is instructed to define the schema of the DynamoDB table with the required indexes, partition key, sort key, projected attributes, and others. To minimize cost, the schema must support certain search operations using the least provisioned read capacity units. A Thread attribute contains the user comments in JSON format. The sample data set is shown in the diagram below:The online forum should support searches within ForumName attribute for items where the Subject begins with a particular letter, such as 'a' or 'b'. It should allow fetches of items within the given LastPostDateTime time frame as well as the capability to return the threads that have been posted within the last quarter. Which of the following schema configuration meets the above requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248179,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A software company hosts backend services within a private subnet of an Amazon Virtual Private Cloud (VPC). The company is integrating with a new external partner that requires IPv6 connectivity. The company's security team manages access through a security group configured to allow outbound traffic to all approved partner integrations.</p><p>As part of the integration process, the DevOps team allocated IPv6 addresses to the backend services. They’ve also updated the security group's outbound rules to include the partner's IPv6 CIDR range. Despite these changes, the services failed to establish communication with the partner's systems, even though they continued to communicate internally without issue.</p><p>What additional steps should be implemented to enable outbound connectivity?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>An egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances within the VPC to the internet and prevents the internet from initiating an IPv6 connection with instances within the VPC. Unlike an internet gateway (IGW), which allows both inbound and outbound IPv4 traffic, an egress-only internet gateway allows only outbound IPv6 traffic.</p><p>To ensure backend systems can communicate with an external system, create an egress-only internet gateway in the VPC and then add a route to the route table pointing all IPv6 traffic (<code>::/0</code>) or a specific range of IPv6 addresses to the egress-only internet gateway.</p><p><img src="https://media.tutorialsdojo.com/public/EgressOnlyInternetGateway.png"></p><p>An egress-only internet gateway is stateful: it forwards traffic from the instances in the subnet to the internet or other AWS services and then sends the response back to the instances. Security groups can be used for instances in the private subnet to control traffic to and from the instances. Similarly, network ACL can be used to control traffic to and from the subnet for which the egress-only internet gateway routes traffic.</p><p>Hence, the correct answer is: <strong>Attach an egress-only internet gateway to the VPC. Configure the subnet’s route table to direct all IPv6 traffic (</strong><code><strong>::/0</strong></code><strong>) to the egress-only internet gateway.</strong></p><p>The option that says: <strong>Update the NAT Gateway settings to enable IPv6. Configure the subnet’s route table to direct all IPv6 traffic (</strong><code><strong>::/0</strong></code><strong>) to the NAT gateway. (</strong><code><strong>::/0</strong></code><strong>) to the NAT gateway</strong> is incorrect. NAT Gateways only support IPv4 traffic and not IPv6. Enabling IPv6 in the settings of the NAT gateway is not possible. Although NAT Gateways support IPv6 to IPv4 translation via NAT64, they only allow IPv6 AWS resources to communicate with IPv4 resources, not IPv6-to-IPv6 communication.</p><p>The option that says: <strong>Deploy a NAT instance in the VPC and disable the source/destination check on the instance. Add a new route with </strong><code><strong>0.0.0.0/0</strong></code><strong> as the destination and the NAT instance as the target</strong> is incorrect. This approach describes a solution for IPv4 traffic, not IPv6. Note that 0.0.0.0/0 represents an IPV4 CIDR range.</p><p>The option that says: <strong>Attach a new internet gateway to the VPC. Add a new route with </strong><code><strong>0.0.0.0/0</strong></code><strong> as the destination and select the internet gateway as the target</strong> is incorrect. Although an internet gateway supports both IPv4 and IPv6 traffic, the specified CIDR range of 0.0.0.0/0 pertains only to IPv4. Furthermore, a VPC can have only one internet gateway attached at any given time.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a></p><p><strong>Check out this VPC Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-vpc/?src=udemy">https://tutorialsdojo.com/amazon-vpc</a></p>',
        answers: [
          "<p>Update the NAT Gateway settings to enable IPv6. Configure the subnet’s route table to direct all IPv6 traffic ( <code>::/0</code> ) to the NAT gateway.</p>",
          "<p>Deploy a NAT instance in the VPC and disable the source/destination check on the instance. Add a new route with <code>0.0.0.0/0</code> as the destination and the NAT instance as the target.</p>",
          "<p>Attach an egress-only internet gateway to the VPC. Configure the subnet’s route table to direct all IPv6 traffic ( <code>::/0</code> ) to the egress-only internet gateway.</p>",
          "<p>Attach a new internet gateway to the VPC. Add a new route with <code>0.0.0.0/0</code> as the destination and select the internet gateway as the target.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Security and Compliance",
      question_plain:
        "A software company hosts backend services within a private subnet of an Amazon Virtual Private Cloud (VPC). The company is integrating with a new external partner that requires IPv6 connectivity. The company's security team manages access through a security group configured to allow outbound traffic to all approved partner integrations.As part of the integration process, the DevOps team allocated IPv6 addresses to the backend services. They’ve also updated the security group's outbound rules to include the partner's IPv6 CIDR range. Despite these changes, the services failed to establish communication with the partner's systems, even though they continued to communicate internally without issue.What additional steps should be implemented to enable outbound connectivity?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248181,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company would like to set up an audit process to ensure that the enterprise application is running exclusively on Amazon EC2 Dedicated Hosts. The company is also concerned about the increasing costs of its application software licensing from its third-party vendor. To meet the compliance requirement, a DevOps Engineer must create a workflow to audit the enterprise applications hosted in its Amazon VPC.</p><p>Which of the following options should the Engineer implement to satisfy the requirement with the LEAST administrative overhead?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can use <strong>AWS Config</strong> to record configuration changes for Dedicated Hosts, and instances that are launched, stopped, or terminated on them. You can then use the information captured by AWS Config as a data source for license reporting.</p><p>AWS Config records configuration information for Dedicated Hosts and instances individually and pairs this information through relationships. There are three reporting conditions:</p><p>- AWS Config recording status — When On, AWS Config is recording one or more AWS resource types, which can include Dedicated Hosts and Dedicated Instances. To capture the information required for license reporting, verify that hosts and instances are being recorded with the following fields.</p><p>- Host recording status — When Enabled, the configuration information for Dedicated Hosts is recorded.</p><p>- Instance recording status — When Enabled, the configuration information for Dedicated Instances is recorded.</p><p>If any of these three conditions are disabled, the icon in the Edit Config Recording button is red. To derive the full benefit of this tool, ensure that all three recording methods are enabled. When all three are enabled, the icon is green. To edit the settings, choose Edit Config Recording. You are directed to the <em>Set up AWS Config </em>page in the AWS Config console, where you can set up AWS Config and start recording for your hosts, instances, and other supported resource types. AWS Config records your resources after it discovers them, which might take several minutes.</p><p>After AWS Config starts recording configuration changes to your hosts and instances, you can get the configuration history of any host that you have allocated or released and any instance that you have launched, stopped, or terminated. For example, at any point in the configuration history of a Dedicated Host, you can look up how many instances are launched on that host, along with the number of sockets and cores on the host. For any of those instances, you can also look up the ID of its Amazon Machine Image (AMI). You can use this information to report on licensing for your own server-bound software that is licensed per-socket or per-core.</p><p>You can view configuration histories in any of the following ways.</p><p>- By using the AWS Config console. For each recorded resource, you can view a timeline page, which provides a history of configuration details. To view this page, choose the gray icon in the Config Timeline column of the Dedicated Hosts page.</p><p>- By running AWS CLI commands. First, you can use the <a href="https://docs.aws.amazon.com/cli/latest/reference/configservice/list-discovered-resources.html">list-discovered-resources</a> command to get a list of all hosts and instances. Then, you can use the <a href="https://docs.aws.amazon.com/cli/latest/reference/configservice/get-resource-config-history.html#get-resource-config-history">get-resource-config-history</a> command to get the configuration details of a host or instance for a specific time interval.</p><p>- By using the AWS Config API in your applications. First, you can use the <a href="https://docs.aws.amazon.com/config/latest/APIReference/API_ListDiscoveredResources.html">ListDiscoveredResources</a> action to get a list of all hosts and instances. Then, you can use the <a href="https://docs.aws.amazon.com/config/latest/APIReference/API_GetResourceConfigHistory.html">GetResourceConfigHistory</a> action to get the configuration details of a host or instance for a specific time interval.</p><p><img src="https://media.tutorialsdojo.com/public/TD-AWS-Config-Status-02-05-2025.png"></p><p>Hence, the correct answer is: <strong>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the </strong><code><strong>config-rule-change-triggered</strong></code><strong> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</strong></p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the </strong><code><strong>PutComplianceItems</strong></code><strong> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the </strong><code><strong>ListComplianceSummaries</strong></code><strong> API action </strong>is incorrect because the AWS Systems Manager Configuration Compliance service is primarily used to scan your fleet of managed instances for patch compliance and configuration inconsistencies. A better solution is to use AWS Config to record the status of your Dedicated Hosts.</p><p>The option that says: <strong>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the </strong><code><strong>inspector-scheduled-run</strong></code><strong> blueprint</strong> is incorrect because Amazon Inspector is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not capable of recording the status of your EC2 instances nor detect if they are configured as a Dedicated Host.</p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data </strong>is incorrect. Although this may be a possible solution, it entails a lot of administrative effort in comparison to just using AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html ">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom ">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom</a></p><p><a href="https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/">https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and AWS Config Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href="https://tutorialsdojo.com/aws-config/">https://tutorialsdojo.com/aws-config/</a></p>',
        answers: [
          "<p>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the <code>PutComplianceItems</code> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the <code>ListComplianceSummaries</code> API action.</p>",
          "<p>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the <code>inspector-scheduled-run</code> blueprint.</p>",
          "<p>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the <code>config-rule-change-triggered</code> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</p>",
          "<p>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Incident and Event Response",
      question_plain:
        "A company would like to set up an audit process to ensure that the enterprise application is running exclusively on Amazon EC2 Dedicated Hosts. The company is also concerned about the increasing costs of its application software licensing from its third-party vendor. To meet the compliance requirement, a DevOps Engineer must create a workflow to audit the enterprise applications hosted in its Amazon VPC.Which of the following options should the Engineer implement to satisfy the requirement with the LEAST administrative overhead?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248183,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An organization's internal application gets important assets in AWS Storage Gateway in file gateway mode. The Storage Gateway fronts an Amazon S3 bucket used by several resources. In the morning, internal application users receive an error saying that some of the assets processed by a third party from the previous evening are missing. Upon investigation, the DevOps Engineer stated that the missing assets from the Storage Gateway exist directly in the S3 bucket.</p><p>Which of the following solution will prevent missing assets in the Storage Gateway and will guarantee that the assets are available in the morning?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Storage Gateway</strong> is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. Storage Gateway provides a standard set of storage protocols such as iSCSI, SMB, and NFS, which allow you to use AWS storage without rewriting your existing applications. It provides low-latency performance by caching frequently accessed data on premises, while storing data securely and durably in Amazon cloud storage services.</p><p><strong>File Gateway</strong> enables you to store and retrieve objects in Amazon Simple Storage Service (S3) using file protocols such as Network File System (NFS) and Server Message Block (SMB). Objects written through S3 File Gateway can be directly accessed in S3.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-file-gateway.png"></p><p><strong>RefreshCache </strong>refreshes the cached inventory of objects for the specified file share. This operation finds objects in the Amazon S3 bucket that were added, removed, or replaced since the gateway last listed the bucket\'s contents and cached the results. This operation does not import files into the S3 File Gateway cache storage. It only updates the cached inventory to reflect changes in the inventory of the objects in the S3 bucket. This operation is only supported in the S3 File Gateway types.</p><p>Hence, the correct answer is: <strong>Create an AWS Lambda function that will run the RefreshCache command for Storage Gateway. Schedule a nightly Amazon EventBridge event to trigger the function </strong>as this option leverages an EventBridge event to schedule a nightly execution of RefreshCache through a Lambda function, ensuring that the processed assets in the evening are accessible in the morning.</p><p>The option that says: <strong>Require the third party to use AWS Transfer for SFTP to transfer the processed assets into the S3 bucket</strong> is incorrect because the assets are missing in the Storage Gateway rather than the S3 bucket. Merely transferring them directly to S3 will not address the issue of missing assets in the Storage Gateway.</p><p>The option that says: <strong>Change the Storage Gateway configuration to run in volume gateway mode and call the RefreshCache API</strong> is incorrect because the RefreshCache API is exclusively accessible in file gateway mode and not supported in volume gateway mode.</p><p>The option that says: <strong>Implement an S3 Same-Region replication to copy any modifications done directly in the S3 bucket to Storage Gateway</strong> is incorrect because S3 replication can only be configured across S3 buckets and cannot be set up with Storage Gateway as the replication target.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html">https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html</a></p><p><a href="https://aws.amazon.com/storagegateway/file/">https://aws.amazon.com/storagegateway/file/</a></p><p><a href="https://aws.amazon.com/storagegateway/faqs">https://aws.amazon.com/storagegateway/faqs</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-storage-gateway/?src=udemy">https://tutorialsdojo.com/aws-storage-gateway/</a></p>',
        answers: [
          "<p>Require the third party to use AWS Transfer for SFTP to transfer the processed assets into the S3 bucket.</p>",
          "<p>Change the Storage Gateway configuration to run in volume gateway mode and call the RefreshCache API</p>",
          "<p>Create an AWS Lambda function that will run the RefreshCache command for Storage Gateway. Schedule a nightly Amazon EventBridge event to trigger the function.</p>",
          "<p>Implement an S3 Same-Region replication to copy any modifications done directly in the S3 bucket to Storage Gateway.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Incident and Event Response",
      question_plain:
        "An organization's internal application gets important assets in AWS Storage Gateway in file gateway mode. The Storage Gateway fronts an Amazon S3 bucket used by several resources. In the morning, internal application users receive an error saying that some of the assets processed by a third party from the previous evening are missing. Upon investigation, the DevOps Engineer stated that the missing assets from the Storage Gateway exist directly in the S3 bucket.Which of the following solution will prevent missing assets in the Storage Gateway and will guarantee that the assets are available in the morning?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248185,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company that develops smart home devices has a set of serverless APIs that uses several independent AWS Lambda functions that handle web, mobile, Internet of Things (IoT), and 3rd party API requests. The current CI/CD pipeline builds, tests, packages, and deploys each Lambda function in sequence using AWS CodePipeline and AWS CodeBuild. An Amazon CloudWatch Events rule is used to monitor and ensure that the pipeline execution starts promptly after a code change has been made. The SysOps team monitors all of the AWS resources as well as the performance of each pipeline. The company noticed that for the past few days, the pipeline has taken too long to finish the build and deployment process. The team instructed a DevOps Engineer to implement a solution that will expedite the deployment process of the independent Lambda functions.</p><p>Which of the following is the MOST suitable solution that the DevOps Engineer should implement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>In AWS CodePipeline, an action is part of the sequence in a stage of a pipeline. It is a task performed on the artifact in that stage. Pipeline actions occur in a specified order, in sequence, or in parallel, as determined in the configuration of the stage.</p><p>CodePipeline provides support for six types of actions:</p><p>- Source</p><p>- Build</p><p>- Test</p><p>- Deploy</p><p>- Approval</p><p>- Invoke</p><p>By default, any pipeline you successfully create in AWS CodePipeline has a valid structure. However, if you manually create or edit a JSON file to create a pipeline or update a pipeline from the AWS CLI, you might inadvertently create a structure that is not valid. The following reference can help you better understand the requirements for your pipeline structure and how to troubleshoot issues.</p><p>The default <code>runOrder</code> value for an action is 1. The value must be a positive integer (natural number). You cannot use fractions, decimals, negative numbers, or zero. To specify a serial sequence of actions, use the smallest number for the first action and larger numbers for each of the rest of the actions in sequence. To specify parallel actions, use the same integer for each action you want to run in parallel.</p><p><img src="https://media.tutorialsdojo.com/aws-codepipeline-action-structure.png"></p><p>For example, if you want three actions to run in sequence in a stage, you would give the first action the <code>runOrder</code> value of 1, the second action the <code>runOrder</code> value of 2, and the third the <code>runOrder</code> value of 3. However, if you want the second and third actions to run in parallel, you would give the first action the <code>runOrder</code> value of 1 and both the second and third actions the <code>runOrder</code> value of 2.</p><p>The numbering of serial actions do not have to be in strict sequence. For example, if you have three actions in a sequence and decide to remove the second action, you do not need to renumber the <code>runOrder</code> value of the third action. Because the <code>runOrder</code> value of action (3) is higher than the <code>runOrder</code> value of the first action (1), it runs serially after the first action in the stage.</p><p>Hence, the correct answer is: <strong>Execute actions for each Lambda function in parallel by setting up a configuration that specifies the same </strong><code><strong>runOrder</strong></code><strong> value in CodePipeline.</strong></p><p>The option that says: <strong>Upgrade the compute type of the build environment in CodeBuild pipeline with a higher memory, vCPUs, and disk space</strong> is incorrect. Although it may help speed up the build time, it will only improve the performance of CodeBuild and not the entire pipeline. A better solution is to run the tasks in parallel in CodePipeline.</p><p>The option that says: <strong>Set up a custom CodeBuild execution environment with multiprocessing option that runs builds in parallel </strong>is incorrect because there is no multiprocessing option in CodeBuild. You can upgrade the compute type of the build environment or use local cache, but there is no such thing as multiprocessing in CodeBuild.</p><p>The option that says: <strong>Enable local caching in CodeBuild using a Docker layer cache mode </strong>is incorrect. Although using a local cache can help expedite the build process, the use of Docker layer cache mode is just applicable for containerized applications and not for Lambda functions.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/actions.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/actions.html</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p>',
        answers: [
          "<p>Upgrade the compute type of the build environment in CodeBuild pipeline with a higher memory, vCPUs, and disk space.</p>",
          "<p>Set up a custom CodeBuild execution environment with multiprocessing option that runs builds in parallel.</p>",
          "<p>Execute actions for each Lambda function in parallel by setting up a configuration that specifies the same <code>runOrder</code> value in CodePipeline.</p>",
          "<p>Enable local caching in CodeBuild using a Docker layer cache mode.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company that develops smart home devices has a set of serverless APIs that uses several independent AWS Lambda functions that handle web, mobile, Internet of Things (IoT), and 3rd party API requests. The current CI/CD pipeline builds, tests, packages, and deploys each Lambda function in sequence using AWS CodePipeline and AWS CodeBuild. An Amazon CloudWatch Events rule is used to monitor and ensure that the pipeline execution starts promptly after a code change has been made. The SysOps team monitors all of the AWS resources as well as the performance of each pipeline. The company noticed that for the past few days, the pipeline has taken too long to finish the build and deployment process. The team instructed a DevOps Engineer to implement a solution that will expedite the deployment process of the independent Lambda functions.Which of the following is the MOST suitable solution that the DevOps Engineer should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248187,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A retail company is planning to migrate its on-premises data center to AWS to scale its infrastructure and reach more customers. The multi-tier web applications will be moved to the cloud and will use a variety of AWS services, IAM policies, and custom network configurations. The requirements can be changed anytime by its Solutions Architect, which means there will be a lot of modifications to the AWS components being deployed. AWS CloudFormation will be used to automate, launch, and version-control the new cloud environment in AWS in various regions.</p><p>Which of the following is the MOST recommended way to set up CloudFormation in this scenario?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>When you organize your AWS resources based on lifecycle and ownership, you might want to build a stack that uses resources that are in another stack. You can hard-code values or use input parameters to pass resource names and IDs. However, these methods can make templates difficult to reuse or can increase the overhead to get a stack running. Instead, use cross-stack references to export resources from a stack so that other stacks can use them. Stacks can use the exported resources by calling them using the <code>Fn::ImportValue</code> function.</p><p>For example, you might have a network stack that includes a VPC, a security group, and a subnet. You want all public web applications to use these resources. By exporting the resources, you allow all stacks with public web applications to use them.</p><p>To export resources from one AWS CloudFormation stack to another, create a cross-stack reference. Cross-stack references let you use a layered or service-oriented architecture. Instead of including all resources in a single stack, you create related AWS resources in separate stacks; then, you can refer to required resource outputs from other stacks. By restricting cross-stack references to outputs, you control the parts of a stack that are referenced by other stacks.</p><p><img src="https://media.tutorialsdojo.com/public/2019-11-08_01-13-02-963bb47e80aa0f615a5cdbd90c4b6baf.png"></p><p>For example, you might have a network stack with a VPC, a security group, and a subnet for public web applications, and a separate public web application stack. To ensure that the web applications use the security group and subnet from the network stack, you create a cross-stack reference that allows the web application stack to reference resource outputs from the network stack. With a cross-stack reference, owners of the web application stacks don\'t need to create or maintain networking rules or assets.</p><p>To create a cross-stack reference, use the <code>Export</code> output field to flag the value of a resource output for export. Then, use the <code>Fn::ImportValue</code> intrinsic function to import the value.</p><p>Hence, the correct answer is: <strong>Prepare multiple separate CloudFormation templates for each logical part of the architecture. Use cross-stack references to export resources from one CloudFormation stack to another and maintain the templates in GitHub.</strong></p><p>The option that says: <strong>Prepare a single master CloudFormation template containing all logical parts of the architecture. Store the CloudFormation resource outputs in an Amazon DynamoDB table that will be used by the template. Upload and manage the template in GitHub</strong> is incorrect because it is better to use multiple separate CloudFormation templates to handle each logical part of the architecture, considering that you are deploying multitier web applications that use a variety of AWS services, IAM policies, and custom network configuration. This will provide better management of each part of your architecture. In addition, you can simply use cross-stack references in CloudFormation instead of storing the resource outputs in a DynamoDB table.</p><p>The option that says: <strong>Prepare a single master CloudFormation template containing all logical parts of the architecture. Upload and maintain the template in GitHub. Use AWS App2Container to automate the deployment of containerized workloads</strong> is incorrect because, just as mentioned above, it is better to use multiple separate CloudFormation templates to handle each logical part of the architecture. Additionally, AWS App2Container is used for migrating existing containerized applications, not for managing infrastructure as code with CloudFormation.</p><p>The option that says: <strong>Prepare multiple separate CloudFormation templates for each logical part of the architecture. Store the CloudFormation resource outputs to AWS Systems Manager Parameter Store. Upload and manage the templates in GitHub</strong> is incorrect because it is better to handle each logical part of the architecture on a separate CloudFormation template for easier management. Although you can integrate AWS Systems Manager Parameter Store with CloudFormation, this service is more suitable to store data such as passwords, database strings, and license codes as parameter values only but not resource outputs. You should create a cross-stack reference to export resources from one AWS CloudFormation stack to another.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-reference-resource/">https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-reference-resource/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p>',
        answers: [
          "<p>Prepare a single master CloudFormation template containing all logical parts of the architecture. Store the CloudFormation resource outputs in an Amazon DynamoDB table that will be used by the template. Upload and manage the template in GitHub.</p>",
          "<p>Prepare a single master CloudFormation template containing all logical parts of the architecture. Upload and maintain the template in GitHub. Use AWS App2Container to automate the deployment of containerized workloads.</p>",
          "<p>Prepare multiple separate CloudFormation templates for each logical part of the architecture. Use cross-stack references to export resources from one CloudFormation stack to another and maintain the templates in GitHub.</p>",
          "<p>Prepare multiple separate CloudFormation templates for each logical part of the architecture. Store the CloudFormation resource outputs to AWS Systems Manager Parameter Store. Upload and manage the templates in GitHub.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A retail company is planning to migrate its on-premises data center to AWS to scale its infrastructure and reach more customers. The multi-tier web applications will be moved to the cloud and will use a variety of AWS services, IAM policies, and custom network configurations. The requirements can be changed anytime by its Solutions Architect, which means there will be a lot of modifications to the AWS components being deployed. AWS CloudFormation will be used to automate, launch, and version-control the new cloud environment in AWS in various regions.Which of the following is the MOST recommended way to set up CloudFormation in this scenario?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248189,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is planning to migrate its online customer portal to AWS. It should be hosted in AWS Elastic Beanstalk and use Amazon RDS MySQL in Multi-AZ configuration for its database. A DevOps Engineer was instructed to ensure that the application resources must be at full capacity during deployment by using a new group of instances. The solution should also include a way to roll back the change easily and prevent issues caused by partially completed rolling deployments. The application performance should not be affected while a new version of the app is being deployed. </p><p>Which is the MOST cost-effective deployment set up that the DevOps Engineer should implement to meet these requirements?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Elastic Beanstalk</strong> provides several options for how <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html">deployments</a> are processed, including deployment policies (<strong>All at once</strong>, <strong>Rolling</strong>, <strong>Rolling with additional batch</strong>, and <strong>Immutable</strong>) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it\'s an automatically scaling environment (you didn\'t specify the <code>--single</code> option), it uses rolling deployments.</p><p>With rolling deployments, Elastic Beanstalk splits the environment\'s EC2 instances into batches and deploys the new version of the application to one batch at a time, leaving the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p><p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a <em>rolling deployment with an additional batch</em>. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.</p><p><em>Immutable deployments </em>perform an <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html">immutable update</a> to launch a full set of new instances running the new version of the application in a separate Auto Scaling group alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don\'t pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.</p><p><img src="https://media.tutorialsdojo.com/public/2019-05-28_11-31-43-d1e6e5be5012c77c39ec15676d410483.png">If your application doesn\'t pass all health checks, but still operates correctly at a lower health status, you can allow instances to pass health checks with a lower status, such as <code>Warning</code>, by modifying the <em>Healthy threshold </em>option. If your deployments fail because they don\'t pass health checks and you need to force an update regardless of health status, specify the <em>Ignore health check</em> option.</p><p>When you specify a batch size for rolling updates, Elastic Beanstalk also uses that value for rolling application restarts. Use rolling restarts when you need to restart the proxy and application servers running on your environment\'s instances without downtime.</p><p>AWS Elastic Beanstalk provides support for running Amazon Relational Database Service (Amazon RDS) instances in your Elastic Beanstalk environment. This works great for development and testing environments. However, it isn\'t ideal for a production environment because it ties the lifecycle of the database instance to the lifecycle of your application\'s environment.</p><p>Hence, the correct answer is: <strong>Host the online customer portal using AWS Elastic Beanstalk and integrate it to an external Amazon RDS MySQL database in Multi-AZ deployments configuration. Use immutable updates for application deployments.</strong></p><p>The option that says: <strong>Host the online customer portal using AWS Elastic Beanstalk and integrate it to an external Amazon RDS MySQL database in Multi-AZ deployments configuration. Configure the Elastic Beanstalk to use blue/green deployment for releasing the new application version to a new environment. Swap the CNAME in the two environments to redirect traffic to the new version using the </strong><code><strong>Swap Environment URLs</strong></code><strong> feature. Once the deployment has been successfully implemented, keep the old environment running as a backup</strong> is incorrect. Although using the blue/green deployment configuration is an ideal option, keeping the old environment running is not recommended since it entails a significant cost. Take note that the scenario asks for the most cost-effective solution, which is why the old environment should be deleted.</p><p>The option that says:<strong> Host the online customer portal using AWS Elastic Beanstalk coupled with an Amazon RDS MySQL database. In the Elastic Beanstalk database configuration, set the </strong><code><strong>Availability</strong></code><strong> option to </strong><code><strong>High (Multi-AZ)</strong></code><strong> to run a warm backup in a second Availability Zone. Use the </strong><code><strong>All at once</strong></code><strong> deployment policy to release the new application version</strong> is incorrect because this will deploy the new version to all existing instances and will not create new EC2 instances. Moreover, you should decouple your RDS database from Elastic Beanstalk, as this is tied to the lifecycle of the database instance and to the lifecycle of your application\'s environment.</p><p>The option that says: <strong>Host the online customer portal using AWS Elastic Beanstalk coupled with Amazon RDS MySQL database as part of the environment. For high availability, set the </strong><code><strong>Availability</strong></code><strong> option to </strong><code><strong>High (Multi-AZ)</strong></code><strong> in the Elastic Beanstalk database configuration to run a warm backup in a second Availability Zone. Use the </strong><code><strong>Rolling with additional batch</strong></code><strong> policy for application deployments </strong>is incorrect because this type of configuration could potentially cause partially completed rolling deployments. The new batch of instances is within the same Auto Scaling group and not in a new one. The rollback process is also cumbersome to implement, unlike the Immutable or Blue/Green deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>',
        answers: [
          "<p>Host the online customer portal using AWS Elastic Beanstalk and integrate it to an external Amazon RDS MySQL database in Multi-AZ deployments configuration. Configure the Elastic Beanstalk to use blue/green deployment for releasing the new application version to a new environment. Swap the CNAME in the two environments to redirect traffic to the new version using the <code>Swap Environment URLs</code> feature. Once the deployment has been successfully implemented, keep the old environment running as a backup.</p>",
          "<p>Host the online customer portal using AWS Elastic Beanstalk coupled with an Amazon RDS MySQL database. In the Elastic Beanstalk database configuration, set the <code>Availability</code> option to <code>High (Multi-AZ)</code> to run a warm backup in a second Availability Zone. Use the <code>All at once</code> deployment policy to release the new application version.</p>",
          "<p>Host the online customer portal using AWS Elastic Beanstalk coupled with Amazon RDS MySQL database as part of the environment. For high availability, set the <code>Availability</code> option to <code>High (Multi-AZ)</code> in the Elastic Beanstalk database configuration to run a warm backup in a second Availability Zone. Use the <code>Rolling with additional batch</code> policy for application deployments.</p>",
          "<p>Host the online customer portal using AWS Elastic Beanstalk and integrate it to an external Amazon RDS MySQL database in Multi-AZ deployments configuration. Use immutable updates for application deployments.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A company is planning to migrate its online customer portal to AWS. It should be hosted in AWS Elastic Beanstalk and use Amazon RDS MySQL in Multi-AZ configuration for its database. A DevOps Engineer was instructed to ensure that the application resources must be at full capacity during deployment by using a new group of instances. The solution should also include a way to roll back the change easily and prevent issues caused by partially completed rolling deployments. The application performance should not be affected while a new version of the app is being deployed. Which is the MOST cost-effective deployment set up that the DevOps Engineer should implement to meet these requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248191,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company has an internal application that is hosted on an Amazon EC2 instance running in a VPC. As part of the setup, the application needed to download an object from the S3 bucket. However, the application received an AccessDenied error in logs when trying to download the object from the restricted S3 bucket.</p><p>Which of the following should the DevOps Engineer investigate to identify the cause of the issue? (Select TWO.)</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p><strong>S3 bucket policy</strong> allows for securing access to objects in buckets so that only users with the necessary permissions can access them. Even authenticated users can be restricted from accessing Amazon S3 resources if they lack the appropriate permissions.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-s3.png"></p><p>An <strong>IAM role</strong> is a type of IAM identity that can be created in an AWS account with defined permissions. It shares similarities with IAM users, as it also has permission policies that determine its actions in AWS. However, unlike an IAM user, an IAM role is not uniquely associated with a specific individual and can be assumed by anyone who requires its permission. Moreover, an IAM role does not have standard long-term credentials such as a password or access keys; instead, it provides temporary security credentials for the duration of the role session.</p><p>Hence, the correct answers are the option that says:</p><p>- <strong>The S3 bucket policy has permission errors.</strong></p><p><strong>- The IAM role attached to the EC2 instance has configuration errors.</strong></p><p>The following options will not cause the error:</p><p><strong>- The object has been moved to a different storage class</strong></p><p><strong>- The S3 Object Lock is enabled</strong></p><p><strong>- The S3 bucket has default encryption enabled</strong></p><p><br></p><p><strong>References:</strong></p><p><a href="https://repost.aws/knowledge-center/ec2-instance-access-s3-bucket">https://repost.aws/knowledge-center/ec2-instance-access-s3-bucket</a></p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a></p><p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><a href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>The object has been moved to a different storage class.</p>",
          "<p>The S3 Object Lock is enabled.</p>",
          "<p>The S3 bucket policy has permission errors.</p>",
          "<p>The S3 bucket has default encryption enabled.</p>",
          "<p>The IAM role attached to the EC2 instance has configuration errors.</p>",
        ],
      },
      correct_response: ["c", "e"],
      section: "Security and Compliance",
      question_plain:
        "A company has an internal application that is hosted on an Amazon EC2 instance running in a VPC. As part of the setup, the application needed to download an object from the S3 bucket. However, the application received an AccessDenied error in logs when trying to download the object from the restricted S3 bucket.Which of the following should the DevOps Engineer investigate to identify the cause of the issue? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248193,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A business has a Go-based on-premises application that needs to be migrated to AWS. The business was excited about the migration and wanted to take advantage of the new opportunities that AWS provided. Its development team desires to enable blue/green deployments. This would allow them to have two identical environments running side-by-side, with one being the current production environment (blue) and the other being a new environment (green) where they could test new features and changes via A/B testing.</p><p><br>Which of the following is the MOST appropriate solution that the DevOps Engineer should implement to meet the requirements?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Elastic Beanstalk</strong> enables users to deploy and administer applications in the AWS Cloud promptly without the need to comprehend the underlying infrastructure. It streamlines management complexities while still granting flexibility and authority to users. The user is only required to upload their application, and Elastic Beanstalk will automatically take care of tasks such as capacity provisioning, load balancing, scaling, and application health monitoring.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-elastic-beanstalk-blue-green.png"></p><p>A diverse selection of platforms is available on AWS Elastic Beanstalk for building applications. Users choose a platform for their web application, and Elastic Beanstalk deploys their code to the chosen platform version, creating a functional application environment.</p><p><strong>Elastic Beanstalk</strong> provides platforms for programming languages (Go, Java, Node.js, PHP, Python, Ruby), application servers (Tomcat, Passenger, Puma), and Docker containers.</p><p>In the scenario, the application is built in Go which is supported by Elastic Beanstalk and can leverage the built-in deployment options for blue/green deployment for less overhead.</p><p>Hence, the correct answer is the option that says: <strong>Host the application using AWS Elastic Beanstalk. Upload a zipped version of the application on an Amazon S3 bucket. Utilize the bucket to implement new versions of the application. Use Elastic Beanstalk to manage the deployment options.</strong></p><p>The option that says: <strong>Host the application using Amazon Lightsail. Upload a zipped version of the application on an Amazon S3 bucket. Utilize the bucket to implement new versions of the application. Use Lightsail deployment options to manage the deployment </strong>is incorrect because using Amazon Lightsail only provides compute, storage, and networking capacity and capabilities to deploy and manage websites and web applications in the cloud. While this approach may be viable, extra configuration is required to establish the Go application, which entails more work.</p><p>The option that says: <strong>Host the application code in AWS CodeArtifact. Deploy the application on a fleet of Amazon EC2 instances using AWS CodeDeploy. To distribute the traffic to the EC2 instances, utilize the Application Load Balancer. When modifying the application, store a new version to CodeArtifact and create a new CodeDeploy deployment </strong>is incorrect because AWS CodeArtifact is primarily used to store artifacts using popular package managers and build tools like Maven, Gradle, npm, Yarn, Twine, pip, and NuGet.</p><p>The option that says: <strong>Host the application on an Amazon EC2 instance and make an AMI of the instance. Create a Launch Template based on this AMI to be used in an Auto Scaling group. Provision an Application Load Balancer to distribute the traffic. When changes are made to the application, create a new AMI, then trigger an EC2 instance refresh </strong>is incorrect because this option will not satisfy the A/B testing requirement since triggering an EC2 instance refresh will replace the instances running the old AMI.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><a href="https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Host the application using Amazon Lightsail. Upload a zipped version of the application on an Amazon S3 bucket. Utilize the bucket to implement new versions of the application. Use Lightsail deployment options to manage the deployment.</p>",
          "<p>Host the application code in AWS CodeArtifact. Deploy the application on a fleet of Amazon EC2 instances using AWS CodeDeploy. To distribute the traffic to the EC2 instances, utilize the Application Load Balancer. When modifying the application, store a new version to CodeArtifact and create a new CodeDeploy deployment.</p>",
          "<p>Host the application using AWS Elastic Beanstalk. Upload a zipped version of the application on an Amazon S3 bucket. Utilize the bucket to implement new versions of the application. Use Elastic Beanstalk to manage the deployment options.</p>",
          "<p>Host the application on an Amazon EC2 instance and make an AMI of the instance. Create a Launch Template based on this AMI to be used in an Auto Scaling group. Provision an Application Load Balancer to distribute the traffic. When changes are made to the application, create a new AMI, then trigger an EC2 instance refresh.</p>",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "A business has a Go-based on-premises application that needs to be migrated to AWS. The business was excited about the migration and wanted to take advantage of the new opportunities that AWS provided. Its development team desires to enable blue/green deployments. This would allow them to have two identical environments running side-by-side, with one being the current production environment (blue) and the other being a new environment (green) where they could test new features and changes via A/B testing.Which of the following is the MOST appropriate solution that the DevOps Engineer should implement to meet the requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248195,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has a hybrid cloud architecture where its on-premises data center is connected to its multiple virtual private clouds in AWS using an AWS Transit Gateway. The company has a service-oriented architecture with several application services distributed across its VPCs as well as on its local data center. Gathering logs from each service takes a significant amount of time, especially if one module experiences a system outage. To rectify this issue, aggregating the system logs from the on-premises servers and Amazon EC2 instances should be implemented. There should also be a way to analyze the logs for audit and review purposes.</p><p>As a DevOps Engineer, which among the following options is the MOST cost-effective solution that entails the LEAST amount of effort to implement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon CloudWatch</strong> monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real-time. You can use CloudWatch to collect and track metrics, which are variables you can measure for your resources and applications. The CloudWatch home page automatically displays metrics about every AWS service you use. You can additionally create custom dashboards to display metrics about your custom applications and display custom collections of metrics that you choose.</p><p>The unified CloudWatch agent enables you to do the following:</p><p>-Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in Metrics Collected by the CloudWatch Agent.</p><p>-Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p><p>-Retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collectd</code> protocols. <code>StatsD</code> is supported on both Linux servers and servers running Windows Server. <code>collectd</code> is supported only on Linux servers.</p><p>-Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p><p>You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is <code>CWAgent</code>, although you can specify a different namespace when you configure the agent.</p><p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs, just like logs collected by the older CloudWatch Logs agent.</p><p><img src="https://media.tutorialsdojo.com/public/EC2%20Systems%20Mgr-01-SSMHeader.png"></p><p><strong>Amazon Data Firehose</strong> is a service that can be used to capture and automatically load streaming data into destinations like Amazon S3. You can configure a Firehose stream to receive log data from CloudWatch Logs and deliver it to an S3 bucket, which can be in a centralized account for log aggregation. Additionally, Storing logs in an S3 bucket allows for cost-effective, durable, and scalable storage, as well as enabling further analysis and processing.</p><p><strong>Amazon Athena</strong> is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.</p><p>Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automatically — executing queries in parallel — so results are fast, even with large datasets and complex queries.</p><p>Hence, the correct answer is: <strong>Install the Unified CloudWatch agent on all on-premises servers and EC2 instances to collect system and application logs. Configure a Firehose stream to deliver the collected data from CloudWatch logs to an S3 bucket in a central account. Set up an Amazon S3 trigger that invokes an AWS Lambda function to analyze the logs as well as to detect any irregularities. Analyze the log data using Amazon Athena.</strong></p><p>The option that says: <strong>Install the Unified CloudWatch agent on all EC2 instances to collect system and application logs. Use batch export from the CloudWatch console to store the collected data in an Amazon S3 bucket in a central account. Analyze the log data using a custom-made Amazon EMR cluster</strong> is incorrect. The CloudWatch agent must also be installed on the on-premises servers. Furthermore, manually exporting logs from the CloudWatch console only introduces significant operational overhead. Building a custom Amazon EMR cluster is also quite complex. Instead, you can simplify the process by using Amazon Athena.</p><p>The option that says: <strong>Develop a shell script that collects and uploads on-premises logs to an S3 bucket. Analyze the log data using Amazon Macie </strong>is incorrect because Amazon Macie is just a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Moreover, you can simply install the Unified CloudWatch Logs Agent to the on-premises data center instead of developing a shell script.</p><p>The option that says: <strong>Install the Unified CloudWatch Logs agent on all on-premises servers and EC2 instances to collect system and application logs. Consolidate all of the collected logs on your on-premises file server. Develop a custom-built solution that uses an open-source ELK stack running Elasticsearch, Logstash, and Kibana to analyze the logs</strong> is incorrect. Although the use of CloudWatch Logs is valid, developing a custom-built ELK stack solution takes a significant amount of time to implement.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><a href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><p><br></p><p><strong>Check out these Amazon CloudWatch and Athena Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><a href="https://tutorialsdojo.com/amazon-athena/?src=udemy">https://tutorialsdojo.com/amazon-athena/</a></p>',
        answers: [
          "<p>Install the Unified CloudWatch agent on all EC2 instances to collect system and application logs. Use batch export from the CloudWatch console to store the collected data in an Amazon S3 bucket in a central account. Analyze the log data using a custom-made Amazon EMR cluster.</p>",
          "<p>Develop a shell script that collects and uploads on-premises logs to an S3 bucket. Analyze the log data using Amazon Macie.</p>",
          "<p>Install the Unified CloudWatch Logs agent on all on-premises servers and EC2 instances to collect system and application logs. Consolidate all of the collected logs on your on-premises file server. Develop a custom-built solution that uses an open-source ELK stack running Elasticsearch, Logstash, and Kibana to analyze the logs.</p>",
          "<p>Install the Unified CloudWatch agent on all on-premises servers and EC2 instances to collect system and application logs. Configure a Firehose stream to deliver the collected data from CloudWatch logs to an S3 bucket in a central account. Set up an Amazon S3 trigger that invokes an AWS Lambda function to analyze the logs as well as to detect any irregularities. Analyze the log data using Amazon Athena.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Monitoring and Logging",
      question_plain:
        "A company has a hybrid cloud architecture where its on-premises data center is connected to its multiple virtual private clouds in AWS using an AWS Transit Gateway. The company has a service-oriented architecture with several application services distributed across its VPCs as well as on its local data center. Gathering logs from each service takes a significant amount of time, especially if one module experiences a system outage. To rectify this issue, aggregating the system logs from the on-premises servers and Amazon EC2 instances should be implemented. There should also be a way to analyze the logs for audit and review purposes.As a DevOps Engineer, which among the following options is the MOST cost-effective solution that entails the LEAST amount of effort to implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248197,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has recently developed a serverless application that is composed of several AWS Lambda functions and an Amazon DynamoDB database. For the CI/CD process, a continuous deployment pipeline has been built using GitHub, AWS CodeBuild, and AWS CodePipeline. The source, build, test, and deployment stages of the pipeline have also been configured. However, upon review, the Lead DevOps engineer requested improvements to the current pipeline configuration to mitigate the risk of failed deployments. The deployment stage should release the new application version to a small subset of users only for verification before fully releasing the change to all users. The pipeline's deployment stage must be modified to meet this requirement.</p><p>Which of the following is the MOST suitable setup to implement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>CodeDeploy</strong> is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application.</p><p><img src="https://media.tutorialsdojo.com/aws-codedeploy-tutorialsdojo.png"></p><p>There are three ways traffic can shift during a deployment:</p><p><strong>Canary</strong>: Traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p><p><strong>Linear</strong>: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment.</p><p><strong>All-at-once</strong>: All traffic is shifted from the original Lambda function to the updated Lambda function version all at once.</p><p>The following table lists the predefined configurations available for AWS Lambda deployments.</p><p><img src="https://media.tutorialsdojo.com/public/2019-12-09_04-19-29-f0f93dd43b7a0d047a3ca8aae4dd728b.png"></p><p>Hence, the correct answer is: <strong>Define and publish the new version on the serverless application using AWS CloudFormation. Deploy the new version of the Lambda functions with AWS CodeDeploy using the </strong><code><strong>CodeDeployDefault.LambdaCanary10Percent5Minutes</strong></code><strong> predefined deployment configuration.</strong></p><p>The option that says: <strong>Publish a new version on the serverless application using AWS CloudFormation. Set up a manual approval action in CodePipeline in order to verify and approve the version that will be deployed. Once the change has been verified, invoke the Lambda functions to use the production alias using CodePipeline</strong> is incorrect. Although this setup allows you to verify the new version of the serverless application before the production deployment, it still fails to meet the requirement of deploying the change to a small subset of users only. This deployment configuration will release the new version to all users.</p><p>The option that says: <strong>Develop a custom script that uses AWS CLI to update the Lambda functions. Integrate the script in CodeBuild that will automatically publish the new version of the application by switching to the production alias </strong>is incorrect because developing a custom script to update the Lambda functions is not necessary. Moreover, AWS CodeBuild is simply not capable of publishing new versions of your Lambda functions. You should use AWS CodeDeploy instead.</p><p>The option that says: <strong>Use AWS CloudFormation to define and publish the new version on the serverless application. Deploy the new version of the Lambda functions with AWS CodeDeploy using the </strong><code><strong>CodeDeployDefault.LambdaAllAtOnce</strong></code><strong> predefined deployment configuration </strong>is incorrect because this configuration will deploy the changes to all users immediately. Keep in mind that the scenario requires that the change should only be deployed to a small subset of users only. You have to use a Canary deployment instead or the <code><strong><em>CodeDeployDefault.LambdaCanary10Percent5Minutes</em></strong></code><strong><em> </em></strong>configuration.<br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps-lambda.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codedeploy/?src=udemy">https://tutorialsdojo.com/aws-codedeploy/</a></p>',
        answers: [
          "<p>Publish a new version on the serverless application using AWS CloudFormation. Set up a manual approval action in CodePipeline in order to verify and approve the version that will be deployed. Once the change has been verified, invoke the Lambda functions to use the production alias using CodePipeline.</p>",
          "<p>Develop a custom script that uses AWS CLI to update the Lambda functions. Integrate the script in CodeBuild that will automatically publish the new version of the application by switching to the production alias.</p>",
          "<p>Define and publish the new version on the serverless application using AWS CloudFormation. Deploy the new version of the Lambda functions with AWS CodeDeploy using the <code>CodeDeployDefault.LambdaCanary10Percent5Minutes</code> predefined deployment configuration.</p>",
          "<p>Use AWS CloudFormation to define and publish the new version on the serverless application. Deploy the new version of the Lambda functions with AWS CodeDeploy using the <code>CodeDeployDefault.LambdaAllAtOnce</code> predefined deployment configuration.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company has recently developed a serverless application that is composed of several AWS Lambda functions and an Amazon DynamoDB database. For the CI/CD process, a continuous deployment pipeline has been built using GitHub, AWS CodeBuild, and AWS CodePipeline. The source, build, test, and deployment stages of the pipeline have also been configured. However, upon review, the Lead DevOps engineer requested improvements to the current pipeline configuration to mitigate the risk of failed deployments. The deployment stage should release the new application version to a small subset of users only for verification before fully releasing the change to all users. The pipeline's deployment stage must be modified to meet this requirement.Which of the following is the MOST suitable setup to implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248199,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading financial company provides GraphQL APIs to allow its various customers and partners to consume its global stock market data. The web service is hosted in an Auto Scaling group of EC2 instances behind an Application Load Balancer. All new versions of the service are deployed via a CI/CD pipeline. A DevOps Engineer has been instructed to track the health of the service in deployment to avoid any issues. If the latency of the service increases more than the defined threshold then the deployment should be halted until the service has been fully recovered. </p><p>Which solution should the Engineer implement to provide the FASTEST detection time for deployment issues?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can create a CloudWatch alarm for an instance or Amazon EC2 Auto Scaling group you are using in your CodeDeploy operations. An alarm watches a single metric over a time period you specify and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. CloudWatch alarms invoke actions when their state changes (for example, from <code>OK</code> to <code>ALARM</code>).</p><p>Using native CloudWatch alarm functionality, you can specify any of the actions supported by CloudWatch when an instance you are using in a deployment fails, such as sending an Amazon SNS notification or stopping, terminating, rebooting, or recovering an instance. For your CodeDeploy operations, you can configure a deployment group to stop a deployment whenever any CloudWatch alarm you associate with the deployment group is activated.</p><p><img src="https://media.tutorialsdojo.com/public/2019-11-08_02-51-47-23834c304f2c4d7cae0d4cdd1eb476db.png"></p><p>You can associate up to ten CloudWatch alarms with a CodeDeploy deployment group. If any of the specified alarms are activated, the deployment stops, and the status is updated to Stopped. You must grant CloudWatch permissions to your CodeDeploy service role to use this option.</p><p>Hence, the correct answer is: <strong>Calculate the average latency using Amazon CloudWatch metrics that monitors the Application Load Balancer. Associate a CloudWatch alarm with the CodeDeploy deployment group. When latency increases beyond the defined threshold, it will automatically trigger an alarm that automatically stops the ongoing deployment.</strong></p><p>The option that says: <strong>Collect the ELB access logs and use a Lambda function to calculate and detect the average latency of the service. When latency increases beyond the defined threshold, trigger the alarm and stop the current deployment</strong> is incorrect. Although ELB access logs contain latency information, you still need to parse the data using Lambda. The calculation process entails a significant amount of time. A faster solution would be to use Amazon CloudWatch metrics.</p><p>The option that says: <strong>Define the thresholds to roll back the deployments based on the latency using the </strong><code><strong>MinimumHealthyHosts</strong></code><strong> deployment configuration in AWS CodeDeploy. Rollback the deployment if the threshold was breached </strong>is incorrect because the <em>MinimumHealthyHosts</em> is just a property of the <em>DeploymentConfig</em> resource that defines how many instances must remain healthy during an AWS CodeDeploy deployment. It doesn\'t calculate the latency of the service.</p><p>The option that says: <strong>Enable Detailed Monitoring in CloudWatch to monitor and detect the latency in the Application Load Balancer. When latency increases beyond the defined threshold, trigger a CloudWatch alarm and stop the current deployment </strong>is incorrect because you cannot enable detailed monitoring in your Application Load Balancer. The detailed monitoring feature in CloudWatch is primarily used to collect data from your EC2 and other resources in 1-minute frequency for an additional cost. This level of frequency is already available for your load balancers. If there are requests flowing through the load balancer, Elastic Load Balancing measures and sends its metrics in 60-second intervals.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html ">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-cloudwatch-metrics.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-cloudwatch-metrics.html</a></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/elb-latency-troubleshooting/ ">https://aws.amazon.com/premiumsupport/knowledge-center/elb-latency-troubleshooting/</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy and Elastic Load Balancing Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/aws-codedeploy/?src=udemy">https://tutorialsdojo.com/aws-codedeploy/</a></p><p><a href="https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p>',
        answers: [
          "<p>Calculate the average latency using Amazon CloudWatch metrics that monitors the Application Load Balancer. Associate a CloudWatch alarm with the CodeDeploy deployment group. When latency increases beyond the defined threshold, it will automatically trigger an alarm that automatically stops the on-going deployment.</p>",
          "<p>Collect the ELB access logs and use a Lambda function to calculate and detect the average latency of the service. When latency increases beyond the defined threshold, trigger the alarm and stop the current deployment.</p>",
          "<p>Define the thresholds to roll back the deployments based on the latency using the <code>MinimumHealthyHosts</code> deployment configuration in AWS CodeDeploy. Rollback the deployment if the threshold was breached.</p>",
          "<p>Enable Detailed Monitoring in CloudWatch to monitor and detect the latency in the Application Load Balancer. When latency increases beyond the defined threshold, trigger a CloudWatch alarm and stop the current deployment.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A leading financial company provides GraphQL APIs to allow its various customers and partners to consume its global stock market data. The web service is hosted in an Auto Scaling group of EC2 instances behind an Application Load Balancer. All new versions of the service are deployed via a CI/CD pipeline. A DevOps Engineer has been instructed to track the health of the service in deployment to avoid any issues. If the latency of the service increases more than the defined threshold then the deployment should be halted until the service has been fully recovered. Which solution should the Engineer implement to provide the FASTEST detection time for deployment issues?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248201,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A DevOps Engineer in a leading aerospace engineering company has a hybrid cloud architecture that connects its on-premises data center with AWS via Direct Connect Gateway. There is a new requirement to implement an automated OS patching solution for all of the Windows servers hosted on-premises as well as in AWS Cloud. The AWS Systems Manager service should be utilized to automate the patching of the servers.</p><p>Which combination of steps should be set up to satisfy this requirement? (Select TWO.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>A hybrid environment includes on-premises servers and virtual machines (VMs) that have been configured for use with Systems Manager, including VMs in other cloud environments. After following the steps below, the users who have been granted permissions by the AWS account administrator can use AWS Systems Manager to configure and manage their organization\'s on-premises servers and virtual machines (VMs).</p><p>To configure your hybrid servers and VMs for AWS Systems Manager, just follow these provided steps:</p><p>1. Complete General Systems Manager Setup Steps<br>2. Create an IAM Service Role for a Hybrid Environment<br>3. Install a TLS certificate on On-Premises Servers and VMs<br>4. Create a Managed-Instance Activation for a Hybrid Environment<br>5. Install SSM Agent for a Hybrid Environment (Windows)<br>6. Install SSM Agent for a Hybrid Environment (Linux)<br>7. (Optional) Enable the Advanced-Instances Tier</p><p><br></p><p>Configuring your hybrid environment for Systems Manager enables you to do the following:</p><p>- Create a consistent and secure way to remotely manage your hybrid workloads from one location using the same tools or scripts.</p><p>- Centralize access control for actions that can be performed on your servers and VMs by using AWS Identity and Access Management (IAM).</p><p>- Centralize auditing and your view into the actions performed on your servers and VMs by recording all actions in AWS CloudTrail.</p><p>- Centralize monitoring by configuring CloudWatch Events and Amazon SNS to send notifications about service execution success.</p><p><img src="https://media.tutorialsdojo.com/public/how-it-works.png">After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as <em>managed instances</em>. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix "mi-". Amazon EC2 instance IDs use the prefix "i-".</p><p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants <code>AssumeRole</code> trust to the Systems Manager service. You only need to create the service role for a hybrid environment once for each AWS account.</p><p>Hence, the correct answers are:</p><p><strong>- Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRole</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</strong></p><p><strong>- Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>mi-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager.</strong></p><p>The option that says: <strong>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRoleWithSAML</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation </strong>is incorrect because you have to execute the <code><em>AssumeRole </em></code>operation instead and not the <code><em>AssumeRoleWithSAML</em></code><em> </em>operation<em>. </em>Moreover, you only need to set up a single IAM service role.</p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager </strong>is incorrect because the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix in the SSM console and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix<strong><em>.</em></strong></p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager State Manager</strong> is incorrect because the AWS Systems Manager State Manager is just a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. You have to apply the patches using the Systems Manager Patch Manager instead. In addition, the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS <code>AssumeRoleWithSAML</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
          "<p>Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS <code>AssumeRole</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
          "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>mi-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
          "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
          "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager State Manager.</p>",
        ],
      },
      correct_response: ["b", "c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A DevOps Engineer in a leading aerospace engineering company has a hybrid cloud architecture that connects its on-premises data center with AWS via Direct Connect Gateway. There is a new requirement to implement an automated OS patching solution for all of the Windows servers hosted on-premises as well as in AWS Cloud. The AWS Systems Manager service should be utilized to automate the patching of the servers.Which combination of steps should be set up to satisfy this requirement? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248203,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A financial company has several accounting applications that are hosted in AWS and used by thousands of small and medium businesses. As part of its Business Continuity Plan, the company is required to set up an automatic DNS failover for its applications to a disaster recovery (DR) environment. The DevOps team was instructed to configure Amazon Route 53 to automatically route to an alternate endpoint when the primary application stack in the us-west-1 region experiences an outage or degradation of service.</p><p>What steps should the team take to satisfy this requirement? (Select TWO.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.</p><p>To create an active-passive failover configuration with one primary record and one secondary record, you just create the records and specify <strong>Failover</strong> for the routing policy. When the primary resource is healthy, Route 53 responds to DNS queries using the primary record. When the primary resource is unhealthy, Route 53 responds to DNS queries using the secondary record.</p><p>You can configure a health check that monitors an endpoint that you specify either by IP address or by domain name. At regular intervals that you specify, Route 53 submits automated requests over the Internet to your application, server, or other resources to verify that it\'s reachable, available, and functional. Optionally, you can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.</p><p><img src="https://media.tutorialsdojo.com/public/td-amazon-route53-evaluate-target-health.png"></p><p>When Route 53 checks the health of an endpoint, it sends an HTTP, HTTPS, or TCP request to the IP address and port that you specified when you created the health check. For a health check to succeed, your router and firewall rules must allow inbound traffic from the IP addresses that the Route 53 health checkers use.</p><p>Hence, the correct answers are:</p><p><strong>- Set up health checks in Route 53 for non-alias records to each service endpoint. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks.</strong></p><p><strong>- Use a Failover routing policy configuration. Set up alias records in Route 53 that route traffic to AWS resources. Set the </strong><code><strong>Evaluate Target Health</strong></code><strong> option to </strong><code><strong>Yes</strong></code><strong>, then create all of the required non-alias records.</strong></p><p>The option that says: <strong>Set up a record in Route 53 with a Weighted routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints</strong> is incorrect because Weighted routing simply lets you associate multiple resources with a single domain name (pasigcity.com) or subdomain name (blog.pasigcity.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.</p><p>The option that says:<strong> Set up an Amazon CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom AWS Lambda function. Execute the </strong><code><strong>ChangeResourceRecordSets</strong></code><strong> API call using the function to initiate the failover to the secondary DNS record</strong> is incorrect because you only have to use a Failover routing policy. Calling the Route 53 API is not applicable nor useful at all in this scenario.</p><p>The option that says: <strong>Set up a record in Route 53 with a latency routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints</strong> is incorrect because the Latency routing policy simply improves the application performance for your users by serving their requests from the AWS Region that provides the lowest latency. You have to use a Failover routing policy instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html</a></p><p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html</a></p><p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-route-53/?src=udemy">https://tutorialsdojo.com/amazon-route-53/</a></p>',
        answers: [
          "<p>Set up health checks in Route 53 for non-alias records to each service endpoint. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks.</p>",
          "<p>Use a Failover routing policy configuration. Set up alias records in Route 53 that route traffic to AWS resources. Set the <code>Evaluate Target Health</code> option to <code>Yes</code>, then create all of the required non-alias records.</p>",
          "<p>Set up a record in Route 53 with a Weighted routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints.</p>",
          "<p>Set up an Amazon CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom AWS Lambda function. Execute the <code>ChangeResourceRecordSets</code> API call using the function to initiate the failover to the secondary DNS record.</p>",
          "<p>Set up a record in Route 53 with a latency routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints.</p>",
        ],
      },
      correct_response: ["a", "b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A financial company has several accounting applications that are hosted in AWS and used by thousands of small and medium businesses. As part of its Business Continuity Plan, the company is required to set up an automatic DNS failover for its applications to a disaster recovery (DR) environment. The DevOps team was instructed to configure Amazon Route 53 to automatically route to an alternate endpoint when the primary application stack in the us-west-1 region experiences an outage or degradation of service.What steps should the team take to satisfy this requirement? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248205,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A leading food and beverage company is currently migrating its Docker-based application hosted on-premises to AWS. The application will be hosted in an Amazon ECS cluster with multiple ECS services to run its various workloads. The cluster is configured to use an Application Load Balancer to distribute traffic evenly across the tasks in your service. A DevOps Engineer was instructed to configure the cluster to automatically collect logs from all of the services and upload them to an S3 bucket for near-real-time analysis.</p><p>How should the Engineer configure the ECS setup to satisfy these requirements? (Select THREE.)</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p>You can configure the containers in your tasks to send log information to CloudWatch Logs. If you are using the Fargate launch type for your tasks, this allows you to view the logs from your containers. If you are using the EC2 launch type, this enables you to view different logs from your containers in one convenient location, and it prevents your container logs from taking up disk space on your container instances.</p><p>The type of information that is logged by the containers in your task depends mostly on their <code>ENTRYPOINT</code> command. By default, the logs that are captured show the command output that you would normally see in an interactive terminal if you ran the container locally, which are the <code>STDOUT</code> and <code>STDERR</code> I/O streams. The <code>awslogs</code> log driver simply passes these logs from Docker to CloudWatch Logs.</p><p><img src="https://media.tutorialsdojo.com/public/awslogs-console-config.png"></p><p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as a Amazon Kinesis stream, Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. To begin subscribing to log events, create the receiving source, such as a Kinesis stream, where the events will be delivered. A subscription filter defines the filter pattern to use for filtering which log events get delivered to your AWS resource, as well as information about where to send matching log events to.</p><p>Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client\'s IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p><p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. You can disable access logging at any time.</p><p>Each access log file is automatically encrypted before it is stored in your S3 bucket and decrypted when you access it. You do not need to take any action; the encryption and decryption is performed transparently. Each log file is encrypted with a unique key, which is itself encrypted with a master key that is regularly rotated.</p><p>Hence, the correct answers are:</p><p><strong>- Create the required IAM Policy and attach it to the </strong><code><strong>ecsInstanceRole</strong></code><strong>. Install the Amazon CloudWatch Logs agent on the Amazon ECS instances. Use the </strong><code><strong>awslogs</strong></code><strong> Log Driver in the Amazon ECS task definition.</strong></p><p><strong>- Capture detailed information about requests sent to your load balancer by enabling access logging on the Application Load Balancer. Configure it to store the logs to the S3 bucket.</strong></p><p><strong>- Create a CloudWatch Logs subscription filter integrated with Amazon Kinesis to analyze the logs. Configure the CloudWatch Logs to export the logs to an S3 bucket.</strong></p><p>The option that says: <strong>Set up Amazon Macie to analyze the access logs in the S3 bucket</strong> is incorrect because Amazon Macie is just a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. It can\'t analyze logs in near-real-time unlike Kinesis.</p><p>The option that says: <strong>Integrate a Lambda function with CloudWatch Events to run a process every minute that invokes the </strong><code><strong>CreateLogGroup</strong></code><strong> and </strong><code><strong>CreateExportTask</strong></code><strong> CloudWatch Logs API to push the logs to the S3 bucket</strong> is incorrect. Although this step may work, it is still better to use the <em>awslogs</em> Log Driver instead of developing a custom scheduled job. It is unnecessary since you only need to change the log driver in your task definition.</p><p>The option that says: <strong>Capture detailed information about requests sent to your load balancer by using Detailed Monitoring in CloudWatch. Configure it to store the logs to the S3 bucket</strong> is incorrect because the Detailed Monitoring feature simply sends the metric data for your instance to CloudWatch in 1-minute periods.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html</a></p><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p><p><br></p><p><strong>Check out these Amazon ECS and AWS Elastic Load Balancing Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><a href="https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p>',
        answers: [
          "<p>Create the required IAM Policy and attach it to the <code>ecsInstanceRole</code>. Install the Amazon CloudWatch Logs agent on the Amazon ECS instances. Use the <code>awslogs</code> Log Driver in the Amazon ECS task definition.</p>",
          "<p>Set up Amazon Macie to analyze the access logs in the S3 bucket.</p>",
          "<p>Integrate a Lambda function with CloudWatch Events to run a process every minute that invokes the <code>CreateLogGroup</code> and <code>CreateExportTask</code> CloudWatch Logs API to push the logs to the S3 bucket.</p>",
          "<p>Capture detailed information about requests sent to your load balancer by enabling access logging on the Application Load Balancer. Configure it to store the logs to the S3 bucket.</p>",
          "<p>Capture detailed information about requests sent to your load balancer by using Detailed Monitoring in CloudWatch. Configure it to store the logs to the S3 bucket.</p>",
          "<p>Create a CloudWatch Logs subscription filter integrated with Amazon Kinesis to analyze the logs.<strong> </strong>Configure the CloudWatch Logs to export the logs to an S3 bucket.</p>",
        ],
      },
      correct_response: ["a", "d", "f"],
      section: "Monitoring and Logging",
      question_plain:
        "A leading food and beverage company is currently migrating its Docker-based application hosted on-premises to AWS. The application will be hosted in an Amazon ECS cluster with multiple ECS services to run its various workloads. The cluster is configured to use an Application Load Balancer to distribute traffic evenly across the tasks in your service. A DevOps Engineer was instructed to configure the cluster to automatically collect logs from all of the services and upload them to an S3 bucket for near-real-time analysis.How should the Engineer configure the ECS setup to satisfy these requirements? (Select THREE.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248101,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A DevOps engineer has been tasked to implement a reliable solution to maintain all of their Windows and Linux servers both in AWS and in on-premises data center. There should be a system that allows them to easily update the operating systems of their servers and apply the core application patches with minimum management overhead. The patches must be consistent across all levels in order to meet the company’s security compliance. </p><p>Which of the following is the MOST suitable solution that you should implement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with both security-related and other types of updates. You can use the Patch Manager to apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications.) You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src="https://media.tutorialsdojo.com/public/patch-groups-how-it-works.png"></p><p>Patch Manager uses <strong><em>patch baselines</em></strong>, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p>A <strong><em>resource group</em></strong> is a collection of AWS resources that are all in the same AWS Region and that match criteria provided in a query. You build queries in the AWS Resource Groups (Resource Groups) console or pass them as arguments to Resource Groups commands in the AWS CLI.</p><p>With AWS Resource Groups, you can create a custom console that organizes and consolidates information based on criteria that you specify in tags. After you add resources to a group you created in Resource Groups, use AWS Systems Manager tools such as Automation to simplify management tasks on your resource group. You can also use the resource groups you create as the basis for viewing monitoring and configuration insights in Systems Manager.</p><p>Hence, the correct answer is: <strong>Configure and install AWS Systems Manager agent on all of the EC2 instances in your VPC as well as your physical servers on-premises. Use the Systems Manager Patch Manager service and specify the required Systems Manager Resource Groups for your hybrid architecture. Utilize a preconfigured patch baseline and then run scheduled patch updates during maintenance windows.</strong></p><p>The option that says: <strong>Configure and install the AWS CodeDeploy agent on all of your existing EC2 instances in your VPC and your on-premises servers. Launch a new Elastic Beanstalk environment for each OS to automate the execution of the patch commands during maintenance windows. Integrate Elastic Beanstalk and CodeDeploy for deployment as well as maintaining your systems with the latest OS and application patches</strong> is incorrect because AWS Elastic Beanstalk is not capable to manage servers both in AWS and on-premises data centers. In addition, you cannot directly integrate Elastic Beanstalk and CodeDeploy for the purposes of application deployment. You either use one of these two but not both.</p><p>The option that says: <strong>Develop a custom python script to install the latest OS patches on the Linux servers. Set up a scheduled job to automatically run this script using the cron scheduler on Linux servers. Enable Windows Update in order to automatically patch Windows servers or set up a scheduled task using Windows Task Scheduler to periodically run the python script</strong> is incorrect because this solution entails a high management overhead since you need to develop a new script and maintain a number of <em>cron</em> schedulers in your Linux servers and Windows Task Scheduler jobs on your Windows servers.</p><p>The option that says: <strong>Store the login credentials of each Linux and Windows servers on the AWS Systems Manager Parameter Store. Use Systems Manager Resource Groups to set up one group for your Linux servers and another one for your Windows servers. Remotely login, run, and deploy the patch updates to all of your servers using the credentials stored in the Systems Manager Parameter Store and through the use of the Systems Manager Run Command</strong> is incorrect because this is not a suitable service to use to handle the patching activities of your servers. You have to use AWS Systems Manager Patch Manager instead.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-resource-groups.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-resource-groups.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "Configure and install AWS Systems Manager agent on all of the EC2 instances in your VPC as well as your physical servers on-premises. Use the Systems Manager Patch Manager service and specify the required Systems Manager Resource Groups for your hybrid architecture. Utilize a preconfigured patch baseline and then run scheduled patch updates during maintenance windows.",
          "<p>Configure and install the AWS CodeDeploy agent on all of your existing EC2 instances in your VPC and your on-premises servers. Launch a new Elastic Beanstalk environment for each OS to automate the execution of the patch commands during maintenance windows. Integrate Elastic Beanstalk and CodeDeploy for deployment as well as maintaining your systems with the latest OS and application patches.</p>",
          "Develop a custom python script to install the latest OS patches on the Linux servers. Set up a scheduled job to automatically run this script using the cron scheduler on Linux servers. Enable Windows Update in order to automatically patch Windows servers or set up a scheduled task using Windows Task Scheduler to periodically run the python script.",
          "Store the login credentials of each Linux and Windows servers on the AWS Systems Manager Parameter Store. Use Systems Manager Resource Groups to set up one group for your Linux servers and another one for your Windows servers. Remotely login, run, and deploy the patch updates to all of your servers using the credentials stored in the Systems Manager Parameter Store and through the use of the Systems Manager Run Command.",
        ],
      },
      correct_response: ["a"],
      section: "Security and Compliance",
      question_plain:
        "A DevOps engineer has been tasked to implement a reliable solution to maintain all of their Windows and Linux servers both in AWS and in on-premises data center. There should be a system that allows them to easily update the operating systems of their servers and apply the core application patches with minimum management overhead. The patches must be consistent across all levels in order to meet the company’s security compliance. Which of the following is the MOST suitable solution that you should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248103,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A fast-growing company has multiple AWS accounts which are consolidated using AWS Organizations, and the company expects to add new accounts soon. The DevOps engineer was instructed to design a centralized logging solution to deliver all Amazon VPC Flow Logs and Amazon CloudWatch Logs across all sub-accounts to its dedicated Audit account for compliance purposes. The logs should also be properly indexed in order to perform search, retrieval, and analysis.</p><p>Which of the following is the MOST suitable solution that the engineer should implement to meet the above requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can load streaming data into your Amazon OpenSearch Service domain from many different sources in AWS. Some sources, like Amazon Data Firehose and Amazon CloudWatch Logs, have built-in support for Amazon OpenSearch Service (successor to Amazon ElasticSearch). Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers. The Lambda functions respond to new data by processing it and streaming it to your domain.</p><p><img src="https://media.tutorialsdojo.com/public/TD-Amazon-OpenSearch-Service-02-04-2025.png"></p><p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, Amazon Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. To begin subscribing to log events, create the receiving source, such as a Kinesis stream, where the events will be delivered. A subscription filter defines the filter pattern to use for filtering which log events get delivered to your AWS resource, as well as information about where to send matching log events to.</p><p>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Amazon Kinesis stream to perform custom processing and analysis. Custom processing is especially useful when you collaborate and analyze data across many accounts. For example, a company\'s information security group might want to analyze data for real-time intrusion detection or anomalous behaviors so it could conduct an audit of accounts in all divisions in the company by collecting their federated production logs for central processing.</p><p>A real-time stream of event data across those accounts can be assembled and delivered to the information security groups, who can use Kinesis to attach the data to their existing security analytic systems. Kinesis streams are currently the only resource supported as a destination for cross-account subscriptions.</p><p>Hence, the correct solution is: <strong>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</strong></p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account</strong> is incorrect. While Lambda functions can be used to process logs, relying solely on a Lambda function to handle a large volume of logs might only lead to performance bottlenecks and scalability issues. Lambda has concurrency limits, and processing logs from multiple sub-accounts may exceed these limits, especially as the company grows and more accounts are added.</p><p>The option that says: <strong>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account </strong>is incorrect because the CloudWatch subscription filter doesn\'t directly support SQS. You should use a Kinesis Data Stream, Kinesis Firehose, or Lambda function.</p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account</strong> is incorrect. Although this approach typically works, using a self-hosted OpenSearch cluster on an EC2 instance introduces unnecessary complexity and overhead regarding management, scaling, and availability. AWS-managed OpenSearch Service would be a more efficient and cost-effective option. Additionally, relying on EC2 for hosting OpenSearch adds management and operational burdens, whereas using AWS-managed services like Kinesis Data Streams or Firehose with OpenSearch is more reliable and easier to scale.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html</a></p><p><a href="https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>',
        answers: [
          "<p>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account.</p>",
          "<p>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</p>",
          "<p>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account.</p>",
          "<p>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Monitoring and Logging",
      question_plain:
        "A fast-growing company has multiple AWS accounts which are consolidated using AWS Organizations, and the company expects to add new accounts soon. The DevOps engineer was instructed to design a centralized logging solution to deliver all Amazon VPC Flow Logs and Amazon CloudWatch Logs across all sub-accounts to its dedicated Audit account for compliance purposes. The logs should also be properly indexed in order to perform search, retrieval, and analysis.Which of the following is the MOST suitable solution that the engineer should implement to meet the above requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248105,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A startup prioritizes a serverless approach, using AWS Lambda for new workloads to analyze performance and identify bottlenecks. The startup aims to transition to self-managed services on top of Amazon EC2 later if it is more cost-effective. To do this, a solution for granular monitoring of every component of the call graph, including services and internal functions, for all requests, is required. In addition, the startup wants engineers and other stakeholders to be notified of performance irregularities as soon as such irregularities arise.</p><p>Which option will meet these requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS X-Ray</strong> is a service that analyzes the execution and performance behavior of distributed applications. Traditional debugging methods don’t work so well for microservice-based applications, in which there are multiple, independent components running on different services. X-Ray enables rapid diagnosis of errors, slowdowns, and timeouts by breaking down application latency.</p><p><img alt="AWSXRay_LambdaServiceMap" height="535" src="https://media.tutorialsdojo.com/public/AWSXRay_LambdaServiceMap_22mar2024.png" width="1000"></p><p><em>Insights</em> is a feature of X-Ray that records performance outliers and tracks their impact until resolved. With insights, issues can be identified where they are occurring and what is causing them, and be triaged with the appropriate severity. Insights notifications are sent as the issue changes over time and can be integrated with your monitoring and alerting solution using Amazon EventBridge.</p><p>With an external <strong>AWS Lambda Extension</strong> using the telemetry API and X-Ray active tracing enabled, workflows are broken down into segments corresponding to the unit of work each Lambda function does. This can even be further broken down into subsegments by instrumenting calls to dependencies and related work, such as when a Lambda function requires data from DynamoDB and additional logic to process the response.</p><p>Lambda extensions come in two flavors: external and internal. The main difference is that an external extension runs in a separate process and can run longer to clean up after the Lambda function terminates, whereas an internal one runs in-process.<br><img alt="AWSLambdaExtension_overview_full_sequence" height="670" src="https://media.tutorialsdojo.com/public/AWSLambdaExtension_overview_full_sequence.png" width="1000"></p><p>Hence, the correct answer is: <strong>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms.</strong></p><p>The option that says: <strong>Create an internal extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms</strong> is incorrect. An internal Lambda extension only works in-process. In the scenario, since X-Ray is the solution chosen for tracing and the X-Ray daemon runs as a separate process, an implementation based on an internal Lambda extension will not work.</p><p>The option that says: <strong>Consolidate workflows spanning multiple Lambda functions into 1 function per workflow. Create an external extension and enable AWS X-Ray active tracing to instrument functions into segments. Assign an execution role allowing X-Ray actions. Enable X-Ray insights and set up appropriate Amazon EventBridge rules and Amazon CloudWatch alarms</strong> is incorrect. Aside from adding unnecessary engineering work, this primarily prevents the reuse of functions in different workflows and increases the chance of undesirable duplication. Use X-Ray groups instead to group traces from individual workflows.</p><p>The option that says: <strong>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable Amazon CloudWatch Logs insights. Configure relevant Amazon EventBridge rules and CloudWatch alarms</strong> is incorrect. Although Cloudwatch Logs insights and X-Ray insights both analyze and surface emergent issues from data, they do it on very different types of data -- logs and traces, respectively. As logs do not have graph-like relationships of trace segments/spans, they may require more work or data to surface the same issues.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html">https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html</a></p><p><a href="https://docs.aws.amazon.com/xray/latest/devguide/xray-console-groups.html">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-groups.html</a></p><p><a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-extensions.html">https://docs.aws.amazon.com/lambda/latest/dg/lambda-extensions.html</a></p><p><br></p><p><strong>Check out this AWS X-Ray Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-x-ray/?src=udemy">https://tutorialsdojo.com/aws-x-ray/</a></p>',
        answers: [
          "<p>Create an internal extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms.</p>",
          "<p>Consolidate workflows spanning multiple Lambda functions into 1 function per workflow. Create an external extension and enable AWS X-Ray active tracing to instrument functions into segments. Assign an execution role allowing X-Ray actions. Enable X-Ray insights and set up appropriate Amazon EventBridge rules and Amazon CloudWatch alarms.</p>",
          "<p>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable Amazon CloudWatch Logs insights. Configure relevant Amazon EventBridge rules and CloudWatch alarms.</p>",
          "<p>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Monitoring and Logging",
      question_plain:
        "A startup prioritizes a serverless approach, using AWS Lambda for new workloads to analyze performance and identify bottlenecks. The startup aims to transition to self-managed services on top of Amazon EC2 later if it is more cost-effective. To do this, a solution for granular monitoring of every component of the call graph, including services and internal functions, for all requests, is required. In addition, the startup wants engineers and other stakeholders to be notified of performance irregularities as soon as such irregularities arise.Which option will meet these requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248107,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A DevOps engineer has been tasked with implementing configuration management for the company's infrastructure in AWS. To adhere to the company's strict security policies, the solution must include a near real-time dashboard that displays the compliance status of the systems and can detect any violations.</p><p>Which solution would be able to meet the above requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Config</strong> provides you a visual dashboard to help you quickly spot non-compliant resources and take appropriate action. IT Administrators, Security Experts, and Compliance Officers can see a shared view of your AWS resources compliance posture.</p><p>When you run your applications on AWS, you usually use AWS resources, which you must create and manage collectively. As the demand for your application keeps growing, so does your need to keep track of your AWS resources.</p><p><img src="https://media.tutorialsdojo.com/public/how-AWSconfig-works.png"></p><p>To exercise better governance over your resource configurations and to detect resource misconfigurations, you need fine-grained visibility into what resources exist and how these resources are configured at any time. You can use AWS Config to notify you whenever resources are created, modified, or deleted without having to monitor these changes by polling the calls made to each resource.</p><p>You can use AWS Config rules to evaluate the configuration settings of your AWS resources. When AWS Config detects that a resource violates the conditions in one of your rules, AWS Config flags the resource as non-compliant and sends a notification. AWS Config continuously evaluates your resources as they are created, changed, or deleted.</p><p>Hence, the correct answer is: <strong>Use AWS Config to record all configuration changes and store the data reports to Amazon S3. Use Amazon QuickSight to analyze the dataset.</strong></p><p>The option that says: <strong>Use AWS Service Catalog to create the required resource configurations for its compliance posture. Monitor the compliance and violations of its cloud resources using a custom CloudWatch dashboard with an integrated Amazon SNS to send notifications</strong><em> </em>is incorrect. Although AWS Service Catalog can be used for resource configuration, it is not typically capable of detecting violations of your AWS configuration rules.</p><p>The option that says: <strong>Tag all the resources and use Trusted Advisor to monitor both the compliant and non-compliant resources. Use the AWS Management Console to monitor the status of the compliance posture</strong> is incorrect because the Trusted Advisor service is not suitable for configuration management and automatic violation detection. You should use AWS Config instead.</p><p>The option that says: <strong>Use Amazon Inspector to monitor the compliance posture of the systems and store the reports in Amazon CloudWatch Logs. Use a CloudWatch dashboard with a custom metric filter to monitor and view all of the specific compliance requirements</strong> is incorrect because the Amazon Inspector service is primarily used to help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html">https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html</a></p><p><a href="https://docs.aws.amazon.com/quicksight/latest/user/QS-compliance.html">https://docs.aws.amazon.com/quicksight/latest/user/QS-compliance.html</a></p><p><a href="https://aws.amazon.com/config/features/">https://aws.amazon.com/config/features/</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-config/?src=udemy">https://tutorialsdojo.com/aws-config/</a></p>',
        answers: [
          "<p>Use AWS Service Catalog to create the required resource configurations for its compliance posture. Monitor the compliance and violations of its cloud resources using a custom CloudWatch dashboard with an integrated Amazon SNS to send notifications.</p>",
          "Use AWS Config to record all configuration changes and store the data reports to Amazon S3. Use Amazon QuickSight to analyze the dataset.",
          "<p>Tag all the resources and use Trusted Advisor to monitor both the compliant and non-compliant resources. Use the AWS Management Console to monitor the status of the compliance posture.</p>",
          "<p>Use Amazon Inspector to monitor the compliance posture of the systems and store the reports in Amazon CloudWatch Logs. Use a CloudWatch dashboard with a custom metric filter to monitor and view all of the specific compliance requirements.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Monitoring and Logging",
      question_plain:
        "A DevOps engineer has been tasked with implementing configuration management for the company's infrastructure in AWS. To adhere to the company's strict security policies, the solution must include a near real-time dashboard that displays the compliance status of the systems and can detect any violations.Which solution would be able to meet the above requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248109,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A recent production incident has caused a data breach in one of the company's flagship applications, which is hosted in an Auto Scaling group of Amazon EC2 instances. In order to prevent this from happening again, a DevOps engineer was tasked to implement a solution that would automatically terminate any instance in production that was manually logged into via SSH. All of the EC2 instances that are being used by the application already have an Amazon CloudWatch Logs agent installed.</p><p>Which of the following is the MOST automated solution that the DevOps engineer should implement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, Amazon Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. To begin subscribing to log events, create the receiving source, such as a Kinesis stream, where the events will be delivered. A subscription filter defines the filter pattern to use for filtering which log events get delivered to your AWS resource, as well as information about where to send matching log events to.</p><p>CloudWatch Logs also produces CloudWatch metrics about the forwarding of log events to subscriptions. You can use a subscription filter with Kinesis, Lambda, or Data Firehose.</p><p><img alt="Monitoring CloudWatch Logs subscription Diagram" height="502" src="https://media.tutorialsdojo.com/public/td-Monitoring+CloudWatch+Logs+subscription+Diagram-2Dec2025.png" width="1000"></p><p>Hence, the correct answer is: <strong>Set up a CloudWatch Logs subscription with an AWS Lambda function which is configured to add a </strong><code><strong>FOR_DELETION</strong></code><strong> tag to the EC2 instance that produced the SSH login event. Run another Lambda function every day using the Amazon EventBridge rule to terminate all EC2 instances with the custom tag for deletion.</strong></p><p>The option that says: <strong>Set up and integrate a CloudWatch Logs subscription with AWS Step Functions to add a special </strong><code><strong>FOR_DELETION</strong></code><strong> tag to the specific EC2 instance that had an SSH login event. Create an Amazon EventBridge rule to trigger a second AWS Lambda function everyday at 12 PM that will terminate all of the EC2 instances with this tag</strong> is incorrect because a CloudWatch Logs subscription cannot be directly integrated with an AWS Step Functions application.</p><p>The option that says: <strong>Set up a CloudWatch Alarm that will be triggered when an SSH login event occurs and configure it also to send a notification to an Amazon SNS topic once the alarm is triggered. Instruct the Support and Operations team to subscribe to the SNS topic and then manually terminate the detected EC2 instance as soon as possible</strong> is incorrect. Although you can configure your Amazon CloudWatch Alarms to send a notification to SNS, this solution simply involves a manual process. Remember that the scenario is asking for an automated system for this scenario.</p><p>The option that says: <strong>Set up a CloudWatch Alarm that will be triggered when there is an SSH login event and configure it to send a notification to an Amazon Simple Queue Service (Amazon SQS) queue. Launch a group of EC2 worker instances to consume the messages from the SQS queue and terminate the detected EC2 instances</strong> is incorrect because using SQS as well as worker instances is unnecessary since you can simply use Lambda functions for processing. In addition, Amazon CloudWatch Alarms can only send notifications to SNS and not SQS.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#LambdaFunctionExample">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#LambdaFunctionExample</a></p><p><a href="https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/">https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Scheduled-Rule.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Scheduled-Rule.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>',
        answers: [
          "<p>Set up and integrate a CloudWatch Logs subscription with AWS Step Functions to add a special <code>FOR_DELETION</code> tag to the specific EC2 instance that had an SSH login event. Create an Amazon EventBridge rule to trigger a second AWS Lambda function everyday at 12 PM that will terminate all of the EC2 instances with this tag.</p>",
          "<p>Set up a CloudWatch Alarm that will be triggered when an SSH login event occurs and configure it also to send a notification to an Amazon SNS topic once the alarm is triggered. Instruct the Support and Operations team to subscribe to the SNS topic and then manually terminate the detected EC2 instance as soon as possible.</p>",
          "<p>Set up a CloudWatch Alarm that will be triggered when there is an SSH login event and configure it to send a notification to an Amazon Simple Queue Service (Amazon SQS) queue. Launch a group of EC2 worker instances to consume the messages from the SQS queue and terminate the detected EC2 instances.</p>",
          "<p>Set up a CloudWatch Logs subscription with an AWS Lambda function which is configured to add a <code>FOR_DELETION</code> tag to the EC2 instance that produced the SSH login event. Run another Lambda function every day using the Amazon EventBridge rule to terminate all EC2 instances with the custom tag for deletion.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Monitoring and Logging",
      question_plain:
        "A recent production incident has caused a data breach in one of the company's flagship applications, which is hosted in an Auto Scaling group of Amazon EC2 instances. In order to prevent this from happening again, a DevOps engineer was tasked to implement a solution that would automatically terminate any instance in production that was manually logged into via SSH. All of the EC2 instances that are being used by the application already have an Amazon CloudWatch Logs agent installed.Which of the following is the MOST automated solution that the DevOps engineer should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248111,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is planning to deploy a new version of their legacy application in AWS which is deployed to an Auto Scaling group of EC2 instances with an Application Load Balancer in front. To avoid any disruption of their services, they need to implement canary testing first before all of the traffic is shifted to the new application version.</p><p>Which of the following solutions can meet this requirement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>The purpose of a canary deployment is to reduce the risk of deploying a new version that impacts the <a href="https://wa.aws.amazon.com/wat.concept.workload.en.html" title="The set of components that together deliver business value.">workload</a>. The method will incrementally deploy the new version, making it visible to new users in a slow fashion. As you gain confidence in the deployment, you will deploy it to replace the current version in its entirety.</p><p><img src="https://media.tutorialsdojo.com/public/Upgrades_Image1.jpeg"></p><p>To properly implement the canary deployment, you should do the following steps:</p><p>- Use a router or load balancer that allows you to send a small percentage of users to the new version.</p><p>- Use a dimension on your KPIs to indicate which version is reporting the metrics.</p><p>- Use the metric to measure the success of the deployment; this indicates whether the deployment should continue or rollback.</p><p>- Increase the load on the new version until either all users are on the new version or you have fully rolled back.</p><p><br></p><p>Hence, the correct answer is: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</strong></p><p>The option that says: <strong>Do a Canary deployment using CodeDeploy with a </strong><code><strong>CodeDeployDefault.LambdaCanary10Percent30Minutes</strong></code><strong> deployment configuration</strong> is incorrect because this specific configuration type is only applicable for Lambda functions and for the applications hosted in an Auto Scaling group.</p><p>The option that says: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers</strong> is incorrect because you can\'t use CloudFront to adjust the weight of the incoming traffic to your application. You should use Route 53 instead.</p><p>The option that says: <strong>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment</strong> is incorrect because you can only integrate a Network Load Balancer to your Amazon API Gateway. Moreover, this service is only applicable for APIs, not full-fledged web applications.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html">https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html</a></p><p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html</a></p><p><a href="https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/">https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/</a></p>',
        answers: [
          "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</p>",
          "<p>Do a Canary deployment using CodeDeploy with a <code>CodeDeployDefault.LambdaCanary10Percent30Minutes</code> deployment configuration.</p>",
          "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers.</p>",
          "<p>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A company is planning to deploy a new version of their legacy application in AWS which is deployed to an Auto Scaling group of EC2 instances with an Application Load Balancer in front. To avoid any disruption of their services, they need to implement canary testing first before all of the traffic is shifted to the new application version.Which of the following solutions can meet this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248113,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>Due to regional growth, an e-commerce company has decided to expand its global operations. The app's REST API web services run in an Auto Scaling group of Amazon EC2 instances across multiple Availability Zones behind an Application Load Balancer. The application uses a single Amazon Aurora MySQL database instance in the AWS Region where it is based.</p><p>The company aims to consolidate and store product catalog data into a single data source across all regions. To comply with data privacy regulations, personal information, purchases, and financial data must be stored within each respective region.</p><p>Which of the following options can meet the above requirements and entail the LEAST amount of change to the application?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon Aurora</strong> is a fully managed relational database service that provides high performance and availability. It is compatible with MySQL and PostgreSQL and is designed to offer the speed and reliability of commercial databases while keeping the simplicity and cost-effectiveness of open-source databases. Aurora includes features such as automated backups, snapshots, and seamless scaling, making it a strong solution for modern applications. One of its key features is the ability to create read replicas, allowing for scaling of read operations across multiple Availability Zones and regions. This is especially beneficial for global applications that need low-latency access to data.</p><p><img alt="Amazon Aurora" height="997" src="https://media.tutorialsdojo.com/public/2019-11-07_02-47-26-7a6fab95ae1934c4162bb8a3764f8cba.png" width="1000"></p><p>Aurora Global Database lets you create read replicas in multiple AWS Regions with replication latency typically under one second. This allows a single database to serve users worldwide with low latency and provides resilience in case of regional failures. However, Aurora Global Database replicates all data across regions, which may conflict with data privacy rules requiring certain data to stay within specific regions.</p><p>To meet these data sovereignty requirements, organizations can use Aurora Global Database for shared data like product catalogs, while deploying separate local Aurora clusters in each region to store sensitive personal and financial data.</p><p>Hence, the correct answer is: <strong>Set up multiple read replicas in your Aurora cluster to store the product catalog data. Launch an additional local Aurora instance in each AWS Region to store customers\' personal information and financial data.</strong></p><p>The option that says: <strong>Set up a new Amazon Redshift database to store the product catalog. Launch a new set of Amazon DynamoDB tables to store their customers\' personal information and financial data</strong> is incorrect because this solution only entails a significant overhead of refactoring your application to use Redshift instead of Aurora. Moreover, Redshift is primarily used as a data warehouse solution and is not suitable for OLTP or e-commerce websites.</p><p>The option that says: <strong>Set up an Amazon DynamoDB global table to store the product catalog data of the e-commerce website. Use regional DynamoDB tables to store their customers\' personal information and financial data</strong> is incorrect. Although the use of Global and Regional DynamoDB is acceptable, this solution still entails a lot of changes to the application. There is no assurance that the application can work with a NoSQL database, and even so, you have to implement a series of code changes in order for this solution to work.</p><p>The option that says: <strong>Set up multiple read replicas in your Aurora cluster to store the product catalog data. Launch a new Amazon DynamoDB global table for storing their customers\' personal information and financial data</strong> is incorrect. Although the use of Read Replicas is appropriate, this solution simply requires you to do a lot of code changes since you will use a different database to store your regional data.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database.advantages">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database.advantages</a></p><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html</a></p><p><br></p><p><strong>Check out this Amazon Aurora Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-aurora/?src=udemy">https://tutorialsdojo.com/amazon-aurora/</a></p>',
        answers: [
          "<p>Set up a new Amazon Redshift database to store the product catalog. Launch a new set of Amazon DynamoDB tables to store their customers' personal information and financial data.</p>",
          "<p>Set up an Amazon DynamoDB global table to store the product catalog data of the e-commerce website. Use regional DynamoDB tables to store their customers' personal information and financial data.</p>",
          "<p>Set up multiple read replicas in your Aurora cluster to store the product catalog data. Launch an additional local Aurora instance in each AWS Region to store customers' personal information and financial data.</p>",
          "<p>Set up multiple read replicas in your Aurora cluster to store the product catalog data. Launch a new Amazon DynamoDB global table for storing their customers' personal information and financial data.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "Due to regional growth, an e-commerce company has decided to expand its global operations. The app's REST API web services run in an Auto Scaling group of Amazon EC2 instances across multiple Availability Zones behind an Application Load Balancer. The application uses a single Amazon Aurora MySQL database instance in the AWS Region where it is based.The company aims to consolidate and store product catalog data into a single data source across all regions. To comply with data privacy regulations, personal information, purchases, and financial data must be stored within each respective region.Which of the following options can meet the above requirements and entail the LEAST amount of change to the application?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248115,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company plans to implement a monitoring system to track the cost-effectiveness of its Amazon EC2 resources across multiple AWS accounts. All existing resources are appropriately tagged to reflect the corresponding environment, department, and business unit for cost allocation purposes.</p><p>The company has instructed its DevOps engineer to automate infrastructure cost optimization across various shared environments and accounts, including detecting EC2 instances with low utilization.</p><p>Which is the MOST suitable solution that the DevOps engineer should implement in this scenario?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Trusted Advisor</strong> draws upon best practices learned from serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment, and then makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps. All AWS customers have access to five Trusted Advisor checks. Customers with a Business or Enterprise support plan can view all Trusted Advisor checks.</p><p>AWS Trusted Advisor is integrated with the Amazon EventBridge and Amazon CloudWatch services. You can use Amazon EventBridge to detect and react to changes in the status of Trusted Advisor checks and you can use Amazon CloudWatch to create alarms on Trusted Advisor metrics for check status changes, resource status changes, and service limit utilization.</p><p><img alt="AWS Trusted Advisor" height="436" src="https://media.tutorialsdojo.com/public/AWS-trusted-advisor.5b9909d5f29f680eeb12ccff536e8d88d8701304.png" width="1000"></p><p>You can use Amazon EventBridge to detect and react to changes in the status of Trusted Advisor checks. Then, based on the rules that you create, EventBridge invokes one or more target actions when a check status changes to the value you specify in a rule. Depending on the type of status change, you might want to send notifications, capture status information, take corrective action, initiate events, or take other actions.</p><p>Hence, the correct answer is: <strong>Utilize AWS Trusted Advisor through a Business Support plan and integrate it with Amazon EventBridge to detect EC2 instances with low utilization. Set up an AWS Lambda function to filter the Trusted Advisor data using environment, department, and business unit tags. Create another Lambda trigger to terminate the underutilized instances.</strong></p><p>The option that says: <strong>Develop a custom shell script on an EC2 instance that runs periodically to report the instance utilization of all instances and store the result in an Amazon DynamoDB table. Use an Amazon QuickSight dashboard with DynamoDB as the source data to monitor and identify the underutilized EC2 instances. Integrate QuickSight and AWS Lambda to trigger an EC2 termination command for the underutilized instances</strong> is incorrect because it takes time to build a custom shell script to track the EC2 instance utilization. A more suitable way is to just use AWS Trusted Advisor instead.</p><p>The option that says: <strong>Set up an Amazon CloudWatch dashboard for EC2 instance tags based on the environment, department, and business unit to track the instance utilization. Create a trigger using an Amazon EventBridge rule and AWS Lambda to terminate the underutilized EC2 instances</strong> is incorrect because CloudWatch alone can\'t provide the instance utilization of all of your EC2 servers. You have to use AWS Trusted Advisor to get this specific data.</p><p>The option that says: <strong>Set up an AWS Systems Manager to track the utilization of all of your EC2 instances and report underutilized instances to Amazon CloudWatch. Filter the data in CloudWatch based on tags for each environment, department, and business unit. Create triggers in CloudWatch that will invoke an AWS Lambda function that will terminate underutilized EC2 instances</strong> is incorrect because the Systems Manager service is primarily used to manage your EC2 instances. It doesn\'t provide an easy way to provide the list of under or over-utilized EC2 instances like what Trusted Advisor can.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html ">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p><p><a href="https://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html">https://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html</a></p><p><a href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p><p><br></p><p><strong>Check out this AWS Trusted Advisor Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-trusted-advisor/?src=udemy">https://tutorialsdojo.com/aws-trusted-advisor/</a></p>',
        answers: [
          "<p>Develop a custom shell script on an EC2 instance that runs periodically to report the instance utilization of all instances and store the result in an Amazon DynamoDB table. Use an Amazon QuickSight dashboard with DynamoDB as the source data to monitor and identify the underutilized EC2 instances. Integrate QuickSight and AWS Lambda to trigger an EC2 termination command for the underutilized instances.</p>",
          "<p>Set up an Amazon CloudWatch dashboard for EC2 instance tags based on the environment, department, and business unit to track the instance utilization. Create a trigger using an Amazon EventBridge rule and AWS Lambda to terminate the underutilized EC2 instances.</p>",
          "<p>Utilize AWS Trusted Advisor through a Business Support plan and integrate it with Amazon EventBridge to detect EC2 instances with low utilization. Set up an AWS Lambda function to filter the Trusted Advisor data using environment, department, and business unit tags. Create another Lambda trigger to terminate the underutilized instances.</p>",
          "<p>Set up an AWS Systems Manager to track the utilization of all of your EC2 instances and report underutilized instances to Amazon CloudWatch. Filter the data in CloudWatch based on tags for each environment, department, and business unit. Create triggers in CloudWatch that will invoke an AWS Lambda function that will terminate underutilized EC2 instances.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Monitoring and Logging",
      question_plain:
        "A company plans to implement a monitoring system to track the cost-effectiveness of its Amazon EC2 resources across multiple AWS accounts. All existing resources are appropriately tagged to reflect the corresponding environment, department, and business unit for cost allocation purposes.The company has instructed its DevOps engineer to automate infrastructure cost optimization across various shared environments and accounts, including detecting EC2 instances with low utilization.Which is the MOST suitable solution that the DevOps engineer should implement in this scenario?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248117,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has several legacy systems which use both on-premises servers as well as EC2 instances in AWS. The cluster nodes in AWS are configured to have a local IP address and a fixed hostname in order for the on-premises servers to communicate with them. There is a requirement to automate the configuration of the cluster which consists of 10 nodes to ensure high availability across three Availability Zones. There should also be a corresponding elastic network interface in a specific subnet for each Availability Zone. Each of the cluster node's local IP address and hostname must be static and should not change even if the instance reboots or gets terminated. </p><p>Which of the following solutions below provides the LEAST amount of effort to automate this architecture?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>A stack is a collection of AWS resources that you can manage as a single unit. In other words, you can create, update, or delete a collection of resources by creating, updating, or deleting stacks. All the resources in a stack are defined by the stack\'s AWS CloudFormation template. A stack, for instance, can include all the resources required to run a web application, such as a web server, a database, and networking rules. If you no longer require that web application, you can simply delete the stack, and all of its related resources are deleted.</p><p>AWS CloudFormation ensures all stack resources are created or deleted as appropriate. Because AWS CloudFormation treats the stack resources as a single unit, they must all be created or deleted successfully for the stack to be created or deleted. If a resource cannot be created, AWS CloudFormation rolls the stack back and automatically deletes any resources that were created. If a resource cannot be deleted, any remaining resources are retained until the stack can be successfully deleted.<br><em><img src="https://media.tutorialsdojo.com/public/cfn-console-nested-stacks.png"></em></p><p><em>Nested stacks</em> are stacks created as part of other stacks. You create a nested stack within another stack by using the <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-stack.html"><code>AWS::CloudFormation::Stack</code></a> resource.</p><p>As your infrastructure grows, common patterns can emerge in which you declare the same components in multiple templates. You can separate out these common components and create dedicated templates for them. Then use the resource in your template to reference other templates, creating nested stacks.</p><p>For example, assume that you have a load balancer configuration that you use for most of your stacks. Instead of copying and pasting the same configurations into your templates, you can create a dedicated template for the load balancer. Then, you just use the resource to reference that template from within other templates.</p><p>By setting up both the <code>MinSize</code> and <code>MaxSize</code> parameters of the Auto Scaling group to 1, you can ensure that your EC2 instance can recover again in the event of systems failure with exactly the same parameters defined in the CloudFormation template. This is one of the Auto Scaling strategies which provides high availability with the least possible cost. In this scenario, there is no mention about the scalability of the solution but only its availability.</p><p>Hence, the correct answer is to: <strong>Set up a CloudFormation child stack template which launches an Auto Scaling group consisting of just one EC2 instance then provide a list of ENIs, hostnames and the specific AZs as stack parameters. Set both the </strong><code><strong>MinSize</strong></code><strong> and </strong><code><strong>MaxSize</strong></code><strong> parameters of the Auto Scaling group to 1. Add a user data script that will attach an ENI to the instance once launched. Use CloudFormation nested stacks to provision a total of 10 nodes needed for the cluster, and deploy the stack using a master template.</strong></p><p>The option that <strong>launches an Elastic Beanstalk application with 10 EC2 instances</strong> is incorrect because this involves a lot of manual configuration, which will make it hard for you to replicate the same stack to another AWS region. Moreover, the Elastic Beanstalk health agent is primarily used to monitor the health of your instances and can\'t be used to configure the hostname of the instance nor attach a specific ENI.</p><p>The option that <strong>uses a DynamoDB table to store the list of ENIs and hostnames subnets</strong> is incorrect because you cannot maintain the assignment of the individual local IP address and hostname for each instance using Systems Manager.</p><p>The option that <strong>develops a custom AWS CLI script to launch the EC2 instances then run it via AWS CloudShell </strong>is incorrect because this is not an automated solution. AWS CloudShell may provide an easy and secure way of interacting with AWS resources via browser-based shell but executing a script on this is still a manual process. A better way to meet this requirement is to use CloudFormation.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacks.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacks.html</a></p><p><a href="https://aws.amazon.com/cloudshell/">https://aws.amazon.com/cloudshell/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p>',
        answers: [
          "Launch an Elastic Beanstalk application with 10 EC2 instances, each has a corresponding ENI, hostname, and AZ as input parameters. Use the Elastic Beanstalk health agent daemon process to configure the hostname of the instance and attach a specific ENI based on the current environment.",
          "<p>Set up a CloudFormation child stack template which launches an Auto Scaling group consisting of just one EC2 instance then provide a list of ENIs, hostnames, and the specific AZs as stack parameters. Set both the <code>MinSize</code> and <code>MaxSize</code> parameters of the Auto Scaling group to 1. Add a user data script that will attach an ENI to the instance once launched. Use CloudFormation nested stacks to provision a total of 10 nodes needed for the cluster, and deploy the stack using a master template.</p>",
          "Use a DynamoDB table to store the list of ENIs and hostnames subnets which will be used by the cluster. Set up a single AWS CloudFormation template to manage an Auto Scaling group with a minimum and maximum size of 10. Maintain the assignment of each local IP address and hostname of the instances by using Systems Manager.",
          "<p>Develop a custom AWS CLI script to launch the EC2 instances, each with an attached ENI, a unique name and placed in a specific AZ. Replace the missing EC2 instance by running the script via AWS CloudShell in the event that one of the instances in the cluster got rebooted or terminated.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company has several legacy systems which use both on-premises servers as well as EC2 instances in AWS. The cluster nodes in AWS are configured to have a local IP address and a fixed hostname in order for the on-premises servers to communicate with them. There is a requirement to automate the configuration of the cluster which consists of 10 nodes to ensure high availability across three Availability Zones. There should also be a corresponding elastic network interface in a specific subnet for each Availability Zone. Each of the cluster node's local IP address and hostname must be static and should not change even if the instance reboots or gets terminated. Which of the following solutions below provides the LEAST amount of effort to automate this architecture?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248119,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has an Amazon ECS cluster with Service Auto Scaling which consists of multiple Amazon EC2 instances that runs a Docker-based application. The development team always pushes a new image to a private Docker container registry whenever they publish a new version of their application. They stop and start all of the tasks to ensure that the containers have the latest version of the application. However, the team noticed that the new tasks are occasionally running the old image of the application.</p><p>As the DevOps engineer, what should you do to fix this issue?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can update a running service to change the number of tasks that are maintained by a service, which task definition is used by the tasks, or if your tasks are using the Fargate launch type, you can change the platform version your service uses. If you have an application that needs more capacity, you can scale up your service. If you have unused capacity to scale down, you can reduce the number of desired tasks in your service and free up resources. If you have updated the Docker image of your application, you can create a new task definition with that image and deploy it to your service.</p><p>If your updated Docker image uses the same tag as what is in the existing task definition for your service (for example, <code>my_image:latest</code>), you do not need to create a new revision of your task definition. You can update the service using the procedure below, keep the current settings for your service, and select Force new deployment. The new tasks launched by the deployment pull the current image/tag combination from your repository when they start.</p><p>The service scheduler uses the minimum healthy percent and maximum percent parameters (in the deployment configuration for the service) to determine the deployment strategy. When a new task starts, the Amazon ECS container agent pulls the latest version of the specified image and tag for the container to use. However, subsequent updates to a repository image are not propagated to already running tasks.</p><p><img src="https://media.tutorialsdojo.com/public/overview-fargate.png"></p><p>To have your service use a newly updated Docker image with the same tag as in the existing task definition (for example, <code>my_image:latest</code>) or keep the current settings for your service, select Force new deployment. The new tasks launched by the deployment pull the current image/tag combination from your repository when they start. The Force new deployment option is also used when updating a Fargate task to use a more current platform version when you specify <code>LATEST</code>. For example, if you specified <code>LATEST</code> and your running tasks are using the <code>1.0.0</code> platform version and you want them to relaunch using a newer platform version.</p><p>To verify that the container agent is running on your container instance, run the following command:</p><p><code>sudo systemctl status ecs</code></p><p>If the command output doesn\'t show the service as active, run the following command to restart the service:</p><p><code>sudo systemctl restart ecs</code></p><p>It is mentioned in the scenario that the new tasks are occasionally running the old image of the application. The ECS cluster is also using Service Auto Scaling that automatically launches new tasks based on demand. We can conclude that the root cause is not in the task definition since this issue only occurs occasionally, and the other tasks were properly updated. If the ECS task is still running an old image, then it is possible that the ECS agent is not running properly.</p><p>Hence, the correct answer is: <strong>Restart the ECS agent.</strong></p><p>The option that says: <strong>Ensuring that the </strong><code><strong>latest</strong></code><strong> tag is being used in the Docker image of the task definition</strong> is incorrect. Although this will release you from the burden of constantly updating your task definition, this is still not the solution for the issue. Remember that there are other tasks that were successfully updated, which means that the image tag is not the root cause.</p><p>The option that says: <strong>Configuring the task definition to use the </strong><code><strong>repository-url/image@digest</strong></code><strong> format then manually updating the SHA-256 digest of the image</strong> is incorrect because this will just explicitly fetch the exact Docker image from the registry based on the provided SHA-256 digest and not based on its tag (e.g., latest, 1.0.0). In addition, it is stated in the scenario that the issue only occurs occasionally, which means that the other tasks are updated correctly. Thus, it suggests that the issue has nothing to do with the task definition but more with the ECS agent.</p><p>The option that says: <strong>Migrating your repository from your private Docker container registry to Amazon ECR</strong> is incorrect because the issue will be the same even if you used Amazon ECR if the ECS agent is not running properly in one of the instances.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/ecs-agent-disconnected-linux2-ami/">https://aws.amazon.com/premiumsupport/knowledge-center/ecs-agent-disconnected-linux2-ami/</a></p><p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/update-service.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/update-service.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "Restart the ECS agent.",
          "<p>Ensure that the <code>latest</code> tag is being used in the Docker image of the task definition.</p>",
          "<p>Configure the task definition to use the <code>repository-url/image@digest</code> format then manually update the SHA-256 digest of the image. </p>",
          "Migrate your repository from your private Docker container registry to Amazon ECR.",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "A company has an Amazon ECS cluster with Service Auto Scaling which consists of multiple Amazon EC2 instances that runs a Docker-based application. The development team always pushes a new image to a private Docker container registry whenever they publish a new version of their application. They stop and start all of the tasks to ensure that the containers have the latest version of the application. However, the team noticed that the new tasks are occasionally running the old image of the application.As the DevOps engineer, what should you do to fix this issue?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248121,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company uses a fleet of Linux and Windows servers for its enterprise applications. An automated daily check of each golden AMI owned is needed to monitor the latest Common Vulnerabilities and Exposures (CVE) using Amazon Inspector.</p><p>Which among the options below is the MOST suitable solution that should be implemented?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon Inspector</strong> tests the network accessibility of your Amazon EC2 instances and the security state of your applications that run on those instances. Amazon Inspector assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings that is organized by level of severity.</p><p><img src="https://media.tutorialsdojo.com/public/2019-11-07_05-00-58-e155c91caae56656552ec8b5ffed377e.png"></p><p>With Amazon Inspector, you can automate security vulnerability assessments throughout your development and deployment pipelines or for static production systems. This allows you to make security testing a regular part of development and IT operations.</p><p>Amazon Inspector also offers predefined software called an <em>agent</em> that you can optionally install in the operating system of the EC2 instances that you want to assess. The agent monitors the behavior of the EC2 instances, including network, file system, and process activity. It also collects a wide set of behavior and configuration data (telemetry).</p><p>If you want to set up a recurring schedule for your assessment, you can configure your assessment template to run automatically by creating a Lambda function using the AWS Lambda console. Alternatively, you can select the <em>"Set up recurring assessment runs once every &lt;number_of_days&gt;, starting now"</em> checkbox and specify the recurrence pattern (number of days) using the up and down arrows.</p><p>Hence, the correct answer is: <strong>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI, install the Inspector agent, and add a custom tag for tracking. Configure the Step Functions to trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the Step Functions every day using an Amazon EventBridge rule.</strong></p><p>The option that says: <strong>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will call the Inspector API action </strong><code><strong>StartAssessmentRun</strong></code><strong> after the EC2 instances have booted up, which will run the assessment against all instances with the custom tag you added. Trigger the function every day using an Amazon CloudWatch Alarms</strong><em> </em>is incorrect because you can\'t trigger a Lambda function on a regular basis using CloudWatch Alarms. You have to use Amazon EventBridge instead. Moreover, you typically have to install the Amazon Inspector agent to the EC2 instance in order to run the security assessments.</p><p>The option that says: <strong>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the function every day using an Amazon EventBridge rule<em> </em></strong>is incorrect because, in order for this solution to work, you have to install the Amazon Inspector agent first to the EC2 instance before you can run the security assessments.</p><p>The option that says: <strong>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI and install the Inspector agent. Configure the Step Functions to trigger the Inspector assessment for all instances right after the EC2 instances have booted up. Configure the Step Functions to run daily using the Event Bus in Amazon EventBridge</strong> is incorrect because the Event bus is primarily used to accept events from AWS services, other AWS accounts, and PutEvents API calls. You should also add a custom tag to the EC2 instance in order to run the Amazon Inspector assessments.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/set-up-amazon-inspector/">https://aws.amazon.com/premiumsupport/knowledge-center/set-up-amazon-inspector/</a></p><p><a href="https://docs.aws.amazon.com/inspector/latest/userguide/inspector_assessments.html#assessment_runs-schedule">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_assessments.html#assessment_runs-schedule</a></p><p><a href="https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/">https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/</a></p><p><br></p><p><strong>Check out this Amazon Inspector Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-inspector/?src=udemy">https://tutorialsdojo.com/amazon-inspector/</a></p>',
        answers: [
          "<p>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI, install the Inspector agent, and add a custom tag for tracking. Configure the Step Functions to trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the Step Functions every day using an Amazon EventBridge rule.</p>",
          "<p>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will call the Inspector API action <code>StartAssessmentRun</code> after the EC2 instances have booted up, which will run the assessment against all instances with the custom tag you added. Trigger the function every day using an Amazon CloudWatch Alarms.</p>",
          "<p>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the function every day using an Amazon EventBridge rule.</p>",
          "<p>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI and install the Inspector agent. Configure the Step Functions to trigger the Inspector assessment for all instances right after the EC2 instances have booted up. Configure the Step Functions to run daily using the Event Bus in Amazon EventBridge.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Security and Compliance",
      question_plain:
        "A company uses a fleet of Linux and Windows servers for its enterprise applications. An automated daily check of each golden AMI owned is needed to monitor the latest Common Vulnerabilities and Exposures (CVE) using Amazon Inspector.Which among the options below is the MOST suitable solution that should be implemented?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248123,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A DevOps engineer is migrating the company’s on-premises applications to Amazon EC2 using AWS Application Migration Service (AWS MGN). The applications consist of both Windows and Linux systems requiring shared storage. The shared storage must support SMB (Server Message Block) for Windows applications and NFS (Network File System) for Linux applications. The company also needs to implement a disaster recovery solution with cross-region storage replication to maintain a pilot light disaster recovery (DR) environment. The DR solution should replicate data from the primary region to the DR region efficiently.</p><p>The company requires a storage solution that meets the following requirements:</p><p>-Support for both SMB and NFS protocols.</p><p>-Seamless replication of data between the primary and DR regions.</p><p>-Minimal latency and efficient disaster recovery using a pilot light strategy.</p><p>Which of the following solutions will meet the given requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Amazon FSx for NetApp ONTAP is a fully managed service that provides shared storage with support for both <strong>SMB</strong> and <strong>NFS</strong> protocols. It is designed for applications requiring robust, high-performance shared storage and integrates seamlessly with Windows and Linux environments. FSx for NetApp ONTAP is ideal for migrating on-premises applications to AWS as it offers features like high availability, scalability, and compatibility with existing tools.</p><p><img src="https://media.tutorialsdojo.com/public/Amazon-FSx-for-NetApp-ONTAP-110124.png"></p><p>To meet the requirements, FSx for ONTAP instances are deployed in both the <strong>primary region</strong> and the <strong>disaster recovery (DR) region</strong>. The primary region instance handles the main application workloads, providing shared storage to Amazon EC2 instances. In the DR region, another FSx for ONTAP instance is configured as part of the pilot light DR strategy, maintaining a minimal but functional infrastructure that can be scaled during a disaster.</p><p>NetApp SnapMirror is a replication technology that enables seamless, low-latency data replication between FSx for ONTAP instances in the primary and DR regions. It ensures that the data in the DR region is always up-to-date with the primary region. SnapMirror supports incremental and scheduled replication, making it highly efficient for disaster recovery scenarios.</p><p>Hence, the correct answer is: <strong>Deploy Amazon FSx for NetApp ONTAP in the primary region, configure FSx for ONTAP instances in the DR region, and use NetApp SnapMirror for cross-region replication.</strong></p><p>The option that says: <strong>Use Amazon EFS for shared storage in the primary region with support for NFS, and configure cross-region replication to a secondary region. Use Amazon FSx for Windows File Server for SMB support and manually sync data between the primary and DR regions using AWS DataSync</strong> is incorrect because while Amazon EFS typically supports NFS and Amazon FSx for Windows File Server supports SMB, they are two separate solutions requiring manual synchronization. This introduces complexity and may not be as seamless or efficient as needed for disaster recovery.</p><p>The option that says: <strong>Use Amazon S3 for shared storage and enable cross-region replication. Use Amazon EC2 instances to mount S3 using SMB and NFS protocols</strong> is incorrect because Amazon S3 does not natively support SMB and NFS protocols. While S3 offers cross-region replication, using EC2 instances to mount S3 via SMB and NFS adds unnecessary complexity and may not meet the performance requirements.</p><p>The option that says: <strong>Configure Amazon FSx for Lustre in the primary region and replicate data using AWS Backup to an Amazon FSx for Lustre instance in the DR region</strong> is incorrect because Amazon FSx for Lustre is primarily designed for high-performance computing workloads and does not support SMB protocols. Also, using AWS Backup for replication may not provide the low-latency, efficient disaster recovery needed for a pilot light strategy.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html">https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html</a></p><p><a href="https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/migrating-fsx-ontap-snapmirror.html">https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/migrating-fsx-ontap-snapmirror.html</a></p><p><br></p><p><strong>Check out this Amazon FSx Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-fsx/?src=udemy">https://tutorialsdojo.com/amazon-fsx/</a></p>',
        answers: [
          "<p>Use Amazon EFS for shared storage in the primary region with support for NFS, and configure cross-region replication to a secondary region. Use Amazon FSx for Windows File Server for SMB support and manually sync data between the primary and DR regions using AWS DataSync.</p>",
          "<p>Deploy Amazon FSx for NetApp ONTAP in the primary region, configure FSx for ONTAP instances in the DR region, and use NetApp SnapMirror for cross-region replication.</p>",
          "<p>Use Amazon S3 for shared storage and enable cross-region replication. Use Amazon EC2 instances to mount S3 using SMB and NFS protocols.</p>",
          "<p>Configure Amazon FSx for Lustre in the primary region and replicate data using AWS Backup to an Amazon FSx for Lustre instance in the DR region.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A DevOps engineer is migrating the company’s on-premises applications to Amazon EC2 using AWS Application Migration Service (AWS MGN). The applications consist of both Windows and Linux systems requiring shared storage. The shared storage must support SMB (Server Message Block) for Windows applications and NFS (Network File System) for Linux applications. The company also needs to implement a disaster recovery solution with cross-region storage replication to maintain a pilot light disaster recovery (DR) environment. The DR solution should replicate data from the primary region to the DR region efficiently.The company requires a storage solution that meets the following requirements:-Support for both SMB and NFS protocols.-Seamless replication of data between the primary and DR regions.-Minimal latency and efficient disaster recovery using a pilot light strategy.Which of the following solutions will meet the given requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248125,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company wants to implement a continuous delivery workflow that will facilitate the process of source code promotion in the development, staging, and production environments in AWS. In the event of system degradation or failure, the company should also have the ability to roll back the recent deployment of the application.</p><p>Which of the following CI/CD designs is the MOST suitable one to implement and will incur the LEAST amount of downtime?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>A repository is the fundamental version control object in GitHub or GitLab. It’s where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. In GitHub and GitLab, you can set up notifications to configure notifications so that repository users receive emails about events (for example, another user commenting on code). You can change default settings to customize the default settings for your repository. You can browse contents to easily navigate and browse the contents of your repository. You can create triggers to set up triggers so that code pushes or other events trigger actions, such as emails or code functions. You can also configure a repository on your local computer (a local repo) to push your changes to more than one repository.</p><p><img src="https://media.tutorialsdojo.com/public/setup-cicd-pipeline-100324.png"></p><p>In designing your CI/CD process in AWS, you can use a single repository in GitHub (or GitLab) and create different branches for development, master, and release. You can use CodeBuild to build your application and run tests to verify that all of the core features of your application are working. For deployment, you can either select an in-place or blue/green deployment using CodeDeploy.</p><p>Hence, the correct answer is: <strong>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</strong></p><p>The option that says: <strong>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer’s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy </strong>is incorrect because creating a separate repository for each developer is absurd since they can simply clone the code instead. A single repository will suffice in this scenario which can have several branches for development and production deployment purposes.</p><p>The option that says: <strong>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production</strong> is incorrect because you don\'t need to create two repositories for one application. Instead, you can just create at least two different branches to separate your development and production code.</p><p>The option that says: <strong>Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy</strong> is incorrect because Amazon ECR is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. This is not a suitable service to be used to store your application code.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/devops/continuous-integration/">https://aws.amazon.com/devops/continuous-integration/</a></p><p><a href="https://aws.amazon.com/devops/continuous-delivery/">https://aws.amazon.com/devops/continuous-delivery/</a></p><p><a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html</a></p><p><br></p><p><strong>Check out this AWS CodePipelin Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
          "<p>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer’s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
          "<p>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production.</p>",
          "Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "A company wants to implement a continuous delivery workflow that will facilitate the process of source code promotion in the development, staging, and production environments in AWS. In the event of system degradation or failure, the company should also have the ability to roll back the recent deployment of the application.Which of the following CI/CD designs is the MOST suitable one to implement and will incur the LEAST amount of downtime?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248127,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is planning to host their enterprise web application in an Amazon ECS Cluster which uses the Fargate launch type. The database credentials, API keys, and other sensitive parameters should be provided to the application image by using environment variables. A DevOps engineer was instructed to ensure that the sensitive parameters are highly secured when passed to the image and must be kept in a dedicated storage with lifecycle management. The size of some parameters can exceed up to 12 Kb in size and must be rotated automatically.</p><p>Which of the following is the MOST suitable solution that the DevOps engineer should implement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</p><p>Within your container definition, specify <code>secrets</code> with the name of the environment variable to set in the container and the full ARN of either the Secrets Manager secret or Systems Manager Parameter Store parameter containing the sensitive data to present to the container. The parameter that you reference can be from a different Region than the container using it but must be from within the same account.</p><p><img src="https://media.tutorialsdojo.com/aws-secrets-manager-automatic-rotation-m501.png"></p><p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises.</p><p>Secrets Manager can automatically rotate your secret on a schedule. To rotate a secret, Secrets Manager uses a Lambda function to update the secret information. Rotation reduces the risk from leaving secret information such as credentials unchanged for long periods of time. Users and applications that retrieve this secret from Secrets Manager get the most up-to-date version as soon as the rotation completes.</p><p>As part of the schedule for rotating your secret, you can define a rotation window to ensure your secrets are rotated at the best time for your needs. The schedule you define is in UTC+0 time zone. A rotation window must end before midnight UTC+0 on the same day it began. It can\'t continue into the next day.</p><p>If you want a single store for configuration and secrets, you can use Parameter Store. If you want a dedicated secrets store with lifecycle management, use Secrets Manager. Hence, the correct answer is to: <strong>Keep the credentials using the AWS Secrets Manager and then encrypt them using AWS KMS. Set up an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container. Enable the built-in automatic key rotation for the credentials.</strong></p><p>The option that says: <strong>Keep the credentials using the AWS Systems Manager Parameter Store and then encrypt them using AWS KMS. Set up an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container. Enable the built-in automatic key rotation for the parameters</strong> is incorrect. Although the use of the AWS Systems Manager Parameter Store service in securing sensitive data in ECS is valid, this service doesn\'t provide dedicated storage with lifecycle management and key rotation, unlike Secrets Manager. Moreover, the AWS Systems Manager Parameter Store doesn\'t have a built-in automatic key rotation and can only store up to 8KB of data. Take note that the size of some parameters can exceed up to 12 Kb in size.</p><p>The option that says: <strong>Store the API Keys and other credentials in AWS Key Management Service (AWS KMS) and enable automatic key rotation. Set up an IAM role to the ECS task definition script that allows access to AWS KMS to retrieve the necessary parameters when calling the </strong><code><strong>register-task-definition</strong></code><strong> action in Amazon ECS </strong>is incorrect. Although AWS KMS has a built-in feature to automatically rotate keys, this service is not recommended to store sensitive API keys, database passwords, or any type of credentials. The primary function of AWS KMS is to encrypt customer-managed keys(CMKs).</p><p>The option that says: <strong>Store the credentials using AWS Storage Gateway in the ECS task definition file of the ECS Cluster in order to centrally manage these sensitive data and securely transmit these only to those containers that need access to them. Ensure that the secrets are encrypted and can only be accessed to those services which have been granted explicit access to it via IAM Role, and only while those service tasks are running. Launch a custom rotation function in AWS Lambda and automatically rotate the credentials using Amazon EventBridge </strong>is incorrect. The AWS Storage Gateway service is not meant to store and centrally manage your sensitive parameters. You should use AWS Secrets Manager for this particular use case since it has a built-in key rotation and can store secrets up to 12 Kb in size each. Launching a custom rotation function is actually possible using AWS Secrets Manager but not for AWS Storage Gateway.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html</a></p><p><a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p><p><a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets_strategies.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets_strategies.html</a></p><p><br></p><p><strong>Check out this AWS Secrets Manager vs AWS Systems Manager Parameter Store Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/?src=udemy">https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/</a></p>',
        answers: [
          "<p>Keep the credentials using the AWS Systems Manager Parameter Store and then encrypt them using AWS KMS. Set up an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container. Enable the built-in automatic key rotation for the parameters.</p>",
          "<p>Store the API Keys and other credentials in AWS Key Management Service (AWS KMS) and enable automatic key rotation. Set up an IAM role to the ECS task definition script that allows access to AWS KMS to retrieve the necessary parameters when calling the <code>register-task-definition</code> action in Amazon ECS.</p>",
          "<p>Keep the credentials using the AWS Secrets Manager and then encrypt them using AWS KMS. Set up an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container. Enable the built-in automatic key rotation for the credentials.</p>",
          "<p>Store the credentials using AWS Storage Gateway in the ECS task definition file of the ECS Cluster in order to centrally manage these sensitive data and securely transmit these only to those containers that need access to them. Ensure that the secrets are encrypted and can only be accessed to those services which have been granted explicit access to it via IAM Role, and only while those service tasks are running. Launch a custom rotation function in AWS Lambda and automatically rotate the credentials using Amazon EventBridge.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company is planning to host their enterprise web application in an Amazon ECS Cluster which uses the Fargate launch type. The database credentials, API keys, and other sensitive parameters should be provided to the application image by using environment variables. A DevOps engineer was instructed to ensure that the sensitive parameters are highly secured when passed to the image and must be kept in a dedicated storage with lifecycle management. The size of some parameters can exceed up to 12 Kb in size and must be rotated automatically.Which of the following is the MOST suitable solution that the DevOps engineer should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248129,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has an application hosted in an Auto Scaling group of EC2 instances which calls an external API with a URL of <code>http://api.tutorialsdojo.com</code> as part of its processing. There was a recent deployment that changed the protocol of the URL from HTTP to HTTPS but after that, the application has stopped working properly. The DevOps engineer has verified using his POSTMAN tool that the external API works without any issues and the VPC being utilized is still using the default network ACL. </p><p>Which of the following is the MOST appropriate course of action that the engineer should take to determine the root cause of this problem?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Amazon Virtual Private Cloud</strong> provides features that you can use to increase and monitor the security of your virtual private cloud (VPC):</p><p><strong>Security groups</strong>: Security groups act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level. When you launch an instance, you can associate it with one or more security groups that you\'ve created. Each instance in your VPC could belong to a different set of security groups. If you don\'t specify a security group when you launch an instance, the instance is automatically associated with the default security group for the VPC.</p><p><strong>Network access control lists (ACLs)</strong>: Network ACLs act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level.</p><p><strong>Flow logs</strong>: Flow logs capture information about the IP traffic going to and from network interfaces in your VPC. You can create a flow log for a VPC, subnet, or individual network interface. Flow log data is published to CloudWatch Logs or Amazon S3, and can help you diagnose overly restrictive or overly permissive security group and network ACL rules.</p><p><strong>Traffic mirroring</strong>: You can copy network traffic from an elastic network interface of an Amazon EC2 instance. You can then send the traffic to out-of-band security and monitoring appliances.</p><p>You can use AWS Identity and Access Management to control who in your organization has permission to create and manage security groups, network ACLs, and flow logs. For example, you can give only your network administrators that permission, but not personnel who only need to launch instances.</p><p><img src="https://media.tutorialsdojo.com/security-diagram.png"></p><p>For HTTP traffic, you must add an inbound rule on port 80 from the source address 0.0.0.0/0. For HTTPS traffic, add an inbound rule on port 443 from the source address 0.0.0.0/0. These inbound rules allow traffic from IPv4 addresses. To allow IPv6 traffic, add inbound rules on the same ports from the source address ::/0. Because security groups are stateful, the return traffic from the instance to users is allowed automatically, so you don\'t need to modify the security group\'s outbound rules.</p><p>The default network ACL is configured to allow all traffic to flow in and out of the subnets with which it is associated. Each network ACL also includes a rule whose rule number is an asterisk. This rule ensures that if a packet doesn\'t match any of the other numbered rules, it\'s denied.</p><p>In this scenario, the change of the URL from HTTP to HTTPS means that the application is using port 443 and not port 80 anymore. Since the application is the one that initiates the call to the external API, it makes sense to check if the egress security group rules allow outgoing HTTPS (443) traffic.</p><p>Hence, the correct answer is:<strong> Log in to the AWS Management Console and look for </strong><code><strong>REJECT</strong></code><strong> records in the VPC flow logs which originated from the Auto Scaling group. Verify that the egress security group rules of the Auto Scaling Group allow the outgoing traffic to the external API.</strong></p><p>The option that says: <strong>Log in to the AWS Management Console and then in the VPC flow logs, look for </strong><code><strong>ACCEPT</strong></code><strong> records which were originated from the Auto Scaling group. Verify that the ingress security group rules of the Auto Scaling Group allow the incoming traffic from the external API </strong>is incorrect because you should first check the egress rules (instead of ingress) of your security group first. Remember that it is the Auto Scaling group of EC2 instances that initiates the call to the external API and not the other way around. In addition, it is more effective if you look for <code><em>REJECT</em></code> records instead of <code>ACCEPT</code> records in VPC Flow Logs to view the details of the failed connection to your external API.</p><p>The option that says: <strong>Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the existing egress security group rules of the Auto Scaling Group, as well as the network ACL, allow the outgoing traffic to the external API</strong> is incorrect because, in the first place, the scenario didn\'t mention that the CloudWatch Logs agent is installed in the EC2 instances. Although it is right to check the existing egress security group rules, you don\'t need to check the network ACL since the architecture is already using a default one which is configured to allow all traffic.</p><p>The option that says: <strong>Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the ingress security group rules of the Auto Scaling Group, as well as the network ACL, allow the incoming traffic from the external API</strong> is incorrect because you have to check the egress security group rules first instead. The scenario also didn\'t mention that the CloudWatch Logs agent is installed in the EC2 instances, which means that you might not be able to view the application logs in CloudWatch.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/">https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/</a></p><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><br></p><p><strong>Amazon VPC Overview:</strong></p><p><a href="https://youtu.be/oIDHKeNxvQQ">https://youtu.be/oIDHKeNxvQQ</a><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-vpc/?src=udemy">https://tutorialsdojo.com/amazon-vpc/</a></p>',
        answers: [
          "<p>Log in to the AWS Management Console and then in the VPC flow logs, look for <code>ACCEPT</code> records which were originated from the Auto Scaling group. Verify that the ingress security group rules of the Auto Scaling Group allow the incoming traffic from the external API.</p>",
          "Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the existing egress security group rules of the Auto Scaling Group, as well as the network ACL, allow the outgoing traffic to the external API.",
          "<p>Log in to the AWS Management Console and look for <code>REJECT</code> records in the VPC flow logs which originated from the Auto Scaling group. Verify that the egress security group rules of the Auto Scaling Group allow the outgoing traffic to the external API.</p>",
          "Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the ingress security group rules of the Auto Scaling Group, as well as the network ACL, allow the incoming traffic from the external API.",
        ],
      },
      correct_response: ["c"],
      section: "Incident and Event Response",
      question_plain:
        "A company has an application hosted in an Auto Scaling group of EC2 instances which calls an external API with a URL of http://api.tutorialsdojo.com as part of its processing. There was a recent deployment that changed the protocol of the URL from HTTP to HTTPS but after that, the application has stopped working properly. The DevOps engineer has verified using his POSTMAN tool that the external API works without any issues and the VPC being utilized is still using the default network ACL. Which of the following is the MOST appropriate course of action that the engineer should take to determine the root cause of this problem?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248131,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A dynamic Node.js-based photo sharing application hosted in four Amazon EC2 web servers is using a DynamoDB table for session management and an S3 bucket for storing media files. The users can upload, view, organize, and share their photos using the content management system of the application. When a user uploads an image, a Lambda function will be invoked to process the media file then store it in Amazon S3. Due to the recent growth of the application’s user base in the country, they decided to manually add another six EC2 instances for the web tier to handle the peak load. However, each of the instances took more than half an hour to download the required application libraries and become fully configured.&nbsp; </p><p>Which of the following is the MOST resilient and highly available solution that will also lessen the deployment time of the new servers?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>When you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn\'t included in the standard AMIs.</p><p>Using <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html">configuration files</a> is great for configuring and customizing your environment quickly and consistently. Applying configurations, however, can start to take a long time during environment creation and updates. If you do a lot of server configuration in configuration files, you can reduce this time by making a custom AMI that already has the software and configuration that you need.</p><p><img src="https://media.tutorialsdojo.com/public/aeb-architecture2.png"></p><p>A custom AMI also allows you to make changes to low-level components, such as the Linux kernel, that are difficult to implement or take a long time to apply in configuration files. To create a custom AMI, launch an Elastic Beanstalk platform AMI in Amazon EC2, customize the software and configuration to your needs, and then stop the instance and save an AMI from it.</p><p>Hence, the correct solution is: <strong>Host the entire application in Elastic Beanstalk. Create a custom AMI using AWS Systems Manager Automation which includes all of the required dependencies and web components. Configure the Elastic Beanstalk environment to have an Auto Scaling group of EC2 instances across multiple Availability Zones with a load balancer in front that balances the incoming traffic. Enable Amazon DynamoDB Auto Scaling and point the application DNS record to the Elastic Beanstalk load balancer using Amazon Route 53.</strong></p><p>The option that says: <strong>Migrate the application to Amazon ECS with Fargate launch type. Create a task definition for the Node.js application that includes all required dependencies. Set up a DynamoDB table with Auto Scaling enabled and configure an Application Load Balancer to distribute traffic to the ECS service. Use Amazon Route 53 to point the application DNS record to the Application Load Balancer</strong> is incorrect. While ECS with Fargate can simplify container management, this solution requires the application to be containerized, which might involve significant changes to the existing application architecture. Elastic Beanstalk offers a simpler, more direct approach to this scenario.</p><p>The option that says: <strong>Deploy a Spot Fleet of EC2 instances with a target capacity of 20 then place them behind an Application Load Balancer. Configure Amazon Route 53 to point the application DNS record to the Application Load Balancer. Increase the RCU and WCU of the DynamoDB table</strong> is incorrect because using Spot Instances is susceptible to interruptions and could lead to outages of your application. Moreover, setting an exact number of target capacity is not recommended since your servers won\'t scale up or scale down based on the actual demand.</p><p>The option that says: <strong>Host the entire Node.js application to Amazon S3 as a static website. Create an Amazon CloudFront web distribution with the S3 bucket as its origin. Enable Auto Scaling in the Amazon DynamoDB table. In Route 53, point the application DNS record to the CloudFront URL </strong>is incorrect because the web application is a dynamic site and cannot be migrated to a static S3 website hosting.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs.html</a></p><p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/beanstalk-environment-configuration-advanced.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/beanstalk-environment-configuration-advanced.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>',
        answers: [
          "<p>Migrate the application to Amazon ECS with Fargate launch type. Create a task definition for the Node.js application that includes all required dependencies. Set up a DynamoDB table with Auto Scaling enabled and configure an Application Load Balancer to distribute traffic to the ECS service. Use Amazon Route 53 to point the application DNS record to the Application Load Balancer.</p>",
          "Deploy a Spot Fleet of EC2 instances with a target capacity of 20 then place them behind an Application Load Balancer. Configure Amazon Route 53 to point the application DNS record to the Application Load Balancer. Increase the RCU and WCU of the DynamoDB table. ",
          "Host the entire Node.js application to Amazon S3 as a static website. Create an Amazon CloudFront web distribution with the S3 bucket as its origin. Enable Auto Scaling in the Amazon DynamoDB table. In Route 53, point the application DNS record to the CloudFront URL.",
          "<p>Host the entire application in Elastic Beanstalk. Create a custom AMI using AWS Systems Manager Automation which includes all of the required dependencies and web components. Configure the Elastic Beanstalk environment to have an Auto Scaling group of EC2 instances across multiple Availability Zones with a load balancer in front that balances the incoming traffic. Enable Amazon DynamoDB Auto Scaling and point the application DNS record to the Elastic Beanstalk load balancer using Amazon Route 53.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A dynamic Node.js-based photo sharing application hosted in four Amazon EC2 web servers is using a DynamoDB table for session management and an S3 bucket for storing media files. The users can upload, view, organize, and share their photos using the content management system of the application. When a user uploads an image, a Lambda function will be invoked to process the media file then store it in Amazon S3. Due to the recent growth of the application’s user base in the country, they decided to manually add another six EC2 instances for the web tier to handle the peak load. However, each of the instances took more than half an hour to download the required application libraries and become fully configured.&nbsp; Which of the following is the MOST resilient and highly available solution that will also lessen the deployment time of the new servers?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248207,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A software development company is using GitHub, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline for its CI/CD process. To further improve their systems, they need to implement a solution that automatically detects and reacts to changes in the state of their deployments in AWS CodeDeploy. Any changes must be rolled back automatically if the deployment process fails, and a notification must be sent to the DevOps Team's Slack channel for easy monitoring.</p><p>Which of the following is the MOST suitable configuration that you should implement to satisfy this requirement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can monitor <strong>CodeDeploy</strong> deployments using the following CloudWatch tools: Amazon EventBridge, CloudWatch alarms, and Amazon CloudWatch Logs.</p><p>Reviewing the logs created by the CodeDeploy agent and deployments can help you troubleshoot the causes of deployment failures. As an alternative to reviewing CodeDeploy logs on one instance at a time, you can use CloudWatch Logs to monitor all logs in a central location.</p><p>You can use <strong>Amazon EventBridge </strong>(formerly known as Amazon CloudWatch Events) to detect and react to changes in the state of an instance or a deployment (an "event") in your CodeDeploy operations. Then, based on the rules you create, EventBridge will invoke one or more target actions when a deployment or instance enters the state you specify in a rule. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions.</p><p><img src="https://media.tutorialsdojo.com/public/td-amazon-eventbridge-rule-12-09-2024.png"></p><p>You can select the following types of targets when using EventBridge as part of your CodeDeploy operations:</p><p>- AWS Lambda functions</p><p>- Kinesis streams</p><p>- Amazon SQS queues</p><p>- Built-in targets (CloudWatch alarm actions)</p><p>- Amazon SNS topics</p><p>The following are some use cases:</p><p>- Use a Lambda function to pass a notification to a Slack channel whenever deployments fail.</p><p>- Push data about deployments or instances to a Kinesis stream to support comprehensive, real-time status monitoring.</p><p>- Use CloudWatch alarm actions to automatically stop, terminate, reboot, or recover Amazon EC2 instances when a deployment or instance event you specify occurs.</p><p>Hence, the correct answer is:<strong> Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team\'s Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when a deployment fails</strong></code><strong> setting.</strong></p><p>The option that says: <strong>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team\'s Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when alarm thresholds are met</strong></code><strong> setting</strong> is incorrect because CloudWatch Alarm can\'t directly send a message to a Slack Channel. You have to use an EventBridge with an associated Lambda function to notify the DevOps Team via Slack.</p><p>The option that says:<strong> Configure a CodeDeploy agent to send a notification to the DevOps Team\'s Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful</strong> is incorrect because a CodeDeploy agent is primarily used for deployment and not for sending custom messages to non-AWS resources such as a Slack Channel.</p><p>The option that says: <strong>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team\'s Slack Channel when the </strong><code><strong>PutLifecycleEventHookExecutionStatus</strong></code><strong> API call has been detected. Rollback the changes by using the AWS CLI</strong> is incorrect because this API simply sets the result of a Lambda validation function. This is not a suitable solution since invoking various API calls is not necessary at all. You simply have to integrate an EventBridge rule with an associated Lambda function to your CodeDeploy project in order to meet the specified requirement.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html</a></p><p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codedeploy/?src=udemy">https://tutorialsdojo.com/aws-codedeploy/</a></p>',
        answers: [
          "<p>Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team's Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when a deployment fails</code> setting.</p>",
          "<p>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when alarm thresholds are met</code> setting.</p>",
          "<p>Configure a CodeDeploy agent to send notification to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful.</p>",
          "<p>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team's Slack Channel when the <code>PutLifecycleEventHookExecutionStatus</code> API call has been detected. Rollback the changes by using the AWS CLI.</p>",
        ],
      },
      correct_response: ["a"],
      section: "Monitoring and Logging",
      question_plain:
        "A software development company is using GitHub, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline for its CI/CD process. To further improve their systems, they need to implement a solution that automatically detects and reacts to changes in the state of their deployments in AWS CodeDeploy. Any changes must be rolled back automatically if the deployment process fails, and a notification must be sent to the DevOps Team's Slack channel for easy monitoring.Which of the following is the MOST suitable configuration that you should implement to satisfy this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248209,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company wants to redirect directory requests to its on-premises Microsoft AD without caching any information in AWS. The DevOps Engineer set up AD Connector through custom resource in AWS CloudFormation. The AD Connector was created via the Lambda function, but CloudFormation is not moving from the state of CREATE_IN_PROGRESS to CREATE_COMPLETE.</p><p>Which of the following is the MOST suitable solution should the DevOps Engineer take?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Custom resources</strong> enable users to write custom provisioning logic in templates that AWS CloudFormation runs anytime a user creates, updates (if the custom resource has been changed), or deletes stacks. For instance, a user might want to include resources that are not available as AWS CloudFormation resource types. Users can include those resources by using custom resources. That way, users can still manage all related resources in a single stack.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-custom-resource.png"></p><p>If a <strong>custom resource</strong> is used to invoke a <strong>Lambda function</strong> in AWS CloudFormation, the request will include a <strong>pre-signed URL</strong>. The Lambda function is responsible for returning a response to the pre-signed URL to indicate if the resource creation was successful or not. If the Lambda function fails to respond to the pre-signed URL, the CloudFormation stack will remain in the CREATE_IN_PROGRESS state and wait for a response.</p><p>Hence, the correct answer is the option that says: <strong>Make sure that the pre-signed URL obtains a response from the Lambda function code.</strong></p><p>The option that says: <strong>Make sure that the IAM role assigned to the Lambda function has </strong><code><strong>cloudformation:UpdateStack</strong></code><strong> permissions for the stack\'s ARN </strong>is incorrect because the option only allows the Lambda function to perform an update to the stack. Note that the Lambda function is used to provision the AD connector, not update the stack.</p><p>The option that says: <strong>Confirm that the AWS account\'s IAM role associated with the Lambda function has </strong><code><strong>ds:ConnectDirectory</strong></code><strong> permissions </strong>is incorrect because despite having the correct permissions, the CloudFormation stack with the custom resource will continue to wait for a response from the Lambda function even after creating the AD connector.</p><p>The option that says: <strong>Confirm that the Lambda function code has exited successfully </strong>is incorrect because the CloudFormation stack will remain in CREATE_IN_PROGRESS regardless if the Lambda function exited successfully or not. It will still wait for a response from the Lambda function to progress.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/crpg-ref-responses.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/crpg-ref-responses.html</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Confirm that the Lambda function code has exited successfully.</p>",
          "<p>Make sure that the IAM role assigned to the Lambda function has <code>cloudformation:UpdateStack</code> permissions for the stack's ARN.</p>",
          "<p>Confirm that the AWS account's IAM role associated with the Lambda function has <code>ds:ConnectDirectory</code> permissions.</p>",
          "<p>Make sure that the pre-signed URL obtains a response from the Lambda function code.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company wants to redirect directory requests to its on-premises Microsoft AD without caching any information in AWS. The DevOps Engineer set up AD Connector through custom resource in AWS CloudFormation. The AD Connector was created via the Lambda function, but CloudFormation is not moving from the state of CREATE_IN_PROGRESS to CREATE_COMPLETE.Which of the following is the MOST suitable solution should the DevOps Engineer take?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248211,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A CTO of a leading insurance company has recently decided to migrate its online customer portal to AWS. The customers will use the online portal to view the paid insurance premiums and manage accounts. For improved scalability, the application should be hosted in an Auto Scaling group of On-Demand Amazon EC2 instances with a custom Amazon Machine Image (AMI). The same architecture will also be used for the non-production environments (DEV, TEST, and STAGING). The DevOps Engineer is instructed by the CTO to design a deployment strategy that securely stores the credentials of each environment, expedites the startup time for the EC2 instances, and allows the same AMI to work in all environments.</p><p>How should the DevOps Engineer set up the deployment configuration to accomplish this task?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Systems Manager Parameter Store</strong> provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Highly scalable, available, and durable, Parameter Store is backed by the AWS Cloud.</p><p>Systems Manager Automation simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p>Automation offers one-click automations for simplifying complex tasks such as creating golden Amazon Machines Images (AMIs), and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for a variety of reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img src="https://media.tutorialsdojo.com/public/custom_ami_1.gif"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Add a tag to each EC2 instance based on their environment. Use AWS Systems Manager Automation to preconfigure the AMI by installing all of the required applications and software dependencies. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Systems Manager Parameter Store to store the credentials as Secure String parameters.</strong></p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Session Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials</strong> is incorrect. The Session Manager service is just a fully managed AWS Systems Manager capability that lets you manage your Amazon EC2 instances through an interactive one-click browser-based shell or through the AWS CLI. It is not capable to build a custom AMI, unlike Systems Manager Automation.</p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Use a preconfigured AMI from AWS Marketplace. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials </strong>is incorrect. The company is using a custom AMI and not a public AMI from AWS Marketplace. You have to preconfigure the AMI using the Systems Manager Automation instead.</p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Patch Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use AWS AppConfig to store the environment-specific configuration and credentials securely</strong> is incorrect. The AWS Patch Manager is typically used for patching and managing operating system patches, not for installing applications or preconfiguring the AMI. Additionally, AWS AppConfig is more for managing feature flags or dynamic configurations and is not the best fit for secure storage of environment-specific credentials.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-systems-manager/?src=udemy">https://tutorialsdojo.com/aws-systems-manager/</a></p>',
        answers: [
          "<p>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Session Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials.</p>",
          "<p>Add a tag to each EC2 instance based on their environment. Use AWS Systems Manager Automation to preconfigure the AMI by installing all of the required applications and software dependencies. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Systems Manager Parameter Store to store the credentials as Secure String parameters.</p>",
          "<p>Add a tag to each EC2 instance based on their environment. Use a preconfigured AMI from AWS Marketplace. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials.</p>",
          "<p>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Patch Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use AWS AppConfig to store the environment-specific configuration and credentials securely.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A CTO of a leading insurance company has recently decided to migrate its online customer portal to AWS. The customers will use the online portal to view the paid insurance premiums and manage accounts. For improved scalability, the application should be hosted in an Auto Scaling group of On-Demand Amazon EC2 instances with a custom Amazon Machine Image (AMI). The same architecture will also be used for the non-production environments (DEV, TEST, and STAGING). The DevOps Engineer is instructed by the CTO to design a deployment strategy that securely stores the credentials of each environment, expedites the startup time for the EC2 instances, and allows the same AMI to work in all environments.How should the DevOps Engineer set up the deployment configuration to accomplish this task?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248213,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A global cryptocurrency trading company has a suite of web applications hosted in an Auto Scaling group of Amazon EC2 instances across multiple Available Zones behind an Application Load Balancer to distribute the incoming traffic. The Auto Scaling group is configured to use Elastic Load Balancing health checks for scaling instead of the default EC2 status checks. However, there are several occasions when some instances are automatically terminated after failing the HTTPS health checks in the ALB that purges all the logs stored in the instance.</p><p>To improve system monitoring, a DevOps Engineer must implement a solution that collects all of the application and server logs effectively. The Operations team should be able to perform a root cause analysis based on the logs, even if the Auto Scaling group immediately terminated the instance.</p><p>How can the DevOps Engineer automate the log collection from the EC2 instances with the LEAST amount of effort?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>The EC2 instances in an Auto Scaling group have a path, or lifecycle, that differs from that of other EC2 instances. The lifecycle starts when the Auto Scaling group launches an instance and puts it into service. The lifecycle ends when you terminate the instance, or the Auto Scaling group takes the instance out of service and terminates it.</p><p>You can add a lifecycle hook to your Auto Scaling group so that you can perform custom actions when instances launch or terminate.</p><p>When Amazon EC2 Auto Scaling responds to a scale out event, it launches one or more instances. These instances start in the <code>Pending</code> state. If you added an <code>autoscaling:EC2_INSTANCE_LAUNCHING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Pending</code> state to the <code>Pending:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Pending:Proceed</code> state. When the instances are fully configured, they are attached to the Auto Scaling group and they enter the <code>InService</code> state.</p><p>When Amazon EC2 Auto Scaling responds to a scale in event, it terminates one or more instances. These instances are detached from the Auto Scaling group and enter the <code>Terminating</code> state. If you added an <code>autoscaling:EC2_INSTANCE_TERMINATING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Terminating:Proceed</code> state. When the instances are fully terminated, they enter the <code>Terminated</code> state.</p><p><img src="https://media.tutorialsdojo.com/public/auto_scaling_lifecycle.png"></p><p>Using CloudWatch agent is the most suitable tool to use to collect the logs. The unified CloudWatch agent enables you to do the following:</p><p>- Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">Metrics Collected by the CloudWatch Agent</a>.</p><p>- Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p><p>- Retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collectd</code> protocols. <code>StatsD</code> is supported on both Linux servers and servers running Windows Server. On the other hand, <code>collectd</code> is supported only on Linux servers.</p><p>- Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p><p><img src="https://media.tutorialsdojo.com/public/td-amazon-eventbridge-event-pattern-27Jan2025.png"></p><p>You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is <code>CWAgent</code>, although you can specify a different namespace when you configure the agent.</p><p>Hence, the correct answer is: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance-terminate Lifecycle Action</strong></code><strong> Auto Scaling Event with an associated AWS Systems Manager Automation document. Trigger the Amazon CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.</strong></p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Pending:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance-terminate Lifecycle Action</strong></code><strong> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Automation to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent</strong> is incorrect because the <code><strong><em>Pending:Wait</em></strong></code><strong><em> </em></strong>state simply refers to the scale-out action in Amazon EC2 Auto Scaling and not for scale-in or for terminating the instances.</p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Use AWS Step Functions to collect the application logs and send them to an Amazon CloudWatch Log group. Resume the instance termination once all the logs are sent to CloudWatch Logs</strong> is incorrect because using AWS Step Functions is inappropriate when collecting the logs from your EC2 instances. You should use a CloudWatch agent instead.</p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance Terminate Successful</strong></code><strong> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Run Command to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent</strong> is incorrect. The <code><strong>EC2 Instance Terminate Successful</strong></code> indicates that the ASG has terminated an instance. The automated solution won\'t just work because the target instance is already deleted when the Lambda function is triggered.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/">https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/</a></p><p><a href="https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p><p><a href="https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/">https://docs.aws.amazon.com/autoscaling/ec2/userguide/cloud-watch-events.html#terminate-successful</a></p><p><a href="https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-auto-scaling/?src=udemy">https://tutorialsdojo.com/aws-auto-scaling/</a></p>',
        answers: [
          "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Pending:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance-terminate Lifecycle Action</code> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Automation to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent.</p>",
          "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Use AWS Step Functions to collect the application logs and send them to an Amazon CloudWatch Log group. Resume the instance termination once all the logs are sent to CloudWatch Logs.</p>",
          "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance-terminate Lifecycle Action</code> Auto Scaling Event with an associated AWS Systems Manager Automation document. Trigger the Amazon CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.</p>",
          "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance Terminate Successful</code> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Run Command to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent.</p>",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "A global cryptocurrency trading company has a suite of web applications hosted in an Auto Scaling group of Amazon EC2 instances across multiple Available Zones behind an Application Load Balancer to distribute the incoming traffic. The Auto Scaling group is configured to use Elastic Load Balancing health checks for scaling instead of the default EC2 status checks. However, there are several occasions when some instances are automatically terminated after failing the HTTPS health checks in the ALB that purges all the logs stored in the instance.To improve system monitoring, a DevOps Engineer must implement a solution that collects all of the application and server logs effectively. The Operations team should be able to perform a root cause analysis based on the logs, even if the Auto Scaling group immediately terminated the instance.How can the DevOps Engineer automate the log collection from the EC2 instances with the LEAST amount of effort?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248215,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading media company has recently migrated its .NET web application from an on-premises network to AWS Elastic Beanstalk for easier deployment and management. The application stores static content like images and PDFs in an Amazon S3 bucket, while all application data is stored in Amazon DynamoDB.</p><p>After launching a global marketing campaign, the company experienced unpredictable traffic spikes. Upon investigation, the Operations team found that over 80% of the incoming traffic consists of duplicate read requests, leading to degraded performance. The team also identified that <code>ElastiCachePrimaryEngineCPUUtilization</code> metrics constantly reached high thresholds, indicating potential cache node overutilization.</p><p>How can a DevOps Engineer improve the application's performance for users worldwide?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>CloudFront</strong> speeds up the distribution of your static and dynamic content (HTML, .css, .js, and image files) by routing each user request through the AWS backbone network to the edge location that can best serve your content. By creating a <strong>CloudFront distribution</strong>, CloudFront determines which origin servers to get your files from when users request the files through your website or application. Moreover, to specify the details about how to track and manage content delivery.</p><p><strong>Amazon DynamoDB</strong> is designed for scale and performance. In most cases, the DynamoDB response times can be measured in single-digit milliseconds. However, there are certain use cases that require response times in microseconds. For these use cases, DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data.</p><p>DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX addresses three core scenarios:</p><p>- As an in-memory cache, DAX reduces the response times of eventually consistent read workloads by an order of magnitude, from single-digit milliseconds to microseconds.</p><p>- DAX reduces operational and application complexity by providing a managed service that is API-compatible with DynamoDB. Therefore, it requires only minimal functional changes to use with an existing application.</p><p>- For read-heavy or bursty workloads, DAX provides increased throughput and potential operational cost savings by reducing the need to over-provision read capacity units. This is especially beneficial for applications that require repeated reads for individual keys</p><p><img src="https://media.tutorialsdojo.com/public/dax_high_level.png"></p><p>Hence, the correct answer is: <strong>Create an Amazon CloudFront web distribution to cache images stored in the S3 bucket. Use DynamoDB Accelerator (DAX) to cache repeated read requests on the web application.</strong></p><p>The option that says: <strong>Cache the images stored in the S3 bucket using AWS Elemental MediaStore. Set up a distributed cache layer using Amazon ElastiCache for Redis Cluster to serve the repeated read requests on the web application. Monitor </strong><code><strong>ElastiCachePrimaryEngineCPUUtilization</strong></code><strong> metric to ensure cache nodes are not overloaded and scale accordingly</strong> is incorrect because AWS Elemental MediaStore is designed primarily for video streaming and media workflows, making it unsuitable for caching static web content like images and PDFs. For static content, Amazon CloudFront is the ideal solution, as it is a CDN optimized for caching and delivering static assets from S3 globally with low latency. Additionally, ElastiCache for Redis is more suited for caching dynamic data, not static content, and using it to cache images would be inefficient compared to CloudFront.</p><p>The option that says: <strong>Cache the images stored in the S3 bucket using the AWS Elemental MediaPackage. Set up a distributed cache layer using an Amazon ElastiCache for Memcached Cluster to serve the repeated read requests on the web application</strong> is incorrect because the AWS Elemental MediaPackage is primarily used for videos and not for photos. Moreover, CloudFront is a more suitable service for use as a content delivery network. AWS Elemental MediaPackage is a video origination and just-in-time (JIT) packaging service that allows video providers to securely and reliably deliver live streaming content at scale.</p><p>The option that says: <strong>Use Lambda@Edge to cache the images stored in the S3 bucket. Use DynamoDB Accelerator (DAX) to cache repeated read requests on the web application</strong> is incorrect because Lambda@Edge is primarily used to run code closer to users of your application in order to improve application performance and reduce latency. Serving static content using Lambda@Edge is not a suitable use case.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/dynamodb/dax/">https://aws.amazon.com/dynamodb/dax/</a></p><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-creating-console.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-creating-console.html</a></p><p><br></p><p><strong>Check out these Amazon DynamoDB and Amazon CloudFront Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/amazon-dynamodb/?src=udemy">https://tutorialsdojo.com/amazon-dynamodb/</a></p>',
        answers: [
          "<p>Cache the images stored in the S3 bucket using AWS Elemental MediaStore. Set up a distributed cache layer using Amazon ElastiCache for Redis Cluster to serve the repeated read requests on the web application. Monitor <code>ElastiCachePrimaryEngineCPUUtilization</code> metric to ensure cache nodes are not overloaded and scale accordingly.</p>",
          "<p>Cache the images stored in the S3 bucket using the AWS Elemental MediaPackage. Set up a distributed cache layer using an Amazon ElastiCache for Memcached Cluster to serve the repeated read requests on the web application.</p>",
          "<p>Create an Amazon CloudFront web distribution to cache images stored in the S3 bucket. Use DynamoDB Accelerator (DAX) to cache repeated read requests on the web application.</p>",
          "<p>Use Lambda@Edge to cache the images stored in the S3 bucket. Use DynamoDB Accelerator (DAX) to cache repeated read requests on the web application.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A leading media company has recently migrated its .NET web application from an on-premises network to AWS Elastic Beanstalk for easier deployment and management. The application stores static content like images and PDFs in an Amazon S3 bucket, while all application data is stored in Amazon DynamoDB.After launching a global marketing campaign, the company experienced unpredictable traffic spikes. Upon investigation, the Operations team found that over 80% of the incoming traffic consists of duplicate read requests, leading to degraded performance. The team also identified that ElastiCachePrimaryEngineCPUUtilization metrics constantly reached high thresholds, indicating potential cache node overutilization.How can a DevOps Engineer improve the application's performance for users worldwide?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248217,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An American tech company used an AWS CloudFormation template to deploy its static corporate website hosted on Amazon S3 in the US East (N. Virginia) region. The template defines an Amazon S3 bucket with a Lambda-backed custom resource that downloads the content from a file server into the bucket. There is a new task for the DevOps Engineer to move the website to the US West (Oregon) region to better serve its customers on the West Coast with lower latency. However, the application stack could not be deleted successfully in CloudFormation. </p><p>Which among the following options shows the root cause of this issue, and how can the DevOps Engineer mitigate this problem for current and future versions of the website?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>When you associate a Lambda function with a custom resource, the function is invoked whenever the custom resource is created, updated, or deleted. AWS CloudFormation calls a Lambda API to invoke the function and to pass all the request data (such as the request type and resource properties) to the function. The power and customizability of Lambda functions in combination with AWS CloudFormation enable a wide range of scenarios, such as dynamically looking up AMI IDs during stack creation, or implementing and using utility functions, such as string reversal functions.</p><p>AWS CloudFormation templates that declare an Amazon Elastic Compute Cloud (Amazon EC2) instance must also specify an Amazon Machine Image (AMI) ID, which includes an operating system and other software and configuration information used to launch the instance. The correct AMI ID depends on the instance type and region in which you\'re launching your stack. And IDs can change regularly, such as when an AMI is updated with software updates.</p><p>Normally, you might map AMI IDs to specific instance types and regions. To update the IDs, you manually change them in each of your templates. By using custom resources and AWS Lambda (Lambda), you can create a function that gets the IDs of the latest AMIs for the region and instance type that you\'re using so that you don\'t have to maintain mappings.</p><p><img src="https://media.tutorialsdojo.com/public/CloudFormation-AMIManager-Flow.png"></p><p>You can also run the custom resource to recursively empty the bucket when the CloudFormation stack is triggered for deletion. In CloudFormation, you can only delete empty buckets. Any request for deletion will fail for buckets that still have contents. To control how AWS CloudFormation handles the bucket when the stack is deleted, you can set a deletion policy for your bucket. You can choose to retain the bucket or to delete the bucket.</p><p>Hence, the correct answer is: <strong>The CloudFormation stack deletion fails for an S3 bucket that still has contents. To fix the issue, modify the Lambda function code of the custom resource to recursively empty the bucket if the stack is selected for deletion.</strong></p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket that is used as a static web hosting. To fix the issue, modify the CloudFormation template to remove the website configuration for the S3 bucket </strong>is incorrect because the CloudFormation deletion process will not be hindered simply because your S3 bucket is configured for static web hosting. The primary root cause of this issue is that the CloudFormation stack deletion fails for an S3 bucket that still has contents.</p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket because the </strong><code><strong>DeletionPolicy</strong></code><strong> attribute is set to </strong><code><strong>Snapshot</strong></code><strong>. To fix the issue, set the </strong><code><strong>DeletionPolicy</strong></code><strong> to </strong><code><strong>Delete</strong></code><strong> instead </strong>is incorrect because you can only set the <code><strong><em>DeletionPolicy</em></strong></code> to either <code><strong><em>Retain </em></strong></code>or <code><strong><em>Delete</em></strong></code> for an Amazon S3 resource. In addition, the CloudFormation deletion will still fail as long as the S3 bucket is not empty, even if the <code>DeletionPolicy</code> attribute is already set to <code>Delete</code>.</p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket is not yet empty. To fix the issue, set the </strong><code><strong>DeletionPolicy</strong></code><strong> to </strong><code><strong>ForceDelete</strong></code><strong> instead</strong> is incorrect. Although the provided root cause is accurate, the configuration for <code><strong><em>DeletionPolicy </em></strong></code>remains invalid. <code><strong><em>ForceDelete</em></strong></code> is not a valid value for the deletion policy attribute.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/">https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html</a></p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudformation/?src=udemy">https://tutorialsdojo.com/aws-cloudformation/</a></p>',
        answers: [
          "<p>The CloudFormation stack deletion fails for an S3 bucket that is used as a static web hosting. To fix the issue, modify the CloudFormation template to remove the website configuration for the S3 bucket.</p>",
          "<p>The CloudFormation stack deletion fails for an S3 bucket that still has contents. To fix the issue, modify the Lambda function code of the custom resource to recursively empty the bucket if the stack is selected for deletion.</p>",
          "<p>The CloudFormation stack deletion fails for an S3 bucket because the <code>DeletionPolicy</code> attribute is set to <code>Snapshot</code>. To fix the issue, set the <code>DeletionPolicy</code> to <code>Delete</code> instead.</p>",
          "<p>The CloudFormation stack deletion fails for an S3 bucket because it is not yet empty. To fix the issue, set the <code>DeletionPolicy</code> to <code>ForceDelete</code> instead.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "An American tech company used an AWS CloudFormation template to deploy its static corporate website hosted on Amazon S3 in the US East (N. Virginia) region. The template defines an Amazon S3 bucket with a Lambda-backed custom resource that downloads the content from a file server into the bucket. There is a new task for the DevOps Engineer to move the website to the US West (Oregon) region to better serve its customers on the West Coast with lower latency. However, the application stack could not be deleted successfully in CloudFormation. Which among the following options shows the root cause of this issue, and how can the DevOps Engineer mitigate this problem for current and future versions of the website?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248219,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading game development company is planning to host its latest video game on AWS. It is expected that there will be millions of users around the globe that will play the game. The architecture should allow players to send or receive data on the backend in real-time for a better gaming experience. The application libraries and user data of the game must also comply with the data residency requirement wherein all files must remain in the same region. GitHub, CodeBuild, CodeDeploy, and CodePipeline should be utilized to build the CI/CD process.</p><p>As a DevOps Engineer, which of the following is the MOST suitable and efficient solution that you should implement to satisfy this requirement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS CodePipeline</strong> includes a number of actions that help you configure build, test, and deploy resources for your automated release process. You can add actions to your pipeline that are in an AWS Region different from your pipeline. When an AWS service is the provider for an action, and this action type/provider type are in a different AWS Region from your pipeline, this is a cross-region action. Certain action types in CodePipeline may only be available in certain AWS Regions. Also note that there may AWS Regions where an action type is available, but a specific AWS provider for that action type is not available.</p><p>You can use the console, AWS CLI, or AWS CloudFormation to add cross-region actions in pipelines. If you use the console to create a pipeline or cross-region actions, default artifact buckets are configured by CodePipeline in the Regions where you have actions. When you use the AWS CLI, AWS CloudFormation, or an SDK to create a pipeline or cross-region actions, you provide the artifact bucket for each Region where you have actions. You must create the artifact bucket and encryption key in the same AWS Region as the cross-region action and in the same account as your pipeline.</p><p><img src="https://media.tutorialsdojo.com/public/AWS-CodePipeline.png"></p><p>You cannot create cross-region actions for the following action types: source actions, third-party actions, and custom actions. When a pipeline includes a cross-region action as part of a stage, CodePipeline replicates only the input artifacts of the cross-region action from the pipeline Region to the action\'s Region. The pipeline Region and the Region where your CloudWatch Events change detection resources are maintained remain the same. The Region where your pipeline is hosted does not change.</p><p>Hence, the correct answer is: <strong>Create a new pipeline using AWS CodePipeline and a GitHub repository as the source in your primary AWS region. Configure the repository to trigger the build and deployment actions whenever there is a new code update. Using the AWS Management Console, set up the pipeline to use cross-region actions that will run the build and deployment actions to other regions. The pipeline will automatically store output files on a default artifact bucket on each region.</strong></p><p>The option that says: <strong>Create a new pipeline using AWS CodePipeline and a GitHub repository as the source on multiple AWS regions. Configure the repository of the pipeline for each region to trigger the build and deployment actions whenever there is a new code update. The pipeline will automatically store output files on a default artifact bucket on each region</strong> is incorrect because this deployment set up is not efficient since you have to maintain several pipelines in multiple AWS regions. A better solution would be to simply configure cross-region actions to build and deploy the application on multiple regions.</p><p>The option that says: <strong>Create a new pipeline using AWS CodePipeline and a GitHub repository as the source in your primary AWS region. Configure the repository to trigger the build and deployment actions whenever there is a new code update. Using the AWS Management Console, set up the pipeline to use cross-AZ actions that will run the build and deployment actions to other Availability Zones. The pipeline will automatically store output files on a default artifact bucket on each AZ</strong> is incorrect because there is no such thing as cross-AZ actions but only cross-region actions. You can build, test, and deploy resources to other AWS regions by adding cross-region actions in your pipeline.</p><p>The option that says: <strong>Create a new pipeline using AWS CodePipeline and a GitHub repository as the source in your primary AWS region. Set a CodeBuild test action to run the automated unit and integration tests. Configure the repository to trigger the build and deployment actions whenever there is a new code update. Using the AWS Management Console, set up the pipeline to use cross-region actions that will run the build and deployment actions to other regions. Manually configure the pipeline to automatically store output files on a single S3 bucket</strong> is incorrect because this will typically violate the data residency requirement. Take note that the application libraries and game user data must remain in the same region.</p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html</a></p><p><a href="https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p><p><a href="https://aws.amazon.com/about-aws/whats-new/2018/11/aws-codepipeline-now-supports-cross-region-actions/">https://aws.amazon.com/about-aws/whats-new/2018/11/aws-codepipeline-now-supports-cross-region-actions/</a></p><p><a href="https://aws.amazon.com/blogs/devops/building-a-ci-cd-pipeline-for-multi-region-deployment-with-aws-codepipeline/">https://aws.amazon.com/blogs/devops/building-a-ci-cd-pipeline-for-multi-region-deployment-with-aws-codepipeline/</a></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p>',
        answers: [
          "<p>Create a new pipeline using AWS CodePipeline and a GitHub repository as the source on multiple AWS regions. Configure the repository of the pipeline for each region to trigger the build and deployment actions whenever there is a new code update. The pipeline will automatically store output files on a default artifact bucket on each region.</p>",
          "<p>Create a new pipeline using AWS CodePipeline and a GitHub repository as the source in your primary AWS region. Configure the repository to trigger the build and deployment actions whenever there is a new code update. Using the AWS Management Console, set up the pipeline to use cross-AZ actions that will run the build and deployment actions to other Availability Zones. The pipeline will automatically store output files on a default artifact bucket on each AZ.</p>",
          "<p>Create a new pipeline using AWS CodePipeline and a GitHub repository as the source in your primary AWS region. Set a CodeBuild test action to run the automated unit and integration tests. Configure the repository to trigger the build and deployment actions whenever there is a new code update. Using the AWS Management Console, set up the pipeline to use cross-region actions that will run the build and deployment actions to other regions. Manually configure the pipeline to automatically store output files on a single S3 bucket.</p>",
          "<p>Create a new pipeline using AWS CodePipeline and a GitHub repository as the source in your primary AWS region. Configure the repository to trigger the build and deployment actions whenever there is a new code update. Using the AWS Management Console, set up the pipeline to use cross-region actions that will run the build and deployment actions to other regions. The pipeline will automatically store output files on a default artifact bucket on each region.</p>",
        ],
      },
      correct_response: ["d"],
      section: "SDLC Automation",
      question_plain:
        "A leading game development company is planning to host its latest video game on AWS. It is expected that there will be millions of users around the globe that will play the game. The architecture should allow players to send or receive data on the backend in real-time for a better gaming experience. The application libraries and user data of the game must also comply with the data residency requirement wherein all files must remain in the same region. GitHub, CodeBuild, CodeDeploy, and CodePipeline should be utilized to build the CI/CD process.As a DevOps Engineer, which of the following is the MOST suitable and efficient solution that you should implement to satisfy this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248221,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>A company is re-architecting its monolithic system to a serverless application in AWS to save on cost. The deployment of the succeeding new version of the application must be initially rolled out to a small number of users first for testing before the full release. If the post-hook tests fail, there should be an easy way to roll back the deployment. The DevOps Engineer was assigned to design an efficient deployment setup that mitigates any unnecessary outage that impacts their production environment.</p><p>As a DevOps Engineer, how should you satisfy this requirement? (Select TWO.)</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", "", ""],
        explanation:
          '<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Each alias has a unique ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. Event sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don\'t need to update the mapping when the function version changes. In a resource policy, you can grant permissions for event sources to use your Lambda function. If you specify an alias ARN in the policy, you don\'t need to update the policy when the function version changes.</p><p>Use routing configuration on an alias to send a portion of traffic to a second function version. For example, you can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version. You can point an alias to a maximum of two Lambda function versions.</p><p><img src="https://media.tutorialsdojo.com/public/API_gateway.png"></p><p>In API Gateway, you create a canary release deployment when deploying the API with <a href="https://docs.aws.amazon.com/apigateway/api-reference/link-relation/deployment-create/#canarySettings">canary settings</a> as an additional input to the <a href="https://docs.aws.amazon.com/apigateway/api-reference/link-relation/deployment-create/">deployment creation</a> operation.</p><p>You can also create a canary release deployment from an existing non-canary deployment by making a <a href="https://docs.aws.amazon.com/apigateway/api-reference/link-relation/stage-update/"><code>stage:update</code></a> request to add the canary settings on the stage.</p><p>When creating a non-canary release deployment, you can specify a non-existing stage name. API Gateway creates one if the specified stage does not exist. However, you cannot specify any non-existing stage name when creating a canary release deployment. You will get an error and API Gateway will not create any canary release deployment.</p><p>Hence, the correct answers are:</p><p><strong>- Set up one AWS Lambda Function Alias that points to both the current and new versions. Route 20% of incoming traffic to the new version and once it is considered stable, update the alias to route all traffic to the new version.</strong></p><p>- <strong>Set up a canary deployment in Amazon API Gateway that routes 20% of the incoming traffic to the canary release. Promote the canary release to production once the initial tests have passed.</strong></p><p>The option that says:<strong><em> </em>Launch an Application Load Balancer with an Amazon API Gateway private integration. Attach a single target group to the load balancer and select the "Canary" routing option which will automatically route incoming traffic to the new version</strong> is incorrect because there is no Canary routing option in an Application Load Balancer.</p><p>The option that says: <strong>Launch a Network Load Balancer with an Amazon API Gateway private integration. Attach two target groups to the load balancer. Configure the first target group with the current version and the second target group with the new version. Configure the load balancer to route 20% of the incoming traffic to the new version and once it becomes stable, detach the first target group from the load balancer </strong>is incorrect because the Network Load Balancer does not support weighted target groups, unlike the Application Load Balancer.</p><p>The option that says: <strong>Create a new record in Route 53 with a Failover routing policy. Configure the primary record to route 20% of incoming traffic to the new version and set the secondary record to route the rest of the traffic to the current version. Once the new version stabilizes, update the primary record to route all traffic to the new version </strong>is incorrect because the failover routing policy simply lets you route traffic to a resource when the resource is healthy, or to a different resource when the first resource is unhealthy. This type of routing is not an appropriate setup. A better solution is to use Canary deployment release in API Gateway to deploy the serverless application.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html</a></p><p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p><p><br></p><p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-api-gateway/?src=udemy">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          '<p>Launch an Application Load Balancer with an Amazon API Gateway private integration. Attach a single target group to the load balancer and select the "Canary" routing option which will automatically route incoming traffic to the new version.</p>',
          "<p>Set up one AWS Lambda Function Alias that points to both the current and new versions. Route 20% of incoming traffic to the new version and once it is considered stable, update the alias to route all traffic to the new version.</p>",
          "<p>Create a new record in Route 53 with a Failover routing policy. Configure the primary record to route 20% of incoming traffic to the new version and set the secondary record to route the rest of the traffic to the current version. Once the new version stabilizes, update the primary record to route all traffic to the new version.</p>",
          "<p>Launch a Network Load Balancer with an Amazon API Gateway private integration. Attach two target groups to the load balancer. Configure the first target group with the current version and the second target group with the new version. Configure the load balancer to route 20% of the incoming traffic to the new version and once it becomes stable, detach the first target group from the load balancer.</p>",
          "<p>Set up a canary deployment in Amazon API Gateway that routes 20% of the incoming traffic to the canary release. Promote the canary release to production once the initial tests have passed.</p>",
        ],
      },
      correct_response: ["b", "e"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A company is re-architecting its monolithic system to a serverless application in AWS to save on cost. The deployment of the succeeding new version of the application must be initially rolled out to a small number of users first for testing before the full release. If the post-hook tests fail, there should be an easy way to roll back the deployment. The DevOps Engineer was assigned to design an efficient deployment setup that mitigates any unnecessary outage that impacts their production environment.As a DevOps Engineer, how should you satisfy this requirement? (Select TWO.)",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248223,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A software development company has a hybrid cloud environment where its on-premises data center from multiple sites is connected to a VPC using AWS Site-to-Site VPN. The development department utilizes a proprietary source code analysis tool that follows the Open Web Application Security Project (OWASP) standard. This tool is hosted on a dedicated server in the data center and checks the code quality and security vulnerabilities of ongoing projects. The company plans to use this tool to run checks against the source code as part of the pipeline before compiling it into a deployable package in CodeDeploy. The code checks take approximately an hour to complete.</p><p>As a DevOps Engineer, which among the options below is the MOST suitable solution that you should implement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Site-to-Site VPN </strong>enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). A Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises) side.</p><p><strong>AWS CodePipeline</strong> includes a number of actions that help you configure build, test, and deploy resources for your automated release process. If your release process includes activities that are not included in the default actions, such as an internally developed build process or a test suite, you can create a custom action for that purpose and include it in your pipeline. You can use the AWS CLI to create custom actions in pipelines associated with your AWS account.</p><p>You can create custom actions for the following AWS CodePipeline action categories:</p><p>- A custom build action that builds or transforms the items</p><p>- A custom deploy action that deploys items to one or more servers, websites, or repositories</p><p>- A custom test action that configures and runs automated tests</p><p>- A custom invoke action that runs functions</p><p>When you create a custom action, you must also create a job worker that will poll CodePipeline for job requests for this custom action, execute the job, and return the status result to CodePipeline. This job worker can be located on any computer or resource as long as it has access to the public endpoint for CodePipeline. To easily manage access and security, consider hosting your job worker on an Amazon EC2 instance.</p><p>The following diagram shows a high-level view of a pipeline that includes a custom build action:</p><p><img src="https://media.tutorialsdojo.com/public/PipelineCustomActionCS.png"></p><p>When a pipeline includes a custom action as part of a stage, the pipeline will create a job request. A custom job worker detects that request and performs that job (in this example, a custom process using third-party build software). When the action is complete, the job worker returns either a success result or a failure result. If a success result is received, the pipeline will transition the revision and its artifacts to the next action. If a failure is returned, the pipeline will not transition the revision to the next action in the pipeline.</p><p>Hence, the correct answer is: <strong>Create a pipeline in AWS CodePipeline. Set up a custom action type and create an associated job worker that runs on-premises. Set the pipeline to invoke the custom action after the source stage. Configure the job worker to poll CodePipeline for job requests for the custom action then execute the source code analysis tool and return the status result to CodePipeline.</strong></p><p>The option that says: <strong>Create a pipeline in AWS CodePipeline. Set up an action that invokes a custom Lambda function after the source stage. Configure the function to execute the source code analysis tool, and return the results to CodePipeline. Ensure that the function waits for the execution to complete and store the output in a specified S3 bucket</strong> is incorrect because using a custom Lambda function to execute the source code analysis tool is not an appropriate solution. Remember that Lambda functions can run up to 15 minutes only and the code checks could take approximately an hour to complete. It is likely that the Lambda function will timeout in the middle of the processing.</p><p>The option that says: <strong>Create a pipeline in AWS CodePipeline. Expose the web services of the on-premises source-code analysis tool over the Internet. Set up an action that will run the tool using an API call. Set the pipeline to execute the script after the source stage. After the processing has been done, send the results to a public S3 bucket. Configure the pipeline to poll the contents of the bucket</strong> is incorrect because this setup has a lot of security vulnerabilities. Exposing the web services of the tool can open up attacks to the on-premises data center. The use of a public S3 bucket is inappropriate as well. Moreover, a CodePipeline cannot directly poll the contents of an S3 bucket.</p><p>The option that says: <strong>Create a pipeline in AWS CodePipeline. Create a shell script that clones the code repository from GitHub and run the source code analysis tool on-premises. Create an action in the pipeline that executes the shell script after the source stage and configure it to return the results to CodePipeline</strong> is incorrect because writing a custom shell script is not needed. A better solution is to simply create a custom action type and create an associated job worker that runs on-premises.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-custom-action.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-custom-action.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/actions.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/actions.html</a></p><p><a href="https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html">https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p>',
        answers: [
          "<p>Create a pipeline in AWS CodePipeline. Set up an action that invokes a custom Lambda function after the source stage. Configure the function to execute the source code analysis tool, and return the results to CodePipeline. Ensure that the function waits for the execution to complete and store the output in a specified S3 bucket.</p>",
          "<p>Create a pipeline in AWS CodePipeline. Set up a custom action type and create an associated job worker that runs on-premises. Set the pipeline to invoke the custom action after the source stage. Configure the job worker to poll CodePipeline for job requests for the custom action then execute the source code analysis tool and return the status result to CodePipeline.</p>",
          "<p>Create a pipeline in AWS CodePipeline. Expose the web services of the on-premises source-code analysis tool over the Internet. Set up an action that will run the tool using an API call. Set the pipeline to execute the script after the source stage. After the processing has been done, send the results to a public S3 bucket. Configure the pipeline to poll the contents of the bucket.</p>",
          "<p>Create a pipeline in AWS CodePipeline. Create a shell script that clones the code repository from GitHub and run the source code analysis tool on-premises. Create an action in the pipeline that executes the shell script after the source stage and configure it to return the results to CodePipeline.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A software development company has a hybrid cloud environment where its on-premises data center from multiple sites is connected to a VPC using AWS Site-to-Site VPN. The development department utilizes a proprietary source code analysis tool that follows the Open Web Application Security Project (OWASP) standard. This tool is hosted on a dedicated server in the data center and checks the code quality and security vulnerabilities of ongoing projects. The company plans to use this tool to run checks against the source code as part of the pipeline before compiling it into a deployable package in CodeDeploy. The code checks take approximately an hour to complete.As a DevOps Engineer, which among the options below is the MOST suitable solution that you should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248225,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company has a suite of applications that uses the MERN stack for the presentation tier and NGINX for the web tier. AWS CodeDeploy will be used to automate its application deployments. The DevOps team created a deployment group for their TEST environment and performed functional tests within the application. The team will set up additional deployment groups for STAGING and PROD environments later on. The current log level is configured within the NGINX settings, but the team wants to change this configuration dynamically when the deployment occurs. This will enable them to set different log level configurations depending on the deployment group without having a different application revision for each group. </p><p>Which among the options below provides the LEAST management overhead and does not require different script versions for each deployment group?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>The content in the <code>\'hooks\'</code> section of the AppSpec file varies, depending on the compute platform for your deployment. The <code>\'hooks\'</code> section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The <code>\'hooks\'</code> section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event.</p><p>There is also a set of available environment variables for the hooks. During each deployment lifecycle event, hook scripts can access the following environment variables:</p><p>During each deployment lifecycle event, hook scripts can access the following environment variables:</p><p><strong>APPLICATION_NAME</strong> - The name of the application in CodeDeploy that is part of the current deployment <em>(for example, WordPress_App).</em></p><p><strong>DEPLOYMENT_ID - </strong>The ID CodeDeploy has assigned to the current deployment <em>(for example, d-AB1CDEF23).</em></p><p><strong>DEPLOYMENT_GROUP_NAME</strong> - The name of the deployment group in CodeDeploy that is part of the current deployment <em>(for example, WordPress_DepGroup).</em></p><p><strong>DEPLOYMENT_GROUP_ID</strong> - The ID of the deployment group in CodeDeploy that is part of the current deployment <em>(for example, b1a2189b-dd90-4ef5-8f40-4c1c5EXAMPLE).</em></p><p><strong>LIFECYCLE_EVENT</strong> - The name of the current deployment lifecycle event (for example, AfterInstall).</p><p>These environment variables are local to each deployment lifecycle event.</p><p>The following script changes the listening port on an Apache HTTP server to 9090 instead of 80 if the value of <strong>DEPLOYMENT_GROUP_NAME</strong> is equal to <code>Staging</code>. This script must be invoked during the <code>BeforeInstall</code> deployment lifecycle event:</p><pre class="prettyprint linenums">if [ "$DEPLOYMENT_GROUP_NAME" == "Staging" ]\nthen\n sed -i -e \'s/Listen 80/Listen 9090/g\' /etc/httpd/conf/httpd.conf\nfi\n</pre><p>The following script example changes the verbosity level of messages recorded in its error log from warning to debug if the value of the <strong>DEPLOYMENT_GROUP_NAME</strong> environment variable is equal to <code>Staging</code>. This script must be invoked during the <code>BeforeInstall</code> deployment lifecycle event:</p><pre class="prettyprint linenums">if [ "$DEPLOYMENT_GROUP_NAME" == "Staging" ]\nthen\n sed -i -e \'s/LogLevel warn/LogLevel debug/g\' /etc/httpd/conf/httpd.conf\nfi </pre><p><br></p><p>Hence, the correct answer is: <strong>Develop a custom shell script that uses the </strong><code><strong>DEPLOYMENT_GROUP_NAME</strong></code><strong> environment variable in CodeDeploy to identify which deployment group the Amazon EC2 instance is associated with. In the </strong><code><strong>appspec.yml</strong></code><strong> config file, add a reference to this script as part of the </strong><code><strong>Beforelnstall</strong></code><strong> lifecycle hook. Configure the log level settings of the instance based on the result of the script.</strong></p><p>The option that says: <strong>Use the AWS Resource Groups Tag Editor to add a tag on each EC2 instance based on its deployment group. Attach a shell script in the application revision that will fetch the instance tag using the </strong><code><strong>aws ec2 describe-tags</strong></code><strong> CLI command to determine which deployment group the Amazon EC2 instance is associated with. In the </strong><code><strong>appspec.yml</strong></code><strong> config file, add a reference to this script as part of the </strong><code><strong>ValidateService</strong></code><strong> lifecycle hook. Configure the log level settings of the instance based on the result of the script </strong>is incorrect. It is better to use the DEPLOYMENT_GROUP_NAME environment variable in CodeDeploy instead of adding tags and using AWS CLI for deployment. Take note that you also have to provide your access keys for the AWS CLI in order to run the <code>aws ec2 describe-tags</code> CLI command. This creates an unnecessary management overhead. Moreover, you have to add a reference to the script as part of the <em>Beforelnstall</em> lifecycle hook and not to the <em>ValidateService</em>.</p><p>The option that says: <strong>Set up a custom environment variable in CodeDeploy for each environment with a value of TEST, STAGING or PROD. Attach a shell script in the application revision that will read the custom variable and determine which deployment group the Amazon EC2 instance is associated with. In the </strong><code><strong>appspec.yml</strong></code><strong> config file, add a reference to this script as part of the </strong><code><strong>ValidateService</strong></code><strong> lifecycle hook. Configure the log level settings of the instance based on the result of the script</strong> is incorrect. There is no such thing as custom environment variable in CodeDeploy. During each deployment lifecycle event, hook scripts can only access the following predefined environment variables: APPLICATION_NAME, DEPLOYMENT_ID, DEPLOYMENT_GROUP_NAME, DEPLOYMENT_GROUP_ID and LIFECYCLE_EVENT. In addition, you have to add a reference to the script as part of the <em>Beforelnstall</em> lifecycle hook and not to the <em>ValidateService</em>.</p><p>The option that says: <strong>Develop a custom shell script that uses the </strong><code><strong>DEPLOYMENT_GROUP_ID</strong></code><strong> environment variable in CodeDeploy to identify which deployment group the Amazon EC2 instance is associated with. In the </strong><code><strong>appspec.yml</strong></code><strong> config file, add a reference to this script as part of the </strong><code><strong>ApplicationStart</strong></code><strong> lifecycle hook. Configure the log level settings of the instance based on the result of the script</strong> is incorrect. You have to use the DEPLOYMENT_GROUP_NAME environment variable in CodeDeploy to identify which deployment group the Amazon EC2 instance is associated with and not DEPLOYMENT_GROUP_ID. Moreover, you have to add a reference to the script as part of the Beforelnstall lifecycle hook and not to the <em>ApplicationStart</em>.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/">https://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-environment-variable-availability">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-environment-variable-availability</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codedeploy/?src=udemy">https://tutorialsdojo.com/aws-codedeploy/</a></p>',
        answers: [
          "<p>Use the AWS Resource Groups Tag Editor to add a tag on each EC2 instance based on its deployment group. Attach a shell script in the application revision that will fetch the instance tag using the <code>aws ec2 describe-tags</code> CLI command to determine which deployment group the Amazon EC2 instance is associated with. In the <code>appspec.yml</code> config file, add a reference to this script as part of the <code>ValidateService</code> lifecycle hook. Configure the log level settings of the instance based on the result of the script.</p>",
          "<p>Develop a custom shell script that uses the <code>DEPLOYMENT_GROUP_NAME</code> environment variable in CodeDeploy to identify which deployment group the Amazon EC2 instance is associated with. In the <code>appspec.yml</code> config file, add a reference to this script as part of the <code>Beforelnstall</code> lifecycle hook. Configure the log level settings of the instance based on the result of the script.</p>",
          "<p>Set up a custom environment variable in CodeDeploy for each environment with a value of TEST, STAGING or PROD. Attach a shell script in the application revision that will read the custom variable and determine which deployment group the Amazon EC2 instance is associated with. In the <code>appspec.yml</code> config file, add a reference to this script as part of the <code>ValidateService</code> lifecycle hook. Configure the log level settings of the instance based on the result of the script.</p>",
          "<p>Develop a custom shell script that uses the <code>DEPLOYMENT_GROUP_ID</code> environment variable in CodeDeploy to identify which deployment group the Amazon EC2 instance is associated with. In the <code>appspec.yml</code> config file, add a reference to this script as part of the <code>ApplicationStart</code> lifecycle hook. Configure the log level settings of the instance based on the result of the script.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "A company has a suite of applications that uses the MERN stack for the presentation tier and NGINX for the web tier. AWS CodeDeploy will be used to automate its application deployments. The DevOps team created a deployment group for their TEST environment and performed functional tests within the application. The team will set up additional deployment groups for STAGING and PROD environments later on. The current log level is configured within the NGINX settings, but the team wants to change this configuration dynamically when the deployment occurs. This will enable them to set different log level configurations depending on the deployment group without having a different application revision for each group. Which among the options below provides the LEAST management overhead and does not require different script versions for each deployment group?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248227,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A startup software company has several application teams that develop API services for its business. Each application team is responsible for services separated on different AWS accounts. The VPC of each AWS account was initially provisioned with a <code>192.168.0.0/24</code> CIDR block. The services are deployed on Amazon EC2 instances accessed on a secure HTTPS public endpoint of an Application Load Balancer. Integration between the services routes externally to the public internet.</p><p>As part of a security audit, there is a recommendation from the security team to re-architect the integration between services to communicate on HTTPS on the private network only. A solutions architect is asked to suggest a long-term solution considering the possibility of adding more VPCs in the future.</p><p>What should the solutions architect recommend?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>A transit gateway acts as a Regional virtual router for traffic flowing between your virtual private clouds (VPCs) and on-premises networks. A transit gateway scales elastically based on the volume of network traffic. It is a best practice to use a separate subnet for each transit gateway VPC attachment.</p><p>A transit gateway enables you to attach VPCs and VPN connections and route traffic between them. A transit gateway works across AWS accounts, and you can use AWS RAM to share your transit gateway with other accounts. After you share a transit gateway with another AWS account, the account owner can attach their VPCs to your transit gateway. A user from either account can delete the attachment at any time.</p><p><img alt="Transit Gateway with AWS RAM" src="https://media.tutorialsdojo.com/public/TransitGatewayWithRAM.png" width="1000"></p><p>It is a high recommendation and the best option to renumber IP networks when possible, based on two reasons: cost, and simplicity. Changing network configurations is not easy, but it is beneficial in the long term because it removes the ongoing cost of running required components when connecting overlapping networks. Having non-overlapping IPs also makes troubleshooting easier when things go wrong, as resources can easily be identified to the network they are deployed to. This also removes the complexity of managing firewall rules across the organization.</p><p>Thus, the correct answer is: <strong>Fix the overlapping IP address ranges by renumbering the IP networks. Create a transit gateway in a new AWS account in AWS Organizations, and configure the VPC attachments. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway across the organization. Create transit gateway attachments to each VPC of the AWS accounts. Add new routes with the destination set to each VPC CIDR block, and the target set to the transit gateway attachment. Create a Network Load Balancer (NLB) in each VPC and use this for communication between services.</strong></p><p>The option that says: <strong>Create peering connections between each VPC of the different AWS accounts. Add a new route to the route table for each peering connection, with the destination set to the CIDR block of the VPC and the target set to the ID of the peering connection. Create Network Load Balancers (NLB) in each VPC and use the NLB DNS names for services integration</strong> is incorrect. Although VPC peering will only work if the overlapping IP ranges are fixed, managing peering connections between multiple VPCs can be very complex and difficult to manage as the number of VPCs increases. For each <code>x</code> number of VPCs, <code>x*(x-1)/2</code> number of peering connections needs to be created to establish connectivity across each VPC.</p><p>The option that says: <strong>Create a new AWS account and set up a new VPC. Create resource shares in AWS Resource Access Manager (AWS RAM) to share the private subnets to specified accounts. Advise the application teams to host the services in EC2 instances deployed in the shared private subnets. Create a Network Load Balancer (NLB) for each account and share the DNS names for the services integration</strong> is incorrect. Sharing private subnets across accounts using AWS Resource Access Manager (AWS RAM) can simply add to the management overhead. Each time a new subnet is created, it needs to be shared manually with the specified AWS accounts.</p><p>The option that says: <strong>Create a Network Load Balancer (NLB) in each of the account VPCs. Create an interface type VPC endpoint for </strong><code><strong>com.amazonaws.us-east-1.elasticloadbalancing</strong></code><strong> service and choose the VPC and subnets. In the AWS PrivateLink console, create a subscription for the VPC endpoint and choose the AWS account that will be allowed access to the NLB. Use the VPC endpoint DNS names for communication between services</strong> is incorrect. Although using AWS PrivateLink and VPC endpoints can provide connectivity between VPCs, it would require managing individual VPC endpoints and subscriptions, making management complex. It is better to implement a Transit Gateway solution for the long term.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/">https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/</a><br></p><p><a href="https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html">https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html</a></p><p><a href="https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html</a></p><p><a href="https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-share.html">https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-share.html</a></p><p><br></p><p>Check out this AWS Transit Gateway Cheat Sheet:</p><p><a href="https://tutorialsdojo.com/aws-transit-gateway/?src=udemy">https://tutorialsdojo.com/aws-transit-gateway/</a></p>',
        answers: [
          "<p>Create peering connections between each VPC of the different AWS accounts. Add a new route to the route table for each peering connection, with the destination set to the CIDR block of the VPC and the target set to the ID of the peering connection. Create Network Load Balancers (NLB) in each VPC and use the NLB DNS names for services integration.</p>",
          "<p>Fix the overlapping IP address ranges by renumbering the IP networks. Create a transit gateway in a new AWS account in AWS Organizations, and configure the VPC attachments. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway across the organization. Create transit gateway attachments to each VPC of the AWS accounts. Add new routes with the destination set to each VPC CIDR block, and the target set to the transit gateway attachment. Create a Network Load Balancer (NLB) in each VPC and use this for communication between services.</p>",
          "<p>Create a new AWS account and set up a new VPC. Create resource shares in AWS Resource Access Manager (AWS RAM) to share the private subnets to specified accounts. Advise the application teams to host the services in EC2 instances deployed in the shared private subnets. Create a Network Load Balancer (NLB) for each account and share the DNS names for the services integration.</p>",
          "<p>Create a Network Load Balancer (NLB) in each of the account VPCs. Create an interface type VPC endpoint for <code>com.amazonaws.us-east-1.elasticloadbalancing</code> service and choose the VPC and subnets. In the AWS PrivateLink console, create a subscription for the VPC endpoint and choose the AWS account that will be allowed access to the NLB. Use the VPC endpoint DNS names for communication between services.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Security and Compliance",
      question_plain:
        "A startup software company has several application teams that develop API services for its business. Each application team is responsible for services separated on different AWS accounts. The VPC of each AWS account was initially provisioned with a 192.168.0.0/24 CIDR block. The services are deployed on Amazon EC2 instances accessed on a secure HTTPS public endpoint of an Application Load Balancer. Integration between the services routes externally to the public internet.As part of a security audit, there is a recommendation from the security team to re-architect the integration between services to communicate on HTTPS on the private network only. A solutions architect is asked to suggest a long-term solution considering the possibility of adding more VPCs in the future.What should the solutions architect recommend?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248229,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A multinational investment bank is implementing regulatory compliance checks over its AWS accounts. All API calls made on each AWS resource across the company's accounts must be monitored and tracked for auditing purposes. AWS CloudTrail will be used to monitor all API activities and detect sensitive security issues in the company's AWS accounts. The DevOps Team was assigned to come up with a solution to prevent CloudTrail from being disabled on some AWS accounts automatically.</p><p>What solution should be applied to ensure CloudTrail log deliveries experience the LEAST amount of downtime?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Config</strong> provides <em>AWS managed rules</em>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly assess whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p><strong><img src="https://media.tutorialsdojo.com/public/td-aws-config-diagram-13Jan2025.png"></strong>You can customize the behavior of a managed rule to suit your needs. For example, you can define the rule\'s scope to constrain which resources trigger an evaluation for the rule, such as EC2 instances or volumes. You can customize the rule\'s parameters to define attributes that your resources must have to comply with the rule. For example, you can customize a parameter to specify that your security group should block incoming traffic to a specific port number.</p><p>After you activate a rule, AWS Config compares your resources to the rule\'s conditions. After this initial evaluation, AWS Config continues to run evaluations each time one is triggered. The evaluation triggers are defined as part of the rule, and they can include the following types:</p><p><strong>Configuration changes</strong> – AWS Config triggers the evaluation when any resource that matches the rule\'s scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p><p><strong>Periodic</strong> – AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p><p>The cloudtrail-enabled checks whether AWS CloudTrail is enabled in your AWS account. Optionally, you can specify which S3 bucket, SNS topic, and Amazon CloudWatch Logs ARN to use.</p><p><img src="https://media.tutorialsdojo.com/aws-config-cloudtrail-enabled.JPG"></p><p>Hence, the correct answer is: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a </strong><code><strong>StopLogging</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>StartLogging</strong></code><strong> API on the resource ARN.</strong></p><p>The option that says: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of </strong><code><strong>Configuration changes</strong></code><strong>. This managed rule will automatically remediate the accounts that disabled its CloudTrail </strong>is incorrect because, by default, AWS Config will not automatically remediate the accounts that disabled its CloudTrail. You must manually set this up using an Amazon EventBridge rule and a custom Lambda function that calls the StartLogging API to enable CloudTrail back again. Furthermore, the <code><strong>cloudtrail-enabled</strong></code> AWS Config managed rule is only available for the <code>periodic trigger</code> type and not <code>Configuration changes</code>.</p><p>The option that says: <strong>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications</strong> is incorrect. AWS Cloud Development Kit (AWS CDK) is only an open-source software development framework for building cloud applications and infrastructure using programming languages. It isn\'t used to check whether the CloudTrail is enabled in an AWS account.</p><p>The option that says: <strong>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a </strong><code><strong>DeleteTrail</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>CreateTrail</strong></code><strong> API on the resource ARN</strong> is incorrect. Instead, you should detect the <code>StopLogging</code> event and call the StartLogging API to enable CloudTrail again. The <code>DeleteTrail</code> and <code>CreateTrail</code> events, as their name implies, are simply for deleting and creating the trails respectively.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/">https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/</a></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href="https://docs.aws.amazon.com/cdk/v2/guide/home.html">https://docs.aws.amazon.com/cdk/v2/guide/home.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-config/?src=udemy">https://tutorialsdojo.com/aws-config/</a></p>',
        answers: [
          "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of <code>Configuration changes</code>. By default, this managed rule will automatically remediate the accounts that disabled its CloudTrail.</p>",
          "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a <code>StopLogging</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>StartLogging</code> API on the resource ARN.</p>",
          "<p>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications.</p>",
          "<p>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a <code>DeleteTrail</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>CreateTrail</code> API on the resource ARN.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Monitoring and Logging",
      question_plain:
        "A multinational investment bank is implementing regulatory compliance checks over its AWS accounts. All API calls made on each AWS resource across the company's accounts must be monitored and tracked for auditing purposes. AWS CloudTrail will be used to monitor all API activities and detect sensitive security issues in the company's AWS accounts. The DevOps Team was assigned to come up with a solution to prevent CloudTrail from being disabled on some AWS accounts automatically.What solution should be applied to ensure CloudTrail log deliveries experience the LEAST amount of downtime?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248231,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading digital payments company uses AWS to host its web application suite, which uses external APIs for credit and debit transactions. The current architecture is using AWS CloudTrail with several trails to log all API actions. Each trail is protected with an IAM policy to restrict access from unauthorized users. In order to maintain the system's PCI DSS compliance, a solution must be implemented that allows the tracing of the integrity of each file and prevents the files from being tampered with.</p><p>Which of the following is the MOST suitable solution with the LEAST amount of effort to implement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation. This feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete, or forge CloudTrail log files without detection. You can use the AWS CLI to validate the files in the location where CloudTrail delivered them.</p><p>Validated log files are invaluable in security and forensic investigations. For example, a validated log file enables you to assert positively that the log file itself has not changed, or that particular user credentials performed specific API activity. The CloudTrail log file integrity validation process also lets you know if a log file has been deleted or changed, or assert positively that no log files were delivered to your account during a given period of time.</p><p>When you enable log file integrity validation, CloudTrail creates a hash for every log file that it delivers. Every hour, CloudTrail also creates and delivers a file that references the log files for the last hour and contains a hash of each. This file is called a digest file. CloudTrail signs each digest file using the private key of a public and private key pair. After delivery, you can use the public key to validate the digest file. CloudTrail uses different key pairs for each AWS region.</p><p><img src="https://media.tutorialsdojo.com/public/cloudtrail_enable_log_file_validation_3.png"></p><p>The digest files are delivered to the same Amazon S3 bucket associated with your trail as your CloudTrail log files. If your log files are delivered from all regions or from multiple accounts into a single Amazon S3 bucket, CloudTrail will deliver the digest files from those regions and accounts into the same bucket.</p><p>The digest files are put into a folder separate from the log files. This separation of digest files and log files enables you to enforce granular security policies and permits existing log processing solutions to continue to operate without modification. Each digest file also contains the digital signature of the previous digest file if one exists. The signature for the current digest file is in the metadata properties of the digest file Amazon S3 object.</p><p>Hence, the correct answer is: <strong>In CloudTrail, enable the log file integrity feature on the trail that will automatically generate a digest file for every log file that CloudTrail delivers. Verify the integrity of the delivered CloudTrail files using the generated digest files.</strong></p><p>The option that says: <strong>Use AWS Systems Manager State Manager to directly enable the log file integrity feature in CloudTrail. This will automatically generate a digest file for every log file that CloudTrail delivers. Verify the integrity of the delivered CloudTrail files using the generated digest files</strong> is incorrect because there is simply no direct way to enable the log file integrity feature in CloudTrail using AWS Systems Manager State Manager. This feature must be manually enabled using either the AWS Console or the AWS CLI. Systems Manager State Manager is primarily designed to automate tasks like system configuration and patch management, not for configuring CloudTrail\'s logging features.</p><p>The option that says: <strong>In the Amazon S3 bucket of the trail, enable the log file integrity feature that will automatically generate a digest file for every log file that CloudTrail delivers. Use an IAM policy to grant the IT Security team with </strong><code><strong>PowerUserAccess</strong></code><strong>, allowing the team to download, verify, and manage the log integrity files stored in the S3 bucket</strong> is incorrect because CloudTrail\'s log file integrity feature is not managed through Amazon S3 bucket settings. The feature is primarily enabled and managed within CloudTrail itself. Although CloudTrail logs are stored in S3, the bucket configuration does not include an option to activate integrity validation or generate digest files. Additionally, granting PowerUserAccess to the IT Security team has no bearing on enabling this feature.</p><p>The option that says: <strong>Use AWS Config to directly enable the log file integrity feature in CloudTrail. This will automatically generate a digest file for every log file that CloudTrail delivers. Verify the integrity of the delivered CloudTrail files using the generated digest files</strong> is incorrect because there is no direct way that you can enable the log file integrity feature in CloudTrail using AWS Config. You have to manually enable it.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p><p><a href="https://aws.amazon.com/blogs/aws/aws-cloudtrail-update-sse-kms-encryption-log-file-integrity-verification/">https://aws.amazon.com/blogs/aws/aws-cloudtrail-update-sse-kms-encryption-log-file-integrity-verification/</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-cloudtrail/?src=udemy">https://tutorialsdojo.com/aws-cloudtrail/</a></p>',
        answers: [
          "<p>Use AWS Systems Manager State Manager to directly enable the log file integrity feature in CloudTrail. This will automatically generate a digest file for every log file that CloudTrail delivers. Verify the integrity of the delivered CloudTrail files using the generated digest files.</p>",
          "<p>In the Amazon S3 bucket of the trail, enable the log file integrity feature that will automatically generate a digest file for every log file that CloudTrail delivers. Use an IAM policy to grant the IT Security team with <code>PowerUserAccess</code>, allowing the team to download, verify, and manage the log integrity files stored in the S3 bucket.</p>",
          "<p>In CloudTrail, enable the log file integrity feature on the trail that will automatically generate a digest file for every log file that CloudTrail delivers. Verify the integrity of the delivered CloudTrail files using the generated digest files.</p>",
          "<p>Use AWS Config to directly enable the log file integrity feature in CloudTrail. This will automatically generate a digest file for every log file that CloudTrail delivers. Verify the integrity of the delivered CloudTrail files using the generated digest files.</p>",
        ],
      },
      correct_response: ["c"],
      section: "Monitoring and Logging",
      question_plain:
        "A leading digital payments company uses AWS to host its web application suite, which uses external APIs for credit and debit transactions. The current architecture is using AWS CloudTrail with several trails to log all API actions. Each trail is protected with an IAM policy to restrict access from unauthorized users. In order to maintain the system's PCI DSS compliance, a solution must be implemented that allows the tracing of the integrity of each file and prevents the files from being tampered with.Which of the following is the MOST suitable solution with the LEAST amount of effort to implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248233,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A leading IT consultancy firm has several Python-based Flask and Django web applications hosted in AWS. Some of the firm's developers are freelance contractors located overseas. The firm wants to automate remediation actions for issues relating to the health of its AWS resources by using the AWS Health Dashboard and the AWS Health API. Additionally, there is a requirement to automatically detect any IAM access key owned by the firm that is accidentally or deliberately exposed on a public GitHub repository. Upon detection, the IAM access key must be immediately deleted, and a notification must be sent to the DevOps team. Once detected, the IAM access key must be immediately deleted, and a notification should be sent to the DevOps team.</p><p>What solution can a DevOps Engineer do to meet this requirement?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>AWS Step Functions</strong> lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker AI, into feature-rich applications. Workflows are made up of a series of steps, with the output of one step acting as input into the next. Application development is simpler and more intuitive using Step Functions, because it translates your workflow into a state machine diagram that is easy to understand, easy to explain to others, and easy to change.</p><p>AWS proactively monitors popular code repository sites for exposed AWS Identity and Access Management (IAM) access keys. On detection of an exposed IAM access key, AWS Health generates an <strong>AWS_RISK_CREDENTIALS_EXPOSED </strong>Event. In response to this event, you can set up an automated workflow deletes the exposed IAM Access Key, summarizes the recent API activity for the exposed key, and sends the summary message to an Amazon Simple Notification Service (SNS) Topic to notify the subscribers which are all orchestrated by an AWS Step Functions state machine.</p><p><img alt="Step Function Automation Diagram using EventBridge" height="555" src="https://media.tutorialsdojo.com/public/td-automation-diagram-using -step-function-eventbridge-01-09-25.png" width="1000"></p><p>You can use Amazon EventBridge to detect and react to changes in the status of AWS Personal Health Dashboard (AWS Health) events. Then, based on the rules that you create, EventBridge invokes one or more target actions when an event matches the values that you specify in a rule. Depending on the type of event, you can send notifications, capture event information, take corrective action, initiate events, or take other actions.</p><p>You can automate actions in response to new scheduled events for your EC2 instances. For example, you can create EventBridge rules for EC2 scheduled events generated by the AWS Health service. These rules can then trigger targets, such as AWS Systems Manager Automation documents, to automate actions.</p><p>Hence, the correct answer is: <strong>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends notification to the IT Security team using Amazon SNS. Create a EventBridge rule with an </strong><code><strong>aws.health</strong></code><strong> event source and the </strong><code><strong>AWS_RISK_CREDENTIALS_EXPOSED</strong></code><strong> event to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</strong></p><p>The option that says: <strong>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Use a combination of Amazon GuardDuty and Amazon Macie to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule</strong> is incorrect. You can\'t monitor any exposed IAM keys from the Internet using Amazon GuardDuty and Amazon Macie. Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads while Amazon Macie is simply a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS.</p><p>The option that says: <strong>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Create an AWS Config rule for the </strong><code><strong>AWS_RISK_CREDENTIALS_EXPOSED</strong></code><strong> event with Multi-Account Multi-Region Data Aggregation to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule </strong>is incorrect. The use of Multi-Account Multi-Region Data Aggregation in CloudTrail will not satisfy the requirement. An aggregator is simply an AWS Config resource type that collects AWS Config configuration and compliance data from multiple accounts across multiple regions.</p><p>The option that says: <strong>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Create an AWS Personal Health Dashboard rule for the </strong><code><strong>AWS_RISK_CREDENTIALS_EXPOSED</strong></code><strong> event to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule</strong> is incorrect. You have to use the AWS Health API instead of the AWS Personal Health Dashboard. The <code><strong><em>AWS_RISK_CREDENTIALS_EXPOSED</em></strong></code><strong><em> </em></strong>event is only applicable from an <code>aws.health</code> event source and not from an AWS Personal Health Dashboard rule.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/">https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/</a></p><p><a href="https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><br></p><p><strong>Check out these AWS Health and Amazon CloudWatch Cheat Sheets:</strong></p><p><a href="https://tutorialsdojo.com/aws-health/?src=udemy">https://tutorialsdojo.com/aws-health/</a></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>',
        answers: [
          "<p>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Use a combination of Amazon GuardDuty and Amazon Macie to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</p>",
          "<p>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Create an AWS Config rule for the <code>AWS_RISK_CREDENTIALS_EXPOSED</code> event with Multi-Account Multi-Region Data Aggregation to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</p>",
          "<p>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Create an AWS Personal Health Dashboard rule for the <code>AWS_RISK_CREDENTIALS_EXPOSED</code> event to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</p>",
          "<p>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends notification to the IT Security team using Amazon SNS. Create a EventBridge rule with an &lt;code&gt;aws.health&lt;/code&gt; event source and the <code>AWS_RISK_CREDENTIALS_EXPOSED</code> event to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</p>",
        ],
      },
      correct_response: ["d"],
      section: "Monitoring and Logging",
      question_plain:
        "A leading IT consultancy firm has several Python-based Flask and Django web applications hosted in AWS. Some of the firm's developers are freelance contractors located overseas. The firm wants to automate remediation actions for issues relating to the health of its AWS resources by using the AWS Health Dashboard and the AWS Health API. Additionally, there is a requirement to automatically detect any IAM access key owned by the firm that is accidentally or deliberately exposed on a public GitHub repository. Upon detection, the IAM access key must be immediately deleted, and a notification must be sent to the DevOps team. Once detected, the IAM access key must be immediately deleted, and a notification should be sent to the DevOps team.What solution can a DevOps Engineer do to meet this requirement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248235,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A company is developing a serverless application that uses AWS Lambda, AWS SAM, and Amazon API Gateway. There is a requirement to fully automate the backend Lambda deployment in such a way that the deployment will automatically run whenever a new commit is pushed to a GitHub repository. There should also be a separate environment pipeline for TEST and PROD environments. In addition, the TEST environment should be the only one that allows automatic deployment.</p><p>How can a DevOps Engineer satisfy these requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>In <strong>AWS CodePipeline</strong>, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p>If the action is approved, the pipeline execution resumes. If the action is rejected—or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping—the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p><img src="https://media.tutorialsdojo.com/public/PipelineFlow.png"></p><p>Hence, the correct answer is: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</strong></p><p>The option that says: <strong>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline</strong> is incorrect. You should add a manual approval step on the PROD pipeline as mentioned in the requirements of the scenario.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline</strong> is incorrect. You don\'t need to create separate GitHub repositories for the two environments. You just need to create two different branches from a single repository.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline </strong>is incorrect. You should add the manual approval step on the PROD pipeline only, excluding the TEST pipeline. Moreover, you don\'t need to create separate GitHub repositories for the two environments. You only need to create two different branches from a single repository.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html ">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline.</p>",
          "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
          "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
          "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
        ],
      },
      correct_response: ["c"],
      section: "SDLC Automation",
      question_plain:
        "A company is developing a serverless application that uses AWS Lambda, AWS SAM, and Amazon API Gateway. There is a requirement to fully automate the backend Lambda deployment in such a way that the deployment will automatically run whenever a new commit is pushed to a GitHub repository. There should also be a separate environment pipeline for TEST and PROD environments. In addition, the TEST environment should be the only one that allows automatic deployment.How can a DevOps Engineer satisfy these requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248237,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>The Development team of a leading IT consultancy company would like to add a manual approval action before their new application versions are deployed to their production environment. The approval action must be strictly enforced even if the unit and integration tests are all successful. They have set up a pipeline using CodePipeline to orchestrate the workflow of their continuous integration and continuous delivery processes. The new versions of the application are built using CodeBuild and are deployed to a fleet of Amazon EC2 instances using CodeDeploy. </p><p>Which of the following provides the SIMPLEST and the MOST cost-effective solution?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>You can automate your release process by using AWS CodePipeline to test your code and run your builds with CodeBuild. You can create reports in CodeBuild that contain details about tests that are run during builds.</p><p>You can create tests such as unit tests, configuration tests, and functional tests. The test file format can be JUnit XML or Cucumber JSON. Create your test cases with any test framework that can create files in one of those formats (for example, Surefire JUnit plugin, TestNG, and Cucumber). To create a test report, you add a report group name to the buildspec file of a build project with information about your test cases. When you run the build project, the test cases are run and a test report is created. You do not need to create a report group before you run your tests. If you specify a report group name, CodeBuild creates a report group for you when you run your reports. If you want to use a report group that already exists, you specify its ARN in the buildspec file.</p><p>In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p><img src="https://media.tutorialsdojo.com/public/pipeline.png"></p><p>If the action is approved, the pipeline execution resumes. If the action is rejected—or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping—the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p>Hence, the correct answer is: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</strong></p><p>The option that says: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application\'s functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. You can just simply set up a manual approval action instead of creating a custom action. That takes a lot of effort to configure including the development of a custom job worker.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application\'s functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. It is tedious to automatically perform the unit and integration tests using AWS Step Functions. You can just use CodeBuild to handle all of the tests.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application\'s functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. This solution entails an additional burden to install, configure and launch a third-party CI/CD tool in Amazon EC2. A more simple solution is to just use CodeBuild for tests.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codepipeline/?src=udemy">https://tutorialsdojo.com/aws-codepipeline/</a></p>',
        answers: [
          "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</p>",
          "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
          "<p>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
          "<p>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
        ],
      },
      correct_response: ["a"],
      section: "SDLC Automation",
      question_plain:
        "The Development team of a leading IT consultancy company would like to add a manual approval action before their new application versions are deployed to their production environment. The approval action must be strictly enforced even if the unit and integration tests are all successful. They have set up a pipeline using CodePipeline to orchestrate the workflow of their continuous integration and continuous delivery processes. The new versions of the application are built using CodeBuild and are deployed to a fleet of Amazon EC2 instances using CodeDeploy. Which of the following provides the SIMPLEST and the MOST cost-effective solution?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248239,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A government agency recently decided to modernize its network infrastructure using AWS. They are developing a solution to store confidential files containing Personally Identifiable Information (PII) and other sensitive financial records of its citizens. All data in the storage solution must be encrypted both at rest and in transit. In addition, all of its data must also be replicated in two locations that are at least 450 miles apart from each other. </p><p>As a DevOps Engineer, what solution should you implement to meet these requirements?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p><strong>Availability Zones</strong> give customers the ability to operate production applications and databases that are more highly available, fault-tolerant, and scalable than would be possible from a single data center. AWS maintains multiple AZs around the world and more zones are added at a fast pace. Each AZ can be multiple data centers (typically 3), and at full scale can be hundreds of thousands of servers. They are fully isolated partitions of the AWS Global Infrastructure. With their own power infrastructure, the AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles of each other).</p><p>All AZs are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. The network performance is sufficient to accomplish synchronous replication between AZs. AWS Availability Zones are also powerful tools for helping build highly available applications. AZs make partitioning applications about as easy as it can be. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more.</p><p><img src="https://media.tutorialsdojo.com/public/Amazon-S3.png"></p><p>By default, Amazon S3 allows both HTTP and HTTPS requests. To comply with the <strong>s3-bucket-ssl-requests-only</strong> rule, confirm that your bucket policies explicitly deny access to HTTP requests. Bucket policies that allow HTTPS requests without explicitly denying HTTP requests might not comply with the rule.</p><p>To determine HTTP or HTTPS requests in a bucket policy, use a condition that checks for the key <strong>"aws:SecureTransport"</strong>. When this key is <strong>true</strong>, this means that the request is sent through HTTPS. To be sure to comply with the <strong>s3-bucket-ssl-requests-only</strong> rule, create a bucket policy that explicitly denies access when the request meets the condition <strong>"aws:SecureTransport": "false"</strong>. This policy explicitly denies access to HTTP requests.</p><p>In this scenario, you should use AWS Regions since AZs are physically separated by only 100 km (60 miles) from each other. Within each AWS Region, S3 operates in a minimum of three AZs, each separated by miles to protect against local events like fires, floods et cetera. Take note that you can\'t launch an AZ-based S3 bucket.</p><p>Hence, the correct answer is: <strong>Set up primary and secondary S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</strong></p><p>The option that says: <strong>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3 SSE-C encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets</strong> is incorrect. You can\'t create Amazon S3 buckets in two separate Availability Zones since this is a regional service.</p><p>The option that says: <strong>Set up primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create an IAM role to enforce access to the buckets only through HTTPS. Set up a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets</strong> is incorrect. You have to use the bucket policy to enforce access to the bucket using HTTPS only and not an IAM role.</p><p>The option that says: <strong>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce AWS KMS encryption on all objects uploaded to the bucket. Enable Transfer Acceleration between the two buckets. Set up a KMS Key in the primary region for encrypting objects</strong> is incorrect. You have to enable Cross-Region replication and not Transfer Acceleration. This feature simply enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket but not data replication.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html ">https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html</a></p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/ ">https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/</a></p><p><a href="https://aws.amazon.com/about-aws/global-infrastructure/regions_az/">https://aws.amazon.com/about-aws/global-infrastructure/regions_az/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-s3/?src=udemy">https://tutorialsdojo.com/amazon-s3/</a></p>',
        answers: [
          "<p>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3 SSE-C encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
          "<p>Set up primary and secondary S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
          "<p>Set up primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create an IAM role to enforce access to the buckets only through HTTPS. Set up a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
          "<p>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce AWS KMS encryption on all objects uploaded to the bucket. Enable Transfer Acceleration between the two buckets. Set up a KMS Key in the primary region for encrypting objects.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Resilient Cloud Solutions",
      question_plain:
        "A government agency recently decided to modernize its network infrastructure using AWS. They are developing a solution to store confidential files containing Personally Identifiable Information (PII) and other sensitive financial records of its citizens. All data in the storage solution must be encrypted both at rest and in transit. In addition, all of its data must also be replicated in two locations that are at least 450 miles apart from each other. As a DevOps Engineer, what solution should you implement to meet these requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248241,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>A commercial bank has a hybrid cloud architecture in AWS where its online banking platform is hosted. The CTO instructed its Lead DevOps Engineer to implement a suitable deployment set up that minimizes the impact on their production environment. The CI/CD process should be configured as follows: </p><p>- A new fleet of Amazon EC2 instances should be automatically launched first before the actual production deployment. The additional instances will serve traffic during the deployment. </p><p>- All available EC2 instances across various Availability Zones must be load-balanced and must automatically heal if it becomes impaired due to an underlying hardware failure in Amazon EC2. </p><p>- At least half of the incoming traffic must be rerouted to the new application version that is hosted to the new instances. </p><p>- The deployment should be considered successful if traffic is rerouted to at least half of the available EC2 instances. </p><p>- All temporary files must be deleted before routing traffic to the new fleet of instances. Ensure that any other files that were automatically generated during the deployment process are removed. </p><p>- To reduce costs, the EC2 instances that host the old version in the deployment group must be terminated immediately. </p><p>What should the Engineer do to satisfy these requirements?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>The content in the <code>\'hooks\'</code> section of the AppSpec file varies, depending on the compute platform for your deployment. The <code>\'hooks\'</code> section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The <code>\'hooks\'</code> section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line. Here are descriptions of the hooks available for use in your AppSpec file.</p><p><code><strong>ApplicationStop</strong></code> – This deployment lifecycle event occurs even before the application revision is downloaded. You can specify scripts for this event to gracefully stop the application or remove currently installed packages in preparation for a deployment. The AppSpec file and scripts used for this deployment lifecycle event are from the previous successfully deployed application revision.</p><p><code><strong>DownloadBundle</strong></code> – During this deployment lifecycle event, the CodeDeploy agent copies the application revision files to a temporary location:</p><p><code>/opt/codedeploy-agent/deployment-root/<em>deployment-group-id</em>/<em>deployment-id</em>/deployment-archive</code> folder on Amazon Linux, Ubuntu Server, and RHEL Amazon EC2 instances.</p><p><code>C:\\ProgramData\\Amazon\\CodeDeploy\\<em>deployment-group-id</em>\\<em>deployment-id</em>\\deployment-archive</code> folder on Windows Server Amazon EC2 instances.</p><p>This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>BeforeInstall</strong></code> – You can use this deployment lifecycle event for preinstall tasks, such as decrypting files and creating a backup of the current version.</p><p><code><strong>Install</strong></code> – During this deployment lifecycle event, the CodeDeploy agent copies the revision files from the temporary location to the final destination folder. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterInstall</strong></code> – You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions.</p><p><code><strong>ApplicationStart</strong></code> – You typically use this deployment lifecycle event to restart services that were stopped during <code>ApplicationStop</code>.</p><p><code><strong>ValidateService</strong></code> – This is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.</p><p><code><strong>BeforeBlockTraffic</strong></code> – You can use this deployment lifecycle event to run tasks on instances before they are deregistered from a load balancer.</p><p><code><strong>BlockTraffic</strong></code> – During this deployment lifecycle event, internet traffic is blocked from accessing instances that are currently serving traffic. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterBlockTraffic</strong></code> – You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><code><strong>BeforeAllowTraffic</strong></code> – You can use this deployment lifecycle event to run tasks on instances before they are registered with a load balancer.</p><p><code><strong>AllowTraffic</strong></code> – During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterAllowTraffic</strong></code> – You can use this deployment lifecycle event to run tasks on instances after they are registered with a load balancer.</p><p><img src="https://img-c.udemycdn.com/redactor/raw/2019-12-16_03-04-56-691d69e91f667f1c9bb8b0554c2da933.png"></p><p>Hence, the correct answer is: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BeforeAllowTraffic</strong></code><strong> Traffic hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files.</strong></p><p>The option that says: <strong>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the </strong><code><strong>CodeDeployDefault.OneAtAtime</strong></code><strong> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the </strong><code><strong>AllowTraffic</strong></code><strong> hook within the </strong><code><strong>appspec.yml</strong></code><strong> configuration file to purge the temporary files</strong> is incorrect because you should use blue/green deployment instead of in-place. In addition, the <code><strong><em>AllowTraffic</em></strong></code> event just allows the incoming traffic to the instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p>The option that says: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.HalfAtAtime</strong></code><strong> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appspec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because the <code>BlockTraffic</code> event is reserved for the CodeDeploy agent and cannot be used to run custom scripts such as deleting the temporary files.</p><p>The option that says: <strong>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.AllatOnce</strong></code><strong> as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because you should use a blue/green deployment instead of in-place. It is also incorrect to use the <code>CodeDeployDefault AllatOnce</code> deployment configuration as this attempts to deploy the application revision to as many instances as possible at once.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures</a></p><p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codedeploy/?src=udemy">https://tutorialsdojo.com/aws-codedeploy/</a></p>',
        answers: [
          "<p>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the <code>CodeDeployDefault.OneAtAtime</code> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the <code>AllowTraffic</code> hook within the <code>appspec.yml</code> configuration file to purge the temporary files.</p>",
          "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the <code>BeforeAllowTraffic</code> Traffic hook within <code>appspec.yml</code> to purge the temporary files.</p>",
          "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the <code>CodeDeployDefault.HalfAtAtime</code> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>",
          "<p>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the &lt;code&gt;CodeDeployDefault AllatOnce&lt;/code&gt; as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>",
        ],
      },
      correct_response: ["b"],
      section: "SDLC Automation",
      question_plain:
        "A commercial bank has a hybrid cloud architecture in AWS where its online banking platform is hosted. The CTO instructed its Lead DevOps Engineer to implement a suitable deployment set up that minimizes the impact on their production environment. The CI/CD process should be configured as follows: - A new fleet of Amazon EC2 instances should be automatically launched first before the actual production deployment. The additional instances will serve traffic during the deployment. - All available EC2 instances across various Availability Zones must be load-balanced and must automatically heal if it becomes impaired due to an underlying hardware failure in Amazon EC2. - At least half of the incoming traffic must be rerouted to the new application version that is hosted to the new instances. - The deployment should be considered successful if traffic is rerouted to at least half of the available EC2 instances. - All temporary files must be deleted before routing traffic to the new fleet of instances. Ensure that any other files that were automatically generated during the deployment process are removed. - To reduce costs, the EC2 instances that host the old version in the deployment group must be terminated immediately. What should the Engineer do to satisfy these requirements?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248243,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An online data analytics application is launched to 12 On-Demand EC2 instances across three Availability Zones using a golden AMI in AWS. Each instance has only 10% utilization after business hours but increases to 30% utilization during peak hours. There are also some third-party applications that use the application from all over the globe with no specific schedule. In the morning, there is always a sudden CPU utilization increase on the EC2 instances due to the number of users logging in to use the application. However, its CPU utilization usually stabilizes after a few hours. A DevOps Engineer has been instructed to reduce costs and improve the overall reliability of the system.</p><p>Which among the following options provides the MOST suitable solution in this scenario?</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>Scaling based on a schedule allows you to set your own scaling schedule for predictable load changes. For example, every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date.</p><p>To configure your Auto Scaling group to scale based on a schedule, you create a scheduled action. The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size specified by the scaling action.</p><p>You can create scheduled actions for scaling one time only or for scaling on a recurring schedule.</p><p><img src="https://media.tutorialsdojo.com/public/as-basic-diagram.png"></p><p>Hence, the correct answer is: <strong>Set up an Auto Scaling group using the golden AMI with a scaling action based on the CPU Utilization average. Configure a scheduled action for the group to adjust the minimum number of Amazon EC2 instances to three after business hours end, and reset to six before business hours begin.</strong></p><p>The option that says: <strong>Set up two Amazon Eventbridge rules and two Lambda functions. Configure each Amazon Eventbridge rule to invoke a Lambda function and regularly run before and after the peak hours. The first function should stop nine instances after the peak hours end while the second function should restart the nine instances before the business day begins</strong> is incorrect because you can simply configure a scheduled action for the Auto Scaling group to adjust the minimum number of the available EC2 instances without using Amazon EventBridge or a Lambda function.</p><p>The option that says: <strong>Set up two AWS Config rules and two Lambda functions. Configure each rule to invoke a Lambda function and regularly run before and after the peak hours. The first function should stop nine instances after the peak hours end while the second function should restart the nine instances before the business day begins </strong>is incorrect because using AWS Config is typically not an appropriate service to use in this scenario. A better solution is to configure a scheduled action in the Auto Scaling group.</p><p>The option that says: <strong>Launch a group of Scheduled Reserved Instances that regularly run before and after peak hours. Integrate Amazon EventBridge and AWS Lambda to regularly stop nine instances after the peak hours every day and restart the nine instances before the business day begins</strong> is incorrect. Although your operating costs will be decreased by using Scheduled Reserved Instances, this setup is still not appropriate since the traffic to the portal is not entirely predictable. Take note that there are third-party applications that use the application from all over the globe with no specific schedule. Hence, the use of Scheduled Reserved Instances is not recommended.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p><p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html</a></p><p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-auto-scaling/?src=udemy">https://tutorialsdojo.com/aws-auto-scaling/</a></p>',
        answers: [
          "<p>Set up two Amazon Eventbridge rules and two Lambda functions. Configure each Amazon Eventbridge rule to invoke a Lambda function and regularly run before and after the peak hours. The first function should stop nine instances after the peak hours end while the second function should restart the nine instances before the business day begins</p>",
          "<p>Set up an Auto Scaling group using the golden AMI with a scaling action based on the CPU Utilization average. Configure a scheduled action for the group to adjust the minimum number of Amazon EC2 instances to three after business hours end, and reset to six before business hours begin.</p>",
          "<p>Set up two AWS Config rules and two Lambda functions. Configure each rule to invoke a Lambda function and regularly run before and after the peak hours. The first function should stop nine instances after the peak hours end while the second function should restart the nine instances before the business day begins.</p>",
          "<p>Launch a group of Scheduled Reserved Instances that regularly run before and after peak hours. Integrate Amazon EventBridge and AWS Lambda to regularly stop nine instances after the peak hours every day and restart the nine instances before the business day begins.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Configuration Management and Infrastructure as Code",
      question_plain:
        "An online data analytics application is launched to 12 On-Demand EC2 instances across three Availability Zones using a golden AMI in AWS. Each instance has only 10% utilization after business hours but increases to 30% utilization during peak hours. There are also some third-party applications that use the application from all over the globe with no specific schedule. In the morning, there is always a sudden CPU utilization increase on the EC2 instances due to the number of users logging in to use the application. However, its CPU utilization usually stabilizes after a few hours. A DevOps Engineer has been instructed to reduce costs and improve the overall reliability of the system.Which among the following options provides the MOST suitable solution in this scenario?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248245,
      assessment_type: "multiple-choice",
      prompt: {
        question:
          "<p>An organization has several accounts managed within AWS Organizations. The DevOps Engineer in charge of the network AWS account has established an AWS Transit Gateway to direct all of the company's outgoing traffic, which is then directed through a firewall appliance for inspection. The firewall appliance logs all types of event severities such as <code>CRITICAL</code>, <code>ERROR</code>, <code>HIGH</code>, <code>MID</code>, <code>LOW</code>, and <code>ERROR</code> which are all sent to Amazon CloudWatch Logs.</p><p>During a recent security audit, no alerts were discovered for events that could cause disruptions to business continuity. The company has required that the security team receive an alert for immediate remediation when any <code>CRITICAL</code> events occur.</p><p>Which of the following is the MOST suitable solution that the DevOps Engineer should implement?</p>",
        relatedLectureIds: "",
        feedbacks: ["", "", "", ""],
        explanation:
          '<p>The ability to search and filter the log data arriving at CloudWatch Logs is achieved by creating one or more <strong>metric filters</strong>.</p><p><strong>Metric filters</strong> define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-cloudwatch-metric-filter.png"></p><p>When creating a metric from a log filter, it has the capability to assign dimensions and a unit to the metric. If specifying a unit, be sure to specify the correct one when creating the filter. Changing the unit for the filter later will have no effect.</p><p>In this scenario, the firewall appliance already pushes logs to <strong>CloudWatch Logs</strong>. We can leverage <strong>CloudWatch metric filter </strong>to search and filter for the CRITICAL events and set a <strong>CloudWatch alarm </strong>to send notifications to the security team.</p><p>Hence, the correct answer is the option that says: <strong>Set up a CloudWatch metric filter that searches for </strong><code><strong>CRITICAL</strong></code><strong> events. Create a custom metric for the findings, then associate a CloudWatch alarm that will send a notification to an SNS topic. Subscribe the security team\'s email address to the SNS topic.</strong></p><p>The option that says: <strong>Set up a CloudWatch Synthetics canary to monitor the firewall status. If the firewall enters a </strong><code><strong>CRITICAL</strong></code><strong> state or logs a </strong><code><strong>CRITICAL</strong></code><strong> event, configure a CloudWatch alarm to publish to an SNS topic. Subscribe the security team\'s email address to the SNS topic </strong>is incorrect because CloudWatch Synthetics canary is used to create canaries, configurable scripts that run on a schedule, to monitor endpoints and APIs. In the scenario, the firewall appliance is already pushing to CloudWatch Logs. Metric filter is a much more suitable solution as it can filter the CRITICAL events.</p><p>The option that says: <strong>Monitor flow logs with Amazon Inspector to the network AWS account. Set up an EventBridge event rule in CloudWatch that will be triggered by </strong><code><strong>CRITICAL</strong></code><strong> Inspector events. Create an SNS topic as the target, then subscribe the security team\'s email address to the SNS topic </strong>is incorrect because Amazon Inspector is a vulnerability management service that is commonly used to scan AWS workloads for software vulnerabilities and unintended network exposure. This service does not filter logs.</p><p>The option that says: <strong>Utilize AWS Firewall Manager to enforce uniform policies across all accounts. Set up an EventBridge event rule in CloudWatch that will be triggered by </strong><code><strong>CRITICAL</strong></code><strong> Firewall Manager events. Create an SNS topic as the target, then subscribe the security team\'s email address to the SNS topic</strong> is incorrect because AWS Firewall Manager is primarily used to centrally configure and manage firewall rules across accounts and applications in AWS Organizations. It does not have a filter capability like CloudWatch metric filter.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html#search-filter-concepts">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html#search-filter-concepts</a></p><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_alarm_log_group_metric_filter.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_alarm_log_group_metric_filter.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Monitor flow logs with Amazon Inspector to the network AWS account. Set up an EventBridge event rule in CloudWatch that will be triggered by <code>CRITICAL</code> Inspector events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic.</p>",
          "<p>Set up a CloudWatch metric filter that searches for <code>CRITICAL</code> events. Create a custom metric for the findings, then associate a CloudWatch alarm that will send a notification to an SNS topic. Subscribe the security team's email address to the SNS topic.</p>",
          "<p>Utilize AWS Firewall Manager to enforce uniform policies across all accounts. Set up an EventBridge event rule in CloudWatch that will be triggered by <code>CRITICAL</code> Firewall Manager events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic.</p>",
          "<p>Set up a CloudWatch Synthetics canary to monitor the firewall status. If the firewall enters a <code>CRITICAL</code> state or logs a <code>CRITICAL</code> event, configure a CloudWatch alarm to publish to an SNS topic. Subscribe the security team's email address to the SNS topic.</p>",
        ],
      },
      correct_response: ["b"],
      section: "Incident and Event Response",
      question_plain:
        "An organization has several accounts managed within AWS Organizations. The DevOps Engineer in charge of the network AWS account has established an AWS Transit Gateway to direct all of the company's outgoing traffic, which is then directed through a firewall appliance for inspection. The firewall appliance logs all types of event severities such as CRITICAL, ERROR, HIGH, MID, LOW, and ERROR which are all sent to Amazon CloudWatch Logs.During a recent security audit, no alerts were discovered for events that could cause disruptions to business continuity. The company has required that the security team receive an alert for immediate remediation when any CRITICAL events occur.Which of the following is the MOST suitable solution that the DevOps Engineer should implement?",
      related_lectures: [],
    },
    {
      _class: "assessment",
      id: 138248247,
      assessment_type: "multi-select",
      prompt: {
        question:
          "<p>An organization has a Java-based application that is built using Apache Maven. The organization decided to automate the build process for the Java project. The project's code is stored on GitHub, and every time the repository is updated, the code must be compiled, tested, and then uploaded to Amazon S3.</p><p>Which set of steps should the DevOps Engineer implement to meet the requirements? (Select THREE.)</p>",
        relatedLectureIds: [],
        links: [],
        feedbacks: ["", "", "", "", "", ""],
        explanation:
          '<p><strong>AWS CodeBuild</strong> is a fully managed build service in the cloud that compiles source code, runs unit tests, and produces deployment-ready artifacts. The service eliminates the need for users to provision, manage, and scale their own build servers. Additionally, it offers prepackaged build environments for popular programming languages and build tools, including Apache Maven, Gradle, and others.</p><p><img src="https://media.tutorialsdojo.com/public/dop-c02-codebuild.png"></p><p>A <strong>buildspec</strong> is a YAML-formatted collection of build commands and settings that are used to execute a build. Users can either include a buildspec as part of the source code or define it when creating a build project.</p><p><strong>AWS CodeBuild</strong> supports <strong>webhooks</strong> when the <strong>source repository is GitHub</strong>. This means that for a CodeBuild build project that has its source code stored in a GitHub repository, webhooks can be used to rebuild the source code every time a code change is pushed to the repository.</p><p>Hence, the correct answers are:</p><p>- <strong>Set up a GitHub webhook that will initiate a build process whenever there is a modification made to the code, and it is pushed to the repository.</strong></p><p><strong>- Provision an AWS CodeBuild project that utilizes GitHub as the source repository.</strong></p><p><strong>- Include a buildspec.yml file in the source code with the commands necessary to compile, build, and test the project.</strong></p><p>The option that says: <strong>Launch an AWS CodeDeploy application utilizing the Amazon EC2/On-Premises compute platform </strong>is incorrect because the scenario requires automation of the build process, which is why AWS CodeBuild is recommended, as it is designed for that purpose. AWS CodeDeploy, on the other hand, is primarily used for deploying applications.</p><p>The option that says: <strong>Create an Amazon EC2 instance and install dependencies via user data, then use it to execute the build </strong>is incorrect because the solution may be technically feasible, but it is not cost-effective because it utilizes an EC2 instance for the build process, which incurs additional operational overhead and costs.</p><p>The option that says: <strong>Set up an AWS Lambda function triggered by an Amazon S3 event to compile the code and run tests</strong> is incorrect because Lambda is not typically used for long-running build and test processes, which are better suited for CodeBuild.</p><p><br></p><p><strong>References:</strong></p><p><a href="https://docs.aws.amazon.com/codebuild/latest/APIReference/Welcome.html">https://docs.aws.amazon.com/codebuild/latest/APIReference/Welcome.html</a></p><p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p><p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html</a></p><p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/github-webhook.html">https://docs.aws.amazon.com/codebuild/latest/userguide/github-webhook.html</a></p><p><br></p><p><strong>Check out this AWS CodeBuild Cheat Sheet:</strong></p><p><a href="https://tutorialsdojo.com/aws-codebuild/?src=udemy">https://tutorialsdojo.com/aws-codebuild/</a></p><p><br></p><p><strong>Tutorials Dojo\'s AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href="https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>',
        answers: [
          "<p>Set up a GitHub webhook that will initiate a build process whenever there is a modification made to the code, and it is pushed to the repository.</p>",
          "<p>Launch an AWS CodeDeploy application utilizing the Amazon EC2/On-Premises compute platform.</p>",
          "<p>Provision an AWS CodeBuild project that utilizes GitHub as the source repository.</p>",
          "<p>Include a buildspec.yml file in the source code with the commands necessary to compile, build, and test the project.</p>",
          "<p>Create an Amazon EC2 instance and install dependencies via user data, then use it to execute the build.</p>",
          "<p>Set up an AWS Lambda function triggered by an Amazon S3 event to compile the code and run tests.</p>",
        ],
      },
      correct_response: ["a", "c", "d"],
      section: "SDLC Automation",
      question_plain:
        "An organization has a Java-based application that is built using Apache Maven. The organization decided to automate the build process for the Java project. The project's code is stored on GitHub, and every time the repository is updated, the code must be compiled, tested, and then uploaded to Amazon S3.Which set of steps should the DevOps Engineer implement to meet the requirements? (Select THREE.)",
      related_lectures: [],
    },
  ],
};
