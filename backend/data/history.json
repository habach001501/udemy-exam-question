[
  {
    "id": "1769619233089",
    "date": "2026-01-28T16:53:53.089Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 15,
    "incorrect": 5,
    "unanswered": 0,
    "total": 20,
    "percent": 75,
    "duration": 2519968,
    "questions": [
      {
        "id": 75949046,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company manages a continuous integration and continuous delivery (CI/CD) pipeline that includes a Jenkins implementation that runs on Amazon EC2. The security team has requested that all build artifacts are encrypted as they contain company sensitive data.</p><p>Which changes should a DevOps engineer make to meet these requirements whilst reducing operational overhead for ongoing management?</p>",
          "answers": [
            "<p>Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes.</p>",
            "<p>Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption.</p>",
            "<p>Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group.</p>",
            "<p>Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM).</p>"
          ],
          "explanation": "<p>The existing Jenkins implementation runs on an Amazon EC2 instance, and this requires more operational management compared to using AWS CodeBuild which is a managed service. The DevOps engineer should replace Jenkins with AWS CodeBuild which is a fully managed build service.</p><p>CodeBuild compiles source code, runs unit tests, and produces artifacts that are ready to deploy. Encryption for build artifacts such as a cache, logs, exported raw test report data files, and build results, is enabled by default, and uses AWS managed keys (AWS KMS).</p><p><strong>CORRECT: </strong>\"Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes\" is incorrect.</p><p>This solution may be secure, but it does require more operational management compared to using AWS CodeBuild and is therefore not the best option.</p><p><strong>INCORRECT:</strong> \"Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group\" is incorrect.</p><p>Moving Jenkins to an Auto Scaling group does not reduce the operational management. This is also an option that requires more overhead to manage compared to using a managed service such as AWS CodeBuild.</p><p><strong>INCORRECT:</strong> \"Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM)\" is incorrect.</p><p>Artifacts cannot be encrypted using ACM as that is a service that issues SSL/TLS certificates which are used for encryption in-transit rather than encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 67357100,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.</p>\n\n<p>As a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?</p>\n",
          "answers": [
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>AWS::EC2::Volume</code> that returns a compliance failure if the custom tag is not applied on the EBS volume. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong></p>\n\n<p>Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type.</p>\n\n<p>AWS Config managed rule that checks required-tags:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p>This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes.</p>\n\n<p>AWS Config rules remediation with automation runbooks architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it's possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n"
        }
      },
      {
        "id": 75949146,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
          "answers": [
            "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
            "<p>The new AZ has not been enabled for the ALB.</p>",
            "<p>The AMI has not been updated to include the new AZ.</p>",
            "<p>There are no subnets associated with the new AZ.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 99528239,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company runs a Java application in an on-premises data center. The application is not currently containerized but the company plans to migrate it to the AWS Cloud and modernize the application into a containerized deployment. A CI/CD pipeline should also be created to automate application updates.</p><p>Which solution can a DevOps engineer use to meet all these requirements?</p>",
          "answers": [
            "<p>Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates.</p>",
            "<p>Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy.</p>",
            "<p>Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository.</p>",
            "<p>Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy.</p>"
          ],
          "explanation": "<p>App2Container (A2C) is a command-line tool for modernizing .NET and Java applications into containerized applications. A2C analyzes and builds an inventory of all applications running in virtual machines on-premises or in the cloud.</p><p>With A2C you can simply select the application you want to containerize, and A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the ECS task and Kubernetes pod definitions.</p><p>App2Container creates CI/CD pipelines for Amazon DevOps services such as CodeBuild and CodeDeploy to build and deploy containers. If you have existing CI/CD tooling (for example, Azure DevOps and Jenkins), then you can integrate A2C provided artifacts \u2013 dockerfile, ECS task definition, EKS deployment YAML - into your existing CI/CD workflows.</p><p><strong>CORRECT: </strong>\"Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates\" is incorrect.</p><p>AWS Copilot is a command line interface (CLI) that you can use to quickly launch and manage containerized applications on AWS. It simplifies running applications on Amazon Elastic Container Service (ECS), AWS Fargate, and AWS App Runner. It is not able to inventory and analyze existing applications or migrate them to containers.</p><p><strong>INCORRECT:</strong> \"Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository\" is incorrect.</p><p>AWS Proton is used to create application stack templates and build infrastructure for applications using the CLI, API, or UI. It is not used for refactoring applications or migrating them to containerized deployments.</p><p><strong>INCORRECT:</strong> \"Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy\" is incorrect.</p><p>Amazon AppFlow is used to automate bi-directional data flows between SaaS applications and AWS services. It is not used to migrate applications between on-premises deployments and Amazon ECS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/app2container/features/\">https://aws.amazon.com/app2container/features/</a></p>"
        }
      },
      {
        "id": 75949108,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>",
          "answers": [
            "<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"
          ],
          "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949138,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
            "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
            "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
            "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
            "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
            "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"
          ],
          "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949100,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company is deploying a new serverless application that uses AWS Lambda functions. A DevOps engineer must create a continuous deployment pipeline for the application. The deployment preferences must be configured to minimize the impact of failed deployments.</p><p>Which deployment configuration will meet these requirements?</p>",
          "answers": [
            "<p>Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type.</p>",
            "<p>Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch.</p>",
            "<p>Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version.</p>",
            "<p>Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version.</p>"
          ],
          "explanation": "<p>The benefits of using AWS SAM to create the serverless application include that it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM can perform the following actions:</p><ul><li><p>Deploys new versions of the Lambda function, and automatically creates aliases that point to the new version.</p></li><li><p>Gradually shifts customer traffic to the new version until you\u2019re satisfied that it's working as expected, or you roll back the update.</p></li><li><p>Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly, and your application operates as expected.</p></li><li><p>Rolls back the deployment if CloudWatch alarms are triggered.</p></li></ul><p>The DevOps engineer can choose the Deployment Preference Type. The following options are available:</p><ul><li><p><strong>Canary:</strong> Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p></li><li><p><strong>Linear:</strong> Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment.</p></li><li><p><strong>All-at-once:</strong> All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</p></li></ul><p>The best option to minimize the impact of failed deployments is to use the canary deployment type. This will ensure that only a small amount of traffic reaches the new Lambda function in the first shift and if any issues occur the deployment can be stopped.</p><p><strong>CORRECT: </strong>\"Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch\" is incorrect.</p><p>The all-at-once deployment preference type would shift all traffic across to the new functions which would increase the impact of failed deployments. Also, CloudFormation is not a continuous deployment tool and is not a suitable substitute for CodePipeline and CodeDeploy.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version\" is incorrect.</p><p>As above, CloudFormation is not designed for continuous delivery and should not be used in this scenario. Creating a CodePipeline that leverages CodeDeploy and a source control service such as CodeCommit would be a better solution.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version\" is incorrect.</p><p>This option requires more operational overhead and testing on behalf of the DevOps team. The better solution is an automated pipeline that shifts some traffic across to the new functions so actual end users are attempting to use the application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949142,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.</p><p>How can the company meet these requirements with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns.</p>",
            "<p>Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket.</p>"
          ],
          "explanation": "<p>The simplest solution for this request is to enable server access logging on the bucket and then analyze the data that is logged using Amazon Athena. Server access logging provides detailed records for the requests that are made to a bucket. This includes the information requested by the company.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p><strong>CORRECT: </strong>\"Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket\" is incorrect $</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/\">https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>"
        }
      },
      {
        "id": 75949172,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A service provider has created business relationships with several companies. The service provider plans to deploy an application to multiple AWS accounts managed by these partner companies using AWS CloudFormation. Each partner company has granted the permissions to create IAM roles with permissions for the deployment in their respective accounts. The organization must minimize operational overhead and stack management.</p><p>Which actions should be taken to deploy the application across these accounts?</p>",
          "answers": [
            "<p>Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template.</p>",
            "<p>Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application.</p>"
          ],
          "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf</p><p>In this case the best solution is to use self-managed permissions as the partner companies have only granted the ability to create IAM roles. They would certainly not want their entire account to be controlled by the service provider through AWS Organizations just so they can deploy a single application.</p><p><strong>CORRECT: </strong>\"Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>You cannot have a role in one account that has permissions in another account. The roles must be created in each AWS account and self-managed permissions must be used with AWS CloudFormation stack sets.</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>The partner companies would not want the service provider to control their accounts with AWS Organizations just for a single application. They have granted the ability to create roles which is enough for the service provider to deploy the application using stack sets with self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template\" is incorrect.</p><p>The partner companies have granted the ability to create roles, not user accounts. Also, this would be inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 75949092,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is developing an application that calls AWS Lambda functions. The functions must connect to a database and credentials must not be stored in source code. The credentials for connection to the database must be regularly rotated to meet security policy requirements.</p><p>What should a DevOps engineer do to meet these requirements?</p>",
          "answers": [
            "<p>Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID.</p>",
            "<p>Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID.</p>",
            "<p>Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution.</p>",
            "<p>Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID.</p>"
          ],
          "explanation": "<p>AWS Secrets Manager is ideal for this scenario as it can be used to securely store the secrets. Automatic rotation can be enabled for several AWS databases and can be configured through custom Lambda functions for other databases.</p><p><strong>CORRECT: </strong>\"Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID\" is incorrect.</p><p>CloudHSM is used for storing encryption keys, not connection credentials.</p><p><strong>INCORRECT:</strong> \"Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution\" is incorrect.</p><p>This is not a secure method of storing the credentials and Secrets Manager is more secure.</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID\" is incorrect.</p><p>KMS is used for storing encryption keys, not connection credentials.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>"
        }
      },
      {
        "id": 115961513,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>",
          "answers": [
            "<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>",
            "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>",
            "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>",
            "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"
          ],
          "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 99528229,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.</p><p>Which actions should the engineer take?</p>",
          "answers": [
            "<p>Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL.</p>",
            "<p>Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield.</p>",
            "<p>Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL.</p>",
            "<p>Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty.</p>"
          ],
          "explanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website.</p><p>AWS Web Application Firewall (WAF) can be used to protect the static website from web exploits. To do this, the engineer should create an AWS WAF web ACL.</p><p><strong>CORRECT: </strong>\"Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield\" is incorrect.</p><p>Both EC2 and ElastiCache use EC2 instances so they are not serverless services. AWS Shield protects against DDoS attacks but AWS WAF should be used for web exploits.</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL\" is incorrect.</p><p>ECS with Fargate is serverless but you cannot use a Redis cluster to cache content served by a task running on Amazon ECS. ElastiCache is used for caching database content.</p><p><strong>INCORRECT:</strong> \"Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty\" is incorrect.</p><p>Lambda with a REST API as the frontend is definitely possible, however the static website on S3 would be easier to deploy and manage. AWS GuardDuty does not protect against web exploits.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>"
        }
      },
      {
        "id": 75949128,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>The DevOps team at an e-commerce company introduced multiple stages of security to the code release process. As an additional measure, they want to add additional SAST &amp; DAST tools into an automated pipeline. These tools should be invoked for every code push in an AWS CodeCommit repository. The code must be sent via an external API.</p><p>Which actions should a DevOps engineer take to achieve these requirements MOST efficiently?</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API.</p>",
            "<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected.</p>",
            "<p>Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API.</p>",
            "<p>Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API.</p>"
          ],
          "explanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that can be set up, you can match events and route them to one or more target functions or streams. Custom application-level events can be generated and published to CloudWatch Events. Scheduled events can be set up that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event Rules cannot have S3 buckets as a target. It is more efficient to directly invoke the Lambda function from the CloudWatch Event rule than use S3 as a temporary store and then trigger a function execution.</p><p><strong>INCORRECT:</strong> \"Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected\" is incorrect.</p><p>You cannot create a CodeCommit hook on an EC2 instance, so this is not possible</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So, this option is ruled out.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/\">https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949174,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
          "answers": [
            "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
            "<p>Add test stages before and after the deploy action.</p>",
            "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
            "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"
          ],
          "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528205,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team has prohibited unauthenticated requests to S3 buckets for this project.</p><p>How can this issue be resolved in the <strong>MOST secure</strong> manner?</p>",
          "answers": [
            "<p>Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.</p>",
            "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</p>"
          ],
          "explanation": "<p>Removing unauthenticated access from the bucket through a bucket policy ensures that the S3 bucket is protected from unauthorized access. Granting the CodeBuild project's service role explicit permissions to access the S3 bucket aligns with the principle of least privilege. Using the AWS CLI within the build spec ensures secure and seamless access to the bucket.<br><br></p><p><strong>CORRECT: </strong>\"Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script\" is incorrect.</p><p>Adding the bucket name to the AllowedBuckets section does not address the security issue. The CodeBuild project needs proper IAM permissions to access the S3 bucket securely. Relying on unauthenticated access remains insecure and against the security team's requirements.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script\" is incorrect.</p><p>Enabling HTTPS basic authentication for the S3 bucket is not supported by AWS. Amazon S3 uses IAM policies and bucket policies for access control, not basic authentication. Additionally, using cURL to pass tokens does not align with AWS's recommended best practices.</p><p><strong>INCORRECT:</strong> \"Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key\" is incorrect.</p><p>Using IAM access keys and secret access keys directly is less secure than granting a CodeBuild service role the necessary permissions. Hardcoding access keys in the build spec or environment can lead to security vulnerabilities, such as accidental exposure of credentials.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html</a></p>"
        }
      },
      {
        "id": 75949134,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An automotive organization is planning to migrate their website into AWS across multiple accounts. The current infrastructure uses an on-premises Microsoft IIS web server and Microsoft SQL server for the data persistence layer.</p><p>They want to be able to scale their infrastructure based on demand. Along with the current website, they also want to collect user interest data from ad clicks that occur on the website. Amazon RedShift has been chosen for the consumption and aggregation of data.</p><p>Which of the below architectures best suits their needs?</p>",
          "answers": [
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.</p>"
          ],
          "explanation": "<p>Amazon Kinesis Data Firehose is used to reliably load streaming data into data lakes, data stores, and analytics tools like Amazon Redshift. Process the incoming data from Firehose with Kinesis Data Analytics to provide real-time dashboarding of website activity.</p><p><strong>CORRECT: </strong>\"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>To load data into S3, Kinesis Firehose is a more suitable tool.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>As per the option above, Kinesis Data Firehose is a better service for this use case.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>Athena is more suited to data analysis within S3 buckets and cannot be used for loading data into S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/\">https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>"
        }
      },
      {
        "id": 75949074,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A retail company has hired a DevOps engineer to provide consultancy services. The company runs Oracle and PostgreSQL services on Amazon RDS for storing large quantities of data generated by manufacturing equipment.</p><p>The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team.</p><p>To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p><p>Which solution should the DevOps engineer recommend to achieve these requirements?</p>",
          "answers": [
            "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift.</p>"
          ],
          "explanation": "<p>The AWS Database Migration Service (DMS) helps migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS DMS the data can be continuously replicated with high availability and the multiple databases can be consolidated into a petabyte-scale data warehouse.</p><p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance. During a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-02-48-34cbd839e896555130df72dfa20efc35.jpg\"><p><strong>CORRECT: </strong>\u201cUse AWS Database Migration Service to replicate the data from the databases into Amazon Redshift\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>AWS Glue is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.</p><p><strong>INCORRECT:</strong> \"Use Amazon EMR to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon EMR (previously called Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon Kinesis Data Streams is a fully managed, serverless data streaming service that stores and ingests various streaming data in real time at any scale. It is not suitable for migrating data between databases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>"
        }
      },
      {
        "id": 99528223,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A data intelligence and analytics company has implemented a CI/CD pipeline using AWS CodePipeline which takes code from an AWS CodeCommit repository and then builds it using AWS CodeBuild. During the deploy stage, the application is deployed onto an Amazon ECS cluster. During deployment, the application is only partly updated on some ECS tasks which are running an older version of the image.</p><p>A DevOps engineer investigated and found that terminating the task or clearing the local Docker cache fixes the issue, but a more robust solution is required that provides visibility and identification to track where container images are deployed. Also, the start-up time of the containers needs to be optimized.</p><p>Which actions should the DevOps engineer take to achieve these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Move all the dependencies into a single image and pull them from a single container registry.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task.</p>",
            "<p>Move secondary dependencies to be downloaded at application startup rather than including them within a static container image.</p>"
          ],
          "explanation": "<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. The IMAGEID property is the SHA digest for the Docker image used to start the container.</p><p>Ideally, a container image is intended to be a complete snapshot of everything that the application requires to function. With a complete container image, the application could be run by downloading one container image from one place.</p><p>There is no need to download several separate pieces from different locations. Therefore, as a best practice, store all application dependencies as static files inside the container image. This will improve performance and start up time.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-00-17-d1b2e063cff66b89f7caee81435b0971.jpg\"><p><strong>CORRECT: </strong>\"Move all the dependencies into a single image and pull them from a single container registry\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time\" is incorrect.</p><p>The exact image tag needs to be pulled because the latest tag will only pull the image that was pushed last.</p><p><strong>INCORRECT:</strong> \"After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task\" is incorrect.</p><p>Using Lambda to SSH onto instances is not a workable solution. SSM Run Command may be better but it\u2019s also not ideal.</p><p><strong>INCORRECT:</strong> \"Move secondary dependencies to be downloaded at application startup rather than including them within a static container image\" is incorrect.</p><p>This would delay the image spin up time rather than improving it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html\">https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528211,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. It should not be possible to create IAM users unless the user account making the requires is specific on an exception list. A DevOps engineer must apply these restrictions.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>",
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>"
          ],
          "explanation": "<p>With AWS Organizations Service Control Policies (SCPs) you can restrict access to specific resources or define conditions for when SCPs are in effect. The condition element is supported when the Effect element has a value of Deny.</p><p>The StringNotLike condition will check the exception list for the aws:username that made the request. If the username is not present the request will be denied.</p><p><strong>CORRECT: </strong>\"Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list\" is incorrect.</p><p>The API action iam:CreateLoginProfile creates a password for the specified IAM user. This is not used to create the user account.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The API action iam:GetUser retrieves information about the specified IAM user, including the user's creation date, path, unique ID, and ARN.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The function should be used to validate that the iam:CreateUser request is being initiated by a user on the exception list. It should not delete the requesting user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>"
        }
      },
      {
        "id": 75949048,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company has several AWS accounts and an on-premises data center. Several microservices applications run across the accounts and data center. The distributed architecture results in challenges with investigating application issues as the logs are saved in a variety of locations. A DevOps engineer must configure a solution that centralizes and aggregates the logs for analytics.</p><p>What is the MOST efficient and cost-effective solution?</p>",
          "answers": [
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account.</p>"
          ],
          "explanation": "<p>The most efficient and cost-effective solution is to use the CloudWatch agent to collect log files from both AWS resources and on-premises servers and save that data to a centralized Amazon S3 bucket. S3 event notifications can be used to trigger an AWS Lambda function that analyzes the data looking for anomalies.</p><p>Amazon Athena is ideal for running ad-hoc SQL queries on data stored in S3. This can be used by the company when they have specific queries they need to run against the data.</p><p><strong>CORRECT: </strong>\"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center\" is incorrect.</p><p>The best solution is not to store the data on-premises, it should be in the AWS Cloud. You would then be able to use the ELK stack to analyze the data.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket\" is incorrect.</p><p>You cannot use S3 APIs to export log files from on-premises servers. You also cannot use KDA to analyze data in S3, it is used for analyzing data in other Kinesis services.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account\" is incorrect.</p><p>As above, you cannot use the S3 API with on-premises resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p><p><a href=\"https://aws.amazon.com/solutions/implementations/centralized-logging/\">https://aws.amazon.com/solutions/implementations/centralized-logging/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      }
    ],
    "answers": {
      "67357100": [
        "d"
      ],
      "75949046": [
        "b"
      ],
      "75949048": [
        "d"
      ],
      "75949074": [
        "c"
      ],
      "75949092": [
        "b"
      ],
      "75949100": [
        "a"
      ],
      "75949108": [
        "a"
      ],
      "75949128": [
        "c"
      ],
      "75949134": [
        "d"
      ],
      "75949138": [
        "b",
        "c",
        "d"
      ],
      "75949142": [
        "c"
      ],
      "75949146": [
        "a"
      ],
      "75949172": [
        "b"
      ],
      "75949174": [
        "b"
      ],
      "99528205": [
        "c"
      ],
      "99528211": [
        "c"
      ],
      "99528223": [
        "b",
        "d"
      ],
      "99528229": [
        "a"
      ],
      "99528239": [
        "b"
      ],
      "115961513": [
        "b"
      ]
    }
  },
  {
    "id": "1769615736451",
    "date": "2026-01-28T15:55:36.451Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 0,
    "incorrect": 0,
    "unanswered": 10,
    "total": 10,
    "percent": 0,
    "duration": 3481,
    "questions": [
      {
        "id": 82921316,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>You are working as a DevOps Engineer at an e-commerce company and have a deployed a Node.js application on Elastic Beanstalk. You would like to track error rates and specifically, you need to ensure by looking at the application log, that you do not have more than 100 errors in a 5 minutes interval. In case you are getting too many errors, you would like to be alerted via email.</p>\n\n<p>Which of the following options represents the most efficient solution in your opinion?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</strong></p>\n\n<p>You can search and filter the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created.</p>\n\n<p>You can use metric filters to search for and match terms, phrases, or values in your log events. When a metric filter finds one of the terms, phrases, or values in your log events, you can increment the value of a CloudWatch metric. For example, you can create a metric filter to search for and count the occurrence of the word ERROR in your log events.</p>\n\n<p>CloudWatch Logs Metric Filter Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p>For the given use-case, you can have Beanstalk send the logs to CloudWatch Logs, and then create a metric filter. This will create a metric for us (and not an alarm), and on top of the metric, you can create a CloudWatch Alarm. This alarm will send a notification to SNS, which will, in turn, send us emails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - You cannot directly set a CloudWatch Alarm as a target for a CloudWatch Logs Metric Filter. You will first need to create a metric filter which can then be used to create a CloudWatch metric to be eventually used in a CloudWatch Alarm.</p>\n\n<p><strong>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</strong> - The Elastic Beanstalk Health Metrics will not track the errors sent out to a log file, so it does not meet the requirements of the use-case. Besides, CloudWatch alarm cannot be used to work on top of the Elastic Beanstalk Health Metrics.</p>\n\n<p><strong>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - Implementing custom logic in your Node.js application may seem like a good idea, but then you have to remember that your application can be distributed amongst many servers with Beanstalk, and as such it will not be possible to track the \"100 errors\" across all instances using this methodology.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p>\n"
        }
      },
      {
        "id": 82921402,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An IT company is creating an online booking system for hotels. The booking workflow that the company has implemented can take over 3 hours to complete as a manual verification step is required by a 3rd party provider to ensure big transactions are not fraudulent.</p>\n\n<p>As a DevOps Engineer, you need to expose this as a secure API for the end customers. The website must be able to sustain 5000 requests at the same time. How should you implement this in the simplest possible way?</p>\n",
          "answers": [
            "<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</strong></p>\n\n<p>API Gateway APIs can directly invoke an AWS service and pass in a payload. It's a common way to provide a publicly available and secure API for your chosen AWS services.</p>\n\n<p>Amazon API Gateway integrates with AWS Step Functions, allowing you to call Step Functions with APIs that you create to simplify and customize interfaces to your applications. Step Functions makes it easy to coordinate the components of distributed applications and microservices as a series of steps in a visual workflow. You create state machines in the Step Functions Console or through the Step Functions API to specify and execute the steps of your application at scale. API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to implement the payment workflow using Step Functions. A key reason you need this integration is that AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</strong> - AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you. So there is no need to use Lambda as an intermediary for this workflow.</p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</strong></p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</strong></p>\n\n<p>Lambda functions cannot process the booking workflow as it may take 3 hours, which is more than the 15 minutes max timeout limit that Lambda supports. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html\">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/faqs/\">https://aws.amazon.com/step-functions/faqs/</a></p>\n"
        }
      },
      {
        "id": 138248163,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A leading commercial bank has its online banking application hosted in AWS. It uses an encrypted Amazon S3 bucket to store the confidential files of its customers. The DevOps team has configured federated access to a particular Active Directory user group from the bank's on-premises network to allow access to the S3 bucket. For audit purposes, there is a new requirement to automatically detect any policy changes that are related to the restricted federated access of the bucket and to have the ability to revert any accidental changes made by the administrators.</p><p>Which of the following options provides the FASTEST way to detect configuration changes?</p>",
          "answers": [
            "<p>Using Amazon EventBridge, integrate an Event Bus with AWS CloudTrail API in order to trigger an AWS Lambda function that will detect and revert any particular changes.</p>",
            "<p>Set up an AWS Config rule with a configuration change trigger that will detect any changes in the S3 bucket configuration and which will also invoke an AWS Systems Manager Automation document with an AWS Lambda function that will revert any changes.</p>",
            "<p>Integrate Amazon EventBridge and an AWS Lambda function to create a scheduled job that runs every hour to scan the IAM policy attached to the federated access role. Configure the function to detect as well as revert any recent changes made in the current configuration.</p>",
            "<p>Set up an AWS Config rule with a periodic trigger that runs every hour which will detect any changes in the S3 bucket configuration. Associate an AWS Lambda function in the rule that will revert any recent changes made in the bucket.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.</p><p>When you add a rule to your account, you can specify when you want AWS Config to run the rule; this is called a <em>trigger</em>. AWS Config evaluates your resource configurations against the rule when the trigger occurs.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-11-22_22-41-48-8712a29506a6524f89ee82708c97854b.png\"></p><p>There are two types of triggers:</p><ol><li><p>Configuration changes</p></li><li><p>Periodic</p></li></ol><p>If you choose both <em>configuration changes</em> and <em>periodic </em>triggers, AWS Config invokes your Lambda function when it detects a configuration change and also at the frequency that you specify.</p><p><strong>Configuration changes</strong></p><p>AWS Config runs evaluations for the rule when certain types of resources are created, changed, or deleted. You choose which resources trigger the evaluation by defining the rule's <em>scope</em>. The scope can include the following:</p><p>- One or more resource types</p><p>- A combination of a resource type and a resource ID</p><p>- A combination of a tag key and value</p><p>- When any recorded resource is created, updated, or deleted</p><p>AWS Config runs the evaluation when it detects a change to a resource that matches the rule's scope. You can use the scope to constrain which resources trigger evaluations. Otherwise, evaluations are triggered when any recorded resource changes.</p><p><strong>Periodic</strong></p><p>AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p><p>Hence, the correct answer is: <strong>Set up an AWS Config rule with a configuration change trigger that will detect any changes in the S3 bucket configuration and which will also invoke an AWS Systems Manager Automation document with an AWS Lambda function that will revert any changes.</strong></p><p>The option that says: <strong>Using Amazon EventBridge, integrate an Event Bus with AWS CloudTrail API in order to trigger an AWS Lambda function that will detect and revert any particular changes</strong> is incorrect. Although you can track all changes to your configuration using CloudTrail API, it would be difficult to integrate it with EventBridge in order to monitor the changes. There is no direct way of integrating these two services and you have to create a custom mapping in order for this to work.</p><p>The option that says: <strong>Integrate Amazon EventBridge and an AWS Lambda function to create a scheduled job that runs every hour to scan the IAM policy attached to the federated access role. Configure the function to detect as well as revert any recent changes made in the current configuration</strong> is incorrect. Although this solution may work, there would be a significant delay since the Lambda function is only run every one hour. So if the new S3 bucket configuration was applied at 12:05 PM, the change will only be detected at 1:00 PM. Moreover, this entails a lot of overhead since you have to develop a custom function that will scan your IAM policies.</p><p>The option that says: <strong>Set up an AWS Config rule with a periodic trigger that runs every hour which will detect any changes in the S3 bucket configuration. Associate an AWS Lambda function in the rule that will revert any recent changes made in the bucket<em> </em></strong>is incorrect. Although this may work, it is not the fastest way of detecting a change in your resource configurations in AWS. Since the rule is using a periodic trigger, the rule will run every hour and not in near real-time, unlike the <strong><em>Configuration changes</em></strong> trigger. So, say a new configuration was applied at 12:01 PM, the change will only be detected at 1:00 PM after the rule has been run.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_getting-started.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_getting-started.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 82921328,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A global financial services company manages over 100 accounts using AWS Organizations and it has recently come to light that several accounts and regions did not have AWS CloudTrail enabled. It also wants to be able to track the compliance of the CloudTrail enablement as a dashboard, and automatically be alerted in case of issues. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How would you go about implementing a solution for this use-case?</p>\n",
          "answers": [
            "<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create another CloudFormation StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</p>",
            "<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</p>",
            "<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SNS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</p>",
            "<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy it in all your accounts and regions under the AWS organization. Create another StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SQS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create another CloudFormation StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</strong></p>\n\n<p>CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p>\n\n<p>Multiple accounts and multiple regions.</p>\n\n<p>Single account and multiple regions.</p>\n\n<p>An organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/config/latest/developerguide/images/Aggregate_Data_Landing_Page_Diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n\n<p>For the given use-case, we need to enable CloudTrail and AWS Config in all accounts and all regions. For this, we'll need separate StackSets to create CloudTrail and enable Config in all accounts and all regions. Note that we'll also need an AWS Config aggregator in a centralized account. Finally, compliance breaches would generate CloudWatch events that can be subscribed by a Lambda function to further send out notifications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</strong> - The issue with this option is that CloudFormation template is being used only in a centralized account to enable AWS Config, whereas the correct solution must leverage a StackSet to enable Config in all accounts and all regions.</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SNS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</strong> - The issue with this option is that CloudFormation template is being used only in a centralized account to enable AWS Config, whereas the correct solution must leverage a StackSet to enable Config in all accounts and all regions.</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy it in all your accounts and regions under the AWS organization. Create another StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SQS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</strong> - This option has been added as a distractor. There is no such thing as an SQS topic.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n"
        }
      },
      {
        "id": 82921412,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>An IT company is deploying a Python Flask based application and would like to ensure that it has a base AMI that contains the necessary Python runtime, as well as OS patches. That AMI must be used able to be referenced programmatically from across all regions in your account in a scalable way. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>Which of the following options would you recommend for this use-case? (Select two)</p>\n",
          "answers": [
            "<p>Create an SSM Automation document to create the AMI in a repeatable manner</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Use AWS Inspector to create a patched AMI using the latest working AMI</p>",
            "<p>Use AWS Lambda to create a patched AMI using the latest working AMI</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an SSM Automation document to create the AMI in a repeatable manner</strong></p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>You can use AWS Systems Manager to create a maintenance window, and then register an Automation task to automate the creation of the AMIs. This process is applicable for both Windows and Linux instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q46-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</strong></p>\n\n<p>The AMI ID is region-scoped, so the AMI must be copied across regions and therefore each SSM parameter store will have different AMI ID values. But you can still use the same SSM Parameter Store key across all regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</strong> - The AMI ID is region-scoped and the AMI must be copied across regions for the solution to work. This option only copies the value of the AMI ID across regions but the AMI itself stays in one region. So this option is incorrect.</p>\n\n<p><strong>Use AWS Inspector to create a patched AMI using the latest working AMI</strong> - AWS Inspector can be leveraged to analyze EC2 instance OS and network vulnerabilities. You cannot use Inspector to create a patched AMI.</p>\n\n<p><strong>Use AWS Lambda to create a patched AMI using the latest working AMI</strong> - AWS Lambda cannot be used to create AMIs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n"
        }
      },
      {
        "id": 82921330,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As the Lead DevOps Engineer at an e-commerce company, you would like to upgrade the major version of your MySQL database, which is managed by CloudFormation with <code>AWS::RDS::DBInstance</code> and setup using Multi-AZ.</p>\n\n<p>You have a requirement to minimize the downtime as much as possible, what steps should you take to achieve this?</p>\n",
          "answers": [
            "<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>",
            "<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>",
            "<p>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</p>",
            "<p>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong></p>\n\n<p>You can minimize downtime on an upgrade by using a rolling upgrade using read replicas. Amazon RDS doesn\u2019t fully automate one-click rolling upgrades. However, you can still perform a rolling upgrade by creating a read replica, upgrading the replica by using the property <code>EngineVersion</code>, promoting the replica, and then routing traffic to the promoted replica. If you want to create a Read Replica DB instance, specify the ID of the source DB instance. The SourceDBInstanceIdentifier property determines whether a DB instance is a Read Replica.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p>You should also note that a Multi-AZ configuration does not prevent downtime during an upgrade. Multi-AZ is only recommended for a high availability use-case. However, in the case of a MySQL or MariaDB engine upgrade, Multi-AZ doesn\u2019t eliminate downtime. The slow shutdown and the physical changes made on the active server by the mysql_upgrade program require this downtime.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong> - You should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p><strong>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - If you update the <code>EngineVersion</code> property of an AWS::RDS::DBInstance resource type, AWS CloudFormation creates a new resource and replaces the current DB instance resource with the new one, so this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n\n<p><strong>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - Also, you should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n"
        }
      },
      {
        "id": 82921326,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The technology team at a health-care solutions company has developed a REST API which is deployed in an Auto Scaling Group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. Upon doing some analytics, it's found that 85% of the read requests are shared across all users.</p>\n\n<p>As a DevOps Engineer, how can you improve the application performance while decreasing the cost?</p>\n",
          "answers": [
            "<p>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</p>",
            "<p>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</p>",
            "<p>Enable DAX for DynamoDB and ElastiCache Memcached for S3</p>",
            "<p>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</strong></p>\n\n<p>DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement\u2014from milliseconds to microseconds\u2014even at millions of requests per second.</p>\n\n<p>DAX is tightly integrated with DynamoDB\u2014you simply provision a DAX cluster, use the DAX client SDK to point your existing DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache DynamoDB reads.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they\u2019ve requested isn\u2019t yet cached, CloudFront retrieves it from your origin \u2013 for example, the S3 bucket where you\u2019ve stored your content.</p>\n\n<p>So, you can use CloudFront to improve application performance to serve static content from S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p>Although, you can integrate Redis with DynamoDB, it's much more involved than using DAX which is a much better fit.</p>\n\n<p><strong>Enable DAX for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.</p>\n\n<p>ElastiCache cannot be used as a cache to serve static content from S3, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n"
        }
      },
      {
        "id": 82921342,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a social media company, you have deployed your application in an Auto Scaling group (ASG) using CloudFormation. You would like to update the Auto Scaling Group to have all the instances reference the newly created launch configuration, which upgrades the instance type. Your ASG currently contains 6 instances and you need at least 4 instances to be up at all times.</p>\n\n<p>Which configuration should you use in the CloudFormation template?</p>\n",
          "answers": [
            "<p>AutoScalingRollingUpdate</p>",
            "<p>AutoScalingReplacingUpdate</p>",
            "<p>AutoScalingLaunchTemplateUpdate</p>",
            "<p>AutoScalingLaunchConfigurationUpdate</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>AutoScalingRollingUpdate</strong></p>\n\n<p>To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the AutoScalingRollingUpdate policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. AutoScalingRollingUpdate is perfect for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q72-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AutoScalingReplacingUpdate</strong> - To specify how AWS CloudFormation handles replacement updates for an Auto Scaling group, you should use the AutoScalingReplacingUpdate policy. This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group. This option will create a new ASG entirely, so this is ruled out.</p>\n\n<p><strong>AutoScalingLaunchTemplateUpdate</strong></p>\n\n<p><strong>AutoScalingLaunchConfigurationUpdate</strong></p>\n\n<p>AutoScalingLaunchTemplateUpdate and AutoScalingLaunchConfigurationUpdate do not exist, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n"
        }
      },
      {
        "id": 82921428,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An e-commerce company would like to automate the patching of their hybrid fleet and distribute some patches through their internal patch repositories every week. As a DevOps Engineer at the company, you have been tasked to implement this most efficiently.</p>\n\n<p>Which of the following options represents the BEST solution to meet this requirement?</p>\n",
          "answers": [
            "<p>Using SSM, implement a Custom Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>",
            "<p>Using SSM Parameter Store, configure the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>",
            "<p>Manage your instances with AWS OpsWorks. Define a maintenance window and define custom chef cookbooks for the 'configure' lifecycle hook that will patch the instances from the internal patch repositories. Schedule the window with a weekly recurrence</p>",
            "<p>Using SSM, do a RunCommand to install the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Using SSM, implement a Custom Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong></p>\n\n<p>SSM Patch Manager automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p>Patch Manager provides predefined patch baselines for each of the operating systems supported by Patch Manager. You can use these baselines as they are currently configured (you can't customize them) or you can create your own custom patch baselines. Custom patch baselines allow you greater control over which patches are approved or rejected for your environment.</p>\n\n<p>When you use the default repositories configured on an instance for patching operations, Patch Manager scans for or installs security-related patches. This is the default behavior for Patch Manager.</p>\n\n<p>On Linux systems, however, you can also use Patch Manager to install patches that are not related to security, or that are in a different source repository than the default one configured on the instance. You can specify alternative patch source repositories when you create a custom patch baseline. In each custom patch baseline, you can specify patch source configurations for up to 20 versions of a supported Linux operating system. You can then set up a weekly maintenance window and include the Run Command RunPatchBaseline.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q45-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Using SSM Parameter Store, configure the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong> - SSM Parameter Store is used to store parameter values but cannot write configuration files on EC2 instances (the EC2 instances would have to fetch the value from the Parameter Store instead).</p>\n\n<p><strong>Manage your instances with AWS OpsWorks. Define a maintenance window and define custom chef cookbooks for the 'configure' lifecycle hook that will patch the instances from the internal patch repositories. Schedule the window with a weekly recurrence</strong> - Using chef cookbooks via OpsWorks may work for what we need, but the Patch Manager of SSM is a better way of achieving this.</p>\n\n<p><strong>Using SSM, do a RunCommand to install the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong> - Using SSM RunCommand may work for what we need, but the Patch Manager of SSM is a better way of achieving this.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-alt-source-repository.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-alt-source-repository.html</a></p>\n"
        }
      },
      {
        "id": 82921378,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>An Internet-of-Things (IoT) solutions company has decided to release every single application as a Docker container and to use ECS classic (on EC2) as the container orchestration system and ECR as the Docker registry. Part of implementing a monitoring pipeline is to ensure all application logs can be stored in CloudWatch logs.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to provide the simplest possible instructions to accomplish this objective. What are these instructions?</p>\n",
          "answers": [
            "<p>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</p>",
            "<p>Create ECS task definitions for your applications, with a mapping of the <code>/var/log</code> directory onto the local filesystem of the EC2 instance. Install the CloudWatch Agent on the EC2 instance using user-data and track the <code>/var/log/containers</code> directory. Create an EC2 instance role with the necessary permissions to write to CloudWatch logs</p>",
            "<p>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM instance role on the EC2 instance with the necessary permissions to write to CloudWatch logs</p>",
            "<p>Create ECS task definitions for your applications, with a sidecar container which contains the CloudWatch Agent tracking the <code>/var/log/containers</code> directory. Map the application's <code>/var/log</code> directory onto the sidecar filesystem. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM instance role on the EC2 instance with the necessary permissions to write to CloudWatch logs</strong></p>\n\n<p>Here many solutions may work but we're looking for the simplest possible solution. The important thing to remember is that the ECS task definitions can include the <code>awslogs</code> driver and write to CloudWatch Logs natively. But the EC2 instance will be the one writing to CloudWatch, and therefore it must have an EC2 Instance Role with the appropriate permissions to write to CloudWatch. Your Amazon ECS container instances also require logs:CreateLogStream and logs:PutLogEvents permission on the IAM role with which you launch your container instances</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q33-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</strong> - As mentioned in the explanation above, you need to provide the appropriate permissions to the EC2 Instance Role and not to the IAM task role to write to CloudWatch logs.</p>\n\n<p><strong>Create ECS task definitions for your applications, with a mapping of the <code>/var/log</code> directory onto the local filesystem of the EC2 instance. Install the CloudWatch Agent on the EC2 instance using user-data and track the <code>/var/log/containers</code> directory. Create an EC2 instance role with the necessary permissions to write to CloudWatch logs</strong> - This is a roundabout way of getting the container logs to the CloudWatch Logs, so not the best fit for the given use-case.</p>\n\n<p><strong>Create ECS task definitions for your applications, with a sidecar container which contains the CloudWatch Agent tracking the <code>/var/log/containers</code> directory. Map the application's <code>/var/log</code> directory onto the sidecar filesystem. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</strong> - Sidecar containers are a common software pattern that has been embraced by engineering organizations. It\u2019s a way to keep server-side architecture easier to understand by building with smaller, modular containers that each serve a simple purpose. Just like an application can be powered by multiple microservices, each microservice can also be powered by multiple containers that work together. A sidecar container is simply a way to move part of the core responsibility of a service out into a containerized module that is deployed alongside a core application container. This again seems to be a roundabout way of getting the container logs to the CloudWatch Logs, but it's not correct for the given use-case. You should note that you need to provide the appropriate permissions to the EC2 Instance Role and not to the IAM task role to write to CloudWatch logs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html</a></p>\n"
        }
      }
    ],
    "answers": {}
  }
]