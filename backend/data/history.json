[
  {
    "id": "1770588364629",
    "date": "2026-02-08T22:06:04.629Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 30,
    "incorrect": 0,
    "unanswered": 0,
    "total": 30,
    "percent": 100,
    "duration": 46218406,
    "questions": [
      {
        "id": 143860747,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 6",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer has created a CI/CD pipeline in which AWS CodeDeploy is used to deploy an AWS Lambda function as the last stage of the pipeline. For proper execution, the Lambda function relies on an Amazon API Gateway API to have been fully deployed and ready to accept requests.</p><p>The DevOps engineer needs to ensure that the API is ready to accept requests before traffic is shifted to the deployed Lambda function version.</p><p>How can this requirement be met?</p>",
          "answers": [
            "<p>Use the BeforeAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests.</p>",
            "<p>Use the AfterInstall hook in the buildspec file to call AWS CodeBuild and run test commands to check that the API\u2019s production stage endpoint is reachable.</p>",
            "<p>Use the AfterAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests.</p>",
            "<p>Use the ValidateService hook in the AppSpec file to check that the deployment was completed successfully before shifting traffic to the deployed Lambda function.</p>"
          ],
          "explanation": "<p>An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Here are descriptions of the hooks available for use in your AppSpec file.</p><ul><li><p><strong>BeforeAllowTraffic</strong> \u2013 Use to run tasks before traffic is shifted to the deployed Lambda function version.</p></li><li><p><strong>AfterAllowTraffic</strong> \u2013 Use to run tasks after all traffic is shifted to the deployed Lambda function version.</p></li></ul><p>In a serverless Lambda function version deployment, event hooks run in the following order: BeforeAllowTraffic &gt; AllowTraffic &gt; AfterAllowTraffic.</p><p>Use the 'hooks' section to specify a Lambda function that CodeDeploy can call to validate a Lambda deployment. You can use the same function or a different one for the BeforeAllowTraffic and AfterAllowTraffic deployment lifecyle events. Following completion of the validation tests, the Lambda validation function calls back CodeDeploy and delivers a result of Succeeded or Failed.</p><p><strong>CORRECT: </strong>\"Use the BeforeAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AfterAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests\" is incorrect.</p><p>This hook is run after traffic is shifted to the Lambda function, so it doesn\u2019t offer any value here.</p><p><strong>INCORRECT:</strong> \"Use the ValidateService hook in the AppSpec file to check that the deployment was completed successfully before shifting traffic to the deployed Lambda function\" is incorrect.</p><p>The ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully. At that point traffic has already been shifted.</p><p><strong>INCORRECT:</strong> \"Use the AfterInstall hook in the buildspec file to call AWS CodeBuild and run test commands to check that the API\u2019s production stage endpoint is reachable\" is incorrect.</p><p>CodeDeploy is being used as the last stage of the pipeline so any build commands would have already been completed at this point.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 134588413,
        "correct_response": [
          "a",
          "e"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading IT consulting firm is building a GraphQL API service and a mobile application that lets people post photos and videos of the traffic situations and other issues in the city's public roads. Users can include a text report and constructive feedback to the authorities. The department of public works shall rectify the problems based on the data gathered by the system. In order for the mobile app to run on various mobile and tablet devices, the firm decided to develop it using the React Native mobile framework, which will consume and send data to the GraphQL API. The backend service will be responsible for storing the photos and videos in an Amazon S3 bucket. The API will also need access to the Amazon DynamoDB database to store the text reports. The firm has recently deployed the mobile app prototype, however, during testing, the GraphQL API showed a lot of issues. The team decided to remove the API to proceed with the project and refactor the mobile application instead so that it will directly connect to both DynamoDB and S3 as well as handle user authentication. </p><p>Which of the following options provides a cost-effective and scalable architecture for this project? (Select TWO.)</p>",
          "answers": [
            "<p>Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with social identity providers like Facebook, Google or any other OpenID Connect (OIDC)-compatible IdP. Create a new IAM Role and grant permissions to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table.</p>",
            "<p>Create an identity pool in AWS Identity and Access Management (IAM) that will be used to store the end-user identities organized for your mobile app. IAM will automatically create the required IAM roles for authenticated identities as well as for unauthenticated \"guest\" identities that define permissions for the users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use IAM. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client.</p>",
            "<p>Using the STS AssumeRoleWithSAML API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile app to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table.</p>",
            "<p>Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the text-based report to a DynamoDB table.</p>",
            "<p>Create an identity pool in Amazon Cognito that will be used to store the end-user identities organized for your mobile app. Amazon Cognito will automatically create the required IAM roles for authenticated identities as well as for unauthenticated \"guest\" identities that define permissions for Amazon Cognito users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use Amazon Cognito. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client.</p>"
          ],
          "explanation": "<p>With web identity federation, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any of your OpenID Connect (OIDC)-compatible IdP. They can receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Using an IdP helps you keep your AWS account secure because you don't have to embed and distribute long-term security credentials with your application.</p><p>The preferred way to use web identity federation is to use <a href=\"https://aws.amazon.com/cognito/\">Amazon Cognito</a>. For example, You are a developer that builds a game for a mobile device where each user data such as scores and profiles are stored in Amazon S3 and Amazon DynamoDB. You could also store this data locally on the device and use Amazon Cognito to keep it synchronized across devices. You know that for security and maintenance reasons, long-term AWS security credentials should not be distributed with the game. You might also know that the game might have a large number of users. For all of these reasons, you don't want to create new user identities in IAM for each player. Instead, you build the game so that users can sign in using an identity that they've already established with a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible IdP. Your game can take advantage of the authentication mechanism from one of these providers to validate the user's identity.</p><p><img src=\"https://media.tutorialsdojo.com/aws-cognito-diagram-tutorialsdojo.png\"></p><p>To enable the mobile app to access your AWS resources, you should first register for a developer ID with your chosen IdPs. You can also configure the application with each of these providers. In your AWS account that contains the Amazon S3 bucket and DynamoDB table for the game, you should use Amazon Cognito to create IAM roles that precisely define permissions that the game needs. If you are using an OIDC IdP, you can also create an IAM OIDC identity provider entity to establish trust between your AWS account and the IdP.</p><p>In the app's code, you can call the sign-in interface for the IdP that you configured previously. The IdP handles all the details of letting the user sign in, and the app gets an OAuth access token or OIDC ID token from the provider. Your mobile app can trade this authentication information for a set of temporary security credentials that consist of an AWS access key ID, a secret access key, and a session token. The app can then use these credentials to access web services offered by AWS. The app is limited to the permissions that are defined in the role that it assumes.</p><p>In this scenario, you have a mobile app that needs to have access to the DynamoDB and S3 bucket. You can achieve this by using Web Identity Federation with AssumeRoleWithWebIdentity API which provides temporary security credentials and an IAM role. You can also use Amazon Cognito to simplify the process.</p><p>Hence, the correct answers are:</p><p><strong>- Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with social identity providers like Facebook, Google or any other OpenID Connect (OIDC)-compatible IdP. Create a new IAM Role and grant permissions to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table.</strong></p><p><strong>- Create an identity pool in Amazon Cognito that will be used to store the end-user identities organized for your mobile app. Amazon Cognito will automatically create the required IAM roles for authenticated identities as well as for unauthenticated \"guest\" identities that define permissions for Amazon Cognito users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use Amazon Cognito. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client.</strong></p><p>The option that says: <strong>Create an identity pool in AWS Identity and Access Management (IAM) that will be used to store the end-user identities organized for your mobile app. IAM will automatically create the required IAM roles for authenticated identities as well as for unauthenticated \"guest\" identities that define permissions for the users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use IAM. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client</strong> is incorrect because you cannot create identity pools with guest identities using the AWS Identity and Access Management (IAM) service. You can only implement this using Amazon Cognito.</p><p>The option that says: <strong>Using the STS AssumeRoleWithSAML API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile app to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table</strong> is incorrect because you should have used the AssumRoleWithWebIdentity API instead of <em>AssumeRoleWithSAML</em>, as this is used in SAML authentication response and not for web identity authentication.</p><p>The option that says: <strong>Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the text-based report to a DynamoDB table</strong> is incorrect because even though the use of Amazon Cognito is valid, it is wrong to store and use the AWS access and secret keys from the mobile app itself. This is a security risk and you should use the temporary security credentials instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>"
        }
      },
      {
        "id": 75949084,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps manager requires a disaster recovery (DR) solution for a workload deployed on Amazon EC2 instances in an Amazon VPC within the us-east-1 Region. The DR solution should enable data replication to a staging area subnet in another AWS Region. The DR solution minimize operational overhead and enable fast failover and failback.</p><p>Which solution meets these requirements?</p>",
          "answers": [
            "<p>Use AWS Elastic Disaster Recovery to replicate data to a staging area subnet in another Region using the replication agent installed on the EC2 instance.</p>",
            "<p>Use AWS Resource Access Manager (RAM) to share the VPC across Regions and configure AWS DataSync to synchronize data between source and target systems.</p>",
            "<p>Create a resource group for the EC2 instances and configure DR protection using AWS Resilience Hub. Configure RPO/RTO settings and enable automated recovery.</p>",
            "<p>Use AWS Backup to enable cross-Region snapshots for the EC2 instances. Use AWS Fault Injection Simulator to enable automated recovery.</p>"
          ],
          "explanation": "<p>AWS Elastic Disaster Recovery (AWS DRS) minimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applications using affordable storage, minimal compute, and point-in-time recovery.</p><p>You can increase IT resilience when you use AWS Elastic Disaster Recovery to replicate on-premises or cloud-based applications running on supported operating systems. Use the AWS Management Console to configure replication and launch settings, monitor data replication, and launch instances for drills or recovery.</p><p>Set up AWS Elastic Disaster Recovery on your source servers to initiate secure data replication. Your data is replicated to a staging area subnet in your AWS account, in the AWS Region you select. The staging area design reduces costs by using affordable storage and minimal compute resources to maintain ongoing replication.</p><p><strong>CORRECT: </strong>\"Use AWS Elastic Disaster Recovery to replicate data to a staging area subnet in another Region using the replication agent installed on the EC2 instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Resource Access Manager (RAM) to share the VPC across Regions and configure AWS DataSync to synchronize data between source and target systems\" is incorrect.</p><p>AWS RAM cannot be used to share VPCs across Regions. Also, DataSync is not suitable for synchronizing the application data on Amazon EC2 instances.</p><p><strong>INCORRECT:</strong> \"Create a resource group for the EC2 instances and configure DR protection using AWS Resilience Hub. Configure RPO/RTO settings and enable automated recovery\" is incorrect.</p><p>AWS Resilience Hub provides a central place to define, validate, and track the resilience of applications on AWS. It cannot be used to configure DR protection or enable automated recovery.</p><p><strong>INCORRECT:</strong> \"Use AWS Backup to enable cross-Region snapshots for the EC2 instances. Use AWS Fault Injection Simulator to enable automated recovery\" is incorrect.</p><p>You can use AWS Backup to enable cross-Region backups of EC2 instances. However, AWS Fault Injection Simulator is not used for automated recovery, it is used for running fault injection experiments to improve an application\u2019s performance, observability, and resiliency.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html\">https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html</a></p>"
        }
      },
      {
        "id": 134588375,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A startup aims to rearchitect its internal web application hosted on Amazon EC2 into serverless architecture. At present, the startup deploys changes to the application by provisioning a new Auto Scaling group of EC2 instances across multiple Availability Zones and is fronted with a new Application Load Balancer. It then shifts the traffic with the use of Amazon Route 53 weighted routing policy. The DevOps Engineer of the startup will need to design a deployment strategy for serverless architecture similar to the current process that retains the ability to test new features with a limited set of users before making the features accessible to the entire user base. The startup plans to use AWS Lambda and Amazon API Gateway for the serverless architecture.</p><p>Which of the following is the MOST suitable solution to meet the requirements?</p>",
          "answers": [
            "<p>Utilize AWS Elastic Beanstalk to deploy Lambda functions and API Gateway. When there are code changes, a new version of both Lambda functions and API should be deployed. Use Elastic Beanstalk's blue/green deployment strategy to shift traffic gradually.</p>",
            "<p>Deploy Lambda functions with versions and API Gateway using AWS CloudFormation. When there are code changes, update the CloudFormation stack with the new Lambda code then a canary release strategy should be used to update the API versions. Once testing is done, promote the new version.</p>",
            "<p>Use AWS CodeDeploy to deploy the Lambda functions and the API Gateway. When there are code changes, use CodeDeploy\u2019s All at once deployment strategy, then redirect all traffic immediately using Amazon Route 53 simple routing policy.</p>",
            "<p>Deploy Lambda functions and API Gateway via AWS CDK. When there are code changes, update the CloudFormation Stack and deploy the new version of the Lambda functions and APIs. Enable canary release strategy by utilizing Amazon Route 53 failover routing policy.</p>"
          ],
          "explanation": "<p>By introducing alias traffic shifting, implementing <strong>canary deployments</strong> of <strong>Lambda functions</strong> has become effortless. The weightings of additional version can be adjusted on an alias to route invocation traffic to new function versions based on the weight specified.</p><p>In<strong> API Gateway</strong>, a <strong>canary release deployment</strong> uses the deployment stage for the production release of the base version of an API, and attaches to the stage a canary release for the new versions, relative to the base version, of the API. The stage is associated with the initial deployment and the canary with subsequent deployments.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-canary-release-strategy.png\"></p><p>Hence, the correct answer is the option that says: <strong>Deploy Lambda functions with versions and API Gateway using AWS CloudFormation. When there are code changes, update the CloudFormation stack with the new Lambda code then a canary release strategy should be used to update the API versions. Once testing is done, promote the new version.</strong></p><p>The option that says: <strong>Deploy Lambda functions and API Gateway via AWS CDK. When there are code changes, update the CloudFormation Stack and deploy the new version of the Lambda functions and APIs. Enable canary release strategy by utilizing Amazon Route 53 failover routing policy </strong>is incorrect because failover routing policy is primarily used for active-passive failover that lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. In addition, Route 53 cannot set Lambda versions as target.</p><p>The option that says: <strong>Utilize AWS Elastic Beanstalk to deploy Lambda functions and API Gateway. When there are code changes, a new version of both Lambda functions and API should be deployed. Use Elastic Beanstalk's blue/green deployment strategy to shift traffic gradually </strong>is incorrect because Elastic Beanstalk cannot deploy Lambda functions and API Gateway.</p><p>The option that says: <strong>Use AWS CodeDeploy to deploy the Lambda functions and the API Gateway. When there are code changes, use CodeDeploy\u2019s All at once deployment strategy, then redirect all traffic immediately using Amazon Route 53 simple routing policy </strong>is incorrect because the All at once deployment strategy updates all instances simultaneously. All instances in your environment are out of service for a short period of time. This strategy doesn\u2019t allow for testing new features with a limited set of users before making the features accessible to the entire user base, which is a requirement in the question. Also, Amazon Route 53 simple routing policy is used when you have a single resource that performs a given function for your domain, it doesn\u2019t allow for gradual traffic shifting which is required in the context of the question.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html#api-gateway-canary-release-deployment-overview\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html#api-gateway-canary-release-deployment-overview</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/\">https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/performing-canary-deployments-for-service-integrations-with-amazon-api-gateway/\">https://aws.amazon.com/blogs/compute/performing-canary-deployments-for-service-integrations-with-amazon-api-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 138248105,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A startup prioritizes a serverless approach, using AWS Lambda for new workloads to analyze performance and identify bottlenecks. The startup aims to transition to self-managed services on top of Amazon EC2 later if it is more cost-effective. To do this, a solution for granular monitoring of every component of the call graph, including services and internal functions, for all requests, is required. In addition, the startup wants engineers and other stakeholders to be notified of performance irregularities as soon as such irregularities arise.</p><p>Which option will meet these requirements?</p>",
          "answers": [
            "<p>Create an internal extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms.</p>",
            "<p>Consolidate workflows spanning multiple Lambda functions into 1 function per workflow. Create an external extension and enable AWS X-Ray active tracing to instrument functions into segments. Assign an execution role allowing X-Ray actions. Enable X-Ray insights and set up appropriate Amazon EventBridge rules and Amazon CloudWatch alarms.</p>",
            "<p>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable Amazon CloudWatch Logs insights. Configure relevant Amazon EventBridge rules and CloudWatch alarms.</p>",
            "<p>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms.</p>"
          ],
          "explanation": "<p><strong>AWS X-Ray</strong> is a service that analyzes the execution and performance behavior of distributed applications. Traditional debugging methods don\u2019t work so well for microservice-based applications, in which there are multiple, independent components running on different services. X-Ray enables rapid diagnosis of errors, slowdowns, and timeouts by breaking down application latency.</p><p><img alt=\"AWSXRay_LambdaServiceMap\" height=\"535\" src=\"https://media.tutorialsdojo.com/public/AWSXRay_LambdaServiceMap_22mar2024.png\" width=\"1000\"></p><p><em>Insights</em> is a feature of X-Ray that records performance outliers and tracks their impact until resolved. With insights, issues can be identified where they are occurring and what is causing them, and be triaged with the appropriate severity. Insights notifications are sent as the issue changes over time and can be integrated with your monitoring and alerting solution using Amazon EventBridge.</p><p>With an external <strong>AWS Lambda Extension</strong> using the telemetry API and X-Ray active tracing enabled, workflows are broken down into segments corresponding to the unit of work each Lambda function does. This can even be further broken down into subsegments by instrumenting calls to dependencies and related work, such as when a Lambda function requires data from DynamoDB and additional logic to process the response.</p><p>Lambda extensions come in two flavors: external and internal. The main difference is that an external extension runs in a separate process and can run longer to clean up after the Lambda function terminates, whereas an internal one runs in-process.<br><img alt=\"AWSLambdaExtension_overview_full_sequence\" height=\"670\" src=\"https://media.tutorialsdojo.com/public/AWSLambdaExtension_overview_full_sequence.png\" width=\"1000\"></p><p>Hence, the correct answer is: <strong>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms.</strong></p><p>The option that says: <strong>Create an internal extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms</strong> is incorrect. An internal Lambda extension only works in-process. In the scenario, since X-Ray is the solution chosen for tracing and the X-Ray daemon runs as a separate process, an implementation based on an internal Lambda extension will not work.</p><p>The option that says: <strong>Consolidate workflows spanning multiple Lambda functions into 1 function per workflow. Create an external extension and enable AWS X-Ray active tracing to instrument functions into segments. Assign an execution role allowing X-Ray actions. Enable X-Ray insights and set up appropriate Amazon EventBridge rules and Amazon CloudWatch alarms</strong> is incorrect. Aside from adding unnecessary engineering work, this primarily prevents the reuse of functions in different workflows and increases the chance of undesirable duplication. Use X-Ray groups instead to group traces from individual workflows.</p><p>The option that says: <strong>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable Amazon CloudWatch Logs insights. Configure relevant Amazon EventBridge rules and CloudWatch alarms</strong> is incorrect. Although Cloudwatch Logs insights and X-Ray insights both analyze and surface emergent issues from data, they do it on very different types of data -- logs and traces, respectively. As logs do not have graph-like relationships of trace segments/spans, they may require more work or data to surface the same issues.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html</a></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-groups.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-groups.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-extensions.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-extensions.html</a></p><p><br></p><p><strong>Check out this AWS X-Ray Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-x-ray/?src=udemy\">https://tutorialsdojo.com/aws-x-ray/</a></p>"
        }
      },
      {
        "id": 134588381,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has a CI/CD pipeline built in AWS CodePipeline for deployment updates. Part of the deployment process is to perform database schema updates and is performed via AWS CodeBuild. A recent security audit has discovered that the AWS CodeBuild is downloading the database scripts via Amazon S3 in an unauthenticated manner. The security team requires a solution that will enhance the security of the company's CI/CD pipeline.</p><p>What action should the DevOps Engineer take to address the issue in the MOST secure way?</p>",
          "answers": [
            "<p>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>",
            "<p>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>"
          ],
          "explanation": "<p><strong>Service role</strong>: A service role is an AWS Identity and Access Management (IAM) that grants permissions to an AWS service so that the service can access AWS resources. You need an AWS CodeBuild service role so that CodeBuild can interact with dependent AWS services on your behalf. You can create a CodeBuild service role by using the CodeBuild or AWS CodePipeline consoles.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-codebuild-service-role.png\"></p><p>In this scenario, the S3 bucket will be safeguarded from unauthorized access by utilizing a bucket policy. Moreover, CodeBuild leverages the service role for executing S3 actions on your behalf.</p><p>Hence, the correct answer is: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</strong></p><p>The option that says: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI</strong> is incorrect. While the use of IAM access key and secret access key can provide S3 access to CodeBuild, it is not the most secure approach to address the issue.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because an IAM policy alone cannot secure an S3 bucket from unauthorized access. A bucket policy must be used instead. Furthermore, this option uses IAM access key and secret access key, which is not the most secure way.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. It is not used for removing unauthenticated access to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role</a></p><p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>"
        }
      },
      {
        "id": 67357182,
        "correct_response": [
          "d",
          "e"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team for an e-commerce company wants to implement a patching plan on AWS Cloud for a large mixed fleet of Windows and Linux servers. The patching plan has to be auditable and must be implemented securely to ensure compliance with the company's business requirements.</p>\n\n<p>Which of the following options would you recommend to address these requirements with MINIMAL effort? (Select two)</p>\n",
          "answers": [
            "<p>Configure CloudFormation automatic patching support for all applications which will keep the OS up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting</p>",
            "<p>Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting</p>",
            "<p>Apply patch baselines using the AWS-ApplyPatchBaseline SSM document</p>",
            "<p>Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval</p>",
            "<p>Apply patch baselines using the AWS-RunPatchBaseline SSM document</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval</strong></p>\n\n<p><strong>Apply patch baselines using the AWS-RunPatchBaseline SSM document</strong></p>\n\n<p>Systems Manager is an AWS service that you can use to view and control your infrastructure on AWS. Using the Systems Manager console, you can view operational data from multiple AWS services and automate operational tasks across your AWS resources. Systems Manager helps you maintain security and compliance by scanning your managed instances and reporting on (or taking corrective action on) any policy violations it detects. AWS Systems Manager Agent (SSM Agent) is Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for the Systems Manager to update, manage, and configure these resources.</p>\n\n<p><img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\">\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>You can use Patch Manager to apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications). Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches individually or to large groups of instances by using Amazon EC2 tags.</p>\n\n<p>For the given use case, you can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task.</p>\n\n<p>Systems Manager supports an SSM document for Patch Manager, AWS-RunPatchBaseline, which performs patching operations on instances for both security-related and other types of updates. When the document is run, it uses the patch baseline currently specified as the \"default\" for an operating system type.</p>\n\n<p>The AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn't support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudFormation automatic patching support for all applications which will keep the OS up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting</strong> - AWS CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictable, and safely, while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions.</p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - \u201cWhat did my AWS resource look like at xyz point in time?\u201d.</p>\n\n<p>There is no such thing as CloudFormation automatic patching support, as you need to use Systems Manager for patch management. So, this option acts as a distractor.</p>\n\n<p><strong>Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting</strong> - You could use an OS-native patching service to manage the update frequency and release approval for all instances but it would take considerable effort to set up and configure this solution. This violates the minimal effort requirement of the given use case.</p>\n\n<p><strong>Apply patch baselines using the AWS-ApplyPatchBaseline SSM document</strong> - As mentioned in the explanation above, the AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn't support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html</a></p>\n"
        }
      },
      {
        "id": 82921462,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a social media company has created a CodePipeline pipeline and the final step is to use CodeDeploy to update an AWS Lambda function. As a DevOps Engineering Lead at the company, you have decided that for every deployment, the new Lambda function must sustain a small amount of traffic for 10 minutes and then shift all the traffic to the new function. It has also been decided that safety must be put in place to automatically roll-back if the Lambda function experiences too many crashes.</p>\n\n<p>Which of the following recommendations would you provide to address the given use-case? (Select two)</p>\n",
          "answers": [
            "<p>Choose a deployment configuration of <code>LambdaAllAtOnce</code></p>",
            "<p>Create a CloudWatch Alarm on the Lambda CloudWatch metrics and associate it with the CodeDeploy deployment</p>",
            "<p>Choose a deployment configuration of <code>LambdaCanary10Percent10Minutes</code></p>",
            "<p>Choose a deployment configuration of <code>LambdaLinear10PercentEvery10Minutes</code></p>",
            "<p>Create a CloudWatch Event for the Lambda Deployment Monitoring and associate it with the CodeDeploy deployment</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a CloudWatch Alarm on the Lambda CloudWatch metrics and associate it with the CodeDeploy deployment</strong></p>\n\n<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time. You can monitor metrics such as instance CPU utilization. If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover). You can also automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated.</p>\n\n<p>For the given use-case, the CodeDeploy deployment must be associated with a CloudWatch Alarm for automated rollbacks.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p>Configure advanced options for a deployment group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q26-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n\n<p><strong>Choose a deployment configuration of <code>LambdaCanary10Percent10Minutes</code></strong></p>\n\n<p>A deployment configuration is a set of rules and success and failure conditions used by CodeDeploy during a deployment. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q26-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n\n<p>For canary deployments, the traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p>\n\n<p>A canary deployment of <code>LambdaCanary10Percent10Minutes</code> means the traffic is 10% on the new function for 10 minutes, and then all the traffic is shifted to the new version after the time has elapsed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Choose a deployment configuration of <code>LambdaAllAtOnce</code></strong> - An all at once deployment means all the traffic is shifted to the new function right away and this option does not meet the given requirements.</p>\n\n<p><strong>Choose a deployment configuration of <code>LambdaLinear10PercentEvery10Minutes</code></strong> - For linear deployments, traffic is shifted in equal increments with an equal number of minutes between each increment. For example, a linear deployment of <code>LambdaLinear10PercentEvery10Minutes</code> would shift 10 percent of traffic every minute until all traffic is shifted.</p>\n\n<p><strong>Create a CloudWatch Event for the Lambda Deployment Monitoring and associate it with the CodeDeploy deployment</strong> - The CodeDeploy deployment must be associated with a CloudWatch Alarm and not CloudWatch Event for automated rollbacks to work.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n"
        }
      },
      {
        "id": 82921406,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multi-national retail company has defined tagging guidelines and standard for all its resources in AWS and would like to create a dashboard to visualize the compliance of all the resources with the ability to find out the non-compliant resources. The company has hired you as an AWS Certified DevOps Engineer Professional to develop a solution for this requirement.</p>\n\n<p>Which of the following options would you suggest to address the use-case?</p>\n",
          "answers": [
            "<p>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</p>",
            "<p>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</p>",
            "<p>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</p>",
            "<p>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</strong></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p>Here, we can use AWS Config to track resource configuration, and we could create a rule to track the tagging of these resources. All the changes to resource configuration as well as tagging of resources are streamed to an SNS topic.</p>\n\n<p>A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags make it easier to manage, search for, and filter resources. You can use tags to categorize your AWS resources in different ways, for example, by purpose, owner, or environment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p>You can set up the Required-Tag managed rule for Config which requires up to 6 tags with optional values in a single rule. Previously, each rule accepted only a single tag/value combo. Additionally, the Required-Tag managed rule now accepts a comma-separated list of values for each checked tag. This allows for a rule to be compliant if any one of a supplied list of tags is present on the resource.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</strong> - AWS Service Catalog enabling AWS customers to create and deliver standardized services that provide the necessary control, while still empowering developers to choose the services that best fit their needs. You cannot use Service Catalog to get an inventory of all the resources in your account.</p>\n\n<p><strong>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</strong> - SSM inventory will only help with understanding what is installed on your managed instances. To view Systems Manager Inventory history and change tracking for all of your managed instances, you need to use AWS Config itself.</p>\n\n<p><strong>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</strong> - CloudTrail is used to track API calls, not resources. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n"
        }
      },
      {
        "id": 99528209,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A critical production application running on AWS uses automatic scaling. The operations team must run updates on the application that affect only one instance at a time. The deployment process must ensure all remaining instances continue to serve traffic. The deployment must roll back if the update causes the CPU utilization of the updated instance to exceed 85%.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Configure AWS CodeDeploy with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Use the CodeDeployDefault.OneAtAtime configuration for deployment. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>",
            "<p>Configure AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group.</p>",
            "<p>Configure AWS Elastic Beanstalk with a load balancer and use AWS Auto Scaling. Create an alarm based on the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>",
            "<p>Configure AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>"
          ],
          "explanation": "<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS.</p><p>Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period. You can monitor metrics such as instance CPU utilization.</p><p>If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover).</p><p>For this scenario the deployment can use the CodeDeployDefault.OneAtAtime strategy which ensures that only one instance will be taken out of action at a time, and automatic rollbacks if the alarm threshold is exceeded. This meets all requirements.</p><p><strong>CORRECT: </strong>\"Configure AWS CodeDeploy with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Use the CodeDeployDefault.OneAtAtime configuration for deployment. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group\" is incorrect.</p><p>CodeDeploy is a much better solution as it is designed for this purpose. This answer pieces together several services in a more complex solution.</p><p><strong>INCORRECT:</strong> \"Configure AWS Elastic Beanstalk with a load balancer and use AWS Auto Scaling. Create an alarm based on the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is incorrect.</p><p>Elastic Beanstalk uses ELB health checks (basic health), or with enhanced health it monitors application logs and the state of your environment's other resources. It does not use CloudWatch alarms.</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is incorrect.</p><p>Systems Manager cannot perform blue/green updates though it can be used with other AWS Developer Tools as part of a solution for deploying updates in this manner.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 138248125,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company wants to implement a continuous delivery workflow that will facilitate the process of source code promotion in the development, staging, and production environments in AWS. In the event of system degradation or failure, the company should also have the ability to roll back the recent deployment of the application.</p><p>Which of the following CI/CD designs is the MOST suitable one to implement and will incur the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production.</p>",
            "Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy."
          ],
          "explanation": "<p>A repository is the fundamental version control object in GitHub or GitLab. It\u2019s where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. In GitHub and GitLab, you can set up notifications to configure notifications so that repository users receive emails about events (for example, another user commenting on code). You can change default settings to customize the default settings for your repository. You can browse contents to easily navigate and browse the contents of your repository. You can create triggers to set up triggers so that code pushes or other events trigger actions, such as emails or code functions. You can also configure a repository on your local computer (a local repo) to push your changes to more than one repository.</p><p><img src=\"https://media.tutorialsdojo.com/public/setup-cicd-pipeline-100324.png\"></p><p>In designing your CI/CD process in AWS, you can use a single repository in GitHub (or GitLab) and create different branches for development, master, and release. You can use CodeBuild to build your application and run tests to verify that all of the core features of your application are working. For deployment, you can either select an in-place or blue/green deployment using CodeDeploy.</p><p>Hence, the correct answer is: <strong>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</strong></p><p>The option that says: <strong>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy </strong>is incorrect because creating a separate repository for each developer is absurd since they can simply clone the code instead. A single repository will suffice in this scenario which can have several branches for development and production deployment purposes.</p><p>The option that says: <strong>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production</strong> is incorrect because you don't need to create two repositories for one application. Instead, you can just create at least two different branches to separate your development and production code.</p><p>The option that says: <strong>Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy</strong> is incorrect because Amazon ECR is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. This is not a suitable service to be used to store your application code.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/devops/continuous-integration/\">https://aws.amazon.com/devops/continuous-integration/</a></p><p><a href=\"https://aws.amazon.com/devops/continuous-delivery/\">https://aws.amazon.com/devops/continuous-delivery/</a></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html</a></p><p><br></p><p><strong>Check out this AWS CodePipelin Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 75949142,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.</p><p>How can the company meet these requirements with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns.</p>",
            "<p>Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket.</p>"
          ],
          "explanation": "<p>The simplest solution for this request is to enable server access logging on the bucket and then analyze the data that is logged using Amazon Athena. Server access logging provides detailed records for the requests that are made to a bucket. This includes the information requested by the company.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p><strong>CORRECT: </strong>\"Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket\" is incorrect $</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/\">https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>"
        }
      },
      {
        "id": 138248157,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is hosting their high-frequency trading application in AWS which serves millions of investors around the globe. The application is hosted in an Auto Scaling Group of EC2 instances behind an Application Load Balancer with an Amazon DynamoDB database. The architecture was deployed using a CloudFormation template with a Route 53 record. There recently was a production deployment that had caused system degradation and outage, costing the company a significant monetary loss due to their application's unavailability. As a result, the company instructed their DevOps engineer to implement an efficient strategy for deploying updates to their web application with the ability to perform an immediate rollback of the stack. All deployments should maintain the normal number of active EC2 instances to keep the performance of the application.</p><p>Which of the following should the DevOps engineer implement to satisfy these requirements?&nbsp; </p>",
          "answers": [
            "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template to use the <code>AutoScalingReplacingUpdate</code> policy. Update the required properties for the new Auto Scaling group policy. Set the <code>WillReplace</code> property to true.</p>",
            "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template to use the <code>AutoScalingReplacingUpdate</code> policy. Set the <code>WillReplace</code> property to false. Also, specify the <code>AutoScalingRollingUpdate</code> policy to update instances that are in an Auto Scaling group in batches.</p>",
            "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template to use the <code>AutoScalingRollingUpdate</code> policy. Update the required properties for the new Auto Scaling group policy. Set the <code>MinSuccessfulInstancesPercent</code> property, as well as its corresponding <code>WaitOnResourceSignals</code> and <code>PauseTime</code> properties.</p>",
            "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::DeploymentUpdates</code> resource in the CloudFormation template to use the <code>AutoScalingReplacingUpdate</code> policy. Update the required properties for the new Auto Scaling group policy. Set the <code>WillReplace</code> property to false.</p>"
          ],
          "explanation": "<p>If you plan to launch an Auto Scaling group of EC2 instances, you can configure the <code>AWS::AutoScaling::AutoScalingGroup</code> resource type reference in your CloudFormation template to define an Amazon EC2 Auto Scaling group with the specified name and attributes. To configure Amazon EC2 instances launched as part of the group, you can specify a launch template. It is recommended that you use a launch template to make sure that you can use the latest features of Amazon EC2, such as T2 Unlimited instances.</p><p>You can add an UpdatePolicy attribute to your Auto Scaling group to perform rolling updates (or replace the group) when a change has been made to the group.</p><p>To specify how AWS CloudFormation handles replacement updates for an Auto Scaling group, use the <code>AutoScalingReplacingUpdate</code> policy. This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-11-22_03-48-11-a352802536c1d852293fc176ddad72a9.png\">During replacement, AWS CloudFormation retains the old group until it finishes creating the new one. If the update fails, AWS CloudFormation can roll back to the old Auto Scaling group and delete the new Auto Scaling group. While AWS CloudFormation creates the new group, it doesn't detach or attach any instances. After successfully creating the new Auto Scaling group, AWS CloudFormation deletes the old Auto Scaling group during the cleanup process.</p><p>When you set the <code>WillReplace</code> parameter, remember to specify a matching CreationPolicy. If the minimum number of instances (specified by the MinSuccessfulInstancesPercent property) doesn't signal success within the Timeout period (specified in the CreationPolicy policy), the replacement update fails, and AWS CloudFormation rolls back to the old Auto Scaling group.</p><p>Hence, the correct answer is: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy. Update the required properties for the new Auto Scaling group policy. Set the </strong><code><strong>WillReplace</strong></code><strong> property to true.</strong></p><p>The option that says: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy. Set the </strong><code><strong>WillReplace</strong></code><strong> property to false. Also, specify the </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy to update instances that are in an Auto Scaling group in batches</strong> is incorrect because if both the <code>AutoScalingReplacingUpdate</code> and <code>AutoScalingRollingUpdate</code> policies are specified, setting the <code>WillReplace</code> property to <code>true</code> gives <code>AutoScalingReplacingUpdate</code> precedence. But since this property is set to false, then the <code>AutoScalingRollingUpdate</code> policy will take precedence instead.</p><p>The option that says: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy. Update the required properties for the new Auto Scaling group policy. Set the </strong><code><strong>MinSuccessfulInstancesPercent</strong></code><strong> property, you must also enable the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> and </strong><code><strong>PauseTime</strong></code><strong> properties</strong> is incorrect because this type of deployment will affect the existing compute capacity of your application. The rolling update doesn't maintain the total number of active EC2 instances during deployment. A better solution is to use the <em>AutoScalingReplacingUpdate</em> policy instead, which will create a separate Auto Scaling group and is able to perform an immediate rollback of the stack in the event of an update failure.</p><p>The option that says: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::DeploymentUpdates</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy. Update the required properties for the new Auto Scaling group policy. Set the </strong><code><strong>WillReplace</strong></code><strong> property to false</strong> is incorrect because there is no <code>AWS::AutoScaling::DeploymentUpdates</code> resource. You have to use the <code>AWS::AutoScaling::AutoscalingGroup</code> resource instead and set the <code>WillReplace</code> property to true.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 82921354,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company is finishing its migration to AWS and realizes that while some employees have passed the AWS Certified DevOps Engineer Professional certification and know AWS very well, other ones are still beginning and haven't passed their Associate-level certifications yet. The company has established architectural and tagging specific internal rules and would like to minimize the risk of the AWS-beginner employees launching uncompliant resources.</p>\n\n<p>As a DevOps Engineer, how should you implement this requirement while allowing the employees to create the resources they need?</p>\n",
          "answers": [
            "<p>Define commonly used architectures as CloudFormation templates. Create Service Catalog stacks from these templates, and ensure the tagging is done properly. Place the IAM users into a beginner group and allow the users to only launch stacks from Service Catalog, while restricting any write access to other services</p>",
            "<p>Define commonly used architectures as CloudFormation templates. Place the IAM users into a beginner group and allow the users to only launch stacks from these CloudFormation stacks, while restricting any write access to other services</p>",
            "<p>Create AWS Config custom rules that will check for the compliance of your company's resources thanks to a Lambda Function. Update the Lambda Function over time while your company improves its architectural and tagging rules. Provide IAM users full access to AWS</p>",
            "<p>Place the beginner IAM users into a group and create an IAM policy that requires conditional approvals from senior DevOps engineers upon resource creation. Hook an SNS topic into the IAM approval channel</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Define commonly used architectures as CloudFormation templates. Create Service Catalog stacks from these templates, and ensure the tagging is done properly. Place the IAM users into a beginner group and allow the users to only launch stacks from Service Catalog, while restricting any write access to other services</strong></p>\n\n<p>AWS Service Catalog allows IT administrators to create, manage, and distribute catalogs of approved products to end-users, who can then access the products they need in a personalized portal. Administrators can control which users have access to each product to enforce compliance with organizational business policies.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html</a></p>\n\n<p>A product is a service or application for end-users. A portfolio is a collection of products, with configuration information that determines who can use those products and how they can use them. A catalog is a collection of products that the administrator creates, adds to portfolios, and provides updates for using AWS Service Catalog. To create a Service Catalog product, you first need to create an AWS CloudFormation template by using an existing AWS CloudFormation template or creating a custom template. Then you can use the AWS Service Catalog console to upload the template and create the product.</p>\n\n<p>Therefore, for the given use-case, we need to use Service Catalog as it was precisely designed for that purpose and give users only access to the stack they should be able to create in Service Catalog.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/servicecatalog/faqs/\">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/servicecatalog/faqs/\">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define commonly used architectures as CloudFormation templates. Place the IAM users into a beginner group and allow the users to only launch stacks from these CloudFormation stacks, while restricting any write access to other services</strong> - If you let IAM users use the CloudFormation service directly, they will have the power to create any resource through their permissions. You cannot restrict templates using IAM policies in CloudFormation.</p>\n\n<p><strong>Create AWS Config custom rules that will check for the compliance of your company's resources thanks to a Lambda Function. Update the Lambda Function over time while your company improves its architectural and tagging rules. Provide IAM users full access to AWS</strong> - AWS Config Rules would be a way to \"monitor\" the situation but not prevent resources from being created the wrong way.</p>\n\n<p><strong>Place the beginner IAM users into a group and create an IAM policy that requires conditional approvals from senior DevOps engineers upon resource creation. Hook an SNS topic into the IAM approval channel</strong> - An IAM policy cannot have a \"conditional approval\", so this option is a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/servicecatalog/faqs/\">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/\">https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/</a></p>\n"
        }
      },
      {
        "id": 75949108,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 3",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>",
          "answers": [
            "<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"
          ],
          "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 134588503,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A company has confidential files containing patent information stored in an Amazon S3 bucket in the US East (Ohio) region. Among the team members working on the project, actions need to be monitored on the objects, such as PUT, GET, and DELETE operations. The goal is to search and review these actions for auditing purposes easily.</p><p>Which solution will meet this requirement MOST cost-effectively?</p>",
          "answers": [
            "<p>Enable logging on your Amazon S3 bucket to save the access logs on a separate S3 bucket. Import logs to Amazon OpenSearch service to easily search and query the logs.</p>",
            "<p>Enable logging on your S3 bucket. Create an Amazon EventBridge rule that watches the object-level events of your S3 bucket. Create a target on the rule to send the event logs to an Amazon CloudWatch Log group. View and search the logs on CloudWatch Logs.</p>",
            "<p>Create an AWS CloudTrail trail to track and store your S3 API call logs in an S3 bucket. Create an AWS Lambda function that logs data events of your S3 bucket. Trigger this Lambda function using the Amazon EventBridge rule for every action taken on your S3 objects. View the logs on the Amazon CloudWatch Logs group.</p>",
            "<p>Create an Amazon CloudWatch Log group and configure S3 logging to send object-level events to this log group. View and search the event logs on the created CloudWatch log group.</p>"
          ],
          "explanation": "<p>When an event occurs in your account, CloudTrail evaluates whether the event matches the settings for your trails. Only events that match your trail settings are delivered to your Amazon S3 bucket and Amazon CloudWatch Logs log group.</p><p>You can configure your trails to log the following:</p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#logging-data-events\"><strong>Data events</strong></a>: These events provide insight into the resource operations performed on or within a resource. These are also known as data plane operations.</p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#logging-management-events\"><strong>Management events</strong></a>: Management events provide insight into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Management events can also include non-API events that occur in your account. For example, when a user logs in to your account, CloudTrail logs the <code>ConsoleLogin</code> event.</p><p>You can configure multiple trails differently so that the trails process and log only the events that you specify. For example, one trail can log read-only data and management events so that all read-only events are delivered to one S3 bucket. Another trail can log only write-only data and management events, so that all write-only events are delivered to a separate S3 bucket.</p><p>You can also configure your trails to have one trail log and deliver all management events to one S3 bucket, and configure another trail to log and deliver all data events to another S3 bucket.</p><p><img alt=\"AWS CloudTrail\" src=\"https://media.tutorialsdojo.com/public/td-aws-cloudtrail-13Feb2025.jpg\" width=\"1000\"></p><p>Data events provide insight into the resource operations performed on or within a resource. These are also known as data plane operations. Data events are often high-volume activities.</p><p>Example data events include:</p><p>- Amazon S3 object-level API activity (for example, <code>GetObject</code>, <code>DeleteObject</code>, and <code>PutObject</code> API operations)</p><p>- AWS Lambda function execution activity (the <code>Invoke</code> API)</p><p>Data events are disabled by default when you create a trail. To record CloudTrail data events, you must explicitly add the supported resources or resource types for which you want to collect activity to a trail.</p><p>You can log the object-level API operations on your S3 buckets. Before Amazon EventBridge can match these events, you must use AWS CloudTrail to set up a trail configured to receive these events. To log data events for an S3 bucket to AWS CloudTrail and EventBridge, create a trail. A trail captures API calls and related events in your account and delivers the log files to an S3 bucket that you specify. After you create a trail and configure it to capture the log files you want, you need to be able to find the log files and interpret the information they contain.</p><p>Typically, log files appear in your bucket within 15 minutes of the recorded AWS API call or other AWS event. Then you need to create a Lambda function to log data events for your S3 bucket. Finally, you need to create a trigger to run your Lambda function in response to an Amazon S3 data event. You can create this rule on Amazon EventBridge and setting Lambda function as the target. Your logs will show up on the CloudWatch log group, which you can view and search as needed.</p><p>Hence, the correct answer is: <strong>Create an AWS CloudTrail trail to track and store your S3 API call logs in an S3 bucket. Create an AWS Lambda function that logs data events of your S3 bucket. Trigger this Lambda function using the Amazon EventBridge rule for every action taken on your S3 objects. View the logs on the Amazon CloudWatch Logs group.</strong></p><p>The option that says: <strong>Enable logging on your Amazon S3 bucket to save the access logs on a separate S3 bucket. Import logs to Amazon OpenSearch service to easily search and query the logs</strong> is incorrect because this would simply incur more cost if you provision an OpenSearch cluster. Take note that the scenario asks for the most cost-effective solution.</p><p>The option that says: <strong>Enable logging on your S3 bucket. Create an Amazon EventBridge rule that watches the object-level events of your S3 bucket. Create a target on the rule to send the event logs to an Amazon CloudWatch Log group. View and search the logs on CloudWatch Logs</strong> is incorrect because before, Amazon EventBridge could match API call events. You must use AWS CloudTrail first to set up a trail configured to receive these events.</p><p>The option that says: <strong>Create an Amazon CloudWatch Log group and configure S3 logging to send object-level events to this log group. View and search the event logs on the created CloudWatch log group</strong> is incorrect because you can't typically configure an S3 bucket to directly send access logs to a CloudWatch log group.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/cloudtrail-update-capture-and-process-amazon-s3-object-level-api-activity/\">https://aws.amazon.com/blogs/aws/cloudtrail-update-capture-and-process-amazon-s3-object-level-api-activity/</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#example-logging-all-S3-objects\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#example-logging-all-S3-objects</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p>"
        }
      },
      {
        "id": 67357116,
        "correct_response": [
          "a",
          "c",
          "f"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>For deployments across AWS accounts, an e-commerce company has decided to use AWS CodePipeline to deploy an AWS CloudFormation stack in an AWS account (account A) to a different AWS account (account B).</p>\n\n<p>What combination of steps will you take to configure this requirement? (Select three)</p>\n",
          "answers": [
            "<p>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket</p>",
            "<p>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. In account B, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account A access to the bucket</p>",
            "<p>In account B, create a cross-account IAM role. In account A, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B</p>",
            "<p>In account A, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account B, update the CodePipeline configuration to include the resources associated with account A</p>",
            "<p>In account B, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account A</p>",
            "<p>In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket</strong></p>\n\n<p><strong>In account B, create a cross-account IAM role. In account A, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B</strong></p>\n\n<p><strong>In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B</strong></p>\n\n<p>Complete list of steps for configuring the requirement:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/\">https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. In account B, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account A access to the bucket</strong></p>\n\n<p><strong>In account A, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account B, update the CodePipeline configuration to include the resources associated with account A</strong></p>\n\n<p><strong>In account B, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account A</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/\">https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/</a></p>\n"
        }
      },
      {
        "id": 134588395,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company leverages API Gateway REST API to streamline SDK generation and simplify distribution to its partners. Employing an AWS CodePipeline pipeline, updates are made to the REST API multiple times daily. In the event of a deployment failure, the REST API reverts to the previous version, ensuring continuous service. However, engineers still have to manually upload corresponding SDKs to an Amazon S3 bucket. The company wants to eliminate this manual process as it is quite error-prone.</p><p>Which option will fit the requirements?</p>",
          "answers": [
            "<p>Create a Lambda function to download the generated SDKs via API Gateway <code>GetSdk</code> and upload them to Amazon S3. Set up a separate CodePipeline action to invoke the function. Configure the new action to be triggered by a deploy action failure.</p>",
            "<p>Set up a separate action in the pipeline. Create a Lambda function to generate language-specific SDKs and upload them to Amazon S3. Configure the action to invoke this function.</p>",
            "<p>Create a Lambda function to generate and download the SDKs via API Gateway <code>GetSdk</code> and upload them to Amazon S3. Set up an EventBridge rule to filter for API Gateway <code>UpdateStage</code> operation and to invoke the function on such event.</p>",
            "<p>Set up an EventBridge rule to filter for API Gateway <code>CreateDeployment</code> operation. Create a Lambda function to generate the SDKs and upload them to Amazon S3. Configure the EventBridge rule to invoke the function.</p>"
          ],
          "explanation": "<p><strong>AWS CodePipeline</strong> is a continuous delivery service that models, visualizes, and automates the steps required to release software. It allows you to quickly model and configure the different stages of a software release process and automate the steps required to continuously release your software changes.</p><p><img src=\"https://media.tutorialsdojo.com/public/AWSCodePipeline_with_APIGateway_21Mar2024.png\"></p><p><strong>Amazon API Gateway</strong> is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.</p><p>More specifically, the API Gateway REST API offering includes several features that make the distribution and operation of APIs more scalable. One of these is the generation of client SDKs via its <code>GetSdk</code> API. Another useful feature is that API operations, such as <code>UpdateStage</code> create observable lifecycle events, allowing for effects, like retrieving SDKs and uploading them to S3 in this scenario, to be programmatically triggered and decoupled.</p><p>Therefore, the correct answer is: <strong>Create a Lambda function to generate and download the SDKs via API Gateway </strong><code><strong>GetSdk</strong></code><strong> and upload them to Amazon S3. Set up an EventBridge rule to filter for API Gateway </strong><code><strong>UpdateStage</strong></code><strong> operation and to invoke the function on such event.</strong></p><p>The option that says: <strong>Create a Lambda function to download the generated SDKs via API Gateway </strong><code><strong>GetSdk</strong></code><strong> and upload them to Amazon S3. Set up a separate CodePipeline action to invoke the function. Configure the new action to be triggered by a deploy action failure</strong> is incorrect because CodePipeline does not provide a feature for conditional actions. A pipeline action can be retried if it fails at some stage but, in general, a pipeline only \"moves forward\" and either succeeds or fails.</p><p>The option that says: <strong>Set up a separate action in the pipeline. Create a Lambda function to generate language-specific SDKs and upload them to Amazon S3. Configure the action to invoke this function.</strong> is incorrect. Aside from unnecessarily implementing code generation, since the function is now part of the pipeline, it will also not be triggered unless the pipeline is manually triggered and parameterized with an older stable version of the API.</p><p>The option that says: <strong>Set up an EventBridge rule to filter for API Gateway </strong><code><strong>CreateDeployment</strong></code><strong> operation. Create a Lambda function to generate the SDKs and upload them to Amazon S3. Configure the EventBridge rule to invoke the function</strong> is incorrect. Reverting to a previous version of an API/stage does not create a new deployment, so the <code>CreateDeployment</code>-related event will not be emitted, and the EventBridge rule will never be triggered.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-retry.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-retry.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/api/API_GetSdk.html\">https://docs.aws.amazon.com/apigateway/latest/api/API_GetSdk.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/api/API_UpdateStage.html\">https://docs.aws.amazon.com/apigateway/latest/api/API_UpdateStage.html</a></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p>"
        }
      },
      {
        "id": 67357132,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A media application runs on a host of Amazon EC2 instances fronted with an Application Load Balancer (ALB) and Amazon S3 buckets as storage service. For enhanced security, an AWS Web Application Firewall (AWS WAF) has been set up to monitor the requests coming to the ALB. The DevOps team needs to submit a quarterly report on the web requests received by AWS WAF, having detailed information about each web request as well as the details about rules that the request matched. The team has reached out to you for implementing the changes needed for collecting the security data for the coming months.</p>\n\n<p>As DevOps Engineer, how will you implement this requirement?</p>\n",
          "answers": [
            "<p>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</p>",
            "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</p>",
            "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</p>",
            "<p>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data firehose name should start with the prefix <code>aws-waf-logs-</code></p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</strong></p>\n\n<p>You can enable logging AWS WAF web ACL traffic, to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose.</p>\n\n<p>To send your web ACL traffic logs to Amazon S3, you need to set up an Amazon S3 bucket for the logs. When you enable logging for AWS WAF, you provide the bucket ARN. Your web ACLs publish their log files to the Amazon S3 bucket at 5-minute intervals. Each log file contains log records for the traffic recorded in the previous 5 minutes.</p>\n\n<p>The maximum file size for a log file is 75 MB. If the log file reaches the file size limit within the 5-minute period, the log stops adding records to it, publishes it to the Amazon S3 bucket, and then creates a new log file.</p>\n\n<p>Your bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For example, <code>aws-waf-logs-DOC-EXAMPLE-BUCKET-SUFFIX</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</strong> - AWS WAF supports encryption with Amazon S3 buckets for key type Amazon S3 key (SSE-S3) and AWS Key Management Service (SSE-KMS) AWS KMS keys. AWS WAF doesn't support encryption for AWS Key Management Service keys that are managed by AWS.</p>\n\n<p><strong>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as a WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data Firehose name should start with the prefix <code>aws-waf-logs-</code></strong> - AWS recommends that the Kinesis stream should not be used as a source when configuring Data Firehose as a WAF logging destination.</p>\n\n<p><strong>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html</a></p>\n"
        }
      },
      {
        "id": 134588409,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A technology consulting company has an Oracle Real Application Clusters (RAC) database on their on-premises network which they want to migrate to AWS Cloud. Their Chief Technology Officer instructed the DevOps team to automate the patch management process of the operating system in which their database will run. They are also mandated as well to set up scheduled backups to comply with the disaster recovery plan of the company. </p><p>What should the DevOps team do to satisfy the requirement for this scenario with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Migrate the Oracle RAC database to a large EBS-backed Amazon EC2 instance then install the SSM agent. Use the AWS Systems Manager Patch Manager to automate the patch management process. Set up the Amazon Data Lifecycle Manager service to automate the creation of Amazon EBS snapshots from the EBS volumes of the EC2 instance.</p>",
            "<p>Migrate the database that is hosted on-premises to Amazon RDS which provides a multi-AZ failover feature for your Oracle RAC cluster. The RPO and RTO will be reduced in the event of system failure since Amazon RDS offers features such as patch management and maintenance of the underlying host.</p>",
            "<p>Migrate the on-premises database to Amazon Aurora. Enable automated backups for your Aurora RAC cluster. With Amazon Aurora, patching is automatically handled during the system maintenance window.</p>",
            "<p>Migrate the Oracle RAC database to a large EBS-backed Amazon EC2 instance. Launch an AWS Lambda function that will call the <code>CreateSnapshot</code> EC2 API to automate the creation of Amazon EBS snapshots of the database. Integrate CloudWatch Events and Lambda in order to run the function on a regular basis. Set up the CodeDeploy and CodePipeline services to automate the patch management process of the database.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/public/AWS-Systems-Manager-Patch-Manager_3AUG2023.png\"></p><p><strong>Amazon Data Lifecycle Manager (DLM)</strong> for EBS Snapshots provides a simple, automated way to back up data stored on Amazon EBS volumes. You can define backup and retention schedules for EBS snapshots by creating lifecycle policies based on tags. With this feature, you no longer have to rely on custom scripts to create and manage your backups.</p><p><strong>Oracle RAC</strong> is supported via the deployment using Amazon EC2 only since Amazon RDS and Aurora do not support it. Amazon RDS does not support certain features in Oracle such as Multitenant Database, Real Application Clusters (RAC), Unified Auditing, Database Vault and many more. You can use AWS Systems Manager Patch Manager to automate the process of patching managed instances with security-related updates.</p><p>Hence, the correct answer is: <strong>Migrate the Oracle RAC database to a large EBS-backed Amazon EC2 instance then install the SSM agent. Use the AWS Systems Manager Patch Manager to automate the patch management process. Set up the Amazon Data Lifecycle Manager service to automate the creation of Amazon EBS snapshots from the EBS volumes of the EC2 instance.</strong></p><p>The option that says: <strong>Migrate the database that is hosted on-premises to Amazon RDS which provides a multi-AZ failover feature for your Oracle RAC cluster. The RPO and RTO will be reduced in the event of system failure since Amazon RDS offers features such as patch management and maintenance of the underlying host</strong> is incorrect because Amazon RDS doesn't support Oracle RAC.</p><p>The option that says: <strong>Migrate the on-premises database to Amazon Aurora. Enable automated backups for your Aurora RAC cluster. With Amazon Aurora, patching is automatically handled during the system maintenance window</strong> is incorrect because, just like Amazon RDS, the Amazon Aurora doesn't support Oracle RAC as well.</p><p>The option that says: <strong>Migrate the Oracle RAC database to a large EBS-backed Amazon EC2 instance. Launch an AWS Lambda function that will call the </strong><code><strong>CreateSnapshot</strong></code><strong> EC2 API to automate the creation of Amazon EBS snapshots of the database. Integrate CloudWatch Events and Lambda in order to run the function on a regular basis. Set up the CodeDeploy and CodePipeline services to automate the patch management process of the database </strong>is incorrect because CodeDeploy and CodePipeline are CI/CD services and are not suitable for patch management. You should use AWS Systems Manager Patch Manager instead. In addition, the Amazon Data Lifecycle Manager service is the recommended way to automate the creation of Amazon EBS snapshots and not a combination of Lambda and CloudWatch Events.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/rds/oracle/faqs/\">https://aws.amazon.com/rds/oracle/faqs/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>"
        }
      },
      {
        "id": 82921398,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A global health-care company has an EFS filesystem being used in eu-west-1. The company would like to plan for a disaster recovery strategy and backup that EFS file system in ap-southeast-2. It needs to have a hot copy of the data so that the applications can be re-deployed in ap-southeast-2 with a minimum RPO and RTO. The VPCs in each region are not peered with each other.</p>\n\n<p>How should a DevOps engineer implement a solution for this use-case?</p>\n",
          "answers": [
            "<p>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</p>",
            "<p>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create a Lambda Function in ap-southeast-2 for PUT on Amazon S3 and triggers an SSM Run Command to copy the files from S3 into EFS</p>",
            "<p>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Create a standby EFS cluster in ap-southeast-2 and mount it on the same EC2 cluster. Let the replication software perform EFS to EFS replication</p>",
            "<p>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create an EC2 replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</strong></p>\n\n<p>Metrics are the fundamental concept in CloudWatch. A metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time. You can use these metrics to verify that your system is performing as expected.</p>\n\n<p>Using custom metrics for your Auto Scaling groups and instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q68-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>RPO and RTO explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q68-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html</a></p>\n\n<p>For the given use-case, we need to create a custom metric via the application that captures the lag in file reads and then uses it for scaling the ASG managing the EC2 instances to replicate the source EFS cluster into S3. Use another ASG to copy data from S3 into EFS in the target AWS Region. Here we want minimum RPO so we want continuous replication, and minimum RTO so we want a hot EFS system ready to go. Please note that because the RPO and RTO are low, the cost of the solution will be very high.</p>\n\n<p>Side note (for your knowledge) the AWS DataSync service (not covered in the exam) can achieve EFS to EFS replication in a much more native way.</p>\n\n<p>Note: With this solution, as the files are copied to S3, the file Linux permissions would not be replicated.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Create a standby EFS cluster in ap-southeast-2 and mount it on the same EC2 cluster. Let the replication software perform EFS to EFS replication</strong> - As the VPCs are not peered, it's not possible to mount the EFS of two different regions onto the same EC2 cluster. We need to go through S3 for the replication.</p>\n\n<p><strong>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create an EC2 replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</strong></p>\n\n<p><strong>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create a Lambda Function in ap-southeast-2 for PUT on Amazon S3 and triggers an SSM Run Command to copy the files from S3 into EFS</strong></p>\n\n<p>As the target, EFS needs to have a hot copy of the data, so both these options are ruled out since there is a delay of an hour.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n"
        }
      },
      {
        "id": 138248107,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer has been tasked with implementing configuration management for the company's infrastructure in AWS. To adhere to the company's strict security policies, the solution must include a near real-time dashboard that displays the compliance status of the systems and can detect any violations.</p><p>Which solution would be able to meet the above requirements?</p>",
          "answers": [
            "<p>Use AWS Service Catalog to create the required resource configurations for its compliance posture. Monitor the compliance and violations of its cloud resources using a custom CloudWatch dashboard with an integrated Amazon SNS to send notifications.</p>",
            "Use AWS Config to record all configuration changes and store the data reports to Amazon S3. Use Amazon QuickSight to analyze the dataset.",
            "<p>Tag all the resources and use Trusted Advisor to monitor both the compliant and non-compliant resources. Use the AWS Management Console to monitor the status of the compliance posture.</p>",
            "<p>Use Amazon Inspector to monitor the compliance posture of the systems and store the reports in Amazon CloudWatch Logs. Use a CloudWatch dashboard with a custom metric filter to monitor and view all of the specific compliance requirements.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides you a visual dashboard to help you quickly spot non-compliant resources and take appropriate action. IT Administrators, Security Experts, and Compliance Officers can see a shared view of your AWS resources compliance posture.</p><p>When you run your applications on AWS, you usually use AWS resources, which you must create and manage collectively. As the demand for your application keeps growing, so does your need to keep track of your AWS resources.</p><p><img src=\"https://media.tutorialsdojo.com/public/how-AWSconfig-works.png\"></p><p>To exercise better governance over your resource configurations and to detect resource misconfigurations, you need fine-grained visibility into what resources exist and how these resources are configured at any time. You can use AWS Config to notify you whenever resources are created, modified, or deleted without having to monitor these changes by polling the calls made to each resource.</p><p>You can use AWS Config rules to evaluate the configuration settings of your AWS resources. When AWS Config detects that a resource violates the conditions in one of your rules, AWS Config flags the resource as non-compliant and sends a notification. AWS Config continuously evaluates your resources as they are created, changed, or deleted.</p><p>Hence, the correct answer is: <strong>Use AWS Config to record all configuration changes and store the data reports to Amazon S3. Use Amazon QuickSight to analyze the dataset.</strong></p><p>The option that says: <strong>Use AWS Service Catalog to create the required resource configurations for its compliance posture. Monitor the compliance and violations of its cloud resources using a custom CloudWatch dashboard with an integrated Amazon SNS to send notifications</strong><em> </em>is incorrect. Although AWS Service Catalog can be used for resource configuration, it is not typically capable of detecting violations of your AWS configuration rules.</p><p>The option that says: <strong>Tag all the resources and use Trusted Advisor to monitor both the compliant and non-compliant resources. Use the AWS Management Console to monitor the status of the compliance posture</strong> is incorrect because the Trusted Advisor service is not suitable for configuration management and automatic violation detection. You should use AWS Config instead.</p><p>The option that says: <strong>Use Amazon Inspector to monitor the compliance posture of the systems and store the reports in Amazon CloudWatch Logs. Use a CloudWatch dashboard with a custom metric filter to monitor and view all of the specific compliance requirements</strong> is incorrect because the Amazon Inspector service is primarily used to help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\">https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html</a></p><p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/QS-compliance.html\">https://docs.aws.amazon.com/quicksight/latest/user/QS-compliance.html</a></p><p><a href=\"https://aws.amazon.com/config/features/\">https://aws.amazon.com/config/features/</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 82921448,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A cyber-security company has had a dubious distinction of their own AWS account credentials being put in public GitHub repositories. The company wants to implement a workflow to be alerted in case credentials are leaked, generate a report of API calls made recently using the credentials, and de-activate the credentials. All executions of the workflow must be auditable.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a robust solution for this requirement. Which of the following solutions would you implement?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>",
            "<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>",
            "<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Lambda Function that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>",
            "<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Lambda Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong></p>\n\n<p>Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>AWS monitors popular code repository sites for IAM access keys that have been publicly exposed. AWS Health generates an AWS_RISK_CREDENTIALS_EXPOSED event when an IAM access key has been publicly exposed on GitHub. A CloudWatch Events rule further detects this event and invokes a Step Function that orchestrates the automated workflow to delete the exposed IAM access key, and summarize the recent API activity for the exposed key. The workflow will also issue API calls to IAM, CloudTrail, and SNS. The AWS_RISK_CREDENTIALS_EXPOSED is exposed by the Personal Health Dashboard service.</p>\n\n<p>Mitigating security events using AWS Health and CloudTrail:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q61-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Lambda Function that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong></p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Lambda Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong></p>\n\n<p>As the way to react to that event is complex and may have retries, and you want to have a full audit trail of each workflow, you should use a Step Function instead of an AWS Lambda function. So both these options are incorrect.</p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong> - AWS_RISK_CREDENTIALS_EXPOSED event is generated by AWS Health service and NOT CloudTrail, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/health/latest/ug/getting-started-phd.html\">https://docs.aws.amazon.com/health/latest/ug/getting-started-phd.html</a></p>\n"
        }
      },
      {
        "id": 82921356,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A social media company is running its flagship application via an Auto-Scaling group (ASG) which has 15 EC2 instances spanning across 3 Availability Zones (AZs). The current average CPU utilization of the group sits at 15% off-peak time. During peak time, it goes all the way to 45%, and these peak times happen predictably during business hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How can you improve the instance utilization while reducing cost and maintaining application availability?</p>\n",
          "answers": [
            "<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</p>",
            "<p>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</p>",
            "<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</p>",
            "<p>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</strong></p>\n\n<p>With target tracking scaling policies, you choose a scaling metric and set a target value. Application Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value.</p>\n\n<p>Target tracking scaling policies for Amazon EC2 Auto Scaling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size that are specified by the scaling action. For the given use-case, you can create two separate scheduled actions that take care of the required minimum capacity during both peak and off-peak times.</p>\n\n<p>Here, we need a scaling policy that tracks a good CPU usage of 75% and adjusts the minimum desired capacity through scheduled actions so it doesn't disrupt the number of EC2 instances negatively at any time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</strong></p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</strong></p>\n\n<p>If a Lambda function terminates 9 instances because they're in an ASG, the desired capacity won't have changed and the ASG will re-create instances automatically. Therefore both these options are incorrect.</p>\n\n<p><strong>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</strong> - UpdatePolicy for CloudFormation cannot help define Scheduled Actions. There's a special ScheduledActions property for that.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n"
        }
      },
      {
        "id": 75949166,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application sits behind a Network Load Balancer (NLB) that is configured with a TLS listener. The DevOps team must analyze traffic patterns and require information about the connections made by clients. The data that is captured must be stored securely with encryption at rest and should only be accessible to the DevOps team members.</p><p>Which actions should a DevOps engineer take?</p>",
          "answers": [
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing provides access logs that capture detailed information about the TLS requests sent to your Network Load Balancer. You can use these access logs to analyze traffic patterns and troubleshoot issues. The logs are sent to an Amazon S3 bucket you configure as the logging destination. This bucket can be encrypted using one of the available server-side encryption options.</p><p>When you enable access logging, you must specify an S3 bucket for the access logs. The policy must grant permission to the AWS service account \u2018delivery.logs.amazonaws.com\u2019. In this case, the DevOps team also require permissions to access the bucket, and this can be granted through an IAM policy attached to the team members, most likely via an IAM group.</p><p><strong>CORRECT: </strong>\"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account\" is incorrect.</p><p>This will not allow read access for the DevOps team as the only permission is write access.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 67357152,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has hundreds of AWS accounts and has also created an organization in AWS Organizations to manage the accounts. The company wants a dashboard to seamlessly search, visualize, and analyze CloudWatch metrics data, logs data, and traces (from AWS X-Ray) from all the linked accounts into a single security and operations account. The solution should automatically onboard any new AWS accounts created later in the organization.</p>\n\n<p>As a DevOps Engineer, what solution do you suggest to address the given requirements?</p>\n",
          "answers": [
            "<p>Create a CloudWatch alarm for the CloudWatch metrics and trigger an event on Amazon EventBridge. Write the metrics data to the Amazon S3 bucket. Use Amazon Athena to create visualizations and dashboards from CloudWatch metrics data, logs data, and traces</p>",
            "<p>Configure CloudWatch Metric Streams to stream real-time metrics data to Kinesis Data Firehose. Firehose will push the metrics data to Amazon Simple Storage Service (Amazon S3) bucket. configure Amazon Athena to create a dashboard with the metrics data</p>",
            "<p>Use Amazon CloudWatch cross-account observability to set up security and operations account as the monitoring account and link it with rest of the member accounts of the organization using AWS Organizations</p>",
            "<p>Use the Amazon CloudWatch cross-account observability feature from the CloudWatch console to create the monitoring account and connect the individual AWS accounts to the monitoring account</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudWatch cross-account observability to set up a security and operations account as the monitoring account and link it with the rest of the member accounts of the organization using AWS Organizations</strong></p>\n\n<p>With Amazon CloudWatch cross-account observability, you can monitor and troubleshoot applications that span multiple accounts within a Region. Seamlessly search, visualize, and analyze your metrics, logs, and traces in any of the linked accounts without account boundaries.</p>\n\n<p>Set up one or more AWS accounts as monitoring accounts and link them with multiple source accounts. A monitoring account is a central AWS account that can view and interact with observability data generated from source accounts. A source account is an individual AWS account that generates observability data for the resources that reside in it. Source accounts share their observability data with the monitoring account.</p>\n\n<p>The shared observability data can include the following types of telemetry:\n1. Metrics in Amazon CloudWatch\n2. Log groups in Amazon CloudWatch Logs\n3. Traces in AWS X-Ray</p>\n\n<p>There are two options for linking source accounts to your monitoring account. You can use one or both options.\n1. Use AWS Organizations to link accounts in an organization or organizational unit to the monitoring account.\n2. Connect individual AWS accounts to the monitoring account.</p>\n\n<p>AWS recommends that you use Organizations so that new AWS accounts created later in the organization are automatically onboarded to cross-account observability as source accounts. This is our use case requirement and hence choosing Organizations to implement the requirements.</p>\n\n<p>Demonstration of setting up CloudWatch cross-account observability:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the Amazon CloudWatch cross-account observability feature from the CloudWatch console to create the monitoring account and connect the individual AWS accounts to the monitoring account</strong> - While this option is correct, it cannot automatically onboard any new AWS accounts created later in the organization.</p>\n\n<p><strong>Create a CloudWatch alarm for the CloudWatch metrics and trigger an event on Amazon EventBridge. Write the metrics data to the Amazon S3 bucket. Use Amazon Athena to create visualizations and dashboards from CloudWatch metrics data, logs data, and traces</strong> - Amazon EventBridge does not support Amazon S3 bucket as a target. Hence, this option is incorrect.</p>\n\n<p><strong>Configure CloudWatch Metric Streams to stream real-time metrics data to Kinesis Data Firehose. Firehose will push the metrics data to Amazon Simple Storage Service (Amazon S3) bucket. configure Amazon Athena to create a dashboard with the metrics data</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. The use case does not talk about real-time data, so configuring CloudWatch Metric Streams with Firehose is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/cloudwatch-metric-streams-send-aws-metrics-to-partners-and-to-your-apps-in-real-time/\">https://aws.amazon.com/blogs/aws/cloudwatch-metric-streams-send-aws-metrics-to-partners-and-to-your-apps-in-real-time/</a></p>\n"
        }
      },
      {
        "id": 134588367,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A European enterprise has developed a serverless web application hosted on AWS. The application comprises Amazon API Gateway, various AWS Lambda functions, Amazon S3, and an Amazon RDS for MySQL database. The source code consists of AWS Serverless Application Model (AWS SAM) templates and Python code and is stored in GitHub.</p><p>The enterprise's security team recently performed a security audit and discovered that user names and passwords for authentication to the database are hardcoded within GitHub repositories. The DevOps Engineer must implement a solution with the following requirements:</p><p>- Automatically detect and prevent hardcoded secrets.<br>- Automatic secrets rotation should also be implemented.</p><p>What is the MOST secure solution that meets these requirements?</p>",
          "answers": [
            "<p>Integrate Amazon CodeGuru Profiler on the AWS Lambda function by enabling the code profiling feature. Apply the CodeGuru Profiler function decorator <code>@with_lambda_profiler()</code> to your handler function and review the recommendation report manually. Store the secret as a secure string in Parameter Store. Modify the SAM templates and Python code to retrieve the secret from Parameter Store.</p>",
            "<p>Link the GitHub repositories to Amazon CodeGuru Reviewer. Perform a manual evaluation of the code review for any recommendations. Select the option to protect the secret. Revise the SAM templates and Python code to fetch the secret from AWS Secrets Manager.</p>",
            "<p>Integrate Amazon CodeGuru Profiler. Apply the CodeGuru Profiler function decorator <code>@with_lambda_profiler()</code> to your handler function and review the recommendation report manually. Select the option to protect the secret. Modify the SAM templates and Python code to fetch the secret from AWS Secrets Manager.</p>",
            "<p>Link the GitHub repositories to Amazon CodeGuru Reviewer. Perform a manual evaluation of the code review for any recommendations. Store the secret as a string in Parameter Store. Modify the SAM templates and Python code to retrieve the secret from Parameter Store.</p>"
          ],
          "explanation": "<p><strong>Amazon CodeGuru</strong> helps improve code quality and automate code reviews by scanning and profiling your Java and Python applications. <strong>CodeGuru Reviewer</strong> can detect potential defects and bugs in your code. For instance, it recommends improvements regarding security vulnerabilities, resource leaks, concurrency issues, incorrect input validation, and deviation from AWS best practices.</p><p><img src=\"https://media.tutorialsdojo.com/public/codeguru-github-100324.png\"></p><p><strong>Amazon CodeGuru Reviewer Secrets Detector </strong>is an automated tool that helps developers detect secrets in source code or configuration files, such as passwords, API keys, SSH keys, and access tokens. The detectors use machine learning (ML) to identify hardcoded secrets as part of the code review process, ensuring all new code doesn\u2019t contain hardcoded secrets before being merged and deployed. In addition to Java and Python code, secrets detectors scan configuration and documentation files. CodeGuru Reviewer suggests remediation steps to secure secrets with AWS Secrets Manager, a managed service that lets you securely and automatically store, rotate, manage, and retrieve credentials, API keys, and all sorts of secrets.</p><p><img src=\"https://media.tutorialsdojo.com/public/codeguru-detected-secret.png\"></p><p><strong>Secrets Manager</strong> enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise.</p><p>Hence, the correct answer is: <strong>Link the GitHub repositories to Amazon CodeGuru Reviewer. Perform a manual evaluation of the code review for any recommendations. Select the option to protect the secret. Revise the SAM templates and Python code to fetch the secret from AWS Secrets Manager.</strong></p><p>The option that says: <strong>Link the GitHub repositories to Amazon CodeGuru Reviewer. Perform a manual evaluation of the code review for any recommendations. Store the secret as a string in Parameter Store. Modify the SAM templates and Python code to retrieve the secret from Parameter Store</strong> is incorrect. While it utilizes CodeGuru Reviewer, storing a secret as a string in Parameter Store is not considered to be secure. It is recommended to store secrets as a secure string. Furthermore, the Parameter Store typically does not provide automatic secrets rotation. It is recommended to use AWS Secrets Manager instead.</p><p>The option that says: <strong>Integrate Amazon CodeGuru Profiler. Apply the CodeGuru Profiler function decorator &lt;code&gt;@with_lambda_profiler()&lt;/code&gt; to your handler function and review the recommendation report manually. Select the option to protect the secret. Modify the SAM templates and Python code to fetch the secret from AWS Secrets Manage</strong>r is incorrect because CodeGuru Profiler is primarily designed to enhance the performance of production applications and identify the most resource-intensive lines of code. It is not intended to automatically identify hardcoded secrets.</p><p>The option that says: <strong>Integrate Amazon CodeGuru Profiler on the AWS Lambda function by enabling the code profiling feature. Apply the CodeGuru Profiler function decorator &lt;code&gt;@with_lambda_profiler()&lt;/code&gt; to your handler function and review the recommendation report manually. Store the secret as a secure string in Parameter Store. Modify the SAM templates and Python code to retrieve the secret from Parameter Store</strong> is incorrect. Although it stores secrets in Parameter Store as a secure string, it does not fulfill the requirement for automatic secrets rotation. In addition, this option utilizes CodeGuru Profiler instead of CodeGuru Reviewer.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/codeguru-reviewer-secrets-detector-identify-hardcoded-secrets/\">https://aws.amazon.com/blogs/aws/codeguru-reviewer-secrets-detector-identify-hardcoded-secrets/</a></p><p><a href=\"https://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda-command-line.html\">https://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda-command-line.html</a></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p><p><br></p><p><strong>Check out this AWS Secrets Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager/</a></p>"
        }
      },
      {
        "id": 134588435,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading technology company is planning to build a document management portal that will utilize an existing Amazon DynamoDB table. The portal will be launched in Kubernetes and managed via AWS App Runner for easier deployment. The table has an attribute of <code>DocumentName</code> that acts as the partition key and another attribute called <code>Category</code> as its sort key. A DevOps Engineer was instructed to develop a feature that queries the <code>DocumentName</code> attribute yet uses a different sort key other than the existing one. To fetch the latest data, strong read consistency must be used in the database tier.</p><p>Which of the following solutions below should the engineer implement?</p>",
          "answers": [
            "<p>Add a Global Secondary Index which uses the <code>DocumentName</code> attribute and use an alternative sort key as projected attributes.</p>",
            "<p>Set up a new DynamoDB table with a Local Secondary Index that uses the <code>DocumentName</code> attribute with a different sort key. Migrate the data from the existing table to the new table.</p>",
            "<p>Add a Global Secondary Index that uses the <code>DocumentName</code> attribute and a different sort key.</p>",
            "<p>Add a Local Secondary Index that uses the <code>DocumentName</code> attribute and a different sort key.</p>"
          ],
          "explanation": "<p>A <strong><em>local secondary index</em></strong> maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.</p><p>Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to <code>Scan</code> the entire <em>Thread </em>table and discard any posts that were not within the specified time frame. With a local secondary index, a <code>Query</code> operation could use <em>LastPostDateTime</em> as a sort key and find the data quickly.</p><p>To create a Local Secondary Index, make sure that the primary key of the index is the same as the primary key/partition key of the table, just as shown below. Then you must select an alternative sort key that is different from the sort key of the table.</p><p><img src=\"https://media.tutorialsdojo.com/public/Local-Secondary-Index_9AUG2023.png\"></p><p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful. A strongly consistent read might not be available if there is a network delay or outage. Strongly consistent reads are not supported on global secondary indexes.</p><p>The primary key of a local secondary index must be composite (partition key and sort key). A local secondary index lets you query over a single partition, as specified by the partition key value in the query.</p><p>Local secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist.</p><p>Hence, the correct answer is: <strong>Set up a new DynamoDB table with a Local Secondary Index that uses the </strong><code><strong>DocumentName</strong></code><strong> attribute with a different sort key. Migrate the data from the existing table to the new table.</strong></p><p>The option that says: <strong>Add a Global Secondary Index that uses the </strong><code><strong>DocumentName</strong></code><strong> attribute and a different sort key</strong><em> </em>is incorrect. Although it is possible to query data without using a scan command, it is still not enough because GSI does not support strong read consistency which is required in the scenario.</p><p>The option that says: <strong>Add a Global Secondary Index which uses the </strong><code><strong>DocumentName</strong></code><strong> attribute and use an alternative sort key as projected attributes</strong> is incorrect because using a local secondary index is a more appropriate solution to be used in this scenario just as explained above. Moreover, projected attributes are just attributes stored in the index that can be returned by queries and scans performed on the index, hence, these are not useful in satisfying the provided requirement.</p><p>The option that says: <strong>Add a Local Secondary Index that uses the </strong><code><strong>DocumentName</strong></code><strong> attribute and a different sort key</strong> is incorrect. Although it is using the correct type of index, you cannot add a local secondary index to an already existing table.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 115961519,
        "correct_response": [
          "a",
          "e"
        ],
        "source": "Neal Set 5",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer updated the AWS CloudFormation template for an application. The stack update failed and CloudFormation attempted to roll back the stack to its previous state. The roll back process failed and generated a UPDATE_ROLLBACK_FAILED error message.</p><p>What are the most likely causes for this issue? (Select TWO.)</p>",
          "answers": [
            "<p>The user or role that was used to perform the stack update had insufficient permissions.</p>",
            "<p>A change set was not created and executed prior to deploying the updated template to the stack.</p>",
            "<p>An interface VPC endpoint was not operational and CloudFormation could not update resources in the VPC.</p>",
            "<p>Amazon EC2 instances included in the stack were updated recently using the \u2018yum update\u2019 command.</p>",
            "<p>Changes to resources were made outside of CloudFormation and the template was not updated.</p>"
          ],
          "explanation": "<p>This error indicates that a dependent resource can't return to its original state, causing the rollback to fail (UPDATE_ROLLBACK_FAILED state). A dependent resource has likely been changed or cannot be modified. There are several possible causes of this issue which include that a dependent resource has been changed outside of the CloudFormation stack or the resource cannot be modified as the user or role that performed the update has insufficient permissions.</p><p><strong>CORRECT: </strong>\"The user or role that was used to perform the stack update had insufficient permissions\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Changes to resources were made outside of CloudFormation and the template was not updated\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"A change set was not created and executed prior to deploying the updated template to the stack\" is incorrect.</p><p>Change sets are useful but not mandatory. There is no reason that the stack update would fail to roll back because a change set had not been used.</p><p><strong>INCORRECT:</strong> \"An interface VPC endpoint was not operational and CloudFormation could not update resources in the VPC\" is incorrect.</p><p>CloudFormation uses APIs rather than the network, so it is not reliant on interface endpoints to be able to modify resources in a VPC.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 instances included in the stack were updated recently using the \u2018yum update\u2019 command\" is incorrect.</p><p>This command updates the operating system with the latest patches. There is no reason that running this update would have any bearing on CloudFormation\u2019s ability to roll back a failed update.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 67357172,
        "correct_response": [
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company uses AWS CodeDeploy to deploy an AWS Lambda function as the final step of a CI/CD pipeline. The company has developed the Lambda function to handle incoming orders through an order-processing API. The DevOps team has noticed intermittent failures in the API occurring for a brief period after deploying the Lambda function. Upon investigation, the team suspects that the failures are a result of incomplete propagation of database changes before the Lambda function gets invoked.</p>\n\n<p>What measures can help resolve this issue?</p>\n",
          "answers": [
            "<p>Add a <code>BeforeInstall</code> hook to the AppSpec file that tests and waits for any necessary database changes before deploying a new version of the Lambda function</p>",
            "<p>Add a <code>ValidateService</code> hook to the AppSpec file that validates that any necessary database changes are propagated</p>",
            "<p>Add an <code>AfterAllowTestTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes after allowing a test traffic</p>",
            "<p>Add a <code>BeforeAllowTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a <code>BeforeAllowTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function</strong></p>\n\n<p><code>BeforeAllowTraffic</code> hook is used to run tasks before traffic is shifted to the deployed Lambda function version. By using the \"BeforeAllowTraffic\" hook, the DevOps team can ensure that traffic to the new version of the Lambda function is allowed only after necessary database changes have fully propagated, thus avoiding intermittent failures after deployment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a <code>BeforeInstall</code> hook to the AppSpec file that tests and waits for any necessary database changes before deploying a new version of the Lambda function</strong></p>\n\n<p><strong>Add a <code>ValidateService</code> hook to the AppSpec file that validates that any necessary database changes are propagated</strong></p>\n\n<p><strong>Add an <code>AfterAllowTestTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes after allowing test traffic</strong></p>\n\n<p>None of these three lifecycle event hooks are available for an AWS Lambda deployment, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n"
        }
      }
    ],
    "answers": {
      "67357116": [
        "a",
        "c",
        "f"
      ],
      "67357132": [
        "c"
      ],
      "67357152": [
        "c"
      ],
      "67357172": [
        "d"
      ],
      "67357182": [
        "d",
        "e"
      ],
      "75949084": [
        "a"
      ],
      "75949108": [
        "c"
      ],
      "75949142": [
        "c"
      ],
      "75949166": [
        "a"
      ],
      "82921354": [
        "a"
      ],
      "82921356": [
        "a"
      ],
      "82921398": [
        "a"
      ],
      "82921406": [
        "a"
      ],
      "82921448": [
        "a"
      ],
      "82921462": [
        "b",
        "c"
      ],
      "99528209": [
        "a"
      ],
      "115961519": [
        "a",
        "e"
      ],
      "134588367": [
        "b"
      ],
      "134588375": [
        "b"
      ],
      "134588381": [
        "d"
      ],
      "134588395": [
        "c"
      ],
      "134588409": [
        "a"
      ],
      "134588413": [
        "a",
        "e"
      ],
      "134588435": [
        "b"
      ],
      "134588503": [
        "c"
      ],
      "138248105": [
        "d"
      ],
      "138248107": [
        "b"
      ],
      "138248125": [
        "a"
      ],
      "138248157": [
        "a"
      ],
      "143860747": [
        "a"
      ]
    }
  },
  {
    "id": "1770506996258",
    "date": "2026-02-07T23:29:56.258Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 16,
    "incorrect": 4,
    "unanswered": 0,
    "total": 20,
    "percent": 80,
    "duration": 4016573,
    "questions": [
      {
        "id": 82921346,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company is implementing a CodePipeline pipeline in which every push to the CodeCommit master branch gets deployed to development, staging, and production environment consisting of EC2 instances. When deploying to production, traffic should be deployed on a few instances so that metrics can be gathered before a manual approval step is done to deploy to all the instances.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?</p>\n",
          "answers": [
            "<p>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create one CodePipeline and chain up these stages together, introducing a manual approval step after the deployment to the canary instances</p>",
            "<p>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create separate CodePipeline for each deployment group all having the same source being your code repository. Introducing a manual approval step in the pipeline that deploys to production</p>",
            "<p>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create one CodePipeline and chain up these together. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</p>",
            "<p>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create three separate CodePipeline for each deployment group having all the same sources being your code repository. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create one CodePipeline and chain up these stages together, introducing a manual approval step after the deployment to the canary instances</strong></p>\n\n<p>CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>CodeDeploy components overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q9-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html\">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html</a></p>\n\n<p>For the given use-case, you should create four deployment groups for development, staging, canary testing, production and then chain these together using CodePipeline. For EC2, to do a canary deployment, you must create a small deployment group made of few instances from production and deploy to these. Add a manual step after the deployment to the canary testing stage.</p>\n\n<p>Sample workflow for CI/CD with AWS CodeCommit, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q9-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/\">https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/</a></p>\n\n<p>Exam Alert:</p>\n\n<p>The exam may try to trap you on some of the following details on deployment-related processes. Be aware of what's possible.</p>\n\n<ul>\n<li><p>A deployment group contains individually tagged Amazon EC2 instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.</p></li>\n<li><p>Deployments that use the EC2/On-Premises compute platform manage the way in which traffic is directed to instances by using an in-place or blue/green deployment type. During an in-place deployment, CodeDeploy performs a rolling update across Amazon EC2 instances. During a blue/green deployment, the latest application revision is installed on replacement instances.</p></li>\n<li><p>If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only.</p></li>\n<li><p>You CANNOT use canary, linear, or all-at-once configuration for EC2/On-Premises compute platform.</p></li>\n<li><p>You can manage the way in which traffic is shifted to the updated Lambda function versions during deployment by choosing a canary, linear, or all-at-once configuration.</p></li>\n<li><p>You can deploy an Amazon ECS containerized application as a task set. You can manage the way in which traffic is shifted to the updated task set during deployment by choosing a canary, linear, or all-at-once configuration.</p></li>\n<li><p>Amazon ECS blue/green deployments are supported using both CodeDeploy and AWS CloudFormation. For blue/green deployments through AWS CloudFormation, you don't create a CodeDeploy application or deployment group.</p></li>\n<li><p>Your deployable content and the AppSpec file are combined into an archive file (also known as application revision) and then upload it to an Amazon S3 bucket or a GitHub repository. Remember these two locations. AWS Lambda revisions can be stored in Amazon S3 buckets. EC2/On-Premises revisions are stored in Amazon S3 buckets or GitHub repositories.</p></li>\n<li><p>AWS Lambda and Amazon ECS deployments CANNOT use an in-place deployment type.</p></li>\n</ul>\n\n<p>Incorrect options:</p>\n\n<p><strong>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create separate CodePipeline for each deployment group all having the same source being your code repository. Introducing a manual approval step in the pipeline that deploys to production</strong> - Creating separate CodePipelines is not a good idea and won't allow you to create manual approval steps before deploying to production.</p>\n\n<p><strong>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create one CodePipeline and chain up these together. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</strong> - While CodeDeploy does have a Canary Deployment setting, it's only meant for AWS Lambda and ECS platforms and there's no option to pause it manually through an approval step.</p>\n\n<p><strong>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create three separate CodePipeline for each deployment group having all the same sources being your code repository. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</strong> - While CodeDeploy does have a Canary Deployment setting, it's only meant for AWS Lambda and ECS platforms. In addition, creating separate CodePipelines is not a good idea and won't allow you to create manual approval steps before deploying to production.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html\">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/\">https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/</a></p>\n"
        }
      },
      {
        "id": 138248237,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The Development team of a leading IT consultancy company would like to add a manual approval action before their new application versions are deployed to their production environment. The approval action must be strictly enforced even if the unit and integration tests are all successful. They have set up a pipeline using CodePipeline to orchestrate the workflow of their continuous integration and continuous delivery processes. The new versions of the application are built using CodeBuild and are deployed to a fleet of Amazon EC2 instances using CodeDeploy. </p><p>Which of the following provides the SIMPLEST and the MOST cost-effective solution?</p>",
          "answers": [
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</p>",
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>"
          ],
          "explanation": "<p>You can automate your release process by using AWS CodePipeline to test your code and run your builds with CodeBuild. You can create reports in CodeBuild that contain details about tests that are run during builds.</p><p>You can create tests such as unit tests, configuration tests, and functional tests. The test file format can be JUnit XML or Cucumber JSON. Create your test cases with any test framework that can create files in one of those formats (for example, Surefire JUnit plugin, TestNG, and Cucumber). To create a test report, you add a report group name to the buildspec file of a build project with information about your test cases. When you run the build project, the test cases are run and a test report is created. You do not need to create a report group before you run your tests. If you specify a report group name, CodeBuild creates a report group for you when you run your reports. If you want to use a report group that already exists, you specify its ARN in the buildspec file.</p><p>In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p><img src=\"https://media.tutorialsdojo.com/public/pipeline.png\"></p><p>If the action is approved, the pipeline execution resumes. If the action is rejected\u2014or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping\u2014the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p>Hence, the correct answer is: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</strong></p><p>The option that says: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. You can just simply set up a manual approval action instead of creating a custom action. That takes a lot of effort to configure including the development of a custom job worker.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. It is tedious to automatically perform the unit and integration tests using AWS Step Functions. You can just use CodeBuild to handle all of the tests.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. This solution entails an additional burden to install, configure and launch a third-party CI/CD tool in Amazon EC2. A more simple solution is to just use CodeBuild for tests.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p>"
        }
      },
      {
        "id": 67357136,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multi-national company with hundreds of AWS accounts has slowly adopted AWS Organizations with all features enabled. The company has also configured a few Organization Units (OUs) to serve its business objectives. The company has some AWS Identity and Access Management (IAM) roles that need to be configured for every new AWS account created for the company. Also, the security policy mandates enabling AWS CloudTrail for all AWS accounts. The company is looking for an automated solution that can add the mandatory IAM Roles and CloudTrail configurations to all newly created accounts and also delete the resources/configurations when an account leaves the organization without manual intervention.</p>\n\n<p>What should a DevOps engineer do to meet these requirements with the minimal overhead?</p>\n",
          "answers": [
            "<p>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</p>",
            "<p>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</p>",
            "<p>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</p>",
            "<p>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</strong></p>\n\n<p>You can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. For example, you can deploy your centralized AWS Identity and Access Management (IAM) roles, provision Amazon Elastic Compute Cloud (Amazon EC2) instances or AWS Lambda functions across AWS Regions and accounts in your organization. CloudFormation StackSets simplify the configuration of cross-account permissions and allow for the automatic creation and deletion of resources when accounts are joined or removed from your Organization.</p>\n\n<p>You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the AWS Organizations master account to deploy stacks to all accounts in your organization or specific organizational units (OUs). A new service-managed permission model is available with these StackSets. Choosing <code>Service managed permissions</code> allows StackSets to automatically configure the necessary IAM permissions required to deploy your stack to the accounts in your organization.</p>\n\n<p>In addition to setting permissions, CloudFormation StackSets offer the option for automatically creating or removing your CloudFormation stacks when a new AWS account joins or quits your Organization. You do not need to remember to manually connect to the new account to deploy your common infrastructure or to delete infrastructure when an account is removed from your Organization. When an account leaves the organization, the stack will be removed from the management of StackSets. However, you can choose to either delete or retain the resources managed by the stack.</p>\n\n<p>Lastly, you choose whether to deploy a stack to your entire organization or just to one or more Organization Units (OU). You also choose a couple of deployment options: how many accounts will be prepared in parallel, and how many failures you tolerate before stopping the entire deployment.</p>\n\n<p>Use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by an AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</strong> - This option involves using too many services which unnecessarily adds to the complexity and cost of the overall solution. So, this option is incorrect.</p>\n\n<p><strong>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</strong> - While you can use AWS Systems Manager to automate tasks across multiple accounts in AWS Organization, the other details in this option are irrelevant to the given use case.</p>\n\n<p>When you run automation across multiple Regions and accounts, you target resources by using tags or the name of an AWS resource group. The resource group must exist in each target account and Region. The resource group name must be the same in each target account and Region. The automation fails to run on those resources that don't have the specified tag or that aren't included in the specified resource group.</p>\n\n<p><strong>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</strong> - It is possible to enable CloudTrail logging from the management account of AWS Organizations, and is referred to as the organization trail. The use case is to be able to log all Trail events to a commonplace. Also, creating an IAM role in the management account and sharing it across all member accounts is not straightforward and requires manual work. Hence, this option is incorrect for the given use case.</p>\n\n<p>IAM policies usage in AWS Organizations:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>\n"
        }
      },
      {
        "id": 67357104,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A video-sharing application stores its files on an Amazon S3 bucket. During the last year, the user traffic has multiplied by thousands and the company is planning on introducing subscription services for its video sharing application. The company needs the access pattern of the video files to identify the most viewed and downloaded videos.</p>\n\n<p>Which of the following would you identify as the MOST cost-effective solution that can be implemented at the earliest?</p>\n",
          "answers": [
            "<p>Leverage Amazon S3 request metrics from CloudWatch for analysis via an AWS Lambda function. The <code>AWS/S3</code> namespace includes the request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> that can be passed to the Lambda function for analyzing the access pattern of the S3 bucket. The output of the Lambda function can be stored in an S3 bucket and visually analyzed using Amazon QuickSight</p>",
            "<p>Enable server access logging on the S3 bucket. Use Amazon Redshift Spectrum to efficiently query large datasets leveraging the massive parallelism offered by dedicated Amazon Redshift servers</p>",
            "<p>Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena</p>",
            "<p>Enable server access logging on the S3 bucket. Configure file create event notification on Access log S3 bucket to trigger an AWS Lambda function. Configure Lambda to write the data to Kinesis Firehose which then writes the log data to Amazon OpenSearch Service. Use Elasticsearch search and analytics engine for analyzing usage patterns of the S3 access logs</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena</strong></p>\n\n<p>Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.</p>\n\n<p>Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects. If you enable logging on multiple source buckets that identify the same target bucket, the target bucket will have access logs for all those source buckets. However, each log object reports access log records for a specific source bucket.</p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly from Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage and you can start analyzing your data immediately. Athena uses an approach known as schema-on-read, which allows you to project your schema onto your data at the time you execute a query. This eliminates the need for any data loading or ETL.</p>\n\n<p>Amazon Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i1.jpg\">\n<a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p><code>CREATE EXTERNAL TABLE [IF NOT EXISTS]</code> syntax is used to create the table in Athena. EXTERNAL keyword specifies that the table is based on an underlying data file that exists in Amazon S3, in the location that you specify.</p>\n\n<p>You can use queries on Amazon S3 server access logs to identify Amazon S3 object access requests, for operations such as GET, PUT, and DELETE, and discover further information about those requests.</p>\n\n<p>Analysing object access requests using SQL queries in Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon S3 request metrics from CloudWatch for analysis via an AWS Lambda function. The <code>AWS/S3</code> namespace includes the request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> that can be passed to the Lambda function for analyzing the access pattern of the S3 bucket. The output of the Lambda function can be stored in an S3 bucket and visually analyzed using Amazon QuickSight</strong> - The request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> will give the overall count of all Get requests and Bytes downloaded through these requests. It does not help in answering the access pattern questions, such as, which files have been viewed/downloaded the most.</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Use Amazon Redshift Spectrum to efficiently query large datasets leveraging the massive parallelism offered by dedicated Amazon Redshift servers</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. Amazon Redshift Spectrum has a dependency on provisioned Amazon Redshift servers, which represents a cost-intensive solution. Therefore, Amazon Redshift Spectrum is a cost overkill for the simple access pattern analysis that the current use case needs.</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Configure file create event notification on Access log S3 bucket to trigger an AWS Lambda function. Configure Lambda to write the data to Kinesis Firehose which then writes the log data to Amazon OpenSearch Service. Use Elasticsearch search and analytics engine for analyzing usage patterns of the S3 access logs</strong> -  Amazon OpenSearch is a managed service that makes it easier to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. The service provides support for open-source Elasticsearch APIs, managed Kibana, and integration with other AWS services such as Amazon S3 and Amazon Kinesis for loading streaming data into Amazon ES. OpenSearch is well-suited for real-time analysis of logs or clickstreams. This is not a cost-effective solution as it uses AWS Lambda, Kinesis Data Firehose, and Amazon OpenSearch services for the simple access pattern analysis that the current use case needs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html#s3-cloudwatch-metrics\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html#s3-cloudwatch-metrics</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/analyzing-amazon-s3-server-access-logs-using-amazon-opensearch-service/\">https://aws.amazon.com/blogs/big-data/analyzing-amazon-s3-server-access-logs-using-amazon-opensearch-service/</a></p>\n"
        }
      },
      {
        "id": 75949076,
        "correct_response": [
          "a",
          "b",
          "d"
        ],
        "source": "Neal Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company uses a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. Whenever traffic spikes occur, there have been inconsistent errors when the application auto scales. The following error message was generated:</p><p><em>\u201cInstance failed to complete user's Lifecycle Action: Lifecycle Action with token&lt;token-Id&gt; was abandoned: Heartbeat Timeout\u201d.</em></p><p>Which actions should a DevOps engineer take to collect logs for all affected instances and store them for later analysis? (Select THREE.)</p>",
          "answers": [
            "<p>Update the deployment group as the AWS CodeDeploy limits have been reached.</p>",
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create an Amazon EventBridge rule and invoke a Lambda function that uses SSM Run Command to extract the application logs and store them in S3.</p>",
            "<p>Enable Access Logs at the Target Group level and configure an Amazon S3 bucket as the destination.</p>",
            "<p>Analyze the logs directly in the Amazon S3 bucket using Amazon Athena.</p>",
            "<p>Analyze the logs by loading them into an Amazon EMR cluster.</p>",
            "<p>Update the health checks on the Auto Scaling group to use the correct port and protocol for the application.</p>"
          ],
          "explanation": "<p>The lifecycle-action-token is provided by Auto Scaling in the message sent as part of processing the lifecycle hook. You need to get the token from the original message.</p><p>The error message reported usually indicates one of the following:</p><ul><li><p>The maximum number of concurrent AWS CodeDeploy deployments associated with an AWS account was reached.</p></li><li><p>The Auto Scaling group tried to launch too many EC2 instances too quickly. The API calls to RecordLifecycleActionHeartbeat or CompleteLifecycleAction for each new instance were throttled.</p></li><li><p>An application in CodeDeploy was deleted before its associated deployment groups were updated or deleted.</p></li></ul><p>Therefore, the engineer can update the deployment group is limits have been reached and create a solution for extracting the application logs for later analysis. This solution can use Amazon EventBridge, Lambda and SSM Run Command with S3 as the destination.</p><p>Amazon Athena allows for running SQL queries against data in an Amazon S3 bucket. The engineer can then perform analysis to identify there are any application issues that must be fixed.</p><p><strong>CORRECT: \"</strong>Update the deployment group as the AWS CodeDeploy limits have been reached<strong>\" is a correct answer (as explained above.)</strong></p><p><strong>CORRECT: \"</strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create an Amazon EventBridge rule and invoke a Lambda function that uses SSM Run Command to extract the application logs and store them in S3<strong>\" is also a correct answer (as explained above.)</strong></p><p><strong>CORRECT: \"</strong>Analyze the logs directly in the Amazon S3 bucket using Amazon Athena<strong>\" is also a correct answer (as explained above.)</strong></p><p><strong>INCORRECT: \"</strong>Enable Access Logs at the Target Group level and configure an Amazon S3 bucket as the destination<strong>\" is incorrect.</strong></p><p><strong>The access logs will not provide the necessary information to troubleshoot and analyze the issues that are occurring. Access logs record information about the requests from clients.</strong></p><p><strong>INCORRECT: \"</strong>Analyze the logs by loading them into an Amazon EMR cluster<strong>\" is incorrect.</strong></p><p><strong>This won\u2019t be needed as this is not a map reduce use case and data can be analyzed by EMR in Amazon S3.</strong></p><p><strong>INCORRECT: \"</strong>Update the health checks on the Auto Scaling group to use the correct port and protocol for the application<strong>\" is incorrect.</strong></p><p><strong>There is no evidence that health checks are misconfigured from the errors that were generated. Auto scaling must be using the health checks as it is managing to auto scale and bring instances into service.</strong></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-reboot\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-reboot</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 134588381,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has a CI/CD pipeline built in AWS CodePipeline for deployment updates. Part of the deployment process is to perform database schema updates and is performed via AWS CodeBuild. A recent security audit has discovered that the AWS CodeBuild is downloading the database scripts via Amazon S3 in an unauthenticated manner. The security team requires a solution that will enhance the security of the company's CI/CD pipeline.</p><p>What action should the DevOps Engineer take to address the issue in the MOST secure way?</p>",
          "answers": [
            "<p>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>",
            "<p>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>"
          ],
          "explanation": "<p><strong>Service role</strong>: A service role is an AWS Identity and Access Management (IAM) that grants permissions to an AWS service so that the service can access AWS resources. You need an AWS CodeBuild service role so that CodeBuild can interact with dependent AWS services on your behalf. You can create a CodeBuild service role by using the CodeBuild or AWS CodePipeline consoles.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-codebuild-service-role.png\"></p><p>In this scenario, the S3 bucket will be safeguarded from unauthorized access by utilizing a bucket policy. Moreover, CodeBuild leverages the service role for executing S3 actions on your behalf.</p><p>Hence, the correct answer is: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</strong></p><p>The option that says: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI</strong> is incorrect. While the use of IAM access key and secret access key can provide S3 access to CodeBuild, it is not the most secure approach to address the issue.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because an IAM policy alone cannot secure an S3 bucket from unauthorized access. A bucket policy must be used instead. Furthermore, this option uses IAM access key and secret access key, which is not the most secure way.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. It is not used for removing unauthenticated access to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role</a></p><p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>"
        }
      },
      {
        "id": 67357132,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A media application runs on a host of Amazon EC2 instances fronted with an Application Load Balancer (ALB) and Amazon S3 buckets as storage service. For enhanced security, an AWS Web Application Firewall (AWS WAF) has been set up to monitor the requests coming to the ALB. The DevOps team needs to submit a quarterly report on the web requests received by AWS WAF, having detailed information about each web request as well as the details about rules that the request matched. The team has reached out to you for implementing the changes needed for collecting the security data for the coming months.</p>\n\n<p>As DevOps Engineer, how will you implement this requirement?</p>\n",
          "answers": [
            "<p>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</p>",
            "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</p>",
            "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</p>",
            "<p>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data firehose name should start with the prefix <code>aws-waf-logs-</code></p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</strong></p>\n\n<p>You can enable logging AWS WAF web ACL traffic, to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose.</p>\n\n<p>To send your web ACL traffic logs to Amazon S3, you need to set up an Amazon S3 bucket for the logs. When you enable logging for AWS WAF, you provide the bucket ARN. Your web ACLs publish their log files to the Amazon S3 bucket at 5-minute intervals. Each log file contains log records for the traffic recorded in the previous 5 minutes.</p>\n\n<p>The maximum file size for a log file is 75 MB. If the log file reaches the file size limit within the 5-minute period, the log stops adding records to it, publishes it to the Amazon S3 bucket, and then creates a new log file.</p>\n\n<p>Your bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For example, <code>aws-waf-logs-DOC-EXAMPLE-BUCKET-SUFFIX</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</strong> - AWS WAF supports encryption with Amazon S3 buckets for key type Amazon S3 key (SSE-S3) and AWS Key Management Service (SSE-KMS) AWS KMS keys. AWS WAF doesn't support encryption for AWS Key Management Service keys that are managed by AWS.</p>\n\n<p><strong>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as a WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data Firehose name should start with the prefix <code>aws-waf-logs-</code></strong> - AWS recommends that the Kinesis stream should not be used as a source when configuring Data Firehose as a WAF logging destination.</p>\n\n<p><strong>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html</a></p>\n"
        }
      },
      {
        "id": 75949120,
        "correct_response": [
          "c",
          "d"
        ],
        "source": "Neal Set 3",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>An application running on an Amazon EC2 instance stores sensitive data on an attached Amazon EBS volume. The volume is not encrypted. A DevOps engineer must enable encryption at rest for the data.</p><p>Which actions should the engineer take? (Select TWO.)</p>",
          "answers": [
            "<p>Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume.</p>",
            "<p>Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data.</p>",
            "<p>Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted.</p>",
            "<p>Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume.</p>",
            "<p>Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume.</p>"
          ],
          "explanation": "<p>You cannot enable encryption for an existing EBS volume. You must enable encryption of the volume at creation time. There are a couple of ways to enable encryption of data stored on an unencrypted volume:</p><p>1) Create a snapshot of the volume. The snapshot will also be unencrypted, but you can then copy it and enable encryption. Then, you can create an encrypted volume from the snapshot and attach it to the instance.</p><p>2) Create and mount a new, encrypted EBS volume. The engineer would then need to move data onto the volume.</p><p>In both cases the engineer will need to update the application to use the new volume.</p><p><strong>CORRECT: </strong>\"Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume\" is incorrect.</p><p>This has not resulted in any change to the EBS volume, it is still unencrypted.</p><p><strong>INCORRECT:</strong> \"Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data\" is incorrect.</p><p>You cannot enable encryption for existing volumes through any AWS tools.</p><p><strong>INCORRECT:</strong> \"Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume\" is incorrect.</p><p>SSL/TLS certificates are used for enabling encryption in-transit, not encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>"
        }
      },
      {
        "id": 138248119,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has an Amazon ECS cluster with Service Auto Scaling which consists of multiple Amazon EC2 instances that runs a Docker-based application. The development team always pushes a new image to a private Docker container registry whenever they publish a new version of their application. They stop and start all of the tasks to ensure that the containers have the latest version of the application. However, the team noticed that the new tasks are occasionally running the old image of the application.</p><p>As the DevOps engineer, what should you do to fix this issue?</p>",
          "answers": [
            "Restart the ECS agent.",
            "<p>Ensure that the <code>latest</code> tag is being used in the Docker image of the task definition.</p>",
            "<p>Configure the task definition to use the <code>repository-url/image@digest</code> format then manually update the SHA-256 digest of the image. </p>",
            "Migrate your repository from your private Docker container registry to Amazon ECR."
          ],
          "explanation": "<p>You can update a running service to change the number of tasks that are maintained by a service, which task definition is used by the tasks, or if your tasks are using the Fargate launch type, you can change the platform version your service uses. If you have an application that needs more capacity, you can scale up your service. If you have unused capacity to scale down, you can reduce the number of desired tasks in your service and free up resources. If you have updated the Docker image of your application, you can create a new task definition with that image and deploy it to your service.</p><p>If your updated Docker image uses the same tag as what is in the existing task definition for your service (for example, <code>my_image:latest</code>), you do not need to create a new revision of your task definition. You can update the service using the procedure below, keep the current settings for your service, and select Force new deployment. The new tasks launched by the deployment pull the current image/tag combination from your repository when they start.</p><p>The service scheduler uses the minimum healthy percent and maximum percent parameters (in the deployment configuration for the service) to determine the deployment strategy. When a new task starts, the Amazon ECS container agent pulls the latest version of the specified image and tag for the container to use. However, subsequent updates to a repository image are not propagated to already running tasks.</p><p><img src=\"https://media.tutorialsdojo.com/public/overview-fargate.png\"></p><p>To have your service use a newly updated Docker image with the same tag as in the existing task definition (for example, <code>my_image:latest</code>) or keep the current settings for your service, select Force new deployment. The new tasks launched by the deployment pull the current image/tag combination from your repository when they start. The Force new deployment option is also used when updating a Fargate task to use a more current platform version when you specify <code>LATEST</code>. For example, if you specified <code>LATEST</code> and your running tasks are using the <code>1.0.0</code> platform version and you want them to relaunch using a newer platform version.</p><p>To verify that the container agent is running on your container instance, run the following command:</p><p><code>sudo systemctl status ecs</code></p><p>If the command output doesn't show the service as active, run the following command to restart the service:</p><p><code>sudo systemctl restart ecs</code></p><p>It is mentioned in the scenario that the new tasks are occasionally running the old image of the application. The ECS cluster is also using Service Auto Scaling that automatically launches new tasks based on demand. We can conclude that the root cause is not in the task definition since this issue only occurs occasionally, and the other tasks were properly updated. If the ECS task is still running an old image, then it is possible that the ECS agent is not running properly.</p><p>Hence, the correct answer is: <strong>Restart the ECS agent.</strong></p><p>The option that says: <strong>Ensuring that the </strong><code><strong>latest</strong></code><strong> tag is being used in the Docker image of the task definition</strong> is incorrect. Although this will release you from the burden of constantly updating your task definition, this is still not the solution for the issue. Remember that there are other tasks that were successfully updated, which means that the image tag is not the root cause.</p><p>The option that says: <strong>Configuring the task definition to use the </strong><code><strong>repository-url/image@digest</strong></code><strong> format then manually updating the SHA-256 digest of the image</strong> is incorrect because this will just explicitly fetch the exact Docker image from the registry based on the provided SHA-256 digest and not based on its tag (e.g., latest, 1.0.0). In addition, it is stated in the scenario that the issue only occurs occasionally, which means that the other tasks are updated correctly. Thus, it suggests that the issue has nothing to do with the task definition but more with the ECS agent.</p><p>The option that says: <strong>Migrating your repository from your private Docker container registry to Amazon ECR</strong> is incorrect because the issue will be the same even if you used Amazon ECR if the ECS agent is not running properly in one of the instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ecs-agent-disconnected-linux2-ami/\">https://aws.amazon.com/premiumsupport/knowledge-center/ecs-agent-disconnected-linux2-ami/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/update-service.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/update-service.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 75949152,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps team is assisting with the deployment of a web application's infrastructure using AWS CloudFormation. The database management team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. The software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to maintain. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.</p>",
            "<p>Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.</p>",
            "<p>Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.</p>",
            "<p>Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack.</p>"
          ],
          "explanation": "<p>A nested stack is a stack that you create within another stack by using the AWS::CloudFormation::Stack resource. With nested stacks, you deploy and manage all resources from a single stack. You can use outputs from one stack in the nested stack group as inputs to another stack in the group. This differs from exporting values.</p><p>If you want to isolate information sharing to within a nested stack group, AWS suggests that you use nested stacks. To share information with other stacks (not just within the group of nested stacks), you should export values instead.</p><p>Change sets for nested stacks affect the entire stack hierarchy which does not meet the requirements as each team requires resource-level change-set reviews. Therefore, in this scenario it is better to export values rather than use a nested stack.</p><p><strong>CORRECT: </strong>\"Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks\" is incorrect.</p><p>As described above, to meet the requirements an export of values is preferable to using nested stacks in this scenario.</p><p><strong>INCORRECT:</strong> \"Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks\" is incorrect.</p><p>Stack sets are used for extending the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions.</p><p><strong>INCORRECT:</strong> \"Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack\" is incorrect.</p><p>Values should be exported as per the correct answer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html#output-vs-nested\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html#output-vs-nested</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 82921458,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A Big Data analytics company is operating a distributed Cassandra cluster on EC2. Each instance in the cluster must have a list of all the other instance's IP to function correctly, store in a configuration file. As a Devops Engineer at the company, you would like this solution to adapt automatically when newer EC2 instances join the cluster, or when some EC2 instances are terminated.</p>\n\n<p>Which of the following solutions would you recommend for the given requirement?</p>\n",
          "answers": [
            "<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</p>",
            "<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</p>",
            "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</p>",
            "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</strong></p>\n\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n\n<p>A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p>\n\n<p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer's instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes.</p>\n\n<p>The lifecycle hook that is called on ALL instances, whenever an instance comes up or another one goes down, is the <code>configure</code> hook. So this option is the best fit for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</strong> - As mentioned in the explanation above, the <code>setup</code> hook is only used when an instance is first created, so this option is incorrect.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</strong> - Lifecycle hooks on Auto Scaling Groups may seem like a good idea at first, but using AWS Lambda, the solution is not practicable as SSH'ing into the instance via Lambda will not work.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</strong> - EC2 user-data scripts are only triggered on an instance's first launch, so this option just acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/opsworks/\">https://aws.amazon.com/opsworks/</a></p>\n"
        }
      },
      {
        "id": 82921316,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>You are working as a DevOps Engineer at an e-commerce company and have a deployed a Node.js application on Elastic Beanstalk. You would like to track error rates and specifically, you need to ensure by looking at the application log, that you do not have more than 100 errors in a 5 minutes interval. In case you are getting too many errors, you would like to be alerted via email.</p>\n\n<p>Which of the following options represents the most efficient solution in your opinion?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</strong></p>\n\n<p>You can search and filter the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created.</p>\n\n<p>You can use metric filters to search for and match terms, phrases, or values in your log events. When a metric filter finds one of the terms, phrases, or values in your log events, you can increment the value of a CloudWatch metric. For example, you can create a metric filter to search for and count the occurrence of the word ERROR in your log events.</p>\n\n<p>CloudWatch Logs Metric Filter Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p>For the given use-case, you can have Beanstalk send the logs to CloudWatch Logs, and then create a metric filter. This will create a metric for us (and not an alarm), and on top of the metric, you can create a CloudWatch Alarm. This alarm will send a notification to SNS, which will, in turn, send us emails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - You cannot directly set a CloudWatch Alarm as a target for a CloudWatch Logs Metric Filter. You will first need to create a metric filter which can then be used to create a CloudWatch metric to be eventually used in a CloudWatch Alarm.</p>\n\n<p><strong>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</strong> - The Elastic Beanstalk Health Metrics will not track the errors sent out to a log file, so it does not meet the requirements of the use-case. Besides, CloudWatch alarm cannot be used to work on top of the Elastic Beanstalk Health Metrics.</p>\n\n<p><strong>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - Implementing custom logic in your Node.js application may seem like a good idea, but then you have to remember that your application can be distributed amongst many servers with Beanstalk, and as such it will not be possible to track the \"100 errors\" across all instances using this methodology.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p>\n"
        }
      },
      {
        "id": 67357098,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has configured AWS Organizations to manage its multiple AWS accounts. The company uses Amazon Elastic File System (Amazon EFS) as a shared storage service, configured in AWS Account A of the company. To implement a serverless architecture, the company has decided to move its applications to AWS Lambda. The Lambda functions will be managed through another AWS account (Account B). All the Lambda functions will be deployed in a VPC. A DevOps team needs help to continue using Amazon EFS in Account A with the Lambda function in Account B.</p>\n\n<p>How will you reconfigure the existing EFS file system for use with AWS Lambda function? (Select two)</p>\n",
          "answers": [
            "<p>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</p>",
            "<p>Update the Lambda execution roles with permission to access the VPC and the EFS file system</p>",
            "<p>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</p>",
            "<p>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</p>",
            "<p>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region, or cross-AZ connectivity between EFS and Lambda</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</strong></p>\n\n<p><strong>Update the Lambda execution roles with permission to access the VPC and the EFS file system</strong></p>\n\n<p>You can configure a function to mount an Amazon Elastic File System (Amazon EFS) file system to a local directory. With Amazon EFS, your function code can access and modify shared resources safely and at high concurrency.</p>\n\n<p>Execution role and user permissions: If the file system doesn't have a user-configured AWS Identity and Access Management (IAM) policy, EFS uses a default policy that grants full access to any client that can connect to the file system using a file system mount target. If the file system has a user-configured IAM policy, your function's execution role must have the correct <code>elasticfilesystem</code> permissions.</p>\n\n<p>Configuring a file system and access point: To connect an EFS file system with a Lambda function, you use an EFS access point, an application-specific entry point into an EFS file system that includes the operating system user and group to use when accessing the file system, file system permissions, and can limit access to a specific path in the file system. This helps keep file system configuration decoupled from the application code.</p>\n\n<p>You can access the same EFS file system from multiple functions, using the same or different access points. For example, using different EFS access points, each Lambda function can access different paths in a file system, or use different file system permissions.</p>\n\n<p>Connecting to a file system: A function connects to a file system over the local network in a VPC. The subnets that your function connects to can be the same subnets that contain mount points for your file system, or subnets in the same Availability Zone that can route NFS traffic (port 2049) to the file system. To mount an EFS file system, your Lambda functions must be connected to an Amazon Virtual Private Cloud (Amazon VPC) that can reach the EFS mount targets.</p>\n\n<p>A Lambda function in one account can mount a file system in a different account. For this scenario, you configure VPC peering between the function VPC and the file system VPC.</p>\n\n<p>An example showcasing the use of EFS with AWS Lambda:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q8-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/\">https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</strong> - A VPC peering connection is needed since EFS and AWS Lambda are in different AWS accounts. SCPs alone are not sufficient to grant permissions to the accounts in your organization. You should note that no permissions are granted by an SCP. The Lambda execution role will need explicit permissions for access to EFS.</p>\n\n<p><strong>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region or cross-AZ connectivity between EFS and Lambda</strong> - This statement is incorrect. AWS does not support cross-region, or cross AZ connectivity between EFS and Lambda.</p>\n\n<p><strong>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</strong> - It is mentioned in the use case that EFS and AWS Lambda will remain in their respective accounts. Hence, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html</a></p>\n"
        }
      },
      {
        "id": 138248197,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has recently developed a serverless application that is composed of several AWS Lambda functions and an Amazon DynamoDB database. For the CI/CD process, a continuous deployment pipeline has been built using GitHub, AWS CodeBuild, and AWS CodePipeline. The source, build, test, and deployment stages of the pipeline have also been configured. However, upon review, the Lead DevOps engineer requested improvements to the current pipeline configuration to mitigate the risk of failed deployments. The deployment stage should release the new application version to a small subset of users only for verification before fully releasing the change to all users. The pipeline's deployment stage must be modified to meet this requirement.</p><p>Which of the following is the MOST suitable setup to implement?</p>",
          "answers": [
            "<p>Publish a new version on the serverless application using AWS CloudFormation. Set up a manual approval action in CodePipeline in order to verify and approve the version that will be deployed. Once the change has been verified, invoke the Lambda functions to use the production alias using CodePipeline.</p>",
            "<p>Develop a custom script that uses AWS CLI to update the Lambda functions. Integrate the script in CodeBuild that will automatically publish the new version of the application by switching to the production alias.</p>",
            "<p>Define and publish the new version on the serverless application using AWS CloudFormation. Deploy the new version of the Lambda functions with AWS CodeDeploy using the <code>CodeDeployDefault.LambdaCanary10Percent5Minutes</code> predefined deployment configuration.</p>",
            "<p>Use AWS CloudFormation to define and publish the new version on the serverless application. Deploy the new version of the Lambda functions with AWS CodeDeploy using the <code>CodeDeployDefault.LambdaAllAtOnce</code> predefined deployment configuration.</p>"
          ],
          "explanation": "<p><strong>CodeDeploy</strong> is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application.</p><p><img src=\"https://media.tutorialsdojo.com/aws-codedeploy-tutorialsdojo.png\"></p><p>There are three ways traffic can shift during a deployment:</p><p><strong>Canary</strong>: Traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p><p><strong>Linear</strong>: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment.</p><p><strong>All-at-once</strong>: All traffic is shifted from the original Lambda function to the updated Lambda function version all at once.</p><p>The following table lists the predefined configurations available for AWS Lambda deployments.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-12-09_04-19-29-f0f93dd43b7a0d047a3ca8aae4dd728b.png\"></p><p>Hence, the correct answer is: <strong>Define and publish the new version on the serverless application using AWS CloudFormation. Deploy the new version of the Lambda functions with AWS CodeDeploy using the </strong><code><strong>CodeDeployDefault.LambdaCanary10Percent5Minutes</strong></code><strong> predefined deployment configuration.</strong></p><p>The option that says: <strong>Publish a new version on the serverless application using AWS CloudFormation. Set up a manual approval action in CodePipeline in order to verify and approve the version that will be deployed. Once the change has been verified, invoke the Lambda functions to use the production alias using CodePipeline</strong> is incorrect. Although this setup allows you to verify the new version of the serverless application before the production deployment, it still fails to meet the requirement of deploying the change to a small subset of users only. This deployment configuration will release the new version to all users.</p><p>The option that says: <strong>Develop a custom script that uses AWS CLI to update the Lambda functions. Integrate the script in CodeBuild that will automatically publish the new version of the application by switching to the production alias </strong>is incorrect because developing a custom script to update the Lambda functions is not necessary. Moreover, AWS CodeBuild is simply not capable of publishing new versions of your Lambda functions. You should use AWS CodeDeploy instead.</p><p>The option that says: <strong>Use AWS CloudFormation to define and publish the new version on the serverless application. Deploy the new version of the Lambda functions with AWS CodeDeploy using the </strong><code><strong>CodeDeployDefault.LambdaAllAtOnce</strong></code><strong> predefined deployment configuration </strong>is incorrect because this configuration will deploy the changes to all users immediately. Keep in mind that the scenario requires that the change should only be deployed to a small subset of users only. You have to use a Canary deployment instead or the <code><strong><em>CodeDeployDefault.LambdaCanary10Percent5Minutes</em></strong></code><strong><em> </em></strong>configuration.<br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps-lambda.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 134588469,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An international tours and travel company is planning to launch a multi-tier Node.js web portal with a MySQL database to AWS. The portal must be highly available during the deployment of new portal versions in the future and have the ability to roll back the changes if necessary, to improve user experience. There are third-party applications that will also use the same MySQL database that the portal is using. The architecture should allow the IT Operations team to centrally view all the server logs from various EC2 instances and store the data for three months. It should also have a feature that allows the team to search and filter server logs in near-real-time for monitoring purposes. The solution should be cost-effective and preferably has less operational overhead. </p><p>As a DevOps Engineer, which of the following is the BEST solution that you should implement to satisfy the above requirements?</p>",
          "answers": [
            "<p>Using AWS Elastic Beanstalk, host the multi-tier Node.js web portal in a load-balancing and autoscaling environment. Set up an Amazon RDS MySQL instance with a Multi-AZ deployments configuration within the Elastic Beanstalk stack. Set the log options to stream the application logs to Amazon CloudWatch Logs with 90-day data retention.</p>",
            "<p>Host the multi-tier Node.js web portal in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Set up an Amazon RDS MySQL database instance. Configure the CloudWatch Log agent to send the application logs to Amazon CloudWatch Logs with 90-day data retention.</p>",
            "<p>Host the multi-tier Node.js web portal in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Set up an Amazon RDS MySQL database instance. For log monitoring and analysis, configure CloudWatch Logs to fetch the application logs from the instances and send them to an Amazon ES cluster. Purge the contents of the Amazon ES domain every 90 days and then recreate it again.</p>",
            "<p>Using AWS Elastic Beanstalk, host the multi-tier Node.js web portal in a load-balancing and autoscaling environment. Set up an Amazon RDS MySQL database with a Multi-AZ deployments configuration that is decoupled from the Elastic Beanstalk stack. Set the log options to stream the application logs to Amazon CloudWatch Logs with 90-day retention.</p>"
          ],
          "explanation": "<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p><p>Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as Amazon EC2 instances, to run your application.</p><p>You can interact with Elastic Beanstalk by using the AWS Management Console, the AWS Command Line Interface (AWS CLI), or <strong>eb</strong>, a high-level CLI designed specifically for Elastic Beanstalk.</p><p><img src=\"https://media.tutorialsdojo.com/public/aeb-architecture2.png\"></p><p>An Amazon RDS instance attached to an Elastic Beanstalk environment is ideal for development and testing environments. However, it's not ideal for production environments because the lifecycle of the database instance is tied to the lifecycle of your application environment. If you terminate the environment, then you will lose your data because the Amazon RDS instance is deleted by the environment.</p><p>Hence, the correct answer is: <strong>Using AWS Elastic Beanstalk, host the multi-tier Node.js web portal in a load-balancing and autoscaling environment. Set up an Amazon RDS MySQL database with a Multi-AZ deployments configuration that is decoupled from the Elastic Beanstalk stack. Set the log options to stream the application logs to Amazon CloudWatch Logs with a 90-day retention.</strong></p><p>The option that says: <strong>Using AWS Elastic Beanstalk, host the multi-tier Node.js web portal in a load-balancing and autoscaling environment. Set up an Amazon RDS MySQL instance with a Multi-AZ deployments configuration within the Elastic Beanstalk stack. Set the log options to stream the application logs to Amazon CloudWatch Logs with 90-day data retention</strong> is incorrect because it's not ideal to place the database within the Elastic Beanstalk environment because the lifecycle of the database instance is tied to the lifecycle of your application environment in production.</p><p>The option that says: <strong>Host the multi-tier Node.js web portal in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Set up an Amazon RDS MySQL database instance. Configure the CloudWatch Log agent to send the application logs to Amazon CloudWatch Logs with 90-day data retention</strong> is incorrect because it is easier to upload the web portal to Elastic Beanstalk instead. Moreover, you have to enable Multi-AZ deployments in RDS to improve the availability of your database tier.</p><p>The option that says: <strong>Host the multi-tier Node.js web portal in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Set up an Amazon RDS MySQL database instance. For log monitoring and analysis, configure CloudWatch Logs to fetch the application logs from the instances and send them to an Amazon ES cluster. Purge the contents of the Amazon ES domain every 90 days and then recreate it again</strong> is incorrect. Although this solution may work, it entails a lot of operational overhead to maintain the Amazon ES cluster.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 82921370,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An e-commerce company is managing its entire application stack and infrastructure using AWS OpsWorks Stacks. The DevOps team at the company has noticed that a lot of instances have been automatically replaced in the stack and the team would henceforth like to be notified via Slack notifications when these events happen.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you implement to meet this requirement?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-healing</code>. Target a Lambda function that will send notifications out to the Slack channel</p>",
            "<p>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Target a Lambda function that will send notifications out to the Slack channel</p>",
            "<p>Subscribe your OpsWorks auto-healing notifications to an SNS topic. Subscribe a Lambda function that will send notifications out to the Slack channel</p>",
            "<p>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Enable the CloudWatch Event Slack integration for sending out the notifications</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-healing</code>. Target a Lambda function that will send notifications out to the Slack channel</strong></p>\n\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n\n<p>A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p>\n\n<p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer's instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes.</p>\n\n<p>OpsWorks Stacks provides an integrated management experience that spans the entire application lifecycle including resource provisioning, EBS volume setup, configuration management, application deployment, monitoring, and access control. You can send state changes in OpsWorks Stacks, such as instance stopped or deployment failed, to CloudWatch Events. The initiated_by field is only populated when the instance is in the requested, terminating, or stopping states. The initiated_by field can contain one of the following values.</p>\n\n<p>user - A user requested the instance state change by using either the API or AWS Management Console.</p>\n\n<p>auto-scaling - The AWS OpsWorks Stacks automatic scaling feature initiated the instance state change.</p>\n\n<p>auto-healing - The AWS OpsWorks Stacks automatic healing feature initiated the instance state change.</p>\n\n<p>For the given use-case, you need to use CloudWatch Events, and the value of <code>initiated_by</code> must be <code>auto-healing</code>. CloudWatch Events does not have a Slack integration, so you need to configure a Lambda function as the target for the CloudWatch rule which would in turn send the Slack notification.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q53-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Target a Lambda function that will send notifications out to the Slack channel</strong> -  This option is incorrect as <code>auto-scaling</code> is just a supported value but it's not meant for the healing events, instead, it is used for the scaling events.</p>\n\n<p><strong>Subscribe your OpsWorks auto-healing notifications to an SNS topic. Subscribe a Lambda function that will send notifications out to the Slack channel</strong> - Opsworks does not send notifications to SNS directly for auto-healing, so this option is incorrect.</p>\n\n<p><strong>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Enable the CloudWatch Event Slack integration for sending out the notifications</strong> - <code>auto-scaling</code> is just a supported value but it's not meant for the healing events, instead, it is used for the scaling events. Besides, CloudWatch Events does not have a direct integration with Slack , so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html</a></p>\n"
        }
      },
      {
        "id": 82921324,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>Your application is deployed on Elastic Beanstalk and you manage the configuration of the stack using a CloudFormation template. A new golden AMI is created every week and contains a hardened AMI that has all the necessary recent security patches. You have deployed over 100 applications using CloudFormation &amp; Beanstalk this way and you would like to ensure the newer AMI used for EC2 instances is updated every week. There are no standardization or naming conventions made across all the CloudFormation templates.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution for this requirement?</p>\n",
          "answers": [
            "<p>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter that points to the SSM Parameter Store, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</p>",
            "<p>Store the Golden AMI id in an S3 object. Create a CloudFormation parameter that points to the S3 object, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</p>",
            "<p>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter of type String, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should fetch the parameter from the SSM Parameter Store and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</p>",
            "<p>Store the Golden AMI id in an S3 object. Create a CloudFormation mapping to contain the last value of the Golden AMI id. That mapping is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should update the mapping section of every CloudFormation template using a YAML parser, upload the new templates to S3 and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter that points to the SSM Parameter Store, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</strong></p>\n\n<p>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values.</p>\n\n<p>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation. When you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation using the <code>UpdateStack</code> API.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q17-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p>\n\n<p>This question is a hard one as many solutions are feasible with a degree of complexity. It's about identifying the simplest solution.</p>\n\n<p>For the given use-case, by having the CloudFormation parameters directly pointing at SSM Parameter Store, on any refresh made to the CloudFormation template by the Lambda function which is in turn triggered by CloudWatch events, the template itself will fetch the latest value from the SSM Parameter Store and will apply it accordingly. So this solution is the best fit for the given requirement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the Golden AMI id in an S3 object. Create a CloudFormation parameter that points to the S3 object, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</strong> - Storing the AMI id in S3 is possible, but CloudFormation cannot source parameters from S3 and therefore there's no integration possible.</p>\n\n<p><strong>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter of type String, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should fetch the parameter from the SSM Parameter Store and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</strong> - Having a Lambda function fetch the parameter and pass it as a parameter to CloudFormation seems like a good idea, but if we remember the constraint that the parameters are not standardized and that there are no naming conventions, it is difficult for us to imagine a solution that would scale.</p>\n\n<p><strong>Store the Golden AMI id in an S3 object. Create a CloudFormation mapping to contain the last value of the Golden AMI id. That mapping is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should update the mapping section of every CloudFormation template using a YAML parser, upload the new templates to S3 and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</strong> - Creating a Lambda function that would update the mapping section of each template would introduce changes to each template content at every update and would be highly complicated to implement. Additionally, the Lambda function would be hard to write and would have a lot of complexity in updating the mapping as there's no standardization.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ssm-parameter.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ssm-parameter.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p>\n"
        }
      },
      {
        "id": 75949138,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
            "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
            "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
            "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
            "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
            "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"
          ],
          "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 134588513,
        "correct_response": [
          "a",
          "e"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An organization has a fleet of Amazon EC2 instances and uses SSH for remote access. Whenever a DevOps Engineer leaves the organization, the SSH keys are rotated as a security measure. The Chief Information Officer (CIO) required to stop the use of EC2 key pairs for operational efficiency and instead utilize AWS Systems Manager Session Manager. To strengthen security, access to Session Manager should be limited solely through a private network.</p><p>Which set of actions should the new DevOps Engineer take to meet the CIO requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Create an IAM instance profile to be associated with the fleet of EC2 instances and attach an IAM policy with the required Systems Manager permissions.</p>",
            "<p>Enable incoming traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances.</p>",
            "<p>Launch a new EC2 instance that will function as a bastion host for the other EC2 instances in the fleet.</p>",
            "<p>Enable outbound traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances.</p>",
            "<p>Provision a VPC endpoint for Systems Manager in the designated region.</p>"
          ],
          "explanation": "<p>With <strong>Session Manager</strong>, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs).</p><p>By default, AWS Systems Manager doesn't have permission to perform actions on your instances. You must grant access by using AWS Identity and Access Management (IAM).</p><p>An <strong>instance profile</strong> that contains the AWS managed policy <strong>AmazonSSMManagedInstanceCore </strong>is needed to be attached to the EC2 instance for the <strong>Session Manager</strong> to work.</p><p><img src=\" https://media.tutorialsdojo.com/public/dop-c02-ssm-session-manager.png\"></p><p>The security posture of managed instances (including those in a hybrid environment) can be improved by configuring AWS Systems Manager to use an interface <strong>VPC endpoint</strong> in Amazon Virtual Private Cloud (Amazon VPC). An interface VPC endpoint (interface endpoint) can be used to connect to services powered by AWS PrivateLink, which is a technology that enables <strong>private access to Amazon Elastic Compute Cloud (Amazon EC2) and Systems Manager</strong> APIs using private IP addresses.</p><p>Hence, the correct answers are the option that says:</p><p>- <strong>Create an IAM instance profile to be associated with the fleet of EC2 instances and attach an IAM policy with the required Systems Manager permissions</strong></p><p><strong>- Provision a VPC endpoint for Systems Manager in the designated region</strong></p><p>The option that says: <strong>Enable incoming traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances </strong>is incorrect because Session Manager will work without the need to open inbound ports.</p><p>The option that says: <strong>Launch a new EC2 instance that will function as a bastion host for the other EC2 instances in the fleet </strong>is incorrect because a bastion host will not meet the requirement as it will still use SSH key pairs to remote access.</p><p>The option that says: <strong>Enable outbound traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances </strong>is incorrect because Session Manager will work without the need to open outbound ports.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-instance-profile.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-instance-profile.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 134588501,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The company operates a fleet of 400 Amazon EC2 instances to support a High-Performance Computing (HPC) application. The fleet is configured with target tracking scaling and is managed behind an Application Load Balancer (ALB). There is a requirement to create a simple web page hosted on an Amazon S3 bucket that displays the status of the EC2 instances in the fleet. This web page must be updated whenever a new instance is launched or terminated. Additionally, a searchable log of these events is required for review at a later time.</p><p>Which of the following options should be implemented to meet these requirements?</p>",
          "answers": [
            "<p>Create an Amazon EventBridge rule for the scale-in/scale-out event and deliver these logs to Amazon CloudWatch Logs. Manually export the CloudWatch Log group to the S3 bucket to view them.</p>",
            "<p>Write your AWS Lambda function to update the simple webpage on S3 and send event logs to CloudWatch Logs. Create an EventBridge rule to invoke the Lambda for scale-in/scale-out events.</p>",
            "<p>Create an EventBridge rule for the scale-in/scale-out event and create two targets for the event - one target to the webpage S3 bucket and another to deliver event logs to CloudWatch Logs.</p>",
            "<p>No need to do anything. AWS CloudTrail records these events and you can view and search the logs on the console.</p>"
          ],
          "explanation": "<p>You can create an <strong>AWS Lambda function</strong> that logs the changes in state for an Amazon EC2 instance. You can create a rule that runs the function whenever there is a state transition or a transition to one or more states that are of interest. Be sure to assign proper permissions to your Lambda function to write to S3 and to send logs to CloudWatch Logs. After creating the Lambda function, create a rule on Amazon EventBridge that will watch for the scale-in/scale-out events. Set a trigger to run your Lambda function which will then update the S3 webpage and send logs to CloudWatch Logs.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-event-pattern-27Jan2025.png\"></p><p>Hence, the correct answer is: <strong>Write your AWS Lambda function to update the simple webpage on S3 and send event logs to CloudWatch Logs. Create an EventBridge rule to invoke the Lambda for scale-in/scale-out events.</strong></p><p>The option that says: <strong>Create an Amazon EventBridge rule for the scale-in/scale-out event and deliver these logs to Amazon CloudWatch Logs. Manually export the CloudWatch Log group to the S3 bucket to view them </strong>is incorrect because you have to export the logs manually on an S3 bucket. This action is not recommended since you can simply use a Lambda function to log the changes of the EC2 instances to a file in the S3 bucket.</p><p>The option that says: <strong>Create an EventBridge rule for the scale-in/scale-out event and create two targets for the event - one target to the webpage S3 bucket and another to deliver event logs to CloudWatch Logs </strong>is incorrect because you cannot set an S3 bucket or object as a target for an EventBridge rule.</p><p>The option that says: <strong>No need to do anything. AWS CloudTrail records these events and you can view and search the logs on the console </strong>is incorrect. Although this is true, it would be hard to search all the relevant logs from the trail. Moreover, it typically doesn\u2019t provide a solution for the required S3 web page.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Tutorials.html \">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Tutorials.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      }
    ],
    "answers": {
      "67357098": [
        "a",
        "b"
      ],
      "67357104": [
        "c"
      ],
      "67357132": [
        "c"
      ],
      "67357136": [
        "b"
      ],
      "75949076": [
        "a",
        "b",
        "d"
      ],
      "75949120": [
        "c",
        "d"
      ],
      "75949138": [
        "b",
        "c",
        "d"
      ],
      "75949152": [
        "c"
      ],
      "82921316": [
        "a"
      ],
      "82921324": [
        "a"
      ],
      "82921346": [
        "a"
      ],
      "82921370": [
        "a"
      ],
      "82921458": [
        "a"
      ],
      "134588381": [
        "c"
      ],
      "134588469": [
        "d"
      ],
      "134588501": [
        "c"
      ],
      "134588513": [
        "a",
        "e"
      ],
      "138248119": [
        "a"
      ],
      "138248197": [
        "c"
      ],
      "138248237": [
        "a"
      ]
    }
  },
  {
    "id": "1770456841969",
    "date": "2026-02-07T09:34:01.969Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 19,
    "incorrect": 1,
    "unanswered": 0,
    "total": 20,
    "percent": 95,
    "duration": 4365278,
    "questions": [
      {
        "id": 115961513,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>",
          "answers": [
            "<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>",
            "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>",
            "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>",
            "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"
          ],
          "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 134588513,
        "correct_response": [
          "a",
          "e"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An organization has a fleet of Amazon EC2 instances and uses SSH for remote access. Whenever a DevOps Engineer leaves the organization, the SSH keys are rotated as a security measure. The Chief Information Officer (CIO) required to stop the use of EC2 key pairs for operational efficiency and instead utilize AWS Systems Manager Session Manager. To strengthen security, access to Session Manager should be limited solely through a private network.</p><p>Which set of actions should the new DevOps Engineer take to meet the CIO requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Create an IAM instance profile to be associated with the fleet of EC2 instances and attach an IAM policy with the required Systems Manager permissions.</p>",
            "<p>Enable incoming traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances.</p>",
            "<p>Launch a new EC2 instance that will function as a bastion host for the other EC2 instances in the fleet.</p>",
            "<p>Enable outbound traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances.</p>",
            "<p>Provision a VPC endpoint for Systems Manager in the designated region.</p>"
          ],
          "explanation": "<p>With <strong>Session Manager</strong>, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs).</p><p>By default, AWS Systems Manager doesn't have permission to perform actions on your instances. You must grant access by using AWS Identity and Access Management (IAM).</p><p>An <strong>instance profile</strong> that contains the AWS managed policy <strong>AmazonSSMManagedInstanceCore </strong>is needed to be attached to the EC2 instance for the <strong>Session Manager</strong> to work.</p><p><img src=\" https://media.tutorialsdojo.com/public/dop-c02-ssm-session-manager.png\"></p><p>The security posture of managed instances (including those in a hybrid environment) can be improved by configuring AWS Systems Manager to use an interface <strong>VPC endpoint</strong> in Amazon Virtual Private Cloud (Amazon VPC). An interface VPC endpoint (interface endpoint) can be used to connect to services powered by AWS PrivateLink, which is a technology that enables <strong>private access to Amazon Elastic Compute Cloud (Amazon EC2) and Systems Manager</strong> APIs using private IP addresses.</p><p>Hence, the correct answers are the option that says:</p><p>- <strong>Create an IAM instance profile to be associated with the fleet of EC2 instances and attach an IAM policy with the required Systems Manager permissions</strong></p><p><strong>- Provision a VPC endpoint for Systems Manager in the designated region</strong></p><p>The option that says: <strong>Enable incoming traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances </strong>is incorrect because Session Manager will work without the need to open inbound ports.</p><p>The option that says: <strong>Launch a new EC2 instance that will function as a bastion host for the other EC2 instances in the fleet </strong>is incorrect because a bastion host will not meet the requirement as it will still use SSH key pairs to remote access.</p><p>The option that says: <strong>Enable outbound traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances </strong>is incorrect because Session Manager will work without the need to open outbound ports.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-instance-profile.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-instance-profile.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 143860751,
        "correct_response": [
          "a",
          "d"
        ],
        "source": "Neal Set 6",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer is troubleshooting an issue with an AWS CodeDeploy deployment to a deployment group containing Amazon EC2 instances. The engineer noticed that all events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.</p><p>Which of the following as possible reasons for this failure? (Select TWO.)</p>",
          "answers": [
            "<p>The EC2 instances cannot reach the internet or CodeDeploy public endpoints using a NAT gateway or internet gateway.</p>",
            "<p>The IAM user who initiated the deployment does not have the required permissions to interact with CodeDeploy.</p>",
            "<p>The target EC2 instances were not properly registered with the CodeDeploy endpoint.</p>",
            "<p>The target EC2 instances do not have an instance profile attached that provides the required permissions.</p>",
            "<p>The EC2 instances were deployed using instance store volumes rather than EBS volumes.</p>"
          ],
          "explanation": "<p>There are several reasons why the deployment may show a Skipped status for all events. These include the networking configuration of the instances not being correctly setup to include a path to a NAT gateway or internet gateway. This is required for internet access and to connect to the CodeDeploy public endpoints.</p><p>Another possible cause of this issue is that the EC2 instances do not have an instance profile attached that uses an IAM policy to assign the required permissions for the application deployment.</p><p><strong>CORRECT: </strong>\"The EC2 instances cannot reach the internet or CodeDeploy public endpoints using a NAT gateway or internet gateway\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"The target EC2 instances do not have an instance profile attached that provides the required permissions\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The IAM user who initiated the deployment does not have the required permissions to interact with CodeDeploy\" is incorrect.</p><p>The deployment uses a service role rather than the IAM user permissions.</p><p><strong>INCORRECT:</strong> The target EC2 instances were not properly registered with the CodeDeploy endpoint\" is incorrect.</p><p>An agent must be installed, and the instances must have the required permissions for CodeDeploy. However, they do not need to be registered.</p><p><strong>INCORRECT:</strong> \"The EC2 instances were deployed using instance store volumes rather than EBS volumes\" is incorrect.</p><p>There is no requirement to use EBS volumes over instance store volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949138,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
            "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
            "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
            "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
            "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
            "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"
          ],
          "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 134588473,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multinational corporation has multiple AWS accounts that are consolidated using AWS Organizations. A new system should be configured for security purposes to detect suspicious activities in any of its accounts, such as SSH brute force attacks or compromised Amazon EC2 instances that serve malware. All gathered information must be centrally stored in its dedicated security account for audit purposes, and the events should be stored in an Amazon S3 bucket.</p><p>Which solution should a DevOps Engineer implement to meet this requirement?</p>",
          "answers": [
            "<p>Automatically detect SSH brute force or malware attacks by enabling Amazon Macie in every account. Set up the security account as the Macie Administrator for every member account of the organization. Create an Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, which should push the findings to an S3 bucket.</p>",
            "<p>Automatically detect SSH brute force or malware attacks by only enabling Amazon Macie in the security account. Configure the security account as the Macie Administrator for every member account. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket.</p>",
            "<p>Automatically detect SSH brute force or malware attacks by enabling Amazon GuardDuty in every account. Configure the security account as the GuardDuty Administrator for all member accounts in the organization. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p>",
            "<p>Automatically detect SSH brute force or malware attacks by only enabling Amazon GuardDuty in the security account. Set up the security account as the GuardDuty Administrator for every member account. Create a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket.</p>"
          ],
          "explanation": "<p><strong>Amazon GuardDuty</strong> is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. The cloud simplifies the collection and aggregation of account and network activities. Still, it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in the AWS Cloud. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats. GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs. With a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with AWS EventBridge, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.</p><p><img alt=\"Amazon GuardDuty\" height=\"378\" src=\"https://media.tutorialsdojo.com/public/product-page-diagram-Amazon-GuardDuty_how-it-works_2AUG2023.png\" width=\"1000\"></p><p>GuardDuty enables and manages across multiple accounts efficiently. All member account findings can be aggregated through the multi-account feature with a GuardDuty administrator account. This allows the security team to manage all GuardDuty findings across the organization in one account. The aggregated findings are also available through EventBridge, making integration with an existing enterprise event management system easy.</p><p>Hence, the correct answer is: <strong>Automatically detect SSH brute force or malware attacks by enabling Amazon GuardDuty in every account. Configure the security account as the GuardDuty Administrator for all member accounts in the organization. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</strong></p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by enabling Amazon Macie in every account. Set up the security account as the Macie Administrator for every member account of the organization. Create an Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, which should push the findings to an S3 bucket</strong> is incorrect because you have to use Amazon GuardDuty instead of Amazon Macie. Note that Amazon Macie cannot detect SSH brute force or malware attacks.</p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by only enabling Amazon Macie in the security account. Configure the security account as the Macie Administrator for every member account. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket</strong> is incorrect because you don't need to create a custom shell script in Lambda or use Kinesis Data Streams. You can just configure the EventBridge rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by only enabling Amazon GuardDuty in the security account. Set up the security account as the GuardDuty Administrator for every member account. Create a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket </strong>is incorrect. Although using Amazon GuardDuty in this scenario is valid, the implementation for storing the findings is incorrect. You can typically configure the EventBridge rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-manage-amazon-guardduty-security-findings-across-multiple-accounts/\">https://aws.amazon.com/blogs/security/how-to-manage-amazon-guardduty-security-findings-across-multiple-accounts/</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-guardduty/?src=udemy\">https://tutorialsdojo.com/amazon-guardduty/</a></p>"
        }
      },
      {
        "id": 82921346,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company is implementing a CodePipeline pipeline in which every push to the CodeCommit master branch gets deployed to development, staging, and production environment consisting of EC2 instances. When deploying to production, traffic should be deployed on a few instances so that metrics can be gathered before a manual approval step is done to deploy to all the instances.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?</p>\n",
          "answers": [
            "<p>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create one CodePipeline and chain up these stages together, introducing a manual approval step after the deployment to the canary instances</p>",
            "<p>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create separate CodePipeline for each deployment group all having the same source being your code repository. Introducing a manual approval step in the pipeline that deploys to production</p>",
            "<p>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create one CodePipeline and chain up these together. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</p>",
            "<p>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create three separate CodePipeline for each deployment group having all the same sources being your code repository. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create one CodePipeline and chain up these stages together, introducing a manual approval step after the deployment to the canary instances</strong></p>\n\n<p>CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>CodeDeploy components overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q9-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html\">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html</a></p>\n\n<p>For the given use-case, you should create four deployment groups for development, staging, canary testing, production and then chain these together using CodePipeline. For EC2, to do a canary deployment, you must create a small deployment group made of few instances from production and deploy to these. Add a manual step after the deployment to the canary testing stage.</p>\n\n<p>Sample workflow for CI/CD with AWS CodeCommit, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q9-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/\">https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/</a></p>\n\n<p>Exam Alert:</p>\n\n<p>The exam may try to trap you on some of the following details on deployment-related processes. Be aware of what's possible.</p>\n\n<ul>\n<li><p>A deployment group contains individually tagged Amazon EC2 instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.</p></li>\n<li><p>Deployments that use the EC2/On-Premises compute platform manage the way in which traffic is directed to instances by using an in-place or blue/green deployment type. During an in-place deployment, CodeDeploy performs a rolling update across Amazon EC2 instances. During a blue/green deployment, the latest application revision is installed on replacement instances.</p></li>\n<li><p>If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only.</p></li>\n<li><p>You CANNOT use canary, linear, or all-at-once configuration for EC2/On-Premises compute platform.</p></li>\n<li><p>You can manage the way in which traffic is shifted to the updated Lambda function versions during deployment by choosing a canary, linear, or all-at-once configuration.</p></li>\n<li><p>You can deploy an Amazon ECS containerized application as a task set. You can manage the way in which traffic is shifted to the updated task set during deployment by choosing a canary, linear, or all-at-once configuration.</p></li>\n<li><p>Amazon ECS blue/green deployments are supported using both CodeDeploy and AWS CloudFormation. For blue/green deployments through AWS CloudFormation, you don't create a CodeDeploy application or deployment group.</p></li>\n<li><p>Your deployable content and the AppSpec file are combined into an archive file (also known as application revision) and then upload it to an Amazon S3 bucket or a GitHub repository. Remember these two locations. AWS Lambda revisions can be stored in Amazon S3 buckets. EC2/On-Premises revisions are stored in Amazon S3 buckets or GitHub repositories.</p></li>\n<li><p>AWS Lambda and Amazon ECS deployments CANNOT use an in-place deployment type.</p></li>\n</ul>\n\n<p>Incorrect options:</p>\n\n<p><strong>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create separate CodePipeline for each deployment group all having the same source being your code repository. Introducing a manual approval step in the pipeline that deploys to production</strong> - Creating separate CodePipelines is not a good idea and won't allow you to create manual approval steps before deploying to production.</p>\n\n<p><strong>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create one CodePipeline and chain up these together. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</strong> - While CodeDeploy does have a Canary Deployment setting, it's only meant for AWS Lambda and ECS platforms and there's no option to pause it manually through an approval step.</p>\n\n<p><strong>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create three separate CodePipeline for each deployment group having all the same sources being your code repository. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</strong> - While CodeDeploy does have a Canary Deployment setting, it's only meant for AWS Lambda and ECS platforms. In addition, creating separate CodePipelines is not a good idea and won't allow you to create manual approval steps before deploying to production.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html\">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/\">https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/</a></p>\n"
        }
      },
      {
        "id": 134588423,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company runs a popular e-commerce website that provides discounts and weekly sales promotions on various products. It was recently moved from its previous cloud hosting provider to AWS. For its architecture, it uses an Auto Scaling group of Reserved EC2 instances with an Application Load Balancer in front. Their DevOps team needs to set up a CloudFront web distribution that uses a custom domain name where the origin is set to point to the ALB. </p><p>How can the DevOps Engineers properly implement an end-to-end HTTPS connection from the origin to the CloudFront viewers?</p>",
          "answers": [
            "<p>Generate an SSL certificate that is signed by a trusted third-party certificate authority. Import the certificate into AWS Certificate Manager and use it for the Application Load balancer. Set the <code>Viewer Protocol Policy</code> to <code>HTTPS Only</code> in CloudFront and then use an SSL/TLS certificate from a 3rd party certificate authority which was imported to either AWS Certificate Manager or the IAM certificate store.</p>",
            "<p>Generate an SSL self-signed certificate for the Application Load Balancer. Configure the Viewer Protocol Policy setting to <code>HTTPS Only</code> in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority that was imported to either AWS Certificate Manager or the IAM certificate store.</p>",
            "<p>Use a certificate that is signed by a trusted 3rd party certificate authority for the Application Load Balancer, which is then imported into AWS Certificate Manager. Choose the <code>Match Viewer</code> setting for the Viewer Protocol Policy to support both HTTP or HTTPS in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority that was imported to either AWS Certificate Manager or the IAM certificate store.</p>",
            "<p>Utilize an SSL certificate that is signed by a trusted 3rd party certificate authority for the ALB, which is then imported into AWS Certificate Manager. Set the Viewer Protocol Policy to <code>HTTPS Only</code> in CloudFront. Use an SSL/TLS certificate from 3rd party certificate authority which was imported to an Amazon S3 bucket.</p>"
          ],
          "explanation": "<p>For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so that connections are encrypted when CloudFront communicates with viewers. You also can configure CloudFront to use HTTPS to get objects from your origin, so that connections are encrypted when CloudFront communicates with your origin.</p><p>Remember that there are rules on which type of SSL Certificate to use if you are using an EC2 or an ELB as your origin. This question is about setting up an end-to-end HTTPS connection between the Viewers, CloudFront, and your custom origin, which is an ALB instance.</p><p><img src=\"https://media.tutorialsdojo.com/end-to-end-application-load-balancer-to-cloudfront-encryption.png\"></p><p>The certificate issuer you must use depends on whether you want to require HTTPS between viewers and CloudFront or between CloudFront and your origin:</p><p><strong>HTTPS between viewers and CloudFront</strong></p><p>- You can use a certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, Symantec, or other third-party providers.</p><p>- You can use a certificate provided by AWS Certificate Manager (ACM)</p><p><strong>HTTPS between CloudFront and a custom origin</strong></p><p>- If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- If your origin is an ELB load balancer, you can also use a certificate provided by ACM.</p><p>If you're using your own domain name, such as tutorialsdojo.com, you need to change several CloudFront settings. You also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store. Lastly, you should set the Viewer Protocol Policy to <strong>HTTPS Only</strong> in CloudFront.</p><p>Hence, the correct answer is: <strong>Generate an SSL certificate that is signed by a trusted third-party certificate authority. Import the certificate into AWS Certificate Manager and use it for the Application Load balancer. Set the </strong><code><strong>Viewer Protocol Policy</strong></code><strong> to </strong><code><strong>HTTPS Only</strong></code><strong> in CloudFront and then use an SSL/TLS certificate from a 3rd party certificate authority which was imported to either AWS Certificate Manager or the IAM certificate store.</strong></p><p>The option says:<strong> Generate an SSL self-signed certificate for the Application Load Balancer. Configure the Viewer Protocol Policy setting to </strong><code><strong>HTTPS Only</strong></code><strong> in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority that was imported to either AWS Certificate Manager or the IAM certificate store</strong> is incorrect because you cannot directly upload a self-signed certificate in your ALB.</p><p>The option that says: <strong>Use a certificate that is signed by a trusted 3rd party certificate authority for the Application Load Balancer, which is then imported into AWS Certificate Manager. Choose the </strong><code><strong>Match Viewer</strong></code><strong> setting for the Viewer Protocol Policy to support both HTTP or HTTPS in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority which was imported to either AWS Certificate Manager or the IAM certificate store</strong> is incorrect because you have to set the Viewer Protocol Policy to <code>HTTPS Only</code>. With<code><strong><em> Match Viewer</em></strong></code><strong><em>, </em></strong>CloudFront communicates with your custom origin using HTTP or HTTPS, depending on the protocol of the viewer request. For example, if you choose Match Viewer for Origin Protocol Policy and the viewer uses HTTP to request an object from CloudFront, CloudFront also uses the same protocol (HTTP) to forward the request to your origin.</p><p>The option that says: <strong>Utilize an SSL certificate that is signed by a trusted 3rd party certificate authority for the ALB, which is then imported into AWS Certificate Manager. Set the Viewer Protocol Policy to </strong><code><strong>HTTPS Only</strong></code><strong> in CloudFront. Use an SSL/TLS certificate from 3rd party certificate authority which was imported to an Amazon S3 bucket</strong> is incorrect because you cannot use an SSL/TLS certificate from a third-party certificate authority which was imported to S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html \">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>"
        }
      },
      {
        "id": 143860767,
        "correct_response": [
          "b",
          "d"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer is migrating an application running on Docker containers from an on-premises data center to AWS. The application must run with minimal management overhead. Encrypted communications between the data center and AWS must be implemented for secure connectivity with on-premises resources.</p><p>Which actions should the DevOps engineer take to meet these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Migrate the container-based workload to Amazon ECS with the EC2 launch type in a custom VPC.</p>",
            "<p>Migrate the container-based workload to Amazon ECS with the Fargate launch type in a custom VPC.</p>",
            "<p>Implement a VPC endpoint and update security groups to enable access to Amazon ECS.</p>",
            "<p>Implement an AWS Managed VPN to encrypt traffic between the on-premises data center and the VPC.</p>",
            "<p>Implement a Network Load Balancer with IP-based targets and configure an HTTPS listener.</p>"
          ],
          "explanation": "<p>The requirements are for low management overhead of the container-based application on AWS and encryption for traffic between AWS and the on-premises network. To minimize management overhead of the application the DevOps engineer should deploy the container-based application on Amazon ECS with the Fargate launch type. This is the serverless solution for Amazon ECS and will be the easiest to manage.</p><p>To encrypt communications between the on-premises data center and the VPC, the engineer should deploy an AWS Managed VPN which will used IPSec to encrypt the traffic between these networks.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-16-13-2d804006a971e4e66b25f15f06b9dd58.jpg\"><p><strong>CORRECT: </strong>\"Migrate the container-based workload to Amazon ECS with the Fargate launch type in a custom VPC\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Implement an AWS Managed VPN to encrypt traffic between the on-premises data center and the VPC\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the container-based workload to Amazon ECS with the EC2 launch type in a custom VPC\" is incorrect.</p><p>The EC2 launch type has a higher management overhead as you must maintain the EC2 instances that run the container agent.</p><p><strong>INCORRECT:</strong> \"Implement a VPC endpoint and update security groups to enable access to Amazon ECS\" is incorrect.</p><p>With the Fargate launch type there is no need to add a VPC endpoint to enable connectivity for the ECS cluster.</p><p><strong>INCORRECT:</strong> \"Implement a Network Load Balancer with IP-based targets and configure an HTTPS listener\" is incorrect.</p><p>You cannot create an HTTPS listener with an NLB and there is no need to use a load balancer to meet these requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 82921460,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A gaming company would like to be able to receive near real-time notifications when the API call <code>DeleteTable</code> is invoked in DynamoDB.</p>\n\n<p>As a DevOps Engineer at the company, how would you implement this at a minimal cost?</p>\n",
          "answers": [
            "<p>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</p>",
            "<p>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</p>",
            "<p>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</p>",
            "<p>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. Rules in CloudWatch Events work only in the Region in which they are created. If you configure CloudTrail to track API calls in multiple Regions, and you want a rule-based on CloudTrail to trigger in each of those Regions, you must create a separate rule in each Region that you want to track.</p>\n\n<p>For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q40-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</strong> - A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. DynamoDB Streams do not capture <code>DeleteTable</code> API calls, they only capture item-level events.</p>\n\n<p><strong>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</strong> - Sending CloudTrail Logs to CloudWatch Logs and creating a filter on those will work but will be expensive, as we're streaming all the logs from CloudTrail just to extract a single event.</p>\n\n<p><strong>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</strong> - CloudTrail trails do not have event filters and cannot be directly sent to a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p>\n"
        }
      },
      {
        "id": 75949050,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is running an eCommerce application on a fleet of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. There have been issues occurring occasionally where instances fail to launch successfully, and the support team wants to be notified whenever this occurs.</p><p>Which configuration update will achieve these requirements?</p><p>Which action will accomplish this?</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch alarm that sends an Amazon SNS notification when a failed SetInstanceHealth API call is made.</p>",
            "<p>Create an Amazon CloudWatch alarm that sends a notification when an Amazon EC2 status check fails.</p>",
            "<p>Configure an Amazon SNS topic for the Auto Scaling group that sends a notification whenever a failed instance launch occurs.</p>",
            "<p>Add a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired.</p>"
          ],
          "explanation": "<p>You can be notified when Amazon EC2 Auto Scaling is launching or terminating the EC2 instances in your Auto Scaling group. You manage notifications using Amazon Simple Notification Service (Amazon SNS).</p><p>Amazon EC2 Auto Scaling supports sending Amazon SNS notifications when the following events occur:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-25-53-89c65e690caba055c07c38d0fb8c649a.jpg\"><p>The \u201cautoscaling:EC2_INSTANCE_LAUNCH_ERROR\u201d would be the correct event to monitor as this indicates if failed instance launch events have occurred. SNS can then send a notification to the support team to let them know what has happened.</p><p><strong>CORRECT: </strong>\"Configure an Amazon SNS topic for the Auto Scaling group that sends a notification whenever a failed instance launch occurs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm that sends an Amazon SNS notification when a failed SetInstanceHealth API call is made\" is incorrect.</p><p>This API action is used to set the health of an instance and does not monitor failed instance launch events.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm that sends a notification when an Amazon EC2 status check fails\" is incorrect.</p><p>Status checks will show issues with instances that are already running rather than issues with launch events.</p><p><strong>INCORRECT:</strong> \"Add a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired\" is incorrect.</p><p>Health checks can be used to check the status of an instance, but the instance is already running. This does not help if the instance failed to launch.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html#auto-scaling-sns-notifications\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html#auto-scaling-sns-notifications</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 75949082,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A financial company has applications hosted in multiple AWS accounts which are managed by AWS organizations. A security audit was conducted to address any potential security breaches and implement best practices. The security audit report recommended that all security related logging and security findings are collect in a centralized security account.</p><p>How can this be achieved?</p>",
          "answers": [
            "<p>Use Amazon GuardDuty in each organization to detect the attacks on EC2 instances. Specify an organization as the GuardDuty administrator. Create a CloudWatch rule in the GuardDuty administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Firehose to send findings to a designated S3 bucket.</p>",
            "<p>Use Amazon Macie in each organization to detect the attacks on EC2 instances. Specify an organization as the Macie administrator. Create a CloudWatch rule in the Macie administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Firehose to send findings to a designated S3 bucket.</p>",
            "<p>Use Amazon GuardDuty in each account to detect the attacks on EC2 instances. Create a CloudWatch rule in the GuardDuty administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Streams to send findings to a designated S3 bucket.</p>",
            "<p>Use Amazon Inspector in each organization to detect the attacks on EC2 instances. Specify an organization as the Inspector administrator. Create a CloudWatch rule in the Inspector administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Firehose to send findings to a designated S3 bucket.</p>"
          ],
          "explanation": "<p>GuardDuty analyzes and processes VPC flow log, AWS CloudTrail event log, and DNS log data sources. You don\u2019t need to manually manage these data sources because the data is automatically leveraged and analyzed when you activate GuardDuty.</p><p>For example, GuardDuty consumes VPC Flow Log events directly from the VPC Flow Logs feature through an independent and duplicative stream of flow logs. As a result, you don\u2019t incur any operational burden on existing workloads.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-10-01-c38c34b5c636f2d9f839cf769fa2e58c.jpg\">"
        }
      },
      {
        "id": 99528195,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company requires an automated solution that terminates Amazon EC2 instances that have been logged into manually within 24 hours of the login event. The applications running in the account are launched using Auto Scaling groups and the CloudWatch Logs agent is configured on all instances.</p><p>How should a DevOps engineer build the automation?</p>",
          "answers": [
            "<p>Create a CloudWatch alarm that will trigger on AWS API call events in CloudTrail. Configure the alarm to send a message to an Amazon SQS queue. Create an AWS Lambda function that processes messages from the queue and terminates the instances that produced the login event.</p>",
            "<p>Create a CloudWatch alarm that will trigger on login events. Send the notification to a Kinesis Data Firehose stream. Configure the stream to send notifications to an SNS topic and instruct the operations team to subscribe to the topic and terminate EC2 instances that produced login events within 24 hours.</p>",
            "<p>Create a CloudWatch Logs subscription filter that delivers logs to an AWS Lambda function. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged.</p>",
            "<p>Create a CloudWatch Logs subscription filter that delivers logs to an AWS Step Functions state machine. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged.</p>"
          ],
          "explanation": "<p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to Kinesis, Lambda, or Kinesis Data Firehose. When log events are sent to the receiving service, they are base64 encoded and compressed with the gzip format.</p><p>The Lambda function that receives the log events can process the log files looking for entries that indicate that a manual login event occurred and add a tag. Another Lambda function that runs on a schedule can then look for instances that have been tagged and terminate them.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Logs subscription filter that delivers logs to an AWS Lambda function. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs subscription filter that delivers logs to an AWS Step Functions state machine. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged\" is incorrect.</p><p>You cannot configure Step Functions to receive the events.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that will trigger on AWS API call events in CloudTrail. Configure the alarm to send a message to an Amazon SQS queue. Create an AWS Lambda function that processes messages from the queue and terminates the instances that produced the login event\" is incorrect.</p><p>The API calls logged via CloudTrail will not show that a manual login event occurred as that does not require an AWS API call to be made. The manual login data would be included in the logs delivered to CloudWatch Logs by the CloudWatch agent.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that will trigger on login events. Send the notification to a Kinesis Data Firehose stream. Configure the stream to send notifications to an SNS topic and instruct the operations team to subscribe to the topic and terminate EC2 instances that produced login events within 24 hours\" is incorrect.</p><p>You can configure a KDS stream to receive the events but would then need a record processor (KCL worker) to process the events, you cannot send directly to an SNS topic. Also, requiring the operations team to process the terminations is not automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 82921344,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An industrial appliances company would like to take advantage of AWS Systems Manager to manage their on-premise instances and their EC2 instances. This will allow them to run some SSM RunCommand across their hybrid fleet. The company would also like to effectively manage the size of the fleet. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>How would you set up the on-premise server to achieve this objective?</p>\n",
          "answers": [
            "<p>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>",
            "<p>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>",
            "<p>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>",
            "<p>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p>AWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. With Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status.</p>\n\n<p>How Systems Manager Works:\n<img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\">\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants AssumeRole trust to the Systems Manager service. You only need to create a service role for a hybrid environment once for each AWS account.</p>\n\n<p>To set up servers and virtual machines (VMs) in your hybrid environment as managed instances, you need to create a managed-instance activation. After you complete the activation, you immediately receive an Activation Code and Activation ID. You specify this Code/ID combination when you install SSM agents on servers and VMs in your hybrid environment. The Code/ID provides secure access to the Systems Manager service from your managed instances.</p>\n\n<p>In the Instance limit field, specify the total number of on-premises servers or VMs that you want to register with AWS as part of the activation. This means you don't need to create a unique activation Code/ID for each managed instance.</p>\n\n<p>After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as managed instances. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix \"mi-\". Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong> - As mentioned in the explanation earlier, the on-premise instances use the prefix \"mi-\" whereas the Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><strong>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p><strong>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong></p>\n\n<p>Both these options suggest using the Access Key ID and the Secret Access Key ID to register your on-premise servers which is considered a bad practice from a security perspective. Instead, you should use an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html</a></p>\n\n<p><a href=\"#\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p>\n"
        }
      },
      {
        "id": 138248129,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has an application hosted in an Auto Scaling group of EC2 instances which calls an external API with a URL of <code>http://api.tutorialsdojo.com</code> as part of its processing. There was a recent deployment that changed the protocol of the URL from HTTP to HTTPS but after that, the application has stopped working properly. The DevOps engineer has verified using his POSTMAN tool that the external API works without any issues and the VPC being utilized is still using the default network ACL. </p><p>Which of the following is the MOST appropriate course of action that the engineer should take to determine the root cause of this problem?</p>",
          "answers": [
            "<p>Log in to the AWS Management Console and then in the VPC flow logs, look for <code>ACCEPT</code> records which were originated from the Auto Scaling group. Verify that the ingress security group rules of the Auto Scaling Group allow the incoming traffic from the external API.</p>",
            "Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the existing egress security group rules of the Auto Scaling Group, as well as the network ACL, allow the outgoing traffic to the external API.",
            "<p>Log in to the AWS Management Console and look for <code>REJECT</code> records in the VPC flow logs which originated from the Auto Scaling group. Verify that the egress security group rules of the Auto Scaling Group allow the outgoing traffic to the external API.</p>",
            "Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the ingress security group rules of the Auto Scaling Group, as well as the network ACL, allow the incoming traffic from the external API."
          ],
          "explanation": "<p><strong>Amazon Virtual Private Cloud</strong> provides features that you can use to increase and monitor the security of your virtual private cloud (VPC):</p><p><strong>Security groups</strong>: Security groups act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level. When you launch an instance, you can associate it with one or more security groups that you've created. Each instance in your VPC could belong to a different set of security groups. If you don't specify a security group when you launch an instance, the instance is automatically associated with the default security group for the VPC.</p><p><strong>Network access control lists (ACLs)</strong>: Network ACLs act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level.</p><p><strong>Flow logs</strong>: Flow logs capture information about the IP traffic going to and from network interfaces in your VPC. You can create a flow log for a VPC, subnet, or individual network interface. Flow log data is published to CloudWatch Logs or Amazon S3, and can help you diagnose overly restrictive or overly permissive security group and network ACL rules.</p><p><strong>Traffic mirroring</strong>: You can copy network traffic from an elastic network interface of an Amazon EC2 instance. You can then send the traffic to out-of-band security and monitoring appliances.</p><p>You can use AWS Identity and Access Management to control who in your organization has permission to create and manage security groups, network ACLs, and flow logs. For example, you can give only your network administrators that permission, but not personnel who only need to launch instances.</p><p><img src=\"https://media.tutorialsdojo.com/security-diagram.png\"></p><p>For HTTP traffic, you must add an inbound rule on port 80 from the source address 0.0.0.0/0. For HTTPS traffic, add an inbound rule on port 443 from the source address 0.0.0.0/0. These inbound rules allow traffic from IPv4 addresses. To allow IPv6 traffic, add inbound rules on the same ports from the source address ::/0. Because security groups are stateful, the return traffic from the instance to users is allowed automatically, so you don't need to modify the security group's outbound rules.</p><p>The default network ACL is configured to allow all traffic to flow in and out of the subnets with which it is associated. Each network ACL also includes a rule whose rule number is an asterisk. This rule ensures that if a packet doesn't match any of the other numbered rules, it's denied.</p><p>In this scenario, the change of the URL from HTTP to HTTPS means that the application is using port 443 and not port 80 anymore. Since the application is the one that initiates the call to the external API, it makes sense to check if the egress security group rules allow outgoing HTTPS (443) traffic.</p><p>Hence, the correct answer is:<strong> Log in to the AWS Management Console and look for </strong><code><strong>REJECT</strong></code><strong> records in the VPC flow logs which originated from the Auto Scaling group. Verify that the egress security group rules of the Auto Scaling Group allow the outgoing traffic to the external API.</strong></p><p>The option that says: <strong>Log in to the AWS Management Console and then in the VPC flow logs, look for </strong><code><strong>ACCEPT</strong></code><strong> records which were originated from the Auto Scaling group. Verify that the ingress security group rules of the Auto Scaling Group allow the incoming traffic from the external API </strong>is incorrect because you should first check the egress rules (instead of ingress) of your security group first. Remember that it is the Auto Scaling group of EC2 instances that initiates the call to the external API and not the other way around. In addition, it is more effective if you look for <code><em>REJECT</em></code> records instead of <code>ACCEPT</code> records in VPC Flow Logs to view the details of the failed connection to your external API.</p><p>The option that says: <strong>Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the existing egress security group rules of the Auto Scaling Group, as well as the network ACL, allow the outgoing traffic to the external API</strong> is incorrect because, in the first place, the scenario didn't mention that the CloudWatch Logs agent is installed in the EC2 instances. Although it is right to check the existing egress security group rules, you don't need to check the network ACL since the architecture is already using a default one which is configured to allow all traffic.</p><p>The option that says: <strong>Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the ingress security group rules of the Auto Scaling Group, as well as the network ACL, allow the incoming traffic from the external API</strong> is incorrect because you have to check the egress security group rules first instead. The scenario also didn't mention that the CloudWatch Logs agent is installed in the EC2 instances, which means that you might not be able to view the application logs in CloudWatch.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/\">https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><br></p><p><strong>Amazon VPC Overview:</strong></p><p><a href=\"https://youtu.be/oIDHKeNxvQQ\">https://youtu.be/oIDHKeNxvQQ</a><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>"
        }
      },
      {
        "id": 99528241,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer is building a web application that will use federated access to a SAML identity provider (IdP). The web application requires sign up and sign in functionality using a custom webpage with authenticated access to AWS services.</p><p>Which steps should the DevOps engineer take to implement the authentication and access control solution for the web application?</p>",
          "answers": [
            "<p>Use Amazon Cognito and create an identity pool for federated sign-in. Configure the SAML IdP to add a relying party trust between the IdP and AWS. Use the AssumeRoleWithWebIdentity API to call the AWS Security Token Service (STS) and generate temporary security credentials providing access to the appropriate AWS services.</p>",
            "<p>Use an Amazon API Gateway REST API to provide a web endpoint for the application. Use an AWS Lambda authorizer to control access to the API with a bearer token authentication strategy utilizing the SAML IdP. Authorize access to the backend services using identity-based policies.</p>",
            "<p>Use AWS Directory Service to create a Simple AD and configure a trust relationship with the SAML IdP. Create a custom webpage on an Amazon S3 static website and use Amazon CloudFront for sign up and sign in functionality. Use role-based access control to access AWS services through the web application.</p>",
            "<p>Use Amazon Cognito and create a user pool for federated sign-in. Add a SAML IdP and enter identifiers to map the sign-in email addresses to the relevant provider. Generate access tokens and exchange them for temporary security credentials providing access to the appropriate AWS services.</p>"
          ],
          "explanation": "<p>The DevOps engineer should use Amazon Cognito with a user pool to create a custom webpage offering sign in and sign up functionality. The user pool can be integrated with a SAML IdP for federated access.</p><p>The process for configuring this federation is as follows:</p><ol><li><p>Create or select a user pool.</p></li><li><p>Choose the Sign-in experience tab. Locate Federated sign-in and select Add an identity provider.</p></li><li><p>Choose a SAML social identity provider.</p></li><li><p>Enter <strong>Identifiers</strong> separated by commas. An identifier tells Amazon Cognito it should check the email address a user enters when they sign in, and then direct them to the provider that corresponds to their domain.</p></li></ol><p>When a user signs into the app, Amazon Cognito verifies the login information. If the login is successful, Amazon Cognito creates a session and returns an ID, access, and refresh token for the authenticated user. The tokens can be used to grant users access to server-side resources or to the Amazon API Gateway. Or they can be exchanged for temporary AWS credentials to access other AWS services.</p><p><strong>CORRECT: </strong>\"Use Amazon Cognito and create a user pool for federated sign-in. Add a SAML IdP and enter identifiers to map the sign-in email addresses to the relevant provider. Generate access tokens and exchange them for temporary security credentials providing access to the appropriate AWS services\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Cognito and create an identity pool for federated sign-in. Configure the SAML IdP to add a relying party trust between the IdP and AWS. Use the AssumeRoleWithWebIdentity API to call the AWS Security Token Service (STS) and generate temporary security credentials providing access to the appropriate AWS services\" is incorrect.</p><p>Though you can use an identity pool for federated sign in you should use a user pool for creating a custom webpage offering sign in and sign up functionality. Also the API action provided is not suitable for SAML \u2013 AssumeRoleWith SAML should be used instead.</p><p><strong>INCORRECT:</strong> \"Use an Amazon API Gateway REST API to provide a web endpoint for the application. Use an AWS Lambda authorizer to control access to the API with a bearer token authentication strategy utilizing the SAML IdP. Authorize access to the backend services using identity-based policies\" is incorrect.</p><p>API Gateway is not a suitable web endpoint for providing a custom webpage with the required sign in and sign up functionality.</p><p><strong>INCORRECT:</strong> \"Use AWS Directory Service to create a Simple AD and configure a trust relationship with the SAML IdP. Create a custom webpage on an Amazon S3 static website and use Amazon CloudFront for sign up and sign in functionality. Use role-based access control to access AWS services through the web application\" is incorrect.</p><p>You cannot configure trust relationships with any other directory when using Simple AD. Amazon CloudFront also does not offer sign up and sign in functionality.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-configuring-federation-with-saml-2-0-idp.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-configuring-federation-with-saml-2-0-idp.html</a></p>"
        }
      },
      {
        "id": 82921392,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A cyber forensics company would like to ensure that CloudTrail is always enabled in its AWS account. It also needs to have an audit trail of the status for CloudTrail. In the case of compliance breaches, the company would like to automatically resolve them.</p>\n\n<p>As a DevOps Engineer, how can you implement a solution for this requirement?</p>\n",
          "answers": [
            "<p>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</p>",
            "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the 'everyone' group automatically</p>",
            "<p>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>",
            "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/config/faq/\">https://aws.amazon.com/config/faq/</a></p>\n\n<p>You need to have an AWS Config rule to maintain auditability and track compliance over time. You can use the <code>cloudtrail-enabled</code> Config managed rule to check whether AWS CloudTrail is enabled in your AWS account. You can use <code>cloudtrail-security-trail-enabled</code> Config managed rules to check that there is at least one AWS CloudTrail trail defined with security best practices. To be alerted of compliance issues, use a CloudWatch Event rule and then hook it to a Lambda function that will re-enable CloudTrail automatically.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the 'everyone' group automatically</strong></p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong></p>\n\n<p>IAM users in a group with a deny policy sounds like a great idea at first, but then you have to remember you can create IAM roles, and they won't have that restriction, and as such you will be able to assume these roles and then issue API calls on CloudTrail to de-activate it. This solution won't work and therefore both these options are incorrect.</p>\n\n<p><strong>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong> - You need to have an AWS Config rule to maintain auditability and track compliance over time, as using the Lambda function to trigger an API call would tell you about the CloudTrail status only at that point in time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/config/faq/\">https://aws.amazon.com/config/faq/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n"
        }
      },
      {
        "id": 134588421,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An international IT consulting firm has multiple on-premises data centers across the globe. Their technical team regularly uploads financial and regulatory files from each of their respective data centers to a centralized web portal hosted in AWS. It uses an Amazon S3 bucket named <code>financial-tdojo-reports</code> to store the data. Another team downloads various reports from a CloudFront web distribution that uses the same Amazon S3 bucket as the origin. A DevOps Engineer noticed that the staff are using both the CloudFront link and the direct Amazon S3 URLs to download the reports. The IT Security team of the company considered this as a security risk, and they recommended to re-design the architecture. A new system must be implemented that prevents anyone from bypassing the CloudFront distribution and disable direct access from Amazon S3 URLs. </p><p>What should the Engineer do to meet the above requirement?</p>",
          "answers": [
            "<p>Create an Origin Access Control (OAC) and associate it with the S3 bucket origin in the CloudFront distribution. Update the bucket policy for the <code>financial-tdojo-reports</code> bucket to allow access only from the specified CloudFront distribution.</p>",
            "<p>Set up a custom SSL in your CloudFront web distribution instead of the default SSL. For each user, revoke the existing permission to access Amazon S3 URLs to download the objects.</p>",
            "<p>In the CloudFront web distribution, set up a field-level encryption configuration and for each user, revoke the existing permission to access Amazon S3 URLs to download the objects.</p>",
            "<p>Configure the distribution to use Signed URLs and create a Signed URL signing key pair (a public/private key pair). Grant permission to the public key to access the objects in the S3 bucket.</p>"
          ],
          "explanation": "<p><strong>Amazon CloudFront</strong> is a content delivery network (CDN) service provided by AWS. It securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. Amazon CloudFront provides various options to secure content delivery from S3 buckets, including preventing direct access to S3 and forcing downloads through the CloudFront distribution.</p><p><strong>Origin Access Control</strong> is a feature of Amazon CloudFront that enhances security for accessing S3 buckets and other supported origins. It allows you to restrict access to your origin resources, ensuring that they can only be accessed through designated CloudFront distributions. Some of the key points about OAC are:</p><ul><li><p>It uses AWS Identity and Access Management (IAM) service principals for authentication.</p></li><li><p>OAC employs short-term credentials with frequent rotations for improved security.</p></li><li><p>It supports comprehensive HTTP methods and server-side encryption with AWS KMS (SSE-KMS).</p></li><li><p>OAC can be used with S3 buckets in all AWS regions, as well as with AWS Lambda function URL origins.</p></li></ul><p><img src=\"https://media.tutorialsdojo.com/public/CloudFront_6AUG2023.png\"></p><p>By creating an Origin Access Control (OAC) setting in the CloudFront distribution and updating the S3 bucket policy to grant access only to the CloudFront OAC, you effectively prevent direct access to the S3 bucket from any other source, including direct S3 URLs. All access to the objects in the S3 bucket must go through the CloudFront distribution, meeting the security requirement.</p><p>Hence, the correct answer is: <strong>Create an Origin Access Control (OAC) and associate it with the S3 bucket origin in the CloudFront distribution. Update the bucket policy for the </strong><code><strong>financial-tdojo-reports</strong></code><strong> bucket to allow access only from the specified CloudFront distribution.</strong></p><p>The option that says: <strong>Set up a custom SSL in your CloudFront web distribution instead of the default SSL. For each user, revoke the existing permission to access Amazon S3 URLs to download the objects</strong> is incorrect because SSL is not needed in this particular scenario. What you need to implement is an OAC.</p><p>The option that says: <strong>In the CloudFront web distribution, set up a field-level encryption configuration and for each user, revoke the existing permission to access Amazon S3 URLs to download the objects</strong> is incorrect because the field-level encryption configuration is primarily used for safeguarding sensitive fields in your CloudFront. Therefore, it is not suitable for this scenario.</p><p>The option that says: <strong>Configure the distribution to use Signed URLs and create a Signed URL signing key pair (a public/private key pair). Grant permission to the public key to access the objects in the S3 bucket</strong> is incorrect because Signed URLs are used to provide temporary access to specific objects in an S3 bucket, typically for a limited time period. While this approach can restrict direct access to the S3 bucket, it requires generating and managing Signed URLs for each object, which can be cumbersome and not scalable, especially if there are many objects or frequent updates.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 134588417,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A cloud-based payments company is heavily using Amazon EC2 instances to host their applications in AWS Cloud. They would like to improve the security of their cloud resources by ensuring that all of their EC2 instances were launched from pre-approved AMIs only. The list of AMIs is set and managed by their IT Security team. Their Software Development team has an automated CI/CD process that launches several EC2 instances with new and untested AMIs for testing. The development process must not be affected by the new solution, which will be implemented by their Lead DevOps Engineer. </p><p>Which of the following can the Engineer implement to satisfy the requirement with the LEAST impact on the development process? (Select TWO.)</p>",
          "answers": [
            "<p>Set up a centralized IT Systems Operations team that has the required policies, roles, and permissions. The team will manually process the security approval steps to ensure that Amazon EC2 instances are launched from pre-approved AMIs only.</p>",
            "<p>Use AWS Config with a Lambda function that periodically evaluates whether there are EC2 instances launched based on non-approved AMIs. Set up a remediation action using AWS Systems Manager Automation that will automatically terminate the EC2 instance. Publish a message to an Amazon SNS topic to inform the IT Security and Development teams about the occurrence.</p>",
            "<p>Integrate AWS Lambda and CloudWatch Events to schedule a daily process that will search through the list of running Amazon EC2 instances within your VPC. Configure the function to determine if any of these are based on unauthorized AMIs. Publish a new message to an Amazon SNS topic to inform the Security and Development teams that the issue occurred and then automatically terminate the EC2 instance.</p>",
            "<p>Set up IAM policies to restrict the ability of users to launch Amazon EC2 instances based on a specific set of pre-approved AMIs which were tagged by the IT Security team.</p>",
            "<p>Do regular scans using Amazon Inspector via a custom assessment template that determines if the Amazon EC2 instance is based upon a pre-approved AMI or not. Terminate the instances and inform the IT Security team by email about the security breach.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting.</p><p>When you run your applications on AWS, you usually use AWS resources, which you must create and manage collectively. As the demand for your application keeps growing, so does your need to keep track of your AWS resources. AWS Config is designed to help you oversee your application resources.</p><p><img src=\"https://media.tutorialsdojo.com/public/how-AWSconfig-works.png\"></p><p>AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p>AWS Config provides AWS managed rules which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. In this scenario, you can use the <code><strong>approved-amis-by-id</strong></code> AWS manage rule which checks whether running instances are using specified AMIs. You can also use a Lambda function which is scheduled to run regularly to scan all of the running EC2 instances in your VPC and check if there is an instance that was launched using an unauthorized AMI.</p><p>In this scenario, we have to balance two things: security and development operations. The former should always be our utmost priority, this is why the scenario says that all of the company's EC2 instances must be launched from pre-approved AMIs only.</p><p>In the first place, why should the development team deploy a non-approved AMI? That's a clear violation of the company policy as stated above.<em> </em>As a Solutions Architect, a small security risk is still considered a risk and it must always be dealt with. In some organizations, such as banks and financial institutions, the IT Security team has the power to stop or revert back any recent deployments if there is a security issue.</p><p>Hence, the correct answers are:</p><p><strong>- Use AWS Config with a Lambda function that periodically evaluates whether there are EC2 instances launched based on non-approved AMIs. Set up a remediation action using AWS Systems Manager Automation that will automatically terminate the EC2 instance. Publish a message to an Amazon SNS topic to inform the IT Security and Development teams about the occurrence.</strong></p><p><strong>- Integrate AWS Lambda and CloudWatch Events to schedule a daily process that will search through the list of running Amazon EC2 instances within your VPC. Configure the function to determine if any of these are based on unauthorized AMIs. Publish a new message to an Amazon SNS topic to inform the Security and Development teams that the issue occurred and then automatically terminate the EC2 instance.</strong></p><p>Remember that the question asks for the LEAST impact solution. The development teams will still be able to continue deploying their applications without any disruption. If you are using the Atlassian suite, your Bamboo build and deployment plans would still execute successfully. Take note that the two solutions will not immediately terminate the EC2 instances running with a non-approved AMI. The first solution uses AWS Config with a Lambda function that periodically evaluates the instances while the second has a once-a-day (daily) process. Therefore, these two answers provide the LEAST impact solution while keeping the architecture secure.</p><p>The option that says: <strong>Set up a centralized IT Systems Operations team that has the required policies, roles, and permissions. The team will manually process the security approval steps to ensure that Amazon EC2 instances are launched from pre-approved AMIs only </strong>is incorrect because having manual information security approval will impact the development process. A better solution is to implement an automated process using AWS Config or a scheduled job using AWS Lambda and CloudWatch Events.</p><p>The option that says: <strong>Set up IAM policies to restrict the ability of users to launch Amazon EC2 instances based on a specific set of pre-approved AMIs which were tagged by the IT Security team</strong> is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact their CI/CD process. The scenario clearly says that the solution should not have any interruption in the company's development process.</p><p>The option that says: <strong>Do regular scans using Amazon Inspector via a custom assessment template that determines if the Amazon EC2 instance is based upon a pre-approved AMI or not. Terminate the instances and inform the IT Security team by email about the security breach </strong>is incorrect because Amazon Inspector is just an automated security assessment service that helps improve the security and compliance of applications deployed in AWS. It does not have the capability to detect EC2 instances that are using unapproved AMIs, unlike AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 82921434,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A healthcare technology company provides a Software as a Service (SaaS) solution to hospitals throughout the United States to use the company\u2019s proprietary system to integrate their clinical documentation and coding workflows. The DevOps team at the company would like to enable a CICD pipeline that enables safe deployments to production and the ability to work on new features of the product roadmap.</p>\n\n<p>As an AWS Certified DevOps Engineer, which solution would you recommend for the given use-case?</p>\n",
          "answers": [
            "<p>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</p>",
            "<p>Create the main CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new CodeCommit repository and create pull requests to merge into the main repository. Set an IAM policy on your developer group to prevent direct pushes to the main repository</p>",
            "<p>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set a repository access policy on your repository to prevent direct pushes to master</p>",
            "<p>Create a CodeCommit repository and create a branch for each feature. Create a CICD pipeline for each branch, and the last step of the CICD pipeline should be to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Projects/CICD%20Pipeline/setup-cicd-pipeline2.5cefde1406fa6787d9d3c38ae6ba3a53e8df3be8.png\">\nvia - <a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p>It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. To protect the master branch you need to set a Deny policy on the IAM group that the developer group should be assigned to.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the main CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new CodeCommit repository and create pull requests to merge into the main repository. Set an IAM policy on your developer group to prevent direct pushes to the main repository</strong> - As mentioned in the explanation above, you should not create a separate repository for each new feature. So this option is incorrect.</p>\n\n<p><strong>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set a repository access policy on your repository to prevent direct pushes to master</strong> - This option has been added as a distractor as there is no such thing as a repository access policy.</p>\n\n<p><strong>Create a CodeCommit repository and create a branch for each feature. Create a CICD pipeline for each branch, and the last step of the CICD pipeline should be to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</strong> - Although you can create a separate CICD pipeline for each branch, you cannot merge multiple pipelines into one to make it a \"master\" pipeline or merge multiple branches into a master branch as the last step of a CICD pipeline. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/faqs/\">https://aws.amazon.com/codecommit/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n"
        }
      },
      {
        "id": 115961505,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application uses an Elastic Load Balancer (ELB) in front of an Auto Scaling group of Amazon EC2 instances. A recent update to the application has resulted in longer times to run and complete bootstrap scripts. The instances often become healthy before they are ready to accept traffic resulting in errors. A DevOps engineer must prevent the instances from being registered with Elastic Load Balancing until the instances are ready to accept traffic.</p><p>Which solution meets these requirements?</p>",
          "answers": [
            "<p>Create an AWS Lambda function that uses the Auto Scaling API to suspend the health check processes until the bootstrap scripts are complete.</p>",
            "<p>Increase the health check grace period from 300 seconds to 600 seconds to ensure the instances are not marked as healthy before they are ready to accept traffic.</p>",
            "<p>Use an Auto Scaling lifecycle hook to verify that the bootstrap scripts have completed before registering the instances with the ELB.</p>",
            "<p>Increase the health check timeout from the default value to 120 seconds and configure the healthy threshold count to 5.</p>"
          ],
          "explanation": "<p>Amazon EC2 Auto Scaling offers the ability to add lifecycle hooks to your Auto Scaling groups. These hooks let you create solutions that are aware of events in the Auto Scaling instance lifecycle, and then perform a custom action on instances when the corresponding lifecycle event occurs.</p><p>A popular use of lifecycle hooks is to control when instances are registered with Elastic Load Balancing. By adding a launch lifecycle hook to your Auto Scaling group, you can ensure that your bootstrap scripts have completed successfully and the applications on the instances are ready to accept traffic before they are registered to the load balancer at the end of the lifecycle hook.</p><p>The following illustration shows the transitions between Auto Scaling instance states:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-33-11-47dc5f5dc473e042857e287968db6fd8.jpg\"><p><strong>CORRECT: </strong>\"Use an Auto Scaling lifecycle hook to verify that the bootstrap scripts have completed before registering the instances with the ELB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increase the health check timeout from the default value to 120 seconds and configure the healthy threshold count to 5\" is incorrect.</p><p>The question specifically states that the solution should prevent the instances from being registered with Elastic Load Balancing until the instances are ready to accept traffic. This solution does not meet the requirement.</p><p><strong>INCORRECT:</strong> \"Increase the health check grace period from 300 seconds to 600 seconds to ensure the instances are not marked as healthy before they are ready to accept traffic\" is incorrect.</p><p>The HealthCheckGracePeriod parameter for the Auto Scaling group helps Amazon EC2 Auto Scaling distinguish unhealthy instances from newly launched instances that are not yet ready to serve traffic. This grace period can prevent Amazon EC2 Auto Scaling from marking InService instances as unhealthy and terminating them before they have time to finish initializing. This does not affect registration with the load balancer.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that uses the Auto Scaling API to suspend the health check processes until the bootstrap scripts are complete\" is incorrect.</p><p>A better solution would be to suspend registration with the load balancer but that was not offered. Suspending health check processes does not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      }
    ],
    "answers": {
      "75949050": [
        "c"
      ],
      "75949082": [
        "a"
      ],
      "75949138": [
        "b",
        "c",
        "d"
      ],
      "82921344": [
        "a"
      ],
      "82921346": [
        "a"
      ],
      "82921392": [
        "a"
      ],
      "82921434": [
        "a"
      ],
      "82921460": [
        "c"
      ],
      "99528195": [
        "c"
      ],
      "99528241": [
        "d"
      ],
      "115961505": [
        "c"
      ],
      "115961513": [
        "b"
      ],
      "134588417": [
        "b",
        "c"
      ],
      "134588421": [
        "a"
      ],
      "134588423": [
        "a"
      ],
      "134588473": [
        "c"
      ],
      "134588513": [
        "a",
        "e"
      ],
      "138248129": [
        "c"
      ],
      "143860751": [
        "a",
        "d"
      ],
      "143860767": [
        "b",
        "d"
      ]
    }
  },
  {
    "id": "1770451632401",
    "date": "2026-02-07T08:07:12.401Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 19,
    "incorrect": 1,
    "unanswered": 0,
    "total": 20,
    "percent": 95,
    "duration": 5906746,
    "questions": [
      {
        "id": 75949168,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 4",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A financial organization is using a multi-account strategy with AWS Organizations. The AWS accounts are organized into different organizational units. Amazon CloudWatch Logs is used for logging within each account. The company needs to ship all the logs to a single centralized account for archiving purposes. The solution must be secure, and centralized, and the logs must be stored cost-effectively.</p><p>How can a DevOps Engineer meet these requirements?</p>",
          "answers": [
            "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3.</p>",
            "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon EFS.</p>",
            "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a subscription for the log stream that triggers and AWS Lambda function that copies the log data to Amazon EFS.</p>",
            "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift.</p>"
          ],
          "explanation": "<p>Subscriptions can be used to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are Base64 encoded and compressed with the gzip format.</p><p>For cross-account log data sharing with subscriptions, you can collaborate with an owner of a different AWS account and receive their log events on the AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Kinesis or Kinesis Data Firehose stream to perform custom processing and analysis. Therefore, we must subscribe the log destination to a Kinesis Firehose delivery stream, which in turn has a destination of S3.</p><p><strong>CORRECT: </strong>\"Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon EFS\" is incorrect.</p><p>The issue with this option is that the target for Kinesis Firehose is set as Amazon EFS which is not a supported destination.</p><p><strong>INCORRECT:</strong> \"Create a log destination in the centralized account and create a log subscription on that destination. Create a subscription for the log stream that triggers and AWS Lambda function that copies the log data to Amazon EFS\" is incorrect.</p><p>This solution is possible, but Amazon EFS is not the most cost-efficient storage solution for archiving purposes. Amazon S3 is more suitable.</p><p><strong>INCORRECT:</strong> \"Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift\" is incorrect.</p><p>The issue with this option is that the target for Kinesis Firehose is set as Amazon Redshift which is not suitable for archiving data and would be very expensive. RedShift is used for analytics purposes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>"
        }
      },
      {
        "id": 82921430,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at an auditing firm has deployed its flagship application on Elastic Beanstalk that processes invoices uploaded by customers in CSV form. The invoices can be quite big, with up to 10MB and 1,000,000 records total. Processing is CPU intensive which results in slowing down the application. Customers are sent an email when the processing is done, through the use of a cron job. The auditing firm has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>What do you recommend for the application to ensure a good performance and address scalability requirements?</p>\n",
          "answers": [
            "<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>",
            "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>",
            "<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>",
            "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>AWS Elastic Beanstalk enables you to manage all of the resources that run your application as environments. An environment is a collection of AWS resources running an application version. When you launch an Elastic Beanstalk environment, you need to choose an environment tier. An application that serves HTTP requests runs in a web server environment tier. A backend environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier.</p>\n\n<p>Elastic Beanstalk Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p>When you create a web server environment, Beanstalk provisions the resources required to run your application. AWS resources created for this type of environment include one elastic load balancer, an Auto Scaling group, and one or more Amazon Elastic Compute Cloud (Amazon EC2) instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p>AWS resources created for a worker environment tier include an ASG, one or more Amazon EC2 instances, and an IAM role. For the worker environment tier, Beanstalk also creates and provisions an SQS queue if you don\u2019t already have one. When you launch a worker environment, Beanstalk installs the necessary support files for your programming language of choice and a daemon on each EC2 instance in the ASG. The daemon reads messages from an SQS queue. The daemon sends data from each message that it reads to the web application running in the worker environment for processing.</p>\n\n<p>For the given use-case, the worker tier is used to asynchronously process the invoices from an SQS queue. SQS size limit is 256KB and therefore the files must be uploaded to S3 and a reference to them should be sent to SQS by the web tier. Finally, the <code>cron.yml</code> file must be defined on the worker tier. Using this strategy we have decoupled our processing tier from our web tier, and CPU usage will go down as a result. The worker tier will also be able to easily scale in case many invoices are uploaded.</p>\n\n<p>Elastic Beanstalk Worker environment:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/aeb-architecture_worker.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong> - As mentioned in the explanation above, the worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The <code>cron.yml</code> file must be defined on the worker tier, it is not supported by the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n"
        }
      },
      {
        "id": 82921372,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An online coding platform wants to fully customize the build tasks and automatically run builds concurrently to take the pain out of managing the build environments. The DevOps team at the company wants to use CodeBuild for all build-tasks and would like the artifacts created by CodeBuild to be named based on the branch being tested. The team wants this solution to be scalable to newer branches with a minimal amount of rework.</p>\n\n<p>As a DevOps Engineer, how would you go about implementing the simplest possible solution to address the given use-case?</p>\n",
          "answers": [
            "<p>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>CODEBUILD_SOURCE_VERSION</code> at runtime. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</p>",
            "<p>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>BRANCH_NAME</code> at runtime. For each existing branch and new branch, create a separate CodeBuild and set the <code>BRANCH_NAME</code> variable accordingly. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</p>",
            "<p>Create a <code>buildspec.yml</code> file that will be different for every single branch. Create a new CodeBuild for each branch. Upon adding a new branch, ensure to edit the <code>buildspec.yml</code> file</p>",
            "<p>Create a unique <code>buildspec.yml</code> file that will be the same for each branch and will name the artifacts the same way. When the artifact is uploaded into S3, create an S3 Event that will trigger a Lambda function that will issue an API call against CodeBuild, extract the branch name from it and rename the file on S3</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>CODEBUILD_SOURCE_VERSION</code> at runtime. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</strong></p>\n\n<p>AWS CodeBuild is a fully managed continuous integration service in the cloud. CodeBuild compiles source code, runs tests, and produces packages that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers.</p>\n\n<p>A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build. You can include a buildspec as part of the source code or you can define a buildspec when you create a build project.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q5-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p>For the given use-case, we need to use environment variables. The variable <code>CODEBUILD_SOURCE_VERSION</code> is exposed at runtime directly within CodeBuild and represents the branch name of the code being tested for CodeCommit. This is the best solution.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q5-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>BRANCH_NAME</code> at runtime. For each existing branch and new branch, create a separate CodeBuild and set the <code>BRANCH_NAME</code> variable accordingly. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</strong> - Providing the branch name as <code>BRANCH_NAME</code> and creating separate CodeBuild would be highly tedious to maintain and error-prone. This is certainly not the simplest solution possible.</p>\n\n<p><strong>Create a <code>buildspec.yml</code> file that will be different for every single branch. Create a new CodeBuild for each branch. Upon adding a new branch, ensure to edit the <code>buildspec.yml</code> file</strong> - Maintaining a different <code>buildspec.yml</code> for each branch is not efficient and it's error-prone. So this option is incorrect.</p>\n\n<p><strong>Create a unique <code>buildspec.yml</code> file that will be the same for each branch and will name the artifacts the same way. When the artifact is uploaded into S3, create an S3 Event that will trigger a Lambda function that will issue an API call against CodeBuild, extract the branch name from it and rename the file on S3</strong> - The answer involving a Lambda function would work but is highly convoluted. This is something that can be directly accomplished using the <code>CODEBUILD_SOURCE_VERSION</code> environment variable.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html</a></p>\n"
        }
      },
      {
        "id": 134588387,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has its on-premises data network connected to their AWS VPC via a Direct Connect connection. Their DevOps team is maintaining their Media Asset Management (MAM) system which uses a repository of over 50-TB digital videos and media files that are stored on their on-premises tape library. Due to the sheer size of their data, they want to implement an automated catalog system that will enable them to search their files using facial recognition. A catalog will store the faces of the people who are present in these videos including a still image of each person. Eventually, the media company would like to migrate these media files to AWS including the MAM video contents. </p><p>Which of the following provides a solution which uses the LEAST amount of ongoing management overhead and will cause MINIMAL disruption to the existing system?</p>",
          "answers": [
            "<p>Connect the on-premises file system to AWS Storage Gateway by setting up a file gateway appliance on-premises. Use the MAM solution to extract the media files from the current data store and send them into the file gateway. Populate a collection using Amazon Rekognition by building a catalog of faces from the processed media files. Launch a Lambda function to invoke Amazon Rekognition Javascript SDK to have it fetch the media files from the S3 bucket which is backing the file gateway. Retrieve the needed metadata using the Lambda function and store the information into the MAM solution.</p>",
            "<p>Using Amazon Kinesis Video Streams, create a video ingestion stream and build a collection of faces with Amazon Rekognition. Stream the media files from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed files. Set up a stream consumer to retrieve the required metadata, and store them into the MAM solution. Configure the stream to store the files in an Amazon S3 bucket.</p>",
            "<p>Launch a tape gateway appliance in your on-premises data center and connect it to your AWS Storage Gateway service. Set up the MAM solution to fetch the media files from the current archive and store them into the tape gateway in the AWS Cloud. Build a collection from the catalog of faces using Amazon Rekognition. Set up an AWS Lambda function which invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video directly from the tape gateway in real-time. Retrieve the required metadata and store them into the MAM solution.</p>",
            "<p>Move all of the media files from the on-premises library into an EBS volume mounted on a large Amazon EC2 instance. Set up an open-source facial recognition tool in the instance. Process the media files to retrieve the metadata and store this information into the MAM solution. Copy the media files to an Amazon S3 bucket.</p>"
          ],
          "explanation": "<p><strong>Amazon Rekognition</strong> can store information about detected faces in server-side containers known as collections. You can use the facial information that's stored in a collection to search for known faces in images, stored videos, and streaming videos. Amazon Rekognition supports the <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_IndexFaces.html\">IndexFaces</a> operation. You can use this operation to detect faces in an image and persist information about facial features that are detected in a collection. This is an example of a <em>storage-based</em> API operation because the service persists information on the server.</p><p><img src=\"https://media.tutorialsdojo.com/public/detect-analyze-faces-rekognition_3AUG2023.png\"></p><p><strong>AWS Storage Gateway</strong> offers file-based, volume-based, and tape-based storage solutions. With a tape gateway, you can cost-effectively and durably archive backup data in GLACIER or DEEP_ARCHIVE. A tape gateway provides a virtual tape infrastructure that scales seamlessly with your business needs and eliminates the operational burden of provisioning, scaling, and maintaining a physical tape infrastructure.</p><p>You can run AWS Storage Gateway either on-premises as a VM appliance, as a hardware appliance or in AWS as an Amazon Elastic Compute Cloud (Amazon EC2) instance. You deploy your gateway on an EC2 instance to provision iSCSI storage volumes in AWS. You can use gateways hosted on EC2 instances for disaster recovery, data mirroring, and providing storage for applications hosted on Amazon EC2.</p><p>Hence, the correct answer is: <strong>Connect the on-premises file system to AWS Storage Gateway by setting up a file gateway appliance on-premises. Use the MAM solution to extract the media files from the current data store and send them into the file gateway. Populate a collection using Amazon Rekognition by building a catalog of faces from the processed media files. Launch a Lambda function to invoke Amazon Rekognition Javascript SDK to have it fetch the media files from the S3 bucket which is backing the file gateway. Retrieve the needed metadata using the Lambda function and store the information into the MAM solution.</strong></p><p>The option that says: <strong>Move all of the media files from the on-premises library into an EBS volume mounted on a large Amazon EC2 instance. Set up an open-source facial recognition tool in the instance. Process the media files to retrieve the metadata and store this information into the MAM solution. Copy the media files to an Amazon S3 bucket</strong> is incorrect because it entails a lot of ongoing management overhead instead of just using Amazon Rekognition. Moreover, it is more suitable to use the AWS Storage Gateway service rather than an EBS Volume.</p><p>The option that says: <strong>Launch a tape gateway appliance in your on-premises data center and connect it to your AWS Storage Gateway service. Set up the MAM solution to fetch the media files from the current archive and store them into the tape gateway in the AWS Cloud. Build a collection from the catalog of faces using Amazon Rekognition. Set up an AWS Lambda function which invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video directly from the tape gateway in real-time. Retrieve the required metadata and store them into the MAM solution</strong> is incorrect. Although this is using the right combination of AWS Storage Gateway and Amazon Rekognition, take note that you can't directly fetch the media files from your tape gateway in real-time since this is backed up using Glacier. Although the on-premises data center is using a tape gateway, you can still set up a solution to use a file gateway in order to properly process the videos using Amazon Rekognition. Keep in mind that the tape gateway in AWS Storage Gateway service is primarily used as an archive solution.</p><p>The option that says: <strong>Using Amazon Kinesis Video Streams, create a video ingestion stream and build a collection of faces with Amazon Rekognition. Stream the media files from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed files. Set up a stream consumer to retrieve the required metadata, and store them into the MAM solution. Configure the stream to store the files in an Amazon S3 bucket </strong>is incorrect<strong> </strong>because you won't be able to connect your tape gateway directly to your Kinesis Video Streams service. You need to use the AWS Storage Gateway first.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/collections.html\">https://docs.aws.amazon.com/rekognition/latest/dg/collections.html</a></p><p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p><p><br></p><p><strong>Check out this Amazon Rekognition Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-rekognition/?src=udemy\"><strong>https://tutorialsdojo.com/amazon-rekognition/</strong></a></p>"
        }
      },
      {
        "id": 138248201,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps Engineer in a leading aerospace engineering company has a hybrid cloud architecture that connects its on-premises data center with AWS via Direct Connect Gateway. There is a new requirement to implement an automated OS patching solution for all of the Windows servers hosted on-premises as well as in AWS Cloud. The AWS Systems Manager service should be utilized to automate the patching of the servers.</p><p>Which combination of steps should be set up to satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS <code>AssumeRoleWithSAML</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
            "<p>Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS <code>AssumeRole</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>mi-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager State Manager.</p>"
          ],
          "explanation": "<p>A hybrid environment includes on-premises servers and virtual machines (VMs) that have been configured for use with Systems Manager, including VMs in other cloud environments. After following the steps below, the users who have been granted permissions by the AWS account administrator can use AWS Systems Manager to configure and manage their organization's on-premises servers and virtual machines (VMs).</p><p>To configure your hybrid servers and VMs for AWS Systems Manager, just follow these provided steps:</p><p>1. Complete General Systems Manager Setup Steps<br>2. Create an IAM Service Role for a Hybrid Environment<br>3. Install a TLS certificate on On-Premises Servers and VMs<br>4. Create a Managed-Instance Activation for a Hybrid Environment<br>5. Install SSM Agent for a Hybrid Environment (Windows)<br>6. Install SSM Agent for a Hybrid Environment (Linux)<br>7. (Optional) Enable the Advanced-Instances Tier</p><p><br></p><p>Configuring your hybrid environment for Systems Manager enables you to do the following:</p><p>- Create a consistent and secure way to remotely manage your hybrid workloads from one location using the same tools or scripts.</p><p>- Centralize access control for actions that can be performed on your servers and VMs by using AWS Identity and Access Management (IAM).</p><p>- Centralize auditing and your view into the actions performed on your servers and VMs by recording all actions in AWS CloudTrail.</p><p>- Centralize monitoring by configuring CloudWatch Events and Amazon SNS to send notifications about service execution success.</p><p><img src=\"https://media.tutorialsdojo.com/public/how-it-works.png\">After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as <em>managed instances</em>. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix \"mi-\". Amazon EC2 instance IDs use the prefix \"i-\".</p><p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants <code>AssumeRole</code> trust to the Systems Manager service. You only need to create the service role for a hybrid environment once for each AWS account.</p><p>Hence, the correct answers are:</p><p><strong>- Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRole</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</strong></p><p><strong>- Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>mi-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager.</strong></p><p>The option that says: <strong>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRoleWithSAML</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation </strong>is incorrect because you have to execute the <code><em>AssumeRole </em></code>operation instead and not the <code><em>AssumeRoleWithSAML</em></code><em> </em>operation<em>. </em>Moreover, you only need to set up a single IAM service role.</p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager </strong>is incorrect because the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix in the SSM console and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix<strong><em>.</em></strong></p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager State Manager</strong> is incorrect because the AWS Systems Manager State Manager is just a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. You have to apply the patches using the Systems Manager Patch Manager instead. In addition, the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 67357170,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company uses an AWS CodePipeline pipeline to deploy updates to the API several times a month. As part of this process, the DevOps team exports the JavaScript SDK for the API from the API Gateway console and uploads it to an Amazon S3 bucket, which is being used as an origin for an Amazon CloudFront distribution. Web clients access the SDK through the CloudFront distribution's endpoint. The goal is to have an automated solution that ensures the latest SDK is always available to clients whenever there's a new API deployment.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you suggest?</p>\n",
          "answers": [
            "<p>Set up an Amazon EventBridge rule on a schedule that is invoked every 5 minutes. Configure this rule to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</p>",
            "<p>Set up an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure this rule to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</p>",
            "<p>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</p>",
            "<p>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong></p>\n\n<p>AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. By creating a CodePipeline action with an AWS Lambda function immediately after the API deployment stage, the DevOps team can automate the process of downloading the SDK from API Gateway and uploading it to the S3 bucket. Additionally, the Lambda function can create a CloudFront invalidation for the SDK path, ensuring that web clients get the latest SDK without any caching issues.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure this rule to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong></p>\n\n<p><strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong></p>\n\n<p>You cannot use any S3 API to invalidate the CloudFront cache, so both of these options are incorrect.</p>\n\n<p><strong>Set up an Amazon EventBridge rule on a schedule that is invoked every 5 minutes. Configure this rule to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong> - It is wasteful to invoke an EventBridge rule every 5 minutes to invalidate the CloudFront cache. You should only invalidate the cache when the API has actually been updated.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p>\n"
        }
      },
      {
        "id": 134588495,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company needs to transform AWS Network Firewall flow logs and add extra data before storing the flow logs in an existing Amazon S3 bucket. Currently, the flow logs go directly to the S3 bucket, and the company uses Amazon Athena to analyze the data.</p><p>The goal is to process the logs before delivering the processed logs to the S3 bucket.</p><p>How can the company transform the flow logs and deliver them to the existing S3 bucket?</p>",
          "answers": [
            "<p>Set up Amazon SQS to queue the logs before processing. Use a Lambda function triggered by SQS to transform the logs and save the results to the existing S3 bucket. This setup provides a buffer to handle spikes in log volume efficiently.</p>",
            "<p>Use Amazon S3 Event Notifications to trigger an AWS Glue job. Configure the job to transform the logs and write the processed data back to the existing S3 bucket. Ensure the Glue job is set to handle structured and unstructured log formats.</p>",
            "<p>Develop an AWS Lambda function to transform the data and save a new object in the existing S3 bucket. Set up an S3 trigger on this bucket to activate the Lambda function, specifying all object creation events as the trigger type. Ensure that recursive invocation is accounted for.</p>",
            "<p>Set up an Amazon Data Firehose delivery stream with an AWS Lambda transformer. Designate the current S3 bucket as the destination. Modify the Network Firewall logging to use Data Firehose instead of Amazon S3.</p>"
          ],
          "explanation": "<p><strong>Amazon Data Firehose</strong> is a fully managed service for real-time data streaming to destinations such as Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and third-party providers. It enables on-the-fly data transformation using AWS Lambda and supports automatic scaling to handle varying data loads. Data Firehose is commonly used for streaming log data, analytics, and ETL pipelines due to its reliability, scalability, and low-latency delivery.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD_Amazon_Data_Firehose-15Nov2024.png\"></p><p>Amazon Data Firehose allows you to capture, transform, and send data in near real-time. Logs can be pre-processed before being stored in the destination S3 bucket by adding a Lambda function to the delivery stream as a transformer. Data Firehose also natively interfaces with various AWS services, including S3, and includes built-in retry and scaling methods to ensure reliability and efficiency while managing large-scale log ingestion and transformations.</p><p>Hence, the correct answer is: <strong>Set up an Amazon Data Firehose delivery stream with an AWS Lambda transformer. Designate the current S3 bucket as the destination. Modify the Network Firewall logging to use Data Firehose instead of Amazon S3.</strong></p><p>The option that says: <strong>Set up Amazon SQS to queue the logs before processing. Use a Lambda function triggered by SQS to transform the logs and save the results to the existing S3 bucket. This setup provides a buffer to handle spikes in log volume efficiently</strong> is incorrect because using Amazon SQS with a Lambda function can only process logs but does not natively handle the direct integration and delivery pipeline to S3 like Data Firehose. This solution adds operational complexity, requiring custom logic for retries and scaling, which Data Firehose inherently provides.</p><p>The option that says: <strong>Use Amazon S3 Event Notifications to trigger an AWS Glue job. Configure the job to transform the logs and write the processed data back to the existing S3 bucket. Ensure the Glue job is set to handle structured and unstructured log formats</strong> is incorrect because triggering an AWS Glue job with S3 Event Notifications would simply process logs after they are already stored in the bucket, which violates the requirement to transform the logs before storage. Glue is suitable for batch transformations but not for real-time processing.</p><p>The option that says: <strong>Develop an AWS Lambda function to transform the data and save a new object in the existing S3 bucket. Set up an S3 trigger on this bucket to activate the Lambda function, specifying all object creation events as the trigger type. Ensure that recursive invocation is accounted for</strong> is incorrect because setting up a Lambda function with an S3 trigger introduces the risk of recursive invocation which the Lambda function creating new events by writing transformed objects back to the bucket. Additionally, this approach processes logs after they are written to S3, not before, failing to meet the requirement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html\">https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html</a></p><p><a href=\"https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html\">https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html</a></p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html\">https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/#data-firehose/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/#data-firehose</a></p>"
        }
      },
      {
        "id": 115961491,
        "correct_response": [
          "c",
          "e"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>To improve security, a company plans to use AWS Systems Manager Session Manager to manage EC2 instances instead of using key pairs. The company also requires that access to Session Manager goes across private networks only.</p><p>Which combinations of actions will accomplish this? (Select TWO.)</p>",
          "answers": [
            "<p>Update all EC2 instance security groups to allow SSH port TCP 22 inbound from the VPC CIDR.</p>",
            "<p>Deploy an AWS Site to Site VPN in the relevant AWS Region for private access to Systems Manager.</p>",
            "<p>Attach an IAM policy providing the required Systems Manager permissions to an existing IAM instance profile.</p>",
            "<p>Run the \u2018aws configure\u2019 command on all EC2 instances to add access keys that provide the required Systems Manager permissions.</p>",
            "<p>Create VPC endpoints for Systems Manager in the relevant AWS Region to provide private access.</p>"
          ],
          "explanation": "<p>Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage Amazon EC2 instances, edge devices, and on-premises servers and virtual machines (VMs).</p><p>Session Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. You do not need to open SSH ports in security groups and can enable private access using VPC endpoints.</p><p>To manage your instances using Session Manager you must install the Systems Manager agent on them and provide the necessary permissions for management. Permissions should be assigned through IAM instance profiles and policies.</p><p><strong>CORRECT: </strong>\"Attach an IAM policy providing the required Systems Manager permissions to an existing IAM instance profile\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create VPC endpoints for Systems Manager in the relevant AWS Region to provide private access\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update all EC2 instance security groups to allow SSH port TCP 22 inbound from the VPC CIDR\" is incorrect.</p><p>With Systems Manager Session Manager you do not need to open SSH ports.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Site to Site VPN in the relevant AWS Region for private access to Systems Manager\" is incorrect.</p><p>The private access between Session Manager and EC2 instances happens within AWS via VPC endpoints. A VPN cannot be used.</p><p><strong>INCORRECT:</strong> \"Run the \u2018aws configure\u2019 command on all EC2 instances to add access keys that provide the required Systems Manager permissions\" is incorrect.</p><p>This is a less secure method of providing the permissions needed by the EC2 instances. The better option is to use IAM instance profiles and policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 67357176,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>An e-commerce company is deploying its flagship application on Amazon EC2 instances. The DevOps team at the company needs a solution to query both the application logs as well as the AWS account API activity.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you recommend to meet these requirements?</p>\n",
          "answers": [
            "<p>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</p>",
            "<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</p>",
            "<p>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</p>",
            "<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</strong></p>\n\n<p>CloudTrail is enabled by default for your AWS account. You can use Event history in the CloudTrail console to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. This includes activity made through the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. For an ongoing record of events in your AWS account, you can create a trail. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all AWS Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs.</p>\n\n<p>You can also configure CloudTrail with CloudWatch Logs to monitor your trail API logs and be notified when specific activity occurs. When you configure your trail to send events to CloudWatch Logs, CloudTrail sends only the events that match your trail settings. For example, if you configure your trail to log data events only, your trail sends data events only to your CloudWatch Logs log group. CloudTrail supports sending data, Insights, and management events to CloudWatch Logs.</p>\n\n<p>You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is CWAgent, although you can specify a different namespace when you configure the agent. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs</p>\n\n<p>For the given use case, you can have both AWS CloudTrail as well as the Amazon CloudWatch Agent deliver the respective logs to CloudWatch Logs. You can then use the CloudWatch Logs Insights to query both sets of logs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. So, this option is incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</strong> - You cannot deliver the API logs from AWS CloudTrail to Kinesis Data Streams. Similarly, you cannot have the Amazon CloudWatch Agent deliver logs from the EC2 instances to Kinesis Data Streams. So, both these options are incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use Amazon Athena to query logs published to Amazon CloudWatch Logs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html</a></p>\n"
        }
      },
      {
        "id": 82921392,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A cyber forensics company would like to ensure that CloudTrail is always enabled in its AWS account. It also needs to have an audit trail of the status for CloudTrail. In the case of compliance breaches, the company would like to automatically resolve them.</p>\n\n<p>As a DevOps Engineer, how can you implement a solution for this requirement?</p>\n",
          "answers": [
            "<p>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</p>",
            "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the 'everyone' group automatically</p>",
            "<p>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>",
            "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/config/faq/\">https://aws.amazon.com/config/faq/</a></p>\n\n<p>You need to have an AWS Config rule to maintain auditability and track compliance over time. You can use the <code>cloudtrail-enabled</code> Config managed rule to check whether AWS CloudTrail is enabled in your AWS account. You can use <code>cloudtrail-security-trail-enabled</code> Config managed rules to check that there is at least one AWS CloudTrail trail defined with security best practices. To be alerted of compliance issues, use a CloudWatch Event rule and then hook it to a Lambda function that will re-enable CloudTrail automatically.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the 'everyone' group automatically</strong></p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong></p>\n\n<p>IAM users in a group with a deny policy sounds like a great idea at first, but then you have to remember you can create IAM roles, and they won't have that restriction, and as such you will be able to assume these roles and then issue API calls on CloudTrail to de-activate it. This solution won't work and therefore both these options are incorrect.</p>\n\n<p><strong>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong> - You need to have an AWS Config rule to maintain auditability and track compliance over time, as using the Lambda function to trigger an API call would tell you about the CloudTrail status only at that point in time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/config/faq/\">https://aws.amazon.com/config/faq/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n"
        }
      },
      {
        "id": 99528235,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A development team run a fleet of Amazon EC2 instances in a dev/test environment. A manager is concerned about the costs of the workloads. While the developers are unable to determine the exact resource requirements for each workload, they also cannot terminate any instances as all are currently in operational use.</p><p>What is the best way to ensure the most efficient use of the underlying hardware?</p>",
          "answers": [
            "<p>Use AWS Control Tower to inspect the utilization metrics of the workloads and use the reports to right-size the instance types to the workloads. Implement the changes using EC2 Image Builder.</p>",
            "<p>Use Amazon Detective to run statistical analysis on the log data collected from EC2 instances to determine services that are underutilized. Disable unnecessary services automatically using an AWS Lambda function.</p>",
            "<p>Use AWS Compute Optimizer to report on resource utilization on EC2 instances and use the recommendations to manually reconfigure resources for improved utilization.</p>",
            "<p>Use AWS CloudWatch to collect performance metrics and create a dashboard displaying utilization metrics across the EC2 instances. Auto remediate instance types with AWS Systems Manager automation documents.</p>"
          ],
          "explanation": "<p>AWS Compute Optimizer helps you identify the optimal AWS resource configurations, such as Amazon Elastic Compute Cloud (EC2) instance types, Amazon Elastic Block Store (EBS) volume configurations, task sizes of Amazon Elastic Container Service (ECS) services on AWS Fargate, and AWS Lambda function memory sizes, using machine learning to analyze historical utilization metrics.</p><p>The recommendations provided by AWS Computer Optimizer can be used by the development team to right-size the workloads which will ensure cost-efficiency.</p><p><strong>CORRECT: </strong>\"Use AWS Computer Optimizer to report on resource utilization on EC2 instances and use the recommendations to manually reconfigure resources for improved utilization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Control Tower to inspect the utilization metrics of the workloads and use the reports to right-size the instance types to the workloads. Implement the changes using EC2 Image Builder\" is incorrect.</p><p>Control Tower is used for managing multiple AWS accounts and applying best practice baselines and guardrails. It is not used for monitoring instance utilization metrics. EC2 Image Builder is used for automating the image management processes for AMIs.</p><p><strong>INCORRECT:</strong> \"Use Amazon Detective to run statistical analysis on the log data collected from EC2 instances to determine services that are underutilized. Disable unnecessary services automatically using an AWS Lambda function\" is incorrect.</p><p>Amazon Detective is a security service that runs statistical analysis on log data using machine learning algorithms. It is used to identify security issues and does not provide performance recommendations for right-sizing workloads.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudWatch to collect performance metrics and create a dashboard displaying utilization metrics across the EC2 instances. Auto remediate instance types with AWS Systems Manager automation documents\" is incorrect.</p><p>Amazon CloudWatch is used for performance monitoring. However, it would be more efficient for the development team to leverage AWS Computer Optimizer for the purpose of right-sizing workloads as it is a tool that is designed for exactly that purpose.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/compute-optimizer/faqs/\">https://aws.amazon.com/compute-optimizer/faqs/</a></p>"
        }
      },
      {
        "id": 82921374,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.</p>\n\n<p>How can you improve the user experience with the least effort?</p>\n",
          "answers": [
            "<p>Enhance the health check so that the return status code corresponds to the connectivity to the database</p>",
            "<p>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</p>",
            "<p>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</p>",
            "<p>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enhance the health check so that the return status code corresponds to the connectivity to the database</strong></p>\n\n<p>Configuring health checks for the Application Load Balancer (ALB) is an important step to ensure that your AWS Cloud application runs smoothly. The ALB Health Check is configured with a protocol and port number to call on the target instances. A healthy EC2 instance is one that issues a response to a health check call with an HTTP 200 response code. Instances that return a status code that is other than the 2XX range or which time out are designated as being unhealthy and will not receive traffic from the ELB.</p>\n\n<p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q69-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>You could just add a simple health check endpoint to the ALB which accepts a request and immediately responds with an HTTP status of 200. This approach provides for a fast health check, but would not meet the requirement for the given use-case. You need to improve the quality of the health check and make sure it returns a proper status code. As the application depends on the database, you need to ensure that you include health checks for these components when determining the health of your service.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</strong> - Migrating to Beanstalk would require significant effort and even then it won't help gather detailed database-specific health checks.</p>\n\n<p><strong>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</strong> - Health Checks for the ALB are pretty basic and only work with the HTTP return status code, not the payload itself.</p>\n\n<p><strong>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</strong> - Route53 health checks can only be used to prevent DNS records from being returned from a DNS query, so it won't help for routing to specific instances behind an ALB (that's why we have health checks at the ALB level).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf\">https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf</a></p>\n"
        }
      },
      {
        "id": 143860749,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is deploying an application in four AWS Regions across North America, Europe, and Asia. The application will be used by millions of users. The application must allow users to submit data through the application layer in each Region and have it saved in a low-latency database layer. The company also must ensure that the data can be read through the application layer in each Region.</p><p>Which solution will meet these requirements with the LOWEST latency of reads and writes?</p>",
          "answers": [
            "<p>Create a table in Amazon DynamoDB and enable global tables in each of the four Regions.</p>",
            "<p>Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions.</p>",
            "<p>Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions.</p>",
            "<p>Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions.</p>"
          ],
          "explanation": "<p>Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p><p>This is the only workable solution in the list that provides both reads and writes in each Region that are replicated across the other Regions. This is also the best solution as it provides low latency reads and writes.</p><p><strong>CORRECT: </strong>\"Create a table in Amazon DynamoDB and enable global tables in each of the four Regions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions\" is incorrect.</p><p>This solutions does not provide local writes in each Region as the Read Replicas cannot be written to. Therefore, this solution only offers low latency reads in each Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions\" is incorrect.</p><p>Replication groups in ElastiCache are used within a Region and not across Regions so this solution does not work.</p><p><strong>INCORRECT:</strong> \"Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions\" is incorrect.</p><p>This solution provides a database in each Region, there is no mechanism for replication. You can create replica instances in different Regions but that would only provide low latency reads (as with RDS), and not low latency writes.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>"
        }
      },
      {
        "id": 75949114,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 3",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is using AWS CodePipeline to automate the release lifecycle of an application. AWS CodeDeploy used to deploy an application to Amazon ECS using the blue/green deployment model. The company wants to run test scripts to validate the green version of the application before shifting traffic. The scripts will complete in less than 5 minutes. If errors are discovered by the scripts, the application must be rolled back.</p><p>Which strategy will meet these requirements?</p>",
          "answers": [
            "<p>Add a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create an execution environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.</p>",
            "<p>Add a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.</p>",
            "<p>Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to run the test scripts. If errors are found, exit the Lambda function with an error to trigger rollback.</p>",
            "<p>Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment.</p>"
          ],
          "explanation": "<p>You can use the 'hooks' section of the CodeDeploy AppSpec file to specify a Lambda function that CodeDeploy can call to validate an Amazon ECS deployment.</p><p>You can use the same function or a different one for the BeforeInstall, AfterInstall, AfterAllowTestTraffic, BeforeAllowTraffic, and AfterAllowTraffic deployment lifecyle events. Following completion of the validation tests, the Lambda AfterAllowTraffic function calls back CodeDeploy and delivers a result of Succeeded or Failed.</p><p><strong>CORRECT: </strong>\"Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to run the test scripts. If errors are found, exit the Lambda function with an error to trigger rollback\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment\" is incorrect.</p><p>The CLI command is not required, either the Lambda function can use AfterAllowTraffic to call back and deliver a success/failure message or if it does not report back within one hour the deployment is assumed to have failed.</p><p><strong>INCORRECT:</strong> \"Add a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create an execution environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment\" is incorrect.</p><p>CodeBuild is not required for running the test scripts which can be added using hooks instead.</p><p><strong>INCORRECT:</strong> \"Add a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment\" is incorrect.</p><p>There is no need to use an extra stage to run an AWS Lambda function, this can be achieved using hooks in the AppSpec file.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-section-structure-ecs-sample-function\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-section-structure-ecs-sample-function</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 134588499,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A web application is hosted on an Auto Scaling group (ASG) of On-Demand EC2 instances. It has a separate Linux EC2 instance used primarily for batch processing. This particular instance needs to update its configuration file based on the list of the active IP addresses of the instances within the ASG. This is needed in order to run a batch job properly.</p><p>Which of the following options allows effective updating of the configuration file whenever the Auto Scaling group scales in or out?</p>",
          "answers": [
            "<p>Develop a custom script that runs on the background to query active EC2 instances and IP addresses of the ASG. Then the script will update the configuration whenever it detects a change.</p>",
            "<p>Create an Amazon EventBridge rule that is scheduled to run every minute. Set the target to a Lambda function to query the ASG instances and update the configuration file on an S3 bucket. Automatically establish an SSH connection to the Linux EC2 instances by using another Lambda function and then download the master configuration file from S3 to update the local configuration file stored in the instance.</p>",
            "<p>Create a CloudWatch Logs to monitor the Launch/Terminate events of the ASG. Set the target to a Lambda function that will update the configuration file inside the EC2 instance. Add a proper IAM permission to Lambda to access the EC2 instance.</p>",
            "<p>Create an Amazon EventBridge rule for the Launch/Terminate events of the ASG. Set the target to an SSM Run Command that will update the configuration file on the target EC2 instance.</p>"
          ],
          "explanation": "<p>You can use <strong>Amazon EventBridge (Amazon CloudWatch Events)</strong> to invoke AWS Systems Manager Run Command and perform actions on Amazon EC2 instances when certain events happen.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-eventbridge-rule-event-pattern-10Jan2025.png\"></p><p>You can use SSM Run Command to configure instances without having to login to an instance. This setup requires the SSM agent to be installed on your EC2 instance and that proper IAM permission has been granted. Additionally, create an Amazon EventBridge rule to monitor the launch or termination events of the Auto Scaling group. Set the Systems Manager Run Command as the target for this rule.</p><p>On the parameters of SSM Run Command, define the commands you need to update the configuration file. It is important that your target EC2 instance is properly tagged and defined on the SSM Run command as it will be the basis of SSM to identify your instance.</p><p>Hence, the correct answer is: <strong>Create an Amazon EventBridge rule for the Launch/Terminate events of the ASG. Set the target to an SSM Run Command that will update the configuration file on the target EC2 instance.</strong></p><p>The option that says: <strong>Develop a custom script that runs on the background to query active EC2 instances and IP addresses of the ASG. Then the script will update the configuration whenever it detects a change </strong>is incorrect. Although this may be a possible solution, you will still have to write and maintain your own script and that creates an unnecessary operational overhead. Also, running a script with regular intervals is not a good approach if you are just waiting for events.</p><p>The option that says: <strong>Create a CloudWatch Logs to monitor the Launch/Terminate events of the ASG. Set the target to a Lambda function that will update the configuration file inside the EC2 instance. Add a proper IAM permission to Lambda to access the EC2 instance </strong>is incorrect because you can't simply monitor the Auto Scaling Group events using CloudWatch Logs. You have to use Amazon EventBridge instead.</p><p>The option that says: <strong>Create an Amazon EventBridge rule that is scheduled to run every minute. Set the target to a Lambda function to query the ASG instances and update the configuration file on an S3 bucket. Automatically establish an SSH connection to the Linux EC2 instances by using another Lambda function and then download the master configuration file from S3 to update the local configuration file stored in the instance</strong> is incorrect. It is better to create an Amazon EventBridge rule that tracks the Launch/Terminate events of the Auto Scaling group (ASG) instead of running it every minute. AWS Lambda can\u2019t directly establish an SSH connection and run a command inside an EC2 instance to update the configuration file, even with proper IAM permissions. A better solution would be to use the Systems Manager Run Command to let you remotely and securely manage the configuration of your managed instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-prereqs.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/rc-console.html</a></p><p><br></p><p><strong>Check out these Amazon CloudWatch and AWS Systems Manager Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 99528225,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a global retail company wants to deploy the latest application code to through build, staging, beta &amp; prod environments. While doing the staging deployment, an automated functional test suite needs to be executed which runs for approximately two hours to complete regression testing. The code is managed via AWS CodeCommit.</p><p>How can a DevOps engineer optimize the configuration and automate the pipeline?</p>",
          "answers": [
            "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage succeeds, the last stage will deploy the application to production.</p>",
            "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production.</p>",
            "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn\u2019t fail, the last stage will deploy the application to production.</p>",
            "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a Step Function which will run the test suite. Create a CloudWatch Event Rule on the execution termination of the Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn\u2019t fail, the last stage will deploy the application to production.</p>"
          ],
          "explanation": "<p>The solution to these requirements uses an AWS CodePipeline pipeline to automate the entire CI/CD pipeline. When code is committed it will be picked up as a change and the pipeline execution commences. This will automate the testing suites and deployment into production using CodeDeploy.</p><p>Lambda should be ruled out for running the test suite as the maximum timeout for a Lambda function is 15 minutes, so it will not support the given use-case since the functional test suite runs for over two hours.</p><p><strong>INCORRECT:</strong> \"Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn\u2019t fail, the last stage will deploy the application to production\" is incorrect.</p><p><strong>CORRECT: </strong>\"Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage succeeds, the last stage will deploy the application to production\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CodePipeline pointing to the master branch of the CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production\" is incorrect.</p><p>Since CodeBuild Test can\u2019t be as a stage prior to deploying in the staging environment, so this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a Step Function which will run the test suite. Create a CloudWatch Event Rule on the execution termination of the Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn\u2019t fail, the last stage will deploy the application to production\" is incorrect.</p><p>AWS Step Functions is a low-code, visual workflow service that developers use to build distributed applications, automate IT and business processes, and build data and machine learning pipelines using AWS services. Workflows manage failures, retries, parallelization, service integrations, and observability so developers can focus on higher-value business logic. This is not an ideal use case for Step Functions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/\">https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 67357136,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multi-national company with hundreds of AWS accounts has slowly adopted AWS Organizations with all features enabled. The company has also configured a few Organization Units (OUs) to serve its business objectives. The company has some AWS Identity and Access Management (IAM) roles that need to be configured for every new AWS account created for the company. Also, the security policy mandates enabling AWS CloudTrail for all AWS accounts. The company is looking for an automated solution that can add the mandatory IAM Roles and CloudTrail configurations to all newly created accounts and also delete the resources/configurations when an account leaves the organization without manual intervention.</p>\n\n<p>What should a DevOps engineer do to meet these requirements with the minimal overhead?</p>\n",
          "answers": [
            "<p>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</p>",
            "<p>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</p>",
            "<p>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</p>",
            "<p>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</strong></p>\n\n<p>You can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. For example, you can deploy your centralized AWS Identity and Access Management (IAM) roles, provision Amazon Elastic Compute Cloud (Amazon EC2) instances or AWS Lambda functions across AWS Regions and accounts in your organization. CloudFormation StackSets simplify the configuration of cross-account permissions and allow for the automatic creation and deletion of resources when accounts are joined or removed from your Organization.</p>\n\n<p>You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the AWS Organizations master account to deploy stacks to all accounts in your organization or specific organizational units (OUs). A new service-managed permission model is available with these StackSets. Choosing <code>Service managed permissions</code> allows StackSets to automatically configure the necessary IAM permissions required to deploy your stack to the accounts in your organization.</p>\n\n<p>In addition to setting permissions, CloudFormation StackSets offer the option for automatically creating or removing your CloudFormation stacks when a new AWS account joins or quits your Organization. You do not need to remember to manually connect to the new account to deploy your common infrastructure or to delete infrastructure when an account is removed from your Organization. When an account leaves the organization, the stack will be removed from the management of StackSets. However, you can choose to either delete or retain the resources managed by the stack.</p>\n\n<p>Lastly, you choose whether to deploy a stack to your entire organization or just to one or more Organization Units (OU). You also choose a couple of deployment options: how many accounts will be prepared in parallel, and how many failures you tolerate before stopping the entire deployment.</p>\n\n<p>Use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by an AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</strong> - This option involves using too many services which unnecessarily adds to the complexity and cost of the overall solution. So, this option is incorrect.</p>\n\n<p><strong>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</strong> - While you can use AWS Systems Manager to automate tasks across multiple accounts in AWS Organization, the other details in this option are irrelevant to the given use case.</p>\n\n<p>When you run automation across multiple Regions and accounts, you target resources by using tags or the name of an AWS resource group. The resource group must exist in each target account and Region. The resource group name must be the same in each target account and Region. The automation fails to run on those resources that don't have the specified tag or that aren't included in the specified resource group.</p>\n\n<p><strong>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</strong> - It is possible to enable CloudTrail logging from the management account of AWS Organizations, and is referred to as the organization trail. The use case is to be able to log all Trail events to a commonplace. Also, creating an IAM role in the management account and sharing it across all member accounts is not straightforward and requires manual work. Hence, this option is incorrect for the given use case.</p>\n\n<p>IAM policies usage in AWS Organizations:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>\n"
        }
      },
      {
        "id": 138248167,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading telecommunications company is using CloudFormation templates to deploy enterprise applications to their production, staging, and development environments in AWS. Their current process involves manual&nbsp;changes to their CloudFormation templates in order to specify the configuration variables and static attributes for each environment. The DevOps Engineer was tasked to set up automated deployments using AWS CodePipeline and ensure that the CloudFormation template is reusable across multiple pipelines. </p><p>How should the DevOps Engineer satisfy this requirement?</p>",
          "answers": [
            "<p>Set up a Lambda-backed custom resource in the CloudFormation templates. Configure the custom resource to monitor the status of the pipeline in AWS CodePipeline in order to detect which environment was launched. Use the cfn-init helper script to modify the launch template of each application stack based on its environment.</p>",
            "<p>Launch a new pipeline using&nbsp;AWS CodePipeline that has multiple stages for each environment and configure it to use input parameters. Switch the associated <code>UserData</code> of the EC2 instances to match the environment where the application stack is being launched using CloudFormation mappings. Specify&nbsp;parameter overrides&nbsp;for AWS CloudFormation actions.&nbsp;</p>",
            "<p>Launch a new pipeline using&nbsp;AWS CodePipeline for each environment with multiple stages for each application. Trigger the CloudFormation deployments using a Lambda function to dynamically modify the <code>UserData</code> of the EC2 instances that were launched in each environment.</p>",
            "<p>Manually configure the CloudFormation templates to use input parameters. Add a configuration that whenever the CloudFormation stack is updated, it will dynamically modify the <code>LaunchConfiguration</code> and <code>UserData</code> sections of the EC2 instances.</p>"
          ],
          "explanation": "<p>Continuous delivery is a release practice in which code changes are automatically built, tested, and prepared for release to production. With <strong>AWS CloudFormation</strong> and <strong>CodePipeline</strong>, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. This release process lets you rapidly and reliably make changes to your AWS infrastructure.</p><p>For example, you can create a workflow that automatically builds a test stack when you submit an updated template to a code repository. After AWS CloudFormation builds the test stack, you can test it and then decide whether to push the changes to a production stack.</p><p>You can use CodePipeline to build a continuous delivery workflow by building a pipeline for AWS CloudFormation stacks. CodePipeline has built-in integration with AWS CloudFormation, so you can specify AWS CloudFormation-specific actions, such as creating, updating, or deleting a stack within a pipeline.</p><p><img src=\"https://media.tutorialsdojo.com/public/product-page-diagram_CodePipeLine.7b8dd19eb6478b7f6f747d936c2f0b0b66757bbf.png\"></p><p>In a CodePipeline stage, you can specify parameter overrides for AWS CloudFormation actions. Parameter overrides let you specify template parameter values that override values in a template configuration file. AWS CloudFormation provides functions to help you specify dynamic values (values that are unknown until the pipeline runs).</p><p>You can set the <code>Fn::GetArtifactAtt</code> function which retrieves the value of an attribute from an input artifact, such as the S3 bucket name where the artifact is stored. You can use this function to specify attributes of an artifact, such as its filename or S3 bucket name, that can be used in the pipeline.</p><p>Hence, the correct answer is: <strong>Launch a new pipeline using AWS CodePipeline that has multiple stages for each environment and configure it to use input parameters. Switch the associated </strong><code><strong>UserData</strong></code><strong> of the EC2 instances to match the environment where the application stack is being launched using CloudFormation mappings. Specify parameter overrides for AWS CloudFormation actions.</strong></p><p>The option that says: <strong>Set up a Lambda-backed custom resource in the CloudFormation templates. Configure the custom resource to monitor the status of the pipeline in AWS CodePipeline in order to detect which environment was launched. Use the cfn-init helper script to modify the launch template of each application stack based on its environment</strong> is incorrect because monitoring the pipeline using a custom resource in CloudFormation entails a lot of administrative overhead. A better solution would be to use input parameters or parameter overrides for AWS CloudFormation actions.</p><p>The option that says: <strong>Launch a new pipeline using AWS CodePipeline for each environment with multiple stages for each application. Trigger the CloudFormation deployments using a Lambda function to dynamically modify the </strong><code><strong>UserData</strong></code><strong> of the EC2 instances that were launched in each environment</strong> is incorrect because using a Lambda function to modify the <code><strong>UserData</strong></code> of the already running EC2 instances is not a suitable solution. The parameters should have been dynamically populated and set before the resources were launched by using parameter overrides.</p><p>The option that says: <strong>Manually configure the CloudFormation templates to use input parameters. Add a configuration that whenever the CloudFormation stack is updated, it will dynamically modify the </strong><code><strong>LaunchConfiguration</strong></code><strong> and </strong><code><strong>UserData</strong></code><strong> sections of the EC2 instances</strong> is incorrect. Although using input parameters is helpful in this scenario, you should still integrate CloudFormation and CodePipeline in order to properly map the configuration files for each environment.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-parameter-override-functions.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-parameter-override-functions.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p><p><a href=\"https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/?src=udemy\">https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</a></p>"
        }
      },
      {
        "id": 115961521,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A gaming startup company is finishing its migration to AWS and realizes that many DevOps engineers have permissions to delete Amazon DynamoDB tables.</p><p>A solution is required to receive near real-time notifications when the API call DeleteTable is invoked in DynamoDB.</p><p>Which actions should be taken to achieve this requirement most cost-effectively?</p>",
          "answers": [
            "<p>Enable DynamoDB Streams and configure an AWS Lambda function to process events from the stream. Send alerts using Amazon SNS.</p>",
            "<p>Create an AWS CloudTrail trail. Create an Amazon EventBridge rule to track an AWS API call via CloudTrail and use Amazon SNS as a target.</p>",
            "<p>Create an AWS CloudTrail event filter and use an AWS Lambda function to send an Amazon SNS notification.</p>",
            "<p>Create an Amazon CloudWatch event filter that monitors for DeleteTable API actions and sends an alert via Amazon SNS.</p>"
          ],
          "explanation": "<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. You must create an AWS CloudTrail trail. For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p><p><strong>CORRECT: </strong>\"Create an AWS CloudTrail trail. Create an Amazon EventBridge rule to track an AWS API call via CloudTrail and use Amazon SNS as a target\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable DynamoDB Streams and configure an AWS Lambda function to process events from the stream. Send alerts using Amazon SNS\" is incorrect.</p><p>This would be less cost-effective compared to using AWS CloudTrail and Amazon EventBridge.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail event filter and use an AWS Lambda function to send an Amazon SNS notification\" is incorrect.</p><p>Event filters are used with CloudWatch, not with CloudTrail.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch event filter that monitors for DeleteTable API actions and sends an alert via Amazon SNS\" is incorrect.</p><p>API actions are tracked by AWS CloudTrail but not by Amazon CloudWatch.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 134588403,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A business has its AWS accounts managed by AWS Organizations and has employees in different countries. The business is reviewing its AWS account security policies and is looking for a way to monitor its AWS accounts for unusual behavior that is associated with an IAM identity. The business wants to:</p><ul><li><p>send a notification to any employee for whom the unusual activity is detected.</p></li><li><p>send a notification to the user's team leader.</p></li><li><p>an external messaging platform will send the notifications. The platform requires a target user-id for each recipient.</p></li></ul><p>The business already has an API that can be used to retrieve the team leader's and the employee's user-id from IAM user names.<br><br>Which solution will satisfy the requirements?</p>",
          "answers": [
            "<p>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
            "<p>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
            "<p>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
            "<p>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>"
          ],
          "explanation": "<p><strong>Amazon GuardDuty </strong>is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. A <strong>GuardDuty finding</strong> represents a potential security issue detected within your network. GuardDuty generates a finding whenever it detects unexpected and potentially malicious activity in your AWS environment.</p><p><img src=\"https://media.tutorialsdojo.com/public/amazon-guardduty-dop-c02.png\"></p><p>In this scenario, findings from Amazon GuardDuty are published to Amazon EventBridge as events that can be used to trigger a Lambda function which will send notifications to the external messaging platform.</p><p>Hence, the correct answer is: <strong>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and</strong><br><strong>invoke the Lambda function.</strong></p><p>The option that says: <strong>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function</strong> is incorrect because Amazon Detective will not by itself detect unusual activity. Detective provides analysis information related to a given finding.</p><p>The option that says: <strong>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function</strong> is incorrect because Amazon SNS can filter messages by attributes and not by message contents. An EventBridge rule would be required to publish to the SNS topic.</p><p>The option that says: <strong>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function</strong> is incorrect because Amazon Detective will not by itself detect unusual activity. In addition, Amazon SNS can filter messages by attributes and not by message contents. An EventBridge rule would be required to publish to the SNS topic.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings.html</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-guardduty/?src=udemy\">https://tutorialsdojo.com/amazon-guardduty/</a></p>"
        }
      }
    ],
    "answers": {
      "67357136": [
        "c"
      ],
      "67357170": [
        "c"
      ],
      "67357176": [
        "a"
      ],
      "75949114": [
        "c"
      ],
      "75949168": [
        "a"
      ],
      "82921372": [
        "a"
      ],
      "82921374": [
        "a"
      ],
      "82921392": [
        "a"
      ],
      "82921430": [
        "a"
      ],
      "99528225": [
        "c"
      ],
      "99528235": [
        "c"
      ],
      "115961491": [
        "c",
        "e"
      ],
      "115961521": [
        "b"
      ],
      "134588387": [
        "a"
      ],
      "134588403": [
        "d"
      ],
      "134588495": [
        "d"
      ],
      "134588499": [
        "d"
      ],
      "138248167": [
        "b"
      ],
      "138248201": [
        "b",
        "c"
      ],
      "143860749": [
        "a"
      ]
    }
  },
  {
    "id": "1770438013022",
    "date": "2026-02-07T04:20:13.022Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 20,
    "incorrect": 0,
    "unanswered": 0,
    "total": 20,
    "percent": 100,
    "duration": 6990618,
    "questions": [
      {
        "id": 134588501,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>The company operates a fleet of 400 Amazon EC2 instances to support a High-Performance Computing (HPC) application. The fleet is configured with target tracking scaling and is managed behind an Application Load Balancer (ALB). There is a requirement to create a simple web page hosted on an Amazon S3 bucket that displays the status of the EC2 instances in the fleet. This web page must be updated whenever a new instance is launched or terminated. Additionally, a searchable log of these events is required for review at a later time.</p><p>Which of the following options should be implemented to meet these requirements?</p>",
          "answers": [
            "<p>Create an Amazon EventBridge rule for the scale-in/scale-out event and deliver these logs to Amazon CloudWatch Logs. Manually export the CloudWatch Log group to the S3 bucket to view them.</p>",
            "<p>Write your AWS Lambda function to update the simple webpage on S3 and send event logs to CloudWatch Logs. Create an EventBridge rule to invoke the Lambda for scale-in/scale-out events.</p>",
            "<p>Create an EventBridge rule for the scale-in/scale-out event and create two targets for the event - one target to the webpage S3 bucket and another to deliver event logs to CloudWatch Logs.</p>",
            "<p>No need to do anything. AWS CloudTrail records these events and you can view and search the logs on the console.</p>"
          ],
          "explanation": "<p>You can create an <strong>AWS Lambda function</strong> that logs the changes in state for an Amazon EC2 instance. You can create a rule that runs the function whenever there is a state transition or a transition to one or more states that are of interest. Be sure to assign proper permissions to your Lambda function to write to S3 and to send logs to CloudWatch Logs. After creating the Lambda function, create a rule on Amazon EventBridge that will watch for the scale-in/scale-out events. Set a trigger to run your Lambda function which will then update the S3 webpage and send logs to CloudWatch Logs.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-event-pattern-27Jan2025.png\"></p><p>Hence, the correct answer is: <strong>Write your AWS Lambda function to update the simple webpage on S3 and send event logs to CloudWatch Logs. Create an EventBridge rule to invoke the Lambda for scale-in/scale-out events.</strong></p><p>The option that says: <strong>Create an Amazon EventBridge rule for the scale-in/scale-out event and deliver these logs to Amazon CloudWatch Logs. Manually export the CloudWatch Log group to the S3 bucket to view them </strong>is incorrect because you have to export the logs manually on an S3 bucket. This action is not recommended since you can simply use a Lambda function to log the changes of the EC2 instances to a file in the S3 bucket.</p><p>The option that says: <strong>Create an EventBridge rule for the scale-in/scale-out event and create two targets for the event - one target to the webpage S3 bucket and another to deliver event logs to CloudWatch Logs </strong>is incorrect because you cannot set an S3 bucket or object as a target for an EventBridge rule.</p><p>The option that says: <strong>No need to do anything. AWS CloudTrail records these events and you can view and search the logs on the console </strong>is incorrect. Although this is true, it would be hard to search all the relevant logs from the trail. Moreover, it typically doesn\u2019t provide a solution for the required S3 web page.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Tutorials.html \">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Tutorials.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 138248247,
        "correct_response": [
          "a",
          "c",
          "d"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An organization has a Java-based application that is built using Apache Maven. The organization decided to automate the build process for the Java project. The project's code is stored on GitHub, and every time the repository is updated, the code must be compiled, tested, and then uploaded to Amazon S3.</p><p>Which set of steps should the DevOps Engineer implement to meet the requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Set up a GitHub webhook that will initiate a build process whenever there is a modification made to the code, and it is pushed to the repository.</p>",
            "<p>Launch an AWS CodeDeploy application utilizing the Amazon EC2/On-Premises compute platform.</p>",
            "<p>Provision an AWS CodeBuild project that utilizes GitHub as the source repository.</p>",
            "<p>Include a buildspec.yml file in the source code with the commands necessary to compile, build, and test the project.</p>",
            "<p>Create an Amazon EC2 instance and install dependencies via user data, then use it to execute the build.</p>",
            "<p>Set up an AWS Lambda function triggered by an Amazon S3 event to compile the code and run tests.</p>"
          ],
          "explanation": "<p><strong>AWS CodeBuild</strong> is a fully managed build service in the cloud that compiles source code, runs unit tests, and produces deployment-ready artifacts. The service eliminates the need for users to provision, manage, and scale their own build servers. Additionally, it offers prepackaged build environments for popular programming languages and build tools, including Apache Maven, Gradle, and others.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-codebuild.png\"></p><p>A <strong>buildspec</strong> is a YAML-formatted collection of build commands and settings that are used to execute a build. Users can either include a buildspec as part of the source code or define it when creating a build project.</p><p><strong>AWS CodeBuild</strong> supports <strong>webhooks</strong> when the <strong>source repository is GitHub</strong>. This means that for a CodeBuild build project that has its source code stored in a GitHub repository, webhooks can be used to rebuild the source code every time a code change is pushed to the repository.</p><p>Hence, the correct answers are:</p><p>- <strong>Set up a GitHub webhook that will initiate a build process whenever there is a modification made to the code, and it is pushed to the repository.</strong></p><p><strong>- Provision an AWS CodeBuild project that utilizes GitHub as the source repository.</strong></p><p><strong>- Include a buildspec.yml file in the source code with the commands necessary to compile, build, and test the project.</strong></p><p>The option that says: <strong>Launch an AWS CodeDeploy application utilizing the Amazon EC2/On-Premises compute platform </strong>is incorrect because the scenario requires automation of the build process, which is why AWS CodeBuild is recommended, as it is designed for that purpose. AWS CodeDeploy, on the other hand, is primarily used for deploying applications.</p><p>The option that says: <strong>Create an Amazon EC2 instance and install dependencies via user data, then use it to execute the build </strong>is incorrect because the solution may be technically feasible, but it is not cost-effective because it utilizes an EC2 instance for the build process, which incurs additional operational overhead and costs.</p><p>The option that says: <strong>Set up an AWS Lambda function triggered by an Amazon S3 event to compile the code and run tests</strong> is incorrect because Lambda is not typically used for long-running build and test processes, which are better suited for CodeBuild.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/APIReference/Welcome.html\">https://docs.aws.amazon.com/codebuild/latest/APIReference/Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/github-webhook.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/github-webhook.html</a></p><p><br></p><p><strong>Check out this AWS CodeBuild Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codebuild/?src=udemy\">https://tutorialsdojo.com/aws-codebuild/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 82921354,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company is finishing its migration to AWS and realizes that while some employees have passed the AWS Certified DevOps Engineer Professional certification and know AWS very well, other ones are still beginning and haven't passed their Associate-level certifications yet. The company has established architectural and tagging specific internal rules and would like to minimize the risk of the AWS-beginner employees launching uncompliant resources.</p>\n\n<p>As a DevOps Engineer, how should you implement this requirement while allowing the employees to create the resources they need?</p>\n",
          "answers": [
            "<p>Define commonly used architectures as CloudFormation templates. Create Service Catalog stacks from these templates, and ensure the tagging is done properly. Place the IAM users into a beginner group and allow the users to only launch stacks from Service Catalog, while restricting any write access to other services</p>",
            "<p>Define commonly used architectures as CloudFormation templates. Place the IAM users into a beginner group and allow the users to only launch stacks from these CloudFormation stacks, while restricting any write access to other services</p>",
            "<p>Create AWS Config custom rules that will check for the compliance of your company's resources thanks to a Lambda Function. Update the Lambda Function over time while your company improves its architectural and tagging rules. Provide IAM users full access to AWS</p>",
            "<p>Place the beginner IAM users into a group and create an IAM policy that requires conditional approvals from senior DevOps engineers upon resource creation. Hook an SNS topic into the IAM approval channel</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Define commonly used architectures as CloudFormation templates. Create Service Catalog stacks from these templates, and ensure the tagging is done properly. Place the IAM users into a beginner group and allow the users to only launch stacks from Service Catalog, while restricting any write access to other services</strong></p>\n\n<p>AWS Service Catalog allows IT administrators to create, manage, and distribute catalogs of approved products to end-users, who can then access the products they need in a personalized portal. Administrators can control which users have access to each product to enforce compliance with organizational business policies.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html</a></p>\n\n<p>A product is a service or application for end-users. A portfolio is a collection of products, with configuration information that determines who can use those products and how they can use them. A catalog is a collection of products that the administrator creates, adds to portfolios, and provides updates for using AWS Service Catalog. To create a Service Catalog product, you first need to create an AWS CloudFormation template by using an existing AWS CloudFormation template or creating a custom template. Then you can use the AWS Service Catalog console to upload the template and create the product.</p>\n\n<p>Therefore, for the given use-case, we need to use Service Catalog as it was precisely designed for that purpose and give users only access to the stack they should be able to create in Service Catalog.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/servicecatalog/faqs/\">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/servicecatalog/faqs/\">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define commonly used architectures as CloudFormation templates. Place the IAM users into a beginner group and allow the users to only launch stacks from these CloudFormation stacks, while restricting any write access to other services</strong> - If you let IAM users use the CloudFormation service directly, they will have the power to create any resource through their permissions. You cannot restrict templates using IAM policies in CloudFormation.</p>\n\n<p><strong>Create AWS Config custom rules that will check for the compliance of your company's resources thanks to a Lambda Function. Update the Lambda Function over time while your company improves its architectural and tagging rules. Provide IAM users full access to AWS</strong> - AWS Config Rules would be a way to \"monitor\" the situation but not prevent resources from being created the wrong way.</p>\n\n<p><strong>Place the beginner IAM users into a group and create an IAM policy that requires conditional approvals from senior DevOps engineers upon resource creation. Hook an SNS topic into the IAM approval channel</strong> - An IAM policy cannot have a \"conditional approval\", so this option is a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/servicecatalog/faqs/\">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/\">https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/</a></p>\n"
        }
      },
      {
        "id": 82921358,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A retail company uses the open-source tool Jenkins on its on-premise infrastructure to perform CICD. It has decided to move to AWS and take advantage of the elasticity properties of the cloud provider to have more efficient workloads. It needs to ensure the Jenkins setup is highly available, fault-tolerant and also elastic to perform builds. The company has hired you as an AWS Certified DevOps Engineer Professional to build the most cost-effective solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend?</p>\n",
          "answers": [
            "<p>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>",
            "<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>",
            "<p>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</p>",
            "<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p>In the AWS Cloud, a web-accessible application like Jenkins is typically designed for high availability and fault tolerance by spreading instances across multiple AZs and fronting them with an Elastic Load Balancing (ELB) load balancer. Elastic Load Balancing automatically distributes incoming application traffic across multiple Amazon EC2 instances in the cloud. It enables you to achieve greater levels of fault tolerance in your applications and seamlessly provides the required amount of load balancing capacity needed to distribute application traffic. If your business requirements demand a fault-tolerant Jenkins environment, your preferred setup might be a scenario in which multiple masters with their own workers are placed in separate Availability Zones.</p>\n\n<p>You can use the Jenkins plugin for AWS CodeBuild to integrate CodeBuild with your Jenkins build jobs. Instead of sending your build jobs to Jenkins build nodes, you use the plugin to send your build jobs to CodeBuild. This eliminates the need for you to provision, configure, and manage Jenkins build nodes.</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2019/10/20/Diagram2.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/\">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p>For the given use-case, Jenkins must be deployed as a multi-master across multi-AZ to be highly available and fault-tolerant. The Jenkins CodeBuild plugin allows to elastically start CodeBuild builds that run a special docker image that works as a Jenkins slave. It allows you to be fully elastic in the cloud with Jenkins, and only pay exactly for the resources you have used.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p>As mentioned in the explanation above, if configured with EC2 instances in an Auto Scaling Group, the setup will be elastic in some ways, but probably expensive if the EC2 instances are not fully utilized at capacity. So these three options are not the best fit for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/\">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf\">https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf</a></p>\n"
        }
      },
      {
        "id": 75949086,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company requires an extremely high performance in-memory database for an application. The database used should store data durably across multiple availability zones. The application requires that the database support strong consistency and can scale seamlessly to many terabytes in size.</p><p>Which database should the company use for this application?</p>",
          "answers": [
            "<p>Use Amazon ElastiCache for Redis.</p>",
            "<p>Use Amazon ElastiCache for Memcached.</p>",
            "<p>Use Amazon MemoryDB for Redis.</p>",
            "<p>Use Amazon Managed Grafana.</p>"
          ],
          "explanation": "<p>Amazon MemoryDB for Redis is a Redis-compatible, durable, in-memory database service that delivers ultra-fast performance. It is purpose-built for modern applications with microservices architectures.</p><p>Amazon MemoryDB for Redis vs ElastiCache:</p><p>\u2022 Use ElastiCache for caching DB queries.</p><p>\u2022 Use MemoryDB for a full DB solution combining DB and cache.</p><p>\u2022 MemoryDB offers higher performance with lower latency .</p><p>\u2022 MemoryDB offers strong consistency for primary nodes and eventual consistency for replica nodes.</p><p>\u2022 With ElastiCache there can be some inconsistency and latency depending on the engine and caching strategy.</p><p>For this scenario the requirement is for a full DB solution, not a caching solution, so MemoryDB for Redis is the best choice.</p><p><strong>CORRECT: </strong>\"Use Amazon MemoryDB for Redis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon ElastiCache for Redis\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Use Amazon ElastiCache for Memcached\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Use Amazon Managed Grafana\" is incorrect. This service is not a database service, it is used for data visualization of operational metrics, logs, and traces.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/memorydb/features/\">https://aws.amazon.com/memorydb/features/</a></p>"
        }
      },
      {
        "id": 75949100,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 3",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is deploying a new serverless application that uses AWS Lambda functions. A DevOps engineer must create a continuous deployment pipeline for the application. The deployment preferences must be configured to minimize the impact of failed deployments.</p><p>Which deployment configuration will meet these requirements?</p>",
          "answers": [
            "<p>Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type.</p>",
            "<p>Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch.</p>",
            "<p>Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version.</p>",
            "<p>Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version.</p>"
          ],
          "explanation": "<p>The benefits of using AWS SAM to create the serverless application include that it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM can perform the following actions:</p><ul><li><p>Deploys new versions of the Lambda function, and automatically creates aliases that point to the new version.</p></li><li><p>Gradually shifts customer traffic to the new version until you\u2019re satisfied that it's working as expected, or you roll back the update.</p></li><li><p>Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly, and your application operates as expected.</p></li><li><p>Rolls back the deployment if CloudWatch alarms are triggered.</p></li></ul><p>The DevOps engineer can choose the Deployment Preference Type. The following options are available:</p><ul><li><p><strong>Canary:</strong> Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p></li><li><p><strong>Linear:</strong> Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment.</p></li><li><p><strong>All-at-once:</strong> All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</p></li></ul><p>The best option to minimize the impact of failed deployments is to use the canary deployment type. This will ensure that only a small amount of traffic reaches the new Lambda function in the first shift and if any issues occur the deployment can be stopped.</p><p><strong>CORRECT: </strong>\"Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch\" is incorrect.</p><p>The all-at-once deployment preference type would shift all traffic across to the new functions which would increase the impact of failed deployments. Also, CloudFormation is not a continuous deployment tool and is not a suitable substitute for CodePipeline and CodeDeploy.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version\" is incorrect.</p><p>As above, CloudFormation is not designed for continuous delivery and should not be used in this scenario. Creating a CodePipeline that leverages CodeDeploy and a source control service such as CodeCommit would be a better solution.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version\" is incorrect.</p><p>This option requires more operational overhead and testing on behalf of the DevOps team. The better solution is an automated pipeline that shifts some traffic across to the new functions so actual end users are attempting to use the application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 138248101,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer has been tasked to implement a reliable solution to maintain all of their Windows and Linux servers both in AWS and in on-premises data center. There should be a system that allows them to easily update the operating systems of their servers and apply the core application patches with minimum management overhead. The patches must be consistent across all levels in order to meet the company\u2019s security compliance. </p><p>Which of the following is the MOST suitable solution that you should implement?</p>",
          "answers": [
            "Configure and install AWS Systems Manager agent on all of the EC2 instances in your VPC as well as your physical servers on-premises. Use the Systems Manager Patch Manager service and specify the required Systems Manager Resource Groups for your hybrid architecture. Utilize a preconfigured patch baseline and then run scheduled patch updates during maintenance windows.",
            "<p>Configure and install the AWS CodeDeploy agent on all of your existing EC2 instances in your VPC and your on-premises servers. Launch a new Elastic Beanstalk environment for each OS to automate the execution of the patch commands during maintenance windows. Integrate Elastic Beanstalk and CodeDeploy for deployment as well as maintaining your systems with the latest OS and application patches.</p>",
            "Develop a custom python script to install the latest OS patches on the Linux servers. Set up a scheduled job to automatically run this script using the cron scheduler on Linux servers. Enable Windows Update in order to automatically patch Windows servers or set up a scheduled task using Windows Task Scheduler to periodically run the python script.",
            "Store the login credentials of each Linux and Windows servers on the AWS Systems Manager Parameter Store. Use Systems Manager Resource Groups to set up one group for your Linux servers and another one for your Windows servers. Remotely login, run, and deploy the patch updates to all of your servers using the credentials stored in the Systems Manager Parameter Store and through the use of the Systems Manager Run Command."
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with both security-related and other types of updates. You can use the Patch Manager to apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications.) You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/public/patch-groups-how-it-works.png\"></p><p>Patch Manager uses <strong><em>patch baselines</em></strong>, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p>A <strong><em>resource group</em></strong> is a collection of AWS resources that are all in the same AWS Region and that match criteria provided in a query. You build queries in the AWS Resource Groups (Resource Groups) console or pass them as arguments to Resource Groups commands in the AWS CLI.</p><p>With AWS Resource Groups, you can create a custom console that organizes and consolidates information based on criteria that you specify in tags. After you add resources to a group you created in Resource Groups, use AWS Systems Manager tools such as Automation to simplify management tasks on your resource group. You can also use the resource groups you create as the basis for viewing monitoring and configuration insights in Systems Manager.</p><p>Hence, the correct answer is: <strong>Configure and install AWS Systems Manager agent on all of the EC2 instances in your VPC as well as your physical servers on-premises. Use the Systems Manager Patch Manager service and specify the required Systems Manager Resource Groups for your hybrid architecture. Utilize a preconfigured patch baseline and then run scheduled patch updates during maintenance windows.</strong></p><p>The option that says: <strong>Configure and install the AWS CodeDeploy agent on all of your existing EC2 instances in your VPC and your on-premises servers. Launch a new Elastic Beanstalk environment for each OS to automate the execution of the patch commands during maintenance windows. Integrate Elastic Beanstalk and CodeDeploy for deployment as well as maintaining your systems with the latest OS and application patches</strong> is incorrect because AWS Elastic Beanstalk is not capable to manage servers both in AWS and on-premises data centers. In addition, you cannot directly integrate Elastic Beanstalk and CodeDeploy for the purposes of application deployment. You either use one of these two but not both.</p><p>The option that says: <strong>Develop a custom python script to install the latest OS patches on the Linux servers. Set up a scheduled job to automatically run this script using the cron scheduler on Linux servers. Enable Windows Update in order to automatically patch Windows servers or set up a scheduled task using Windows Task Scheduler to periodically run the python script</strong> is incorrect because this solution entails a high management overhead since you need to develop a new script and maintain a number of <em>cron</em> schedulers in your Linux servers and Windows Task Scheduler jobs on your Windows servers.</p><p>The option that says: <strong>Store the login credentials of each Linux and Windows servers on the AWS Systems Manager Parameter Store. Use Systems Manager Resource Groups to set up one group for your Linux servers and another one for your Windows servers. Remotely login, run, and deploy the patch updates to all of your servers using the credentials stored in the Systems Manager Parameter Store and through the use of the Systems Manager Run Command</strong> is incorrect because this is not a suitable service to use to handle the patching activities of your servers. You have to use AWS Systems Manager Patch Manager instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-resource-groups.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-resource-groups.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 134588513,
        "correct_response": [
          "a",
          "e"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An organization has a fleet of Amazon EC2 instances and uses SSH for remote access. Whenever a DevOps Engineer leaves the organization, the SSH keys are rotated as a security measure. The Chief Information Officer (CIO) required to stop the use of EC2 key pairs for operational efficiency and instead utilize AWS Systems Manager Session Manager. To strengthen security, access to Session Manager should be limited solely through a private network.</p><p>Which set of actions should the new DevOps Engineer take to meet the CIO requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Create an IAM instance profile to be associated with the fleet of EC2 instances and attach an IAM policy with the required Systems Manager permissions.</p>",
            "<p>Enable incoming traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances.</p>",
            "<p>Launch a new EC2 instance that will function as a bastion host for the other EC2 instances in the fleet.</p>",
            "<p>Enable outbound traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances.</p>",
            "<p>Provision a VPC endpoint for Systems Manager in the designated region.</p>"
          ],
          "explanation": "<p>With <strong>Session Manager</strong>, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs).</p><p>By default, AWS Systems Manager doesn't have permission to perform actions on your instances. You must grant access by using AWS Identity and Access Management (IAM).</p><p>An <strong>instance profile</strong> that contains the AWS managed policy <strong>AmazonSSMManagedInstanceCore </strong>is needed to be attached to the EC2 instance for the <strong>Session Manager</strong> to work.</p><p><img src=\" https://media.tutorialsdojo.com/public/dop-c02-ssm-session-manager.png\"></p><p>The security posture of managed instances (including those in a hybrid environment) can be improved by configuring AWS Systems Manager to use an interface <strong>VPC endpoint</strong> in Amazon Virtual Private Cloud (Amazon VPC). An interface VPC endpoint (interface endpoint) can be used to connect to services powered by AWS PrivateLink, which is a technology that enables <strong>private access to Amazon Elastic Compute Cloud (Amazon EC2) and Systems Manager</strong> APIs using private IP addresses.</p><p>Hence, the correct answers are the option that says:</p><p>- <strong>Create an IAM instance profile to be associated with the fleet of EC2 instances and attach an IAM policy with the required Systems Manager permissions</strong></p><p><strong>- Provision a VPC endpoint for Systems Manager in the designated region</strong></p><p>The option that says: <strong>Enable incoming traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances </strong>is incorrect because Session Manager will work without the need to open inbound ports.</p><p>The option that says: <strong>Launch a new EC2 instance that will function as a bastion host for the other EC2 instances in the fleet </strong>is incorrect because a bastion host will not meet the requirement as it will still use SSH key pairs to remote access.</p><p>The option that says: <strong>Enable outbound traffic to TCP port 22 from the VPC CIDR range in all associated security groups of EC2 instances </strong>is incorrect because Session Manager will work without the need to open outbound ports.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-instance-profile.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-instance-profile.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 138248233,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading IT consultancy firm has several Python-based Flask and Django web applications hosted in AWS. Some of the firm's developers are freelance contractors located overseas. The firm wants to automate remediation actions for issues relating to the health of its AWS resources by using the AWS Health Dashboard and the AWS Health API. Additionally, there is a requirement to automatically detect any IAM access key owned by the firm that is accidentally or deliberately exposed on a public GitHub repository. Upon detection, the IAM access key must be immediately deleted, and a notification must be sent to the DevOps team. Once detected, the IAM access key must be immediately deleted, and a notification should be sent to the DevOps team.</p><p>What solution can a DevOps Engineer do to meet this requirement?</p>",
          "answers": [
            "<p>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Use a combination of Amazon GuardDuty and Amazon Macie to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</p>",
            "<p>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Create an AWS Config rule for the <code>AWS_RISK_CREDENTIALS_EXPOSED</code> event with Multi-Account Multi-Region Data Aggregation to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</p>",
            "<p>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Create an AWS Personal Health Dashboard rule for the <code>AWS_RISK_CREDENTIALS_EXPOSED</code> event to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</p>",
            "<p>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends notification to the IT Security team using Amazon SNS. Create a EventBridge rule with an &lt;code&gt;aws.health&lt;/code&gt; event source and the <code>AWS_RISK_CREDENTIALS_EXPOSED</code> event to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</p>"
          ],
          "explanation": "<p><strong>AWS Step Functions</strong> lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker AI, into feature-rich applications. Workflows are made up of a series of steps, with the output of one step acting as input into the next. Application development is simpler and more intuitive using Step Functions, because it translates your workflow into a state machine diagram that is easy to understand, easy to explain to others, and easy to change.</p><p>AWS proactively monitors popular code repository sites for exposed AWS Identity and Access Management (IAM) access keys. On detection of an exposed IAM access key, AWS Health generates an <strong>AWS_RISK_CREDENTIALS_EXPOSED </strong>Event. In response to this event, you can set up an automated workflow deletes the exposed IAM Access Key, summarizes the recent API activity for the exposed key, and sends the summary message to an Amazon Simple Notification Service (SNS) Topic to notify the subscribers which are all orchestrated by an AWS Step Functions state machine.</p><p><img alt=\"Step Function Automation Diagram using EventBridge\" height=\"555\" src=\"https://media.tutorialsdojo.com/public/td-automation-diagram-using -step-function-eventbridge-01-09-25.png\" width=\"1000\"></p><p>You can use Amazon EventBridge to detect and react to changes in the status of AWS Personal Health Dashboard (AWS Health) events. Then, based on the rules that you create, EventBridge invokes one or more target actions when an event matches the values that you specify in a rule. Depending on the type of event, you can send notifications, capture event information, take corrective action, initiate events, or take other actions.</p><p>You can automate actions in response to new scheduled events for your EC2 instances. For example, you can create EventBridge rules for EC2 scheduled events generated by the AWS Health service. These rules can then trigger targets, such as AWS Systems Manager Automation documents, to automate actions.</p><p>Hence, the correct answer is: <strong>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends notification to the IT Security team using Amazon SNS. Create a EventBridge rule with an </strong><code><strong>aws.health</strong></code><strong> event source and the </strong><code><strong>AWS_RISK_CREDENTIALS_EXPOSED</strong></code><strong> event to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule.</strong></p><p>The option that says: <strong>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Use a combination of Amazon GuardDuty and Amazon Macie to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule</strong> is incorrect. You can't monitor any exposed IAM keys from the Internet using Amazon GuardDuty and Amazon Macie. Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads while Amazon Macie is simply a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS.</p><p>The option that says: <strong>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Create an AWS Config rule for the </strong><code><strong>AWS_RISK_CREDENTIALS_EXPOSED</strong></code><strong> event with Multi-Account Multi-Region Data Aggregation to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule </strong>is incorrect. The use of Multi-Account Multi-Region Data Aggregation in CloudTrail will not satisfy the requirement. An aggregator is simply an AWS Config resource type that collects AWS Config configuration and compliance data from multiple accounts across multiple regions.</p><p>The option that says: <strong>Set up three Lambda functions in AWS Step Functions that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key using CloudTrail and sends a notification to the IT Security team using Amazon SNS. Create an AWS Personal Health Dashboard rule for the </strong><code><strong>AWS_RISK_CREDENTIALS_EXPOSED</strong></code><strong> event to monitor any exposed IAM keys from the Internet. Set the Step Functions as the target of the EventBridge rule</strong> is incorrect. You have to use the AWS Health API instead of the AWS Personal Health Dashboard. The <code><strong><em>AWS_RISK_CREDENTIALS_EXPOSED</em></strong></code><strong><em> </em></strong>event is only applicable from an <code>aws.health</code> event source and not from an AWS Personal Health Dashboard rule.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/</a></p><p><a href=\"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html\">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><br></p><p><strong>Check out these AWS Health and Amazon CloudWatch Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-health/?src=udemy\">https://tutorialsdojo.com/aws-health/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 134588499,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A web application is hosted on an Auto Scaling group (ASG) of On-Demand EC2 instances. It has a separate Linux EC2 instance used primarily for batch processing. This particular instance needs to update its configuration file based on the list of the active IP addresses of the instances within the ASG. This is needed in order to run a batch job properly.</p><p>Which of the following options allows effective updating of the configuration file whenever the Auto Scaling group scales in or out?</p>",
          "answers": [
            "<p>Develop a custom script that runs on the background to query active EC2 instances and IP addresses of the ASG. Then the script will update the configuration whenever it detects a change.</p>",
            "<p>Create an Amazon EventBridge rule that is scheduled to run every minute. Set the target to a Lambda function to query the ASG instances and update the configuration file on an S3 bucket. Automatically establish an SSH connection to the Linux EC2 instances by using another Lambda function and then download the master configuration file from S3 to update the local configuration file stored in the instance.</p>",
            "<p>Create a CloudWatch Logs to monitor the Launch/Terminate events of the ASG. Set the target to a Lambda function that will update the configuration file inside the EC2 instance. Add a proper IAM permission to Lambda to access the EC2 instance.</p>",
            "<p>Create an Amazon EventBridge rule for the Launch/Terminate events of the ASG. Set the target to an SSM Run Command that will update the configuration file on the target EC2 instance.</p>"
          ],
          "explanation": "<p>You can use <strong>Amazon EventBridge (Amazon CloudWatch Events)</strong> to invoke AWS Systems Manager Run Command and perform actions on Amazon EC2 instances when certain events happen.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-eventbridge-rule-event-pattern-10Jan2025.png\"></p><p>You can use SSM Run Command to configure instances without having to login to an instance. This setup requires the SSM agent to be installed on your EC2 instance and that proper IAM permission has been granted. Additionally, create an Amazon EventBridge rule to monitor the launch or termination events of the Auto Scaling group. Set the Systems Manager Run Command as the target for this rule.</p><p>On the parameters of SSM Run Command, define the commands you need to update the configuration file. It is important that your target EC2 instance is properly tagged and defined on the SSM Run command as it will be the basis of SSM to identify your instance.</p><p>Hence, the correct answer is: <strong>Create an Amazon EventBridge rule for the Launch/Terminate events of the ASG. Set the target to an SSM Run Command that will update the configuration file on the target EC2 instance.</strong></p><p>The option that says: <strong>Develop a custom script that runs on the background to query active EC2 instances and IP addresses of the ASG. Then the script will update the configuration whenever it detects a change </strong>is incorrect. Although this may be a possible solution, you will still have to write and maintain your own script and that creates an unnecessary operational overhead. Also, running a script with regular intervals is not a good approach if you are just waiting for events.</p><p>The option that says: <strong>Create a CloudWatch Logs to monitor the Launch/Terminate events of the ASG. Set the target to a Lambda function that will update the configuration file inside the EC2 instance. Add a proper IAM permission to Lambda to access the EC2 instance </strong>is incorrect because you can't simply monitor the Auto Scaling Group events using CloudWatch Logs. You have to use Amazon EventBridge instead.</p><p>The option that says: <strong>Create an Amazon EventBridge rule that is scheduled to run every minute. Set the target to a Lambda function to query the ASG instances and update the configuration file on an S3 bucket. Automatically establish an SSH connection to the Linux EC2 instances by using another Lambda function and then download the master configuration file from S3 to update the local configuration file stored in the instance</strong> is incorrect. It is better to create an Amazon EventBridge rule that tracks the Launch/Terminate events of the Auto Scaling group (ASG) instead of running it every minute. AWS Lambda can\u2019t directly establish an SSH connection and run a command inside an EC2 instance to update the configuration file, even with proper IAM permissions. A better solution would be to use the Systems Manager Run Command to let you remotely and securely manage the configuration of your managed instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-prereqs.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EC2_Run_Command.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/rc-console.html</a></p><p><br></p><p><strong>Check out these Amazon CloudWatch and AWS Systems Manager Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 67357132,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A media application runs on a host of Amazon EC2 instances fronted with an Application Load Balancer (ALB) and Amazon S3 buckets as storage service. For enhanced security, an AWS Web Application Firewall (AWS WAF) has been set up to monitor the requests coming to the ALB. The DevOps team needs to submit a quarterly report on the web requests received by AWS WAF, having detailed information about each web request as well as the details about rules that the request matched. The team has reached out to you for implementing the changes needed for collecting the security data for the coming months.</p>\n\n<p>As DevOps Engineer, how will you implement this requirement?</p>\n",
          "answers": [
            "<p>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</p>",
            "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</p>",
            "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</p>",
            "<p>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data firehose name should start with the prefix <code>aws-waf-logs-</code></p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</strong></p>\n\n<p>You can enable logging AWS WAF web ACL traffic, to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose.</p>\n\n<p>To send your web ACL traffic logs to Amazon S3, you need to set up an Amazon S3 bucket for the logs. When you enable logging for AWS WAF, you provide the bucket ARN. Your web ACLs publish their log files to the Amazon S3 bucket at 5-minute intervals. Each log file contains log records for the traffic recorded in the previous 5 minutes.</p>\n\n<p>The maximum file size for a log file is 75 MB. If the log file reaches the file size limit within the 5-minute period, the log stops adding records to it, publishes it to the Amazon S3 bucket, and then creates a new log file.</p>\n\n<p>Your bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For example, <code>aws-waf-logs-DOC-EXAMPLE-BUCKET-SUFFIX</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</strong> - AWS WAF supports encryption with Amazon S3 buckets for key type Amazon S3 key (SSE-S3) and AWS Key Management Service (SSE-KMS) AWS KMS keys. AWS WAF doesn't support encryption for AWS Key Management Service keys that are managed by AWS.</p>\n\n<p><strong>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as a WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data Firehose name should start with the prefix <code>aws-waf-logs-</code></strong> - AWS recommends that the Kinesis stream should not be used as a source when configuring Data Firehose as a WAF logging destination.</p>\n\n<p><strong>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html</a></p>\n"
        }
      },
      {
        "id": 138248201,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps Engineer in a leading aerospace engineering company has a hybrid cloud architecture that connects its on-premises data center with AWS via Direct Connect Gateway. There is a new requirement to implement an automated OS patching solution for all of the Windows servers hosted on-premises as well as in AWS Cloud. The AWS Systems Manager service should be utilized to automate the patching of the servers.</p><p>Which combination of steps should be set up to satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS <code>AssumeRoleWithSAML</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
            "<p>Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS <code>AssumeRole</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>mi-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager State Manager.</p>"
          ],
          "explanation": "<p>A hybrid environment includes on-premises servers and virtual machines (VMs) that have been configured for use with Systems Manager, including VMs in other cloud environments. After following the steps below, the users who have been granted permissions by the AWS account administrator can use AWS Systems Manager to configure and manage their organization's on-premises servers and virtual machines (VMs).</p><p>To configure your hybrid servers and VMs for AWS Systems Manager, just follow these provided steps:</p><p>1. Complete General Systems Manager Setup Steps<br>2. Create an IAM Service Role for a Hybrid Environment<br>3. Install a TLS certificate on On-Premises Servers and VMs<br>4. Create a Managed-Instance Activation for a Hybrid Environment<br>5. Install SSM Agent for a Hybrid Environment (Windows)<br>6. Install SSM Agent for a Hybrid Environment (Linux)<br>7. (Optional) Enable the Advanced-Instances Tier</p><p><br></p><p>Configuring your hybrid environment for Systems Manager enables you to do the following:</p><p>- Create a consistent and secure way to remotely manage your hybrid workloads from one location using the same tools or scripts.</p><p>- Centralize access control for actions that can be performed on your servers and VMs by using AWS Identity and Access Management (IAM).</p><p>- Centralize auditing and your view into the actions performed on your servers and VMs by recording all actions in AWS CloudTrail.</p><p>- Centralize monitoring by configuring CloudWatch Events and Amazon SNS to send notifications about service execution success.</p><p><img src=\"https://media.tutorialsdojo.com/public/how-it-works.png\">After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as <em>managed instances</em>. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix \"mi-\". Amazon EC2 instance IDs use the prefix \"i-\".</p><p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants <code>AssumeRole</code> trust to the Systems Manager service. You only need to create the service role for a hybrid environment once for each AWS account.</p><p>Hence, the correct answers are:</p><p><strong>- Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRole</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</strong></p><p><strong>- Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>mi-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager.</strong></p><p>The option that says: <strong>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRoleWithSAML</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation </strong>is incorrect because you have to execute the <code><em>AssumeRole </em></code>operation instead and not the <code><em>AssumeRoleWithSAML</em></code><em> </em>operation<em>. </em>Moreover, you only need to set up a single IAM service role.</p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager </strong>is incorrect because the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix in the SSM console and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix<strong><em>.</em></strong></p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager State Manager</strong> is incorrect because the AWS Systems Manager State Manager is just a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. You have to apply the patches using the Systems Manager Patch Manager instead. In addition, the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 134588385,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is planning to host their enterprise application in an ECS Cluster which uses the Fargate launch type. The database credentials should be provided to the AMI by using environment variables for security purposes. A DevOps engineer was instructed to ensure that the credentials are secure when passed to the image and that the sensitive passwords cannot be viewed on the cluster itself. In addition, the credentials must be kept in a dedicated storage with lifecycle management and key rotation. </p><p>Which of the following is the MOST suitable solution that the engineer should implement with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Upload and manage the database credentials using AWS Systems Manager Parameter Store then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container.</p>",
            "<p>Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt them with KMS. Store the task definition JSON file in a private Amazon S3 bucket. Ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Set up an IAM role to the ECS task definiton script that allows access to the specific S3 bucket and then pass the <code>--cli-input-json</code> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials.</p>",
            "<p>Store the database credentials using the AWS Secrets Manager. Encrypt the credentials using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret, which contains the sensitive data, to present to the container.</p>",
            "<p>Store the database credentials using Docker Secrets in the ECS task definition file of the ECS Cluster where you can centrally manage sensitive data and securely transmit it to only those containers that need access to it. Ensure that the secrets are encrypted during transit and at rest.</p>"
          ],
          "explanation": "<p><strong>Amazon ECS</strong> enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</p><p><img src=\"https://media.tutorialsdojo.com/public/diagram3-1_2AUG2023.png\"></p><p>Within your container definition, specify <code>secrets</code> with the name of the environment variable to set in the container and the full ARN of either the Secrets Manager secret or Systems Manager Parameter Store parameter containing the sensitive data to present to the container. The parameter that you reference can be from a different Region than the container using it, but must be from within the same account.</p><p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises.</p><p>If you want a single store for configuration and secrets, you can use Parameter Store. If you want a dedicated secrets store with lifecycle management, use Secrets Manager.</p><p>Hence, the correct answer is the option that says: <strong>Store the database credentials using the AWS Secrets Manager. Encrypt the credentials using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret, which contains the sensitive data, to present to the container<em>.</em></strong></p><p>The option that says: <strong>Upload and manage the database credentials using AWS Systems Manager Parameter Store then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container </strong>is incorrect. Although the use of Systems Manager Parameter Store in securing sensitive data in ECS is valid, this service doesn't provide dedicated storage with lifecycle management and key rotation, unlike Secrets Manager.</p><p>The option that says: <strong>Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt them with KMS. Store the task definition JSON file in a private Amazon S3 bucket. Ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Set up an IAM role to the ECS task definition script that allows access to the specific S3 bucket and then pass the </strong><code><strong>--cli-input-json</strong></code><strong> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials</strong><em> </em>is incorrect. Although the solution may work, it is not recommended to store sensitive credentials in S3. This entails a lot of overhead and manual configuration steps which can be simplified by simply using the Secrets Manager or Systems Manager Parameter Store.</p><p>The option that says: <strong>Store the database credentials using Docker Secrets in the ECS task definition file of the ECS Cluster where you can centrally manage sensitive data and securely transmit it to only those containers that need access to it. Ensure that the secrets are encrypted during transit and at rest<em> </em></strong>is incorrect. Although you can use Docker Secrets to secure the sensitive database credentials, this feature is only applicable in Docker Swarm. In AWS, the recommended way to secure sensitive data is either through the use of Secrets Manager or Systems Manager Parameter Store.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/\">https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><br></p><p><strong>Check out this AWS Secrets Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager/</a></p>"
        }
      },
      {
        "id": 82921458,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A Big Data analytics company is operating a distributed Cassandra cluster on EC2. Each instance in the cluster must have a list of all the other instance's IP to function correctly, store in a configuration file. As a Devops Engineer at the company, you would like this solution to adapt automatically when newer EC2 instances join the cluster, or when some EC2 instances are terminated.</p>\n\n<p>Which of the following solutions would you recommend for the given requirement?</p>\n",
          "answers": [
            "<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</p>",
            "<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</p>",
            "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</p>",
            "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</strong></p>\n\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n\n<p>A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p>\n\n<p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer's instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes.</p>\n\n<p>The lifecycle hook that is called on ALL instances, whenever an instance comes up or another one goes down, is the <code>configure</code> hook. So this option is the best fit for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</strong> - As mentioned in the explanation above, the <code>setup</code> hook is only used when an instance is first created, so this option is incorrect.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</strong> - Lifecycle hooks on Auto Scaling Groups may seem like a good idea at first, but using AWS Lambda, the solution is not practicable as SSH'ing into the instance via Lambda will not work.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</strong> - EC2 user-data scripts are only triggered on an instance's first launch, so this option just acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/opsworks/\">https://aws.amazon.com/opsworks/</a></p>\n"
        }
      },
      {
        "id": 75949152,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 4",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps team is assisting with the deployment of a web application's infrastructure using AWS CloudFormation. The database management team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. The software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to maintain. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.</p>",
            "<p>Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.</p>",
            "<p>Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.</p>",
            "<p>Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack.</p>"
          ],
          "explanation": "<p>A nested stack is a stack that you create within another stack by using the AWS::CloudFormation::Stack resource. With nested stacks, you deploy and manage all resources from a single stack. You can use outputs from one stack in the nested stack group as inputs to another stack in the group. This differs from exporting values.</p><p>If you want to isolate information sharing to within a nested stack group, AWS suggests that you use nested stacks. To share information with other stacks (not just within the group of nested stacks), you should export values instead.</p><p>Change sets for nested stacks affect the entire stack hierarchy which does not meet the requirements as each team requires resource-level change-set reviews. Therefore, in this scenario it is better to export values rather than use a nested stack.</p><p><strong>CORRECT: </strong>\"Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks\" is incorrect.</p><p>As described above, to meet the requirements an export of values is preferable to using nested stacks in this scenario.</p><p><strong>INCORRECT:</strong> \"Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks\" is incorrect.</p><p>Stack sets are used for extending the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions.</p><p><strong>INCORRECT:</strong> \"Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack\" is incorrect.</p><p>Values should be exported as per the correct answer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html#output-vs-nested\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html#output-vs-nested</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 134588487,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A JavaScript-based online salary calculator hosted on-premises is slated to be migrated to AWS. The application has no server-side code and is just composed of a UI powered by Vue.js and Bootstrap. Since the online calculator may contain sensitive financial data, adding HTTP response headers such as <code>X-Content-Type-Options</code>, <code>X-Frame-Options</code> and <code>X-XSS-Protection</code> should be implemented to comply with the Open Web Application Security Project (OWASP) standards.</p><p>Which of the following is the MOST suitable solution to implement?</p>",
          "answers": [
            "<p>Host the application on an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3 bucket to track user activity. Enable S3 client-side encryption and configure it to return the required security headers.</p>",
            "<p>Host the application on an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3 bucket to track user activity. Configure the bucket policy of the S3 bucket to return the required security headers.</p>",
            "<p>Host the application on an Amazon S3 bucket configured for website hosting. Set up an Amazon CloudFront web distribution and set the S3 bucket as the origin with the origin response event set to trigger a Lambda@Edge function. Add the required security headers in the HTTP response using the AWS Lambda function.</p>",
            "<p>Host the application on an Amazon S3 bucket configured for website hosting. Set up an Amazon CloudFront web distribution and set the S3 bucket as the origin. Set a custom Request and Response Behavior in CloudFront that automatically adds the required security headers in the HTTP response.</p>"
          ],
          "explanation": "<p><strong>Amazon CloudFront</strong> now supports adding HTTP security headers using response headers policies. This feature allows you to configure CloudFront to include headers like <code>X-Content-Type-Options</code>, <code>X-Frame-Options</code>, and <code>X-XSS-Protection</code> in HTTP responses. You can attach these headers by creating a custom policy or using an AWS-managed policy such as <code>SecurityHeadersPolicy</code>. This setup does not require writing code or using Lambda@Edge, making it easier to manage.</p><p><img alt=\"Response Headers Policy\" height=\"423\" src=\"https://media.tutorialsdojo.com/public/td-response-headers-policy-13May2025.png\" width=\"1000\">CloudFront is the content delivery network for static websites hosted on Amazon S3 and can handle header manipulation through its response behavior. Attaching a response headers policy to a cache behavior in your CloudFront distribution adds the headers automatically to all responses from the origin. This approach meets OWASP security requirements and simplifies maintenance because the configuration is handled entirely within CloudFront.</p><p>Therefore, using CloudFront response headers policies is the recommended and most suitable solution for adding security headers to static web applications hosted on S3.</p><p>Hence, the correct answer is: <strong>Host the application on an Amazon S3 bucket configured for website hosting. Set up an Amazon CloudFront web distribution and set the S3 bucket as the origin. Set a custom Request and Response Behavior in CloudFront that automatically adds the required security headers in the HTTP response.</strong></p><p>The option that says: <strong>Host the application on an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3 bucket to track user activity. Enable S3 client-side encryption and configure it to return the required security headers</strong> is incorrect because Amazon S3 does not typically allow adding custom HTTP response headers through client-side encryption. Encryption protects stored data but does not change how responses are sent to users. S3 alone cannot add security headers to the HTTP response.</p><p>The option that says:<strong> Host the application on an Amazon S3 bucket configured for website hosting, then set up server access logging on the S3 bucket to track user activity. Configure the bucket policy of the S3 bucket to return the required security headers</strong> is incorrect because bucket policies control access to S3 content, not HTTP response behavior. You cannot add or modify HTTP headers using a bucket policy. This method cannot meet the requirement of adding OWASP-recommended security headers.</p><p>The option that says: <strong>Host the application on an Amazon S3 bucket configured for website hosting. Set up an Amazon CloudFront web distribution and set the S3 bucket as the origin with the origin response event set to trigger a Lambda@Edge function. Add the required security headers in the HTTP response using the AWS Lambda function</strong> is incorrect. While this method works, it only adds unnecessary complexity. Lambda@Edge requires extra setup, code, and management. CloudFront\u2019s built-in Response Headers Policies now provide a more straightforward way to add security headers, making Lambda@Edge unnecessary for this case.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-response-headers-policies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-response-headers-policies.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/creating-response-headers-policies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/creating-response-headers-policies.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>"
        }
      },
      {
        "id": 82921424,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The development team at a social media company is using AWS CodeCommit to store code. As a Lead DevOps Engineer at the company, you have defined a company-wide rule so that the team should not be able to push to the master branch. You have added all the developers in an IAM group <code>developers</code> and attached the AWS managed IAM policy <code>arn:aws:iam::aws:policy/AWSCodeCommitPowerUser</code> to the group. This policy provides full access to AWS CodeCommit repositories but does not allow repository deletion, however, your developers can still push to the master branch.</p>\n\n<p>How should you prevent the developers from pushing to the master branch?</p>\n",
          "answers": [
            "<p>Add a new IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</p>",
            "<p>Include a CodeCommit repository policy on each repository with an explicit Deny for <code>codecommit:GitPush</code></p>",
            "<p>Modify the AWS managed IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</p>",
            "<p>Include a git commit pre-hook that invokes a Lambda function and checks if the push is done to master</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a new IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</strong></p>\n\n<p>Any CodeCommit repository user who has sufficient permissions to push code to the repository can contribute to any branch in that repository. You can configure a branch so that only some repository users can push or merge code to that branch. For example, you might want to configure a branch used for production code so that only a subset of senior developers can push or merge changes to that branch. Other developers can still pull from the branch, make their own branches, and create pull requests, but they cannot push or merge changes to that branch. You can configure this access by creating a conditional policy that uses a context key for one or more branches in IAM.</p>\n\n<p>For the given use-case, you need to add an extra policy with an explicit Deny. Please note an Explicit Deny always has priority over anything else.</p>\n\n<p>Limit pushes and merges to branches in AWS CodeCommit:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q60-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Include a CodeCommit repository policy on each repository with an explicit Deny for <code>codecommit:GitPush</code></strong> - This option has been added as a distractor since CodeCommit repository policies do not exist.</p>\n\n<p><strong>Modify the AWS managed IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</strong> - You cannot modify an AWS managed IAM policy, so this option is incorrect.</p>\n\n<p><strong>Include a git commit pre-hook that invokes a Lambda function and checks if the push is done to master</strong> - Although it would be cool, CodeCommit still does not have a pre-hook feature to integrate with Lambda.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n"
        }
      },
      {
        "id": 134588421,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An international IT consulting firm has multiple on-premises data centers across the globe. Their technical team regularly uploads financial and regulatory files from each of their respective data centers to a centralized web portal hosted in AWS. It uses an Amazon S3 bucket named <code>financial-tdojo-reports</code> to store the data. Another team downloads various reports from a CloudFront web distribution that uses the same Amazon S3 bucket as the origin. A DevOps Engineer noticed that the staff are using both the CloudFront link and the direct Amazon S3 URLs to download the reports. The IT Security team of the company considered this as a security risk, and they recommended to re-design the architecture. A new system must be implemented that prevents anyone from bypassing the CloudFront distribution and disable direct access from Amazon S3 URLs. </p><p>What should the Engineer do to meet the above requirement?</p>",
          "answers": [
            "<p>Create an Origin Access Control (OAC) and associate it with the S3 bucket origin in the CloudFront distribution. Update the bucket policy for the <code>financial-tdojo-reports</code> bucket to allow access only from the specified CloudFront distribution.</p>",
            "<p>Set up a custom SSL in your CloudFront web distribution instead of the default SSL. For each user, revoke the existing permission to access Amazon S3 URLs to download the objects.</p>",
            "<p>In the CloudFront web distribution, set up a field-level encryption configuration and for each user, revoke the existing permission to access Amazon S3 URLs to download the objects.</p>",
            "<p>Configure the distribution to use Signed URLs and create a Signed URL signing key pair (a public/private key pair). Grant permission to the public key to access the objects in the S3 bucket.</p>"
          ],
          "explanation": "<p><strong>Amazon CloudFront</strong> is a content delivery network (CDN) service provided by AWS. It securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. Amazon CloudFront provides various options to secure content delivery from S3 buckets, including preventing direct access to S3 and forcing downloads through the CloudFront distribution.</p><p><strong>Origin Access Control</strong> is a feature of Amazon CloudFront that enhances security for accessing S3 buckets and other supported origins. It allows you to restrict access to your origin resources, ensuring that they can only be accessed through designated CloudFront distributions. Some of the key points about OAC are:</p><ul><li><p>It uses AWS Identity and Access Management (IAM) service principals for authentication.</p></li><li><p>OAC employs short-term credentials with frequent rotations for improved security.</p></li><li><p>It supports comprehensive HTTP methods and server-side encryption with AWS KMS (SSE-KMS).</p></li><li><p>OAC can be used with S3 buckets in all AWS regions, as well as with AWS Lambda function URL origins.</p></li></ul><p><img src=\"https://media.tutorialsdojo.com/public/CloudFront_6AUG2023.png\"></p><p>By creating an Origin Access Control (OAC) setting in the CloudFront distribution and updating the S3 bucket policy to grant access only to the CloudFront OAC, you effectively prevent direct access to the S3 bucket from any other source, including direct S3 URLs. All access to the objects in the S3 bucket must go through the CloudFront distribution, meeting the security requirement.</p><p>Hence, the correct answer is: <strong>Create an Origin Access Control (OAC) and associate it with the S3 bucket origin in the CloudFront distribution. Update the bucket policy for the </strong><code><strong>financial-tdojo-reports</strong></code><strong> bucket to allow access only from the specified CloudFront distribution.</strong></p><p>The option that says: <strong>Set up a custom SSL in your CloudFront web distribution instead of the default SSL. For each user, revoke the existing permission to access Amazon S3 URLs to download the objects</strong> is incorrect because SSL is not needed in this particular scenario. What you need to implement is an OAC.</p><p>The option that says: <strong>In the CloudFront web distribution, set up a field-level encryption configuration and for each user, revoke the existing permission to access Amazon S3 URLs to download the objects</strong> is incorrect because the field-level encryption configuration is primarily used for safeguarding sensitive fields in your CloudFront. Therefore, it is not suitable for this scenario.</p><p>The option that says: <strong>Configure the distribution to use Signed URLs and create a Signed URL signing key pair (a public/private key pair). Grant permission to the public key to access the objects in the S3 bucket</strong> is incorrect because Signed URLs are used to provide temporary access to specific objects in an S3 bucket, typically for a limited time period. While this approach can restrict direct access to the S3 bucket, it requires generating and managing Signed URLs for each object, which can be cumbersome and not scalable, especially if there are many objects or frequent updates.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 67357174,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company wants to create an automated monitoring solution to generate real-time customized notifications regarding unrestricted security groups in the company's production AWS account. The notification must contain the name and ID of the noncompliant security group. The DevOps team at the company has already activated the restricted-ssh AWS Config managed rule. The team has also set up an Amazon Simple Notification Service (Amazon SNS) topic and subscribed relevant personnel to it.</p>\n\n<p>Which of the following options represents the BEST solution for the given scenario?</p>\n",
          "answers": [
            "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>",
            "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>",
            "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>",
            "<p>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong></p>\n\n<p>You can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes do not comply with the conditions in your rules.</p>\n\n<p>The restricted-ssh rule checks if the incoming SSH traffic for the security groups is accessible. The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0). This rule applies only to IPv4.</p>\n\n<p>For the given use case, you need to monitor for the NON_COMPLIANT evaluation result of the rule, which implies that the rule has failed the conditions of the compliance check. You can then create an Amazon EventBridge rule (with AWS Config configured as a source) that is put in action when it matches the NON_COMPLIANT evaluation result of the restricted-ssh rule. The EventBridge rule, in turn, publishes a notification to the SNS topic.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p>You should note that you can only set up a filter policy on an SNS subscription and NOT on the SNS topic itself. In addition, it is wasteful to set up Amazon EventBridge rule on all AWS Config managed rules, rather than only the restricted-ssh rule. Therefore, both these options are incorrect.</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong> - You get an ERROR evaluation result when one of the required/optional parameters is not valid, or not of the correct type, or is formatted incorrectly. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\">https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html</a></p>\n"
        }
      },
      {
        "id": 134588457,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>You have created a new Elastic Beanstalk environment to be used as a pre-production stage for load testing new code version. Since code changes are committed on a regular basis, you sometimes need to deploy new versions 2 to 3 times each day. You need to deploy a new version as quickly as possible in a cost-effective way to give ample time for the QA team to test it. </p><p>Which of the following implementations is suited for this scenario?</p>",
          "answers": [
            "<p>Use <code>Rolling</code> as the deployment policy to deploy new versions.</p>",
            "<p>Use <code>All at once</code> as deployment policy to deploy new versions.</p>",
            "<p>Use <code>Immutable</code> as the deployment policy to deploy code on new instances.</p>",
            "<p>Implement a blue/green deployment strategy to have the new version ready for quick switching.</p>"
          ],
          "explanation": "<p>In ElasticBeanstalk, you can choose from a variety of deployment methods:</p><p><strong>All at once</strong> \u2013 Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. This is the method that provides the least amount of time for deployment.</p><p><strong>Rolling</strong> \u2013 Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch.</p><p><strong>Rolling with additional batch</strong> \u2013 Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process.</p><p><strong>Immutable</strong> \u2013 Deploy the new version to a fresh group of instances by performing an <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">immutable update</a>.</p><p><strong>Blue/Green</strong> - Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.</p><p><br></p><p>Refer to the table below for the characteristics of each deployment method as well as the amount of time it takes to do the deployment, as seen in the <strong>Deploy Time</strong> column:</p><p><img src=\"https://media.tutorialsdojo.com/public/DeploymentMethods_2AUG2023.png\"></p><p>Hence, the correct answer is:<strong><em> </em>Use </strong><code><strong>All at once</strong></code><strong> as the deployment policy to deploy new versions.</strong></p><p>The option that says: <strong>Use </strong><code><strong>Rolling</strong></code><strong> as the deployment policy to deploy new versions</strong> is incorrect because this deployment type is not as fast as \u201cAll at once\u201d deployment since the code is deployed in batches.</p><p>The option that says: <strong>Use </strong><code><strong>Immutable</strong></code><strong> as the deployment policy to deploy code on new instances</strong> is incorrect because this deployment type takes time as new instances are being provisioned during the deployment.</p><p>The option that says: <strong>Implement a blue/green deployment strategy to have the new version ready for quick switching</strong> is incorrect because just like the Immutable deployment, this type also takes time since the new instances are provisioned first before the actual deployment starts. Plus, it incurs additional cost as 2 sets of instances are running at the same time.</p><p><br><strong>References:</strong> <br><br><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      }
    ],
    "answers": {
      "67357132": [
        "c"
      ],
      "67357174": [
        "b"
      ],
      "75949086": [
        "c"
      ],
      "75949100": [
        "a"
      ],
      "75949152": [
        "a"
      ],
      "82921354": [
        "a"
      ],
      "82921358": [
        "a"
      ],
      "82921424": [
        "a"
      ],
      "82921458": [
        "a"
      ],
      "134588385": [
        "c"
      ],
      "134588421": [
        "a"
      ],
      "134588457": [
        "b"
      ],
      "134588487": [
        "d"
      ],
      "134588499": [
        "d"
      ],
      "134588501": [
        "b"
      ],
      "134588513": [
        "a",
        "e"
      ],
      "138248101": [
        "a"
      ],
      "138248201": [
        "b",
        "c"
      ],
      "138248233": [
        "d"
      ],
      "138248247": [
        "a",
        "c",
        "d"
      ]
    }
  },
  {
    "id": "1770378186182",
    "date": "2026-02-06T11:43:06.182Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 7,
    "incorrect": 3,
    "unanswered": 0,
    "total": 10,
    "percent": 70,
    "duration": 2070546,
    "questions": [
      {
        "id": 134588399,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is planning to launch a mobile marketplace using AWS Amplify and AWS Mobile Hub which will serve millions of users worldwide. The backend APIs will be launched to multiple AWS regions to process the sales and financial transactions in the region closest to the users to lower the latency. A DevOps Engineer was instructed to design the system architecture to ensure that the transactions made in one region are automatically replicated to other regions. In the coming months ahead, it is expected that the marketplace will have millions of users across North America, South America, Europe, and Asia.</p><p>Which of the following is the MOST scalable, cost-effective, and highly available architecture that the Engineer should implement?</p>",
          "answers": [
            "<p>Store the individual transactions in each local region in an Amazon DynamoDB table. Use an AWS Lambda function to read recent writes from the primary DynamoDB table and replay the data to tables in all other regions.</p>",
            "<p>Create an Amazon DynamoDB Global Table across the required AWS Regions. Store the individual transactions in the local region\u2019s replica table. Any changes made in one of the replica tables are automatically replicated across all other replica tables worldwide.</p>",
            "<p>Set up an Amazon DynamoDB Global table in your preferred AWS region and enable the DynamoDB Streams option. Set up replica tables in the other AWS regions where you want to replicate your data. In each local region, store the individual transactions in a DynamoDB replica table in the same region.</p>",
            "<p>Set up an Amazon Aurora Global Database with a primary writer in one AWS Region and read-only replicas in other Regions. Store the transactions in the local Aurora writer instance. Aurora will replicate data asynchronously across Regions via the storage layer, and on failover you can promote a replica to be the new writer.</p>"
          ],
          "explanation": "<p>Amazon DynamoDB is a fully managed, key-value and document database that delivers single-digit millisecond performance at any scale. It is a multi-Region, multi-active database that offers built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and support peaks of more than 20 million requests per second.</p><p><img alt=\"Amazon DynamoDB Global Table\" height=\"770\" src=\"https://media.tutorialsdojo.com/amazon-dynamodb-global-table-dop-c01.png\" width=\"1000\"></p><p>DynamoDB Global Tables build upon DynamoDB\u2019s global footprint to provide a fully managed, multi-Region, and multi-active (active-active) database. They automatically replicate your DynamoDB table data across the AWS Regions you select, eliminating the need to build and maintain your own replication logic. With Global Tables, your applications get fast, local read and write performance while AWS takes care of data replication and conflict resolution in the background.</p><p>If a Region becomes unavailable, your application can continue reading and writing in other Regions without downtime, providing high availability and resiliency.</p><p>Hence, the correct answer is: <strong>Create an Amazon DynamoDB Global Table across the required AWS Regions. Store the individual transactions in the local region\u2019s replica table. Any changes made in one of the replica tables are automatically replicated across all other replica tables worldwide.</strong></p><p>The option says: <strong>Store the individual transactions in each local region in an Amazon DynamoDB table. Use an AWS Lambda function to read recent writes from the primary DynamoDB table and replay the data to tables in all other regions</strong> is incorrect because this requires building and maintaining custom replication logic, which introduces additional operational complexity, higher latency, and greater potential for data inconsistency. DynamoDB Global Tables already provide automatic, fully managed, multi-Region replication without requiring custom Lambda-based replication pipelines, making this option less scalable and more costly.</p><p>The option says: <strong>Set up an Amazon DynamoDB Global table in your preferred AWS region and enable the DynamoDB Streams option. Set up replica tables in the other AWS regions where you want to replicate your data. In each local region, store the individual transactions in a DynamoDB replica table in the same region</strong> is incorrect because DynamoDB Global Tables already enable and use DynamoDB Streams automatically to handle change data capture and cross-Region replication. Customers do not need to manually enable Streams or configure replication themselves. This option incorrectly suggests that manual setup is required, while in reality AWS manages the Streams integration and replication process in the background.</p><p>The option says: <strong>Set up an Amazon Aurora Global Database with a primary writer in one AWS Region and read-only replicas in other Regions. Store the transactions in the local Aurora writer instance. Aurora will replicate data asynchronously across Regions via the storage layer, and on failover you can promote a replica to be the new writer</strong> is incorrect because Aurora Global Database is a single-writer design; only the primary Region accepts writes, and all secondary Regions are read-only until a switchover or failover. This means users in secondary Regions can\u2019t perform low-latency local writes and must route write traffic cross-Region to the primary, introducing latency; replication to secondaries is asynchronous (typically under ~1 second) and promotion of a secondary is an operational event, not simultaneous multi-Region writes. Therefore, it doesn\u2019t meet the requirement for active-active, multi-Region write capability with automatic replication in every Region.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>"
        }
      },
      {
        "id": 138248181,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company would like to set up an audit process to ensure that the enterprise application is running exclusively on Amazon EC2 Dedicated Hosts. The company is also concerned about the increasing costs of its application software licensing from its third-party vendor. To meet the compliance requirement, a DevOps Engineer must create a workflow to audit the enterprise applications hosted in its Amazon VPC.</p><p>Which of the following options should the Engineer implement to satisfy the requirement with the LEAST administrative overhead?</p>",
          "answers": [
            "<p>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the <code>PutComplianceItems</code> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the <code>ListComplianceSummaries</code> API action.</p>",
            "<p>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the <code>inspector-scheduled-run</code> blueprint.</p>",
            "<p>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the <code>config-rule-change-triggered</code> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</p>",
            "<p>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data.</p>"
          ],
          "explanation": "<p>You can use <strong>AWS Config</strong> to record configuration changes for Dedicated Hosts, and instances that are launched, stopped, or terminated on them. You can then use the information captured by AWS Config as a data source for license reporting.</p><p>AWS Config records configuration information for Dedicated Hosts and instances individually and pairs this information through relationships. There are three reporting conditions:</p><p>- AWS Config recording status \u2014 When On, AWS Config is recording one or more AWS resource types, which can include Dedicated Hosts and Dedicated Instances. To capture the information required for license reporting, verify that hosts and instances are being recorded with the following fields.</p><p>- Host recording status \u2014 When Enabled, the configuration information for Dedicated Hosts is recorded.</p><p>- Instance recording status \u2014 When Enabled, the configuration information for Dedicated Instances is recorded.</p><p>If any of these three conditions are disabled, the icon in the Edit Config Recording button is red. To derive the full benefit of this tool, ensure that all three recording methods are enabled. When all three are enabled, the icon is green. To edit the settings, choose Edit Config Recording. You are directed to the <em>Set up AWS Config </em>page in the AWS Config console, where you can set up AWS Config and start recording for your hosts, instances, and other supported resource types. AWS Config records your resources after it discovers them, which might take several minutes.</p><p>After AWS Config starts recording configuration changes to your hosts and instances, you can get the configuration history of any host that you have allocated or released and any instance that you have launched, stopped, or terminated. For example, at any point in the configuration history of a Dedicated Host, you can look up how many instances are launched on that host, along with the number of sockets and cores on the host. For any of those instances, you can also look up the ID of its Amazon Machine Image (AMI). You can use this information to report on licensing for your own server-bound software that is licensed per-socket or per-core.</p><p>You can view configuration histories in any of the following ways.</p><p>- By using the AWS Config console. For each recorded resource, you can view a timeline page, which provides a history of configuration details. To view this page, choose the gray icon in the Config Timeline column of the Dedicated Hosts page.</p><p>- By running AWS CLI commands. First, you can use the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/configservice/list-discovered-resources.html\">list-discovered-resources</a> command to get a list of all hosts and instances. Then, you can use the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/configservice/get-resource-config-history.html#get-resource-config-history\">get-resource-config-history</a> command to get the configuration details of a host or instance for a specific time interval.</p><p>- By using the AWS Config API in your applications. First, you can use the <a href=\"https://docs.aws.amazon.com/config/latest/APIReference/API_ListDiscoveredResources.html\">ListDiscoveredResources</a> action to get a list of all hosts and instances. Then, you can use the <a href=\"https://docs.aws.amazon.com/config/latest/APIReference/API_GetResourceConfigHistory.html\">GetResourceConfigHistory</a> action to get the configuration details of a host or instance for a specific time interval.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-AWS-Config-Status-02-05-2025.png\"></p><p>Hence, the correct answer is: <strong>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the </strong><code><strong>config-rule-change-triggered</strong></code><strong> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</strong></p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the </strong><code><strong>PutComplianceItems</strong></code><strong> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the </strong><code><strong>ListComplianceSummaries</strong></code><strong> API action </strong>is incorrect because the AWS Systems Manager Configuration Compliance service is primarily used to scan your fleet of managed instances for patch compliance and configuration inconsistencies. A better solution is to use AWS Config to record the status of your Dedicated Hosts.</p><p>The option that says: <strong>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the </strong><code><strong>inspector-scheduled-run</strong></code><strong> blueprint</strong> is incorrect because Amazon Inspector is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not capable of recording the status of your EC2 instances nor detect if they are configured as a Dedicated Host.</p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data </strong>is incorrect. Although this may be a possible solution, it entails a lot of administrative effort in comparison to just using AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html \">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom \">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/\">https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and AWS Config Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-config/\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 134588507,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is running a batch job hosted on AWS Fargate to process large ZIP files. The job is triggered whenever files are uploaded on an Amazon S3 bucket. To save costs, the company wants to set the minimum number of ECS Tasks to 1, and only increase the task count when objects are uploaded to the S3 bucket again. Once processing is complete, the S3 bucket should be emptied out and all ECS tasks should be stopped. The object-level logging has already been enabled in the bucket.</p><p>Which of the following options is the EASIEST way to implement this?</p>",
          "answers": [
            "<p>Create an Amazon EventBridge rule to detect S3 object PUT operations and set the target to an AWS Lambda function that will update the desired count of tasks in the ECS service using an ECS capacity provider. Create another EventBridge rule to detect S3 DELETE operations and use the Lambda function to scale down the ECS tasks by updating the capacity provider settings.</p>",
            "<p>Create an EventBridge rule to detect S3 object PUT operations and set the target to the ECS cluster to run a new ECS task. Create another EventBridge rule to detect S3 DELETE operations. Set the target to a Lambda function that will stop all ECS tasks.</p>",
            "<p>Use Amazon CloudWatch Alarms on AWS CloudTrail since the S3 object-level operations are recorded on CloudTrail. Create two Lambda functions for increasing/decreasing the ECS task count. Set these as respective targets for the CloudWatch Alarm depending on the S3 event.</p>",
            "<p>Use CloudWatch Alarms on CloudTrail since this S3 object-level operations are recorded on CloudTrail. Set two alarm actions to update ECS task count to scale-out/scale-in depending on the S3 event.</p>"
          ],
          "explanation": "<p>You can use Amazon EventBridge (formerly Amazon CloudWatch Events) to run Amazon ECS tasks when certain AWS events occur. You can set up an EventBridge rule that runs an Amazon ECS task whenever a file is uploaded to a certain Amazon S3 bucket using the Amazon S3 PUT operation.</p><p>First, you must create an Amazon EventBridge rule for the S3 service that will watch for object-level operations \u2013 PUT and DELETE objects. For object-level operations, it is required to create a CloudTrail trail first. You need two rules \u2013 one for running a task whenever a file is uploaded to S3 and another for stopping all the ECS tasks. For the first rule, select \u201cECS task\u201d as the target and input the needed values such as the cluster name, task definition, and the task count. For the second rule, select a Lambda function as the target. To stop a running task, you need to call the StopTask API, which can be done in a Lambda function.</p><p><img src=\"https://media.tutorialsdojo.com/public/overview-fargate.png\"></p><p>Hence, the correct answer is: <strong>Create an EventBridge rule to detect S3 object PUT operations and set the target to the ECS cluster to run a new ECS task. Create another EventBridge rule to detect S3 DELETE operations. Set the target to a Lambda function that will stop all ECS tasks.</strong></p><p>The option that says: <strong>Create an Amazon EventBridge rule to detect S3 object PUT operations and set the target to an AWS Lambda function that will update the desired count of tasks in the ECS service using an ECS capacity provider. Create another EventBridge rule to detect S3 DELETE operations and use the Lambda function to scale down the ECS tasks by updating the capacity provider settings </strong>is incorrect because ECS capacity providers are primarily designed to manage the underlying infrastructure for ECS tasks, such as EC2 instances or Fargate Spot, rather than to scale task counts based on event triggers directly.</p><p>The option that says: <strong>Use Amazon CloudWatch Alarms on AWS CloudTrail since the S3 object-level operations are recorded on CloudTrail. Create two Lambda functions for increasing/decreasing the ECS task count. Set these as respective targets for the CloudWatch Alarm depending on the S3 event </strong>is incorrect because using CloudTrail, CloudWatch Alarm, and two Lambda functions will only create unnecessary complexity to what you want to achieve. Amazon EventBridge can directly target an ECS task on the Targets section when you create a new rule.</p><p>The option that says: <strong>Use CloudWatch Alarms on CloudTrail since this S3 object-level operations are recorded on CloudTrail. Set two alarm actions to update ECS task count to scale-out/scale-in depending on the S3 event</strong> is incorrect because you can\u2019t directly set CloudWatch Alarms to update the ECS task count. You simply have to use Amazon EventBridge instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-tutorial-ECS.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-tutorial-ECS.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 75949058,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company stores sensitive data in Amazon S3 buckets. Each day the development team create new buckets for projects they are working on, and all existing and future buckets must be secured. The security team requires encryption, logging, and versioning to be enabled. It is also required that buckets should not be publicly accessible.</p><p>What should a DevOps engineer do to meet these requirements?</p>",
          "answers": [
            "<p>Enable AWS CloudTrail and configure automatic remediation using AWS Lambda.</p>",
            "<p>Enable AWS Trusted Advisor and configure automatic remediation using Amazon CloudWatch Events.</p>",
            "<p>Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.</p>",
            "<p>Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents.</p>"
          ],
          "explanation": "<p>You can use the AWS Config Auto Remediation feature to auto remediate any non-compliant S3 buckets using the AWS Config rules. There are several pre-built rules you can leverage for various use cases. For example, the following rules can be used to meet the requirements specified in this question:</p><ul><li><p>s3-bucket-logging-enabled</p></li><li><p>s3-bucket-server-side-encryption-enabled</p></li><li><p>s3-bucket-public-read-prohibited</p></li><li><p>s3-bucket-public-write-prohibited</p></li></ul><p>These rules act as controls to prevent any non-compliant S3 activities. AWS Config uses AWS Systems Manager to implement the remediations and the rules are automation documents that Systems Manager runs.</p><p><strong>CORRECT: </strong>\"Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable AWS Trusted Advisor and configure automatic remediation using Amazon CloudWatch Events\" is incorrect.</p><p>Trusted Advisor will not discover these specific compliance events and CloudWatch Events is not able to remediate them.</p><p><strong>INCORRECT:</strong> \"Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents\" is incorrect.</p><p>Systems Manager automation documents are used for remediation, but Systems Manager is unable to discover the specific compliance events for this scenario. AWS Config should be used with Systems Manager to meet the requirements.</p><p><strong>INCORRECT:</strong> \"Enable AWS CloudTrail and configure automatic remediation using AWS Lambda\" is incorrect.</p><p>CloudTrail can only detect API actions rather than audit compliance with configuration requirements. Though it is possible to use CloudTrail to detect some configuration changes this would be complex to implement.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>"
        }
      },
      {
        "id": 82921410,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The technology team at a leading bank is using software that has a license type that gets billed based on the number of CPU sockets that are being used. The team would like to ensure that they are using the most appropriate EC2 launch mode and create a compliance dashboard that highlights any violation of that decision. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend as the best fit?</p>\n",
          "answers": [
            "<p>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</p>",
            "<p>Launch the EC2 instances on Reserved Instances and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</p>",
            "<p>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</p>",
            "<p>Launch the EC2 instances on Reserved Instance and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</strong></p>\n\n<p>An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated for your use. When you bring your own licenses to Amazon EC2 Dedicated Hosts, you can let AWS take care of all these administrative tasks on your behalf. AWS gives administrators the option to perform a one-time onboarding set up in AWS License Manager.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q50-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/dedicated-hosts/faqs/\">https://aws.amazon.com/ec2/dedicated-hosts/faqs/</a></p>\n\n<p>To get access to the CPU sockets for billing purposes, you need to use EC2 Dedicated Hosts. Reserved Instances are here to save cost on a yearly utilization of EC2. Reserved Instances (RI) provide a significant discount (up to 72%) compared to On-Demand pricing and provide a capacity reservation when used in a specific Availability Zone.</p>\n\n<p>AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time. An AWS Config rule represents your desired configuration settings for specific AWS resources or an entire AWS account. AWS Config provides customizable, predefined rules to help you get started. If a resource violates a rule, AWS Config flags the resource and the rule as noncompliant, and AWS Config notifies you through Amazon SNS.</p>\n\n<p>For the given use-case, you need to create a Config custom rule that will check the application tag and ensure the instance is launched as a Dedicated Host.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q50-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q50-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</strong> -  Service Catalog is used to create stacks backed by CloudFormation through a service portal. To track compliance over time, you must use AWS Config.</p>\n\n<p><strong>Launch the EC2 instances on Reserved Instances and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</strong> - Reserved Instances can only be used to save cost on a yearly utilization of EC2 for example. To get access to the CPU sockets for billing purposes, you need to use EC2 Dedicated Hosts.</p>\n\n<p><strong>Launch the EC2 instances on Reserved Instance and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</strong> - Service Catalog is used to create stacks backed by CloudFormation through a service portal. To track compliance over time, you must use AWS Config. Besides, Reserved Instances can only be used to save cost on a yearly utilization of EC2 for example. To get access to the CPU sockets for billing purposes, you need to use EC2 Dedicated Hosts.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/dedicated-hosts/faqs/\">https://aws.amazon.com/ec2/dedicated-hosts/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n"
        }
      },
      {
        "id": 75949144,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 4",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A new application is being deployed on AWS using Amazon EC2 instances. The operations team must monitor the instance\u2019s application logs and API activity and need a solution for querying both sets of logs.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and use AWS CloudTrail to log API activity to Amazon S3. Use CloudWatch Logs Insights to query both sets of logs.</p>",
            "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon S3 bucket and configure AWS CloudTrail to log API activity to the same S3 bucket. Use Amazon Athena to query both sets of logs.</p>",
            "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon SQS queue and configure AWS CloudTrail to log API activity to the same SQS queue. Create an AWS Lambda function to query messages in the queue.</p>",
            "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and configure AWS CloudTrail to log API activity to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.</p>"
          ],
          "explanation": "<p>The unified CloudWatch agent enables you to collect system logs, application logs, and metrics from your Amazon EC2 instances. The log files can be stored in Amazon CloudWatch Logs.</p><p>AWS CloudTrail logs API activity from your account and can also be configured to stream the log data to an Amazon CloudWatch Logs log group.</p><p>CloudWatch Logs Insights can then be used to query both sets of logs. This service is used to interactively search and analyze your log data in Amazon CloudWatch Logs.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and configure AWS CloudTrail to log API activity to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and use AWS CloudTrail to log API activity to Amazon S3. Use CloudWatch Logs Insights to query both sets of logs\" is incorrect.</p><p>CloudWatch Logs Insights can be used to query log data in CloudWatch Logs log groups but not data that is stored directly in an Amazon S3 bucket.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon S3 bucket and configure AWS CloudTrail to log API activity to the same S3 bucket. Use Amazon Athena to query both sets of logs\" is incorrect.</p><p>The CloudWatch agent can be configured to send the log files to an Amazon CloudWatch Logs log group but cannot send data into an S3 bucket.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon SQS queue and configure AWS CloudTrail to log API activity to the same SQS queue. Create an AWS Lambda function to query messages in the queue\" is incorrect.</p><p>The SQS queue is not a destination you can configure in the CloudWatch Logs agent and it not a suitable storage location as it is meant for temporary storage of data that is awaiting processing.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 138248141,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A government-sponsored health service is running its web application containing information about the clinics, hospitals, medical specialists, and other medical services in the country. The organization also has a set of public web services which enable third-party companies to search medical data for its respective applications and clients. AWS Lambda functions are used for the public APIs. For its database-tier, an Amazon DynamoDB table stores all of the data with an Amazon OpenSearch Service domain, which supports the search feature and stores the indexes. A DevOps engineer has been instructed to ensure that in the event of a failed deployment, there should be no downtime and a system should be in place to prevent subsequent deployments. The service must strictly maintain full capacity during API deployment without any reduced capacity to avoid degradation of service.</p><p>How can the engineer meet the above requirements in the MOST efficient way?</p>",
          "answers": [
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>All at Once</code>.</p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Rolling</code>.</p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Immutable</code></p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication.</p>"
          ],
          "explanation": "<p><strong>AWS Elastic Beanstalk</strong> provides several options for how <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">deployments</a> are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's an automatically scaling environment (you didn't specify the <code>--single</code> option), it uses rolling deployments.</p><p>With rolling deployments, Elastic Beanstalk splits the environment's EC2 instances into batches and deploys the new version of the application to one batch at a time, leaving the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p><p><img src=\"https://media.tutorialsdojo.com/public/environments-mgmt-updates-immutable.png\"></p><p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.''</p><p>Immutable deployments perform an <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">immutable update</a> to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.</p><p>Hence, the correct answer is: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>Immutable.</strong></code></p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>All at Once</strong></code><strong><em> </em></strong>is incorrect because this policy only deploys the new version to all instances simultaneously, which means that the instances in your environment are out of service for a short time while the deployment occurs.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to</strong> <code><strong>Rolling</strong></code><em> </em>is incorrect because this policy will just deploy the new version in batches where each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication</strong> is incorrect because you can't host a dynamic web application in Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 75949162,
        "correct_response": [
          "a",
          "b",
          "f"
        ],
        "source": "Neal Set 4",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company uses GitHub to store the code for their software development. The company\u2019s DevOps team want to automate the build process of an application. When the GitHub repository is updated, the code should be compiled, tested, and then pushed to an Amazon S3 bucket.</p><p>Which combination of steps would address these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Add a buildspec.yml file to the source code with build instructions.</p>",
            "<p>Configure a GitHub webhook to trigger a build every time a code change is pushed to the repository.</p>",
            "<p>Create an AWS CodeDeploy application with the Amazon EC2/On-Premises compute platform.</p>",
            "<p>Create an AWS OpsWorks deployment with the install dependencies command.</p>",
            "<p>Launch an Amazon EC2 instance to perform the build.</p>",
            "<p>Create an AWS CodeBuild project with GitHub as the source repository.</p>"
          ],
          "explanation": "<p>AWS CodeBuild can be used for this project to perform the compilation of code, testing, and pushing the package to S3. AWS CodeBuild supports webhooks when the source repository is GitHub. This means that for a CodeBuild build project that has its source code stored in a GitHub repository, webhooks can be used to rebuild the source code every time a code change is pushed to the repository.</p><p>The buildspec.yml file is used to define the build commands and sequence to run during the build process. This file is placed in the source code.</p><p><strong>CORRECT: </strong>\"Create an AWS CodeBuild project with GitHub as the source repository\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Add a buildspec.yml file to the source code with build instructions\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure a GitHub webhook to trigger a build every time a code change is pushed to the repository\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CodeDeploy application with the Amazon EC2/On-Premises compute platform\" is incorrect.</p><p>CodeDeploy is used for deploying the code. In this case the code needs to be compiled and tested. CodeBuild is used for this purpose. There is no mention in the question about how the code will be deployed to servers.</p><p><strong>INCORRECT:</strong> \"Create an AWS OpsWorks deployment with the install dependencies command\" is incorrect.</p><p>OpsWorks is not needed in this scenario as this is a pure compile and build requirement which is a good use case for CodeBuild.</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance to perform the build\" is incorrect.</p><p>There is no need to launch an instance and CodeBuild can automatically do this on the platform of your choice.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921384,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.</p>\n\n<p>What can be done to improve the performance of the setup?</p>\n",
          "answers": [
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</p>",
            "<p>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong></p>\n\n<p>A golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can also add the web application as part of the golden AMI. You can think of it as an input base AMI for creating a standardized application-specific golden AMI.</p>\n\n<p>Once you create a golden AMI for a product (a product can be a standardized OS-AMI that you want to distribute to accounts in your organization or an application-specific AMI you want to let your business unit(s) deploy in their environment), you can validate whether the AMI meets your expectations, and choose to approve or reject the AMI.</p>\n\n<p>About the golden AMI pipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q66-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</strong> - The local cache warmup can unfortunately not be improved, as caching is dynamic and data may change over time. So creating an AMI with a copy of the local data cache just serves as a distractor.</p>\n\n<p><strong>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong> - You cannot migrate from ElastiCache to DynamoDB for the given use-case, as it's primarily a NoSQL database and not a caching solution (You could use DAX as a caching solution with DynamoDB). Besides, the existing database is RDS MySQL which is a relational database, so DynamoDB does not really fit into this mix.</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</strong> - You cannot use Lambda to configure the instance local cache at boot time as caching is dynamic and data may change over time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n"
        }
      },
      {
        "id": 82921430,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at an auditing firm has deployed its flagship application on Elastic Beanstalk that processes invoices uploaded by customers in CSV form. The invoices can be quite big, with up to 10MB and 1,000,000 records total. Processing is CPU intensive which results in slowing down the application. Customers are sent an email when the processing is done, through the use of a cron job. The auditing firm has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>What do you recommend for the application to ensure a good performance and address scalability requirements?</p>\n",
          "answers": [
            "<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>",
            "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>",
            "<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>",
            "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>AWS Elastic Beanstalk enables you to manage all of the resources that run your application as environments. An environment is a collection of AWS resources running an application version. When you launch an Elastic Beanstalk environment, you need to choose an environment tier. An application that serves HTTP requests runs in a web server environment tier. A backend environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier.</p>\n\n<p>Elastic Beanstalk Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p>When you create a web server environment, Beanstalk provisions the resources required to run your application. AWS resources created for this type of environment include one elastic load balancer, an Auto Scaling group, and one or more Amazon Elastic Compute Cloud (Amazon EC2) instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p>AWS resources created for a worker environment tier include an ASG, one or more Amazon EC2 instances, and an IAM role. For the worker environment tier, Beanstalk also creates and provisions an SQS queue if you don\u2019t already have one. When you launch a worker environment, Beanstalk installs the necessary support files for your programming language of choice and a daemon on each EC2 instance in the ASG. The daemon reads messages from an SQS queue. The daemon sends data from each message that it reads to the web application running in the worker environment for processing.</p>\n\n<p>For the given use-case, the worker tier is used to asynchronously process the invoices from an SQS queue. SQS size limit is 256KB and therefore the files must be uploaded to S3 and a reference to them should be sent to SQS by the web tier. Finally, the <code>cron.yml</code> file must be defined on the worker tier. Using this strategy we have decoupled our processing tier from our web tier, and CPU usage will go down as a result. The worker tier will also be able to easily scale in case many invoices are uploaded.</p>\n\n<p>Elastic Beanstalk Worker environment:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/aeb-architecture_worker.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong> - As mentioned in the explanation above, the worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The <code>cron.yml</code> file must be defined on the worker tier, it is not supported by the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n"
        }
      }
    ],
    "answers": {
      "75949058": [
        "c"
      ],
      "75949144": [
        "b"
      ],
      "75949162": [
        "a",
        "b",
        "f"
      ],
      "82921384": [
        "a"
      ],
      "82921410": [
        "a"
      ],
      "82921430": [
        "a"
      ],
      "134588399": [
        "b"
      ],
      "134588507": [
        "b"
      ],
      "138248141": [
        "d"
      ],
      "138248181": [
        "d"
      ]
    }
  },
  {
    "id": "1770375804803",
    "date": "2026-02-06T11:03:24.803Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 20,
    "incorrect": 0,
    "unanswered": 0,
    "total": 20,
    "percent": 100,
    "duration": 4973041,
    "questions": [
      {
        "id": 115961509,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 5",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps Engineer is responsible for a containerized application deployed on Amazon ECS. The application retrieves JSON-formatted configuration files at runtime. Each file is approximately 3 KB in size, and the number of configuration files is expected to grow significantly with increased customer onboarding.</p><p>The following requirements must be met:</p><p>\u00b7 The configuration files must be updateable dynamically without requiring container restarts.</p><p>\u00b7 A change history must be maintained for auditing and rollback purposes.</p><p>\u00b7 The solution must scale with minimal operational effort and use managed AWS services wherever possible.</p><p>What is the BEST solution for managing and loading the configuration files?</p>",
          "answers": [
            "<p>Store the configuration files in a version-enabled Amazon S3 bucket. Use Amazon EventBridge to schedule a periodic trigger for an AWS Lambda function that loads the configuration into the ECS task as needed.</p>",
            "<p>Store the configuration in source code as a <code>.properties</code> file and deploy new container images every time changes are made. Trigger a container restart to pick up new configurations.</p>",
            "<p>Use AWS Secrets Manager to store the configuration files and configure the ECS task to pull the secrets at runtime.</p>",
            "<p>Use AWS Systems Manager Parameter Store (Advanced Tier) with a hierarchical path structure. Load the configuration at runtime from the container.</p>"
          ],
          "explanation": "<p>Amazon S3 supports storing and versioning files of nearly unlimited size and quantity. Using a version-enabled S3 bucket provides an efficient and scalable solution for storing dynamic configuration files while retaining change history.</p><p>By combining this with Amazon EventBridge and an AWS Lambda function, the engineer can automate periodic updates without restarting containers. This approach uses fully managed services and requires minimal maintenance, making it ideal for a growing number of client-specific configurations.</p><p><strong>CORRECT:</strong> \"Store the configuration files in a version-enabled Amazon S3 bucket. Use Amazon EventBridge to schedule a periodic trigger for an AWS Lambda function that loads the configuration into the ECS task as needed\" is the correct answer.</p><p>This solution is scalable, automates configuration updates using managed services, and ensures history is retained through S3 versioning.</p><p><strong>INCORRECT:</strong> \"Store the configuration in source code as a <code>.properties</code> file and deploy new container images every time changes are made. Trigger a container restart to pick up new configurations\" is incorrect.</p><p>This method introduces high operational effort and does not support dynamic updates, violating key requirements.</p><p><strong>INCORRECT:</strong> \"Use AWS Secrets Manager to store the configuration files and configure the ECS task to pull the secrets at runtime\" is incorrect.</p><p>Although Secrets Manager supports up to 64 KB per secret, it is designed for managing sensitive credentials rather than large, growing sets of configuration files. It is not a scalable or cost-effective solution for this use case.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Parameter Store (Advanced Tier) with a hierarchical path structure. Load the configuration at runtime from the container\" is incorrect.</p><p>Although Parameter Store supports hierarchical data and versioning in the advanced tier, it has a size limit of 8 KB per parameter and becomes harder to manage at scale compared to S3. This approach also incurs higher cost for advanced tier usage as configurations grow.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-advanced-parameters.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-advanced-parameters.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/limits.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/limits.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-scheduled-events.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-scheduled-events.html</a></p>"
        }
      },
      {
        "id": 75949068,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>To increase security, a company plans to use AWS Systems Manager Session Manager to managed Amazon EC2 instances rather than SSH. The connectivity to Session Manager should also use a private network connection.</p><p>Which configuration actions should be taken to implement this? (Select TWO.)</p>",
          "answers": [
            "<p>Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range.</p>",
            "<p>Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager.</p>",
            "<p>Create a VPC endpoint for Systems Manager in the Region where the instances are running.</p>",
            "<p>Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables.</p>",
            "<p>Establish private Session Manager connectivity using the instance IDs of EC2 instances.</p>"
          ],
          "explanation": "<p>Systems Manager Session Manager enables secure remote access to EC2 instances without the need to open ports for SSH or create bastion hosts. You can connect to instances through Session Manager privately by establishing a VPC endpoint in your VPC. This ensures that all connectivity takes place using private addresses.</p><p>The EC2 instances must have the Systems Manager agent running and they must have permissions to be able to communicate with the Systems Manager service. An instance profile can be easily attached to provide these permissions to instances.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-59-28-dac1a7fafdbe3507d5ca4b1fe07ed213.jpg\"><p><strong>CORRECT: </strong>\"Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Systems Manager in the Region where the instances are running\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range\" is incorrect.</p><p>Port 22 is used by the SSH protocol and is not required by Systems Manager Session Manager.</p><p><strong>INCORRECT:</strong> \"Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables\" is incorrect.</p><p>You cannot connect to EC2 instances via a NAT gateway. These gateways are used for outbound (internet) connectivity from the EC2 instances in private subnets.</p><p><strong>INCORRECT:</strong> \"Establish private Session Manager connectivity using the instance IDs of EC2 instances\" is incorrect.</p><p>You cannot connect privately to instances simply by connecting via the instance ID. You must establish a VPC endpoint and attach the necessary permissions to instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248121,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company uses a fleet of Linux and Windows servers for its enterprise applications. An automated daily check of each golden AMI owned is needed to monitor the latest Common Vulnerabilities and Exposures (CVE) using Amazon Inspector.</p><p>Which among the options below is the MOST suitable solution that should be implemented?</p>",
          "answers": [
            "<p>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI, install the Inspector agent, and add a custom tag for tracking. Configure the Step Functions to trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the Step Functions every day using an Amazon EventBridge rule.</p>",
            "<p>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will call the Inspector API action <code>StartAssessmentRun</code> after the EC2 instances have booted up, which will run the assessment against all instances with the custom tag you added. Trigger the function every day using an Amazon CloudWatch Alarms.</p>",
            "<p>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the function every day using an Amazon EventBridge rule.</p>",
            "<p>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI and install the Inspector agent. Configure the Step Functions to trigger the Inspector assessment for all instances right after the EC2 instances have booted up. Configure the Step Functions to run daily using the Event Bus in Amazon EventBridge.</p>"
          ],
          "explanation": "<p><strong>Amazon Inspector</strong> tests the network accessibility of your Amazon EC2 instances and the security state of your applications that run on those instances. Amazon Inspector assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings that is organized by level of severity.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-11-07_05-00-58-e155c91caae56656552ec8b5ffed377e.png\"></p><p>With Amazon Inspector, you can automate security vulnerability assessments throughout your development and deployment pipelines or for static production systems. This allows you to make security testing a regular part of development and IT operations.</p><p>Amazon Inspector also offers predefined software called an <em>agent</em> that you can optionally install in the operating system of the EC2 instances that you want to assess. The agent monitors the behavior of the EC2 instances, including network, file system, and process activity. It also collects a wide set of behavior and configuration data (telemetry).</p><p>If you want to set up a recurring schedule for your assessment, you can configure your assessment template to run automatically by creating a Lambda function using the AWS Lambda console. Alternatively, you can select the <em>\"Set up recurring assessment runs once every &lt;number_of_days&gt;, starting now\"</em> checkbox and specify the recurrence pattern (number of days) using the up and down arrows.</p><p>Hence, the correct answer is: <strong>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI, install the Inspector agent, and add a custom tag for tracking. Configure the Step Functions to trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the Step Functions every day using an Amazon EventBridge rule.</strong></p><p>The option that says: <strong>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will call the Inspector API action </strong><code><strong>StartAssessmentRun</strong></code><strong> after the EC2 instances have booted up, which will run the assessment against all instances with the custom tag you added. Trigger the function every day using an Amazon CloudWatch Alarms</strong><em> </em>is incorrect because you can't trigger a Lambda function on a regular basis using CloudWatch Alarms. You have to use Amazon EventBridge instead. Moreover, you typically have to install the Amazon Inspector agent to the EC2 instance in order to run the security assessments.</p><p>The option that says: <strong>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the function every day using an Amazon EventBridge rule<em> </em></strong>is incorrect because, in order for this solution to work, you have to install the Amazon Inspector agent first to the EC2 instance before you can run the security assessments.</p><p>The option that says: <strong>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI and install the Inspector agent. Configure the Step Functions to trigger the Inspector assessment for all instances right after the EC2 instances have booted up. Configure the Step Functions to run daily using the Event Bus in Amazon EventBridge</strong> is incorrect because the Event bus is primarily used to accept events from AWS services, other AWS accounts, and PutEvents API calls. You should also add a custom tag to the EC2 instance in order to run the Amazon Inspector assessments.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/set-up-amazon-inspector/\">https://aws.amazon.com/premiumsupport/knowledge-center/set-up-amazon-inspector/</a></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_assessments.html#assessment_runs-schedule\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_assessments.html#assessment_runs-schedule</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/\">https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/</a></p><p><br></p><p><strong>Check out this Amazon Inspector Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-inspector/?src=udemy\">https://tutorialsdojo.com/amazon-inspector/</a></p>"
        }
      },
      {
        "id": 143860767,
        "correct_response": [
          "b",
          "d"
        ],
        "source": "Neal Set 6",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer is migrating an application running on Docker containers from an on-premises data center to AWS. The application must run with minimal management overhead. Encrypted communications between the data center and AWS must be implemented for secure connectivity with on-premises resources.</p><p>Which actions should the DevOps engineer take to meet these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Migrate the container-based workload to Amazon ECS with the EC2 launch type in a custom VPC.</p>",
            "<p>Migrate the container-based workload to Amazon ECS with the Fargate launch type in a custom VPC.</p>",
            "<p>Implement a VPC endpoint and update security groups to enable access to Amazon ECS.</p>",
            "<p>Implement an AWS Managed VPN to encrypt traffic between the on-premises data center and the VPC.</p>",
            "<p>Implement a Network Load Balancer with IP-based targets and configure an HTTPS listener.</p>"
          ],
          "explanation": "<p>The requirements are for low management overhead of the container-based application on AWS and encryption for traffic between AWS and the on-premises network. To minimize management overhead of the application the DevOps engineer should deploy the container-based application on Amazon ECS with the Fargate launch type. This is the serverless solution for Amazon ECS and will be the easiest to manage.</p><p>To encrypt communications between the on-premises data center and the VPC, the engineer should deploy an AWS Managed VPN which will used IPSec to encrypt the traffic between these networks.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-16-13-2d804006a971e4e66b25f15f06b9dd58.jpg\"><p><strong>CORRECT: </strong>\"Migrate the container-based workload to Amazon ECS with the Fargate launch type in a custom VPC\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Implement an AWS Managed VPN to encrypt traffic between the on-premises data center and the VPC\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the container-based workload to Amazon ECS with the EC2 launch type in a custom VPC\" is incorrect.</p><p>The EC2 launch type has a higher management overhead as you must maintain the EC2 instances that run the container agent.</p><p><strong>INCORRECT:</strong> \"Implement a VPC endpoint and update security groups to enable access to Amazon ECS\" is incorrect.</p><p>With the Fargate launch type there is no need to add a VPC endpoint to enable connectivity for the ECS cluster.</p><p><strong>INCORRECT:</strong> \"Implement a Network Load Balancer with IP-based targets and configure an HTTPS listener\" is incorrect.</p><p>You cannot create an HTTPS listener with an NLB and there is no need to use a load balancer to meet these requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 138248151,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A software development company is doing an all-in migration of their on-premises resources to AWS. The company has a hybrid architecture that comprises over a thousand on-premises VMware servers and a few EC2 instances in their VPC. The company is using a VMWare vCenter Server for data center management of their vSphere environments and virtual servers. A DevOps engineer is tasked to implement a solution that will collect various information from their on-premises and EC2 instances, such as operating system details, MAC address, IP address, and many others. The Operations team should also be able to analyze the collected data in a visual format.</p><p>Which of the following is the MOST appropriate solution that the engineer should implement with the LEAST amount of effort</p>",
          "answers": [
            "Develop a custom python script and install them on both VMware servers and EC2 instances to collect all of the required information. Push the data to a centralized S3 bucket. Use VMware vSphere to collect the data from your on-premises resources and push the results into a file gateway in order to store the data in Amazon S3. Use Amazon Athena on the S3 bucket to analyze the data.",
            "<p>Register all of the on-premises virtual machines as well as the EC2 instances to AWS Service Catalog where all the required information such as the operating system details and many others will be automatically populated. Export the consolidated data from AWS Service Catalog to an Amazon S3 bucket and then use Amazon QuickSight for analytics.</p>",
            "<p>Install the AWS Systems Manager Agent (SSM Agent) on all on-premises virtual machines and the EC2 instances. Utilize the AWS Systems Manager Inventory service to provide visibility into your Amazon EC2 and on-premises computing environment. Set up an AWS Systems Manager Resource Data Sync to an S3 bucket in order to analyze the data with Amazon QuickSight.</p>",
            "Using the AWS Application Discovery Service, deploy the Agentless Discovery Connector in an OVA file format to your VMware vCenter and then install the AWS Discovery Agents on the EC2 instances to collect the required data. Use the AWS Migration Hub Dashboard to analyze your hybrid infrastructure."
          ],
          "explanation": "<p><strong>AWS Application Discovery Service</strong> helps you plan your migration to the AWS cloud by collecting usage and configuration data about your on-premises servers. Application Discovery Service is integrated with AWS Migration Hub, which simplifies your migration tracking. After performing discovery, you can view the discovered servers, group them into applications, and then track the migration status of each application from the Migration Hub console. The discovered data can be exported for analysis in Microsoft Excel or AWS analysis tools such as Amazon Athena and Amazon QuickSight.</p><p>Using Application Discovery Service APIs, you can export the system performance and utilization data for your discovered servers. You can input this data into your cost model to compute the cost of running those servers in AWS. Additionally, you can export the network connections and process data to understand the network connections that exist between servers. This will help you determine the network dependencies between servers and group them into applications for migration planning.</p><p><img src=\"https://media.tutorialsdojo.com/aws-migration-hub-console.png\"></p><p>Application Discovery Service offers two ways of performing discovery and collecting data about your on-premises servers:</p><p><strong>- Agentless discovery</strong> can be performed by deploying the AWS Agentless Discovery Connector (OVA file) through your VMware vCenter. After the Discovery Connector is configured, it identifies virtual machines (VMs) and hosts associated with vCenter. The Discovery Connector collects the following static configuration data: Server hostnames, IP addresses, MAC addresses, and disk resource allocations. Additionally, it collects the utilization data for each VM and computes average and peak utilization for metrics such as CPU, RAM, and Disk I/O. You can export a summary of the system performance information for all the VMs associated with a given VM host and perform a cost analysis of running them in AWS.</p><p><strong>- Agent-based discovery</strong> can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers. The agent installer is available for both Windows and Linux operating systems. It collects static configuration data, detailed time-series system-performance information, inbound and outbound network connections, and processes that are running. You can export this data to perform a detailed cost analysis and to identify network connections between servers for grouping servers as applications.</p><p>The Agentless discovery uses the AWS Discovery Connector, which is a VMware appliance that can collect information only about VMware virtual machines (VMs). This mode doesn't require you to install a connector on each host. You install the Discovery Connector as a VM in your VMware vCenter Server environment using an Open Virtualization Archive (OVA) file. Because the Discovery Connector relies on VMware metadata to gather server information regardless of the operating system, it minimizes the time required for initial on-premises infrastructure assessment.</p><p>Hence, the correct answer is: <strong>Using the AWS Application Discovery Service, deploy the Agentless Discovery Connector in an OVA file format to your VMware vCenter and then install the AWS Discovery Agents on the EC2 instances to collect the required data. Use the AWS Migration Hub Dashboard to analyze your hybrid infrastructure.</strong></p><p>The option that says:<strong> Develop a custom python script and install them on both VMware servers and EC2 instances to collect all of the required information. Push the data to a centralized S3 bucket. Use VMware vSphere to collect the data from your on-premises resources and push the results into a file gateway in order to store the data in Amazon S3. Use Amazon Athena on the S3 bucket to analyze the data</strong> is incorrect. Although this solution may work, it takes a lot of effort to develop a custom python script as well as to manually install it to over a thousand VMWare servers on the company's on-premises data center.</p><p>The option that says: <strong>Register all of the on-premises virtual machines as well as the EC2 instances to AWS Service Catalog where all the required information such as the operating system details, and many others will be automatically populated. Export the consolidated data from AWS Service Catalog to an Amazon S3, bucket and then use Amazon QuickSight for analytics</strong> is incorrect because the AWS Service Catalog service doesn't have the capability to integrate with the on-premises VMWare servers. This service only allows organizations to create and manage catalogs of IT services that are approved for use on AWS.</p><p>The option that says: <strong>Install the AWS Systems Manager Agent (SSM Agent) on all on-premises virtual machines and the EC2 instances. Utilize the AWS Systems Manager Inventory service to provide visibility into your Amazon EC2 and on-premises computing environment. Set up an AWS Systems Manager Resource Data Sync to an S3 bucket in order to analyze the data with Amazon QuickSight </strong>is incorrect. Although the solution of using the AWS Systems Manager is valid, this is definitely not the one that can be implemented with the least amount of effort. You can use the SSM Agent to fetch all of the required information about your servers, the task of installing it to each and every on-premises VMWare server is a herculean task that entails a lot of execution time. The use of AWS Systems Manager Resource Data Sync for analyzing data is irrelevant too. Moreover, the scenario mentioned that the company is doing an all-in migration of their on-premises resources to AWS, which means that installing the SSM agent is not appropriate. A better solution would be to use the Agentless Discovery Connector of the AWS Application Discovery Service to your on-premises VMware vCenter, which can easily fetch the required information from hundreds of VMware servers.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html\">https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html</a></p><p><a href=\"https://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-connector.html\">https://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-connector.html</a></p><p><a href=\"https://docs.aws.amazon.com/application-discovery/latest/userguide/dashboard.html\">https://docs.aws.amazon.com/application-discovery/latest/userguide/dashboard.html</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 82921350,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>As part of the CICD pipeline, a DevOps Engineer is performing a functional test using a CloudFormation template that will later get deployed to production. That CloudFormation template creates an S3 bucket and a Lambda function which transforms images uploaded into S3 into thumbnails. To test the Lambda function, a few images are automatically uploaded and the thumbnail output is expected from the Lambda function on the S3 bucket. As part of the clean-up of these functional tests, the CloudFormation stack is deleted, but right now the delete fails.</p>\n\n<p>What's the reason and how could this issue be fixed?</p>\n",
          "answers": [
            "<p>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Create an additional Custom Resource backed by a Lambda function that performs a clean-up of the bucket</p>",
            "<p>The Lambda function is still using the S3 bucket and CloudFormation cannot, therefore, delete the S3 bucket. Place a <code>WaitCondition</code> on the Lambda function to fix the issue</p>",
            "<p>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Add the property <code>Delete: Force</code> to your CloudFormation template so that the S3 bucket is emptied before being deleted</p>",
            "<p>A StackPolicy prevents the CloudFormation template to be deleted. Clear the Stack Policy and try again</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Create an additional Custom Resource backed by a Lambda function that performs a clean-up of the bucket</strong></p>\n\n<p>In a CloudFormation template, you can use the AWS::CloudFormation::CustomResource or Custom::String resource type to specify custom resources. Custom resources provide a way for you to write custom provisioning logic in CloudFormation template and have CloudFormation run it during a stack operation, such as when you create, update or delete a stack.</p>\n\n<p>Some resources must be empty before they can be deleted. For example, you must delete all objects in an Amazon S3 bucket or remove all instances in an Amazon EC2 security group before you can delete the bucket or security group.</p>\n\n<p>For this use-case, the issue is that the S3 bucket is not empty before being deleted, therefore you must implement a Custom Resource backed by Lambda which will clean the bucket for you.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q20-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html\">https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Lambda function is still using the S3 bucket and CloudFormation cannot, therefore, delete the S3 bucket. Place a <code>WaitCondition</code> on the Lambda function to fix the issue</strong> - CloudFormation can delete resources while they're being used, and a <code>WaitCondition</code> can be attached to EC2 instances and Auto Scaling Groups and NOT to Lambda function. AWS further recommends that for Amazon EC2 and Auto Scaling resources, you use a CreationPolicy attribute instead of wait conditions. Add a CreationPolicy attribute to those resources, and use the cfn-signal helper script to signal when an instance creation process has completed successfully.</p>\n\n<p><strong>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Add the property <code>Delete: Force</code> to your CloudFormation template so that the S3 bucket is emptied before being deleted</strong> - This option has been added as a distractor. To clean it, you cannot use a <code>Delete: Force</code> as this is not a feature of CloudFormation.</p>\n\n<p><strong>A StackPolicy prevents the CloudFormation template to be deleted. Clear the Stack Policy and try again</strong> -  A stack policy is a JSON document that defines the update actions that can be performed on designated resources. Stack Policies are only used during CloudFormation stack updates.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html\">https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html</a></p>\n\n<p><a href=\"https://stackoverflow.com/questions/40383470/can-i-force-cloudformation-to-delete-non-empty-s3-bucket\">https://stackoverflow.com/questions/40383470/can-i-force-cloudformation-to-delete-non-empty-s3-bucket</a></p>\n"
        }
      },
      {
        "id": 82921460,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A gaming company would like to be able to receive near real-time notifications when the API call <code>DeleteTable</code> is invoked in DynamoDB.</p>\n\n<p>As a DevOps Engineer at the company, how would you implement this at a minimal cost?</p>\n",
          "answers": [
            "<p>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</p>",
            "<p>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</p>",
            "<p>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</p>",
            "<p>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. Rules in CloudWatch Events work only in the Region in which they are created. If you configure CloudTrail to track API calls in multiple Regions, and you want a rule-based on CloudTrail to trigger in each of those Regions, you must create a separate rule in each Region that you want to track.</p>\n\n<p>For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q40-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</strong> - A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. DynamoDB Streams do not capture <code>DeleteTable</code> API calls, they only capture item-level events.</p>\n\n<p><strong>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</strong> - Sending CloudTrail Logs to CloudWatch Logs and creating a filter on those will work but will be expensive, as we're streaming all the logs from CloudTrail just to extract a single event.</p>\n\n<p><strong>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</strong> - CloudTrail trails do not have event filters and cannot be directly sent to a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p>\n"
        }
      },
      {
        "id": 75949172,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A service provider has created business relationships with several companies. The service provider plans to deploy an application to multiple AWS accounts managed by these partner companies using AWS CloudFormation. Each partner company has granted the permissions to create IAM roles with permissions for the deployment in their respective accounts. The organization must minimize operational overhead and stack management.</p><p>Which actions should be taken to deploy the application across these accounts?</p>",
          "answers": [
            "<p>Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template.</p>",
            "<p>Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application.</p>"
          ],
          "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf</p><p>In this case the best solution is to use self-managed permissions as the partner companies have only granted the ability to create IAM roles. They would certainly not want their entire account to be controlled by the service provider through AWS Organizations just so they can deploy a single application.</p><p><strong>CORRECT: </strong>\"Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>You cannot have a role in one account that has permissions in another account. The roles must be created in each AWS account and self-managed permissions must be used with AWS CloudFormation stack sets.</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>The partner companies would not want the service provider to control their accounts with AWS Organizations just for a single application. They have granted the ability to create roles which is enough for the service provider to deploy the application using stack sets with self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template\" is incorrect.</p><p>The partner companies have granted the ability to create roles, not user accounts. Also, this would be inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 82921334,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.</p>\n\n<p>How can you do this most securely?</p>\n",
          "answers": [
            "<p>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</p>",
            "<p>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>",
            "<p>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</p>",
            "<p>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</strong></p>\n\n<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.</p>\n\n<p>You can use Secrets Manager to natively rotate credentials for Amazon Relational Database Service (RDS), Amazon DocumentDB, and Amazon Redshift. You can extend Secrets Manager to rotate other secrets, such as credentials for Oracle databases hosted on EC2 or OAuth refresh tokens, by modifying sample AWS Lambda functions available in the Secrets Manager documentation.</p>\n\n<p>To access PostgreSQL, you can use database credentials and they're best stored in Secrets Manager from a security best-practices perspective. Access to Secrets Manager itself is regulated using an IAM role with the requisite policy. You must write this IAM policy permitting your application on EC2 instances to access specific secrets. Then, in the application source code, you can replace secrets in plain text with code to retrieve these secrets programmatically using the Secrets Manager APIs. To access the DynamoDB table, you should also add the appropriate policy to this IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - As mentioned in the explanation above, Secrets Manager does NOT support DynamoDB, so this option is incorrect.</p>\n\n<p><strong>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</strong> - It's not recommended to store RDS credentials in a DynamoDB, as it can be accessed by everyone who has access to the underlying table. This constitutes a serious security threat.</p>\n\n<p><strong>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - Storing IAM user credentials in Secrets Manager is a distractor as IAM user credentials are not required to build a solution for this use-case. You can just use an IAM instance role for EC2 to access Secrets Manager.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/faqs/\">https://aws.amazon.com/secrets-manager/faqs/</a></p>\n"
        }
      },
      {
        "id": 138248165,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A software development company has a microservices architecture that consists of several AWS Lambda functions with a DynamoDB table as its data store. The current workflow of the development team is to manually deploy the new version of the Lambda function right after the QA team completed the testing. There is a new requirement to improve the workflow by automating the tests as well as the code deployments. Whenever there is a new release, the application traffic to the new versions of each microservice should be incrementally shifted over time after deployment. This will provide them the option to verify the changes to a subset of users in production and easily rollback the changes if needed.&nbsp; </p><p>Which of the following solutions will improve the velocity of the company's development workflow?</p>",
          "answers": [
            "<p>Set up a new pipeline in AWS CodePipeline and configure a post-commit hook to start the pipeline after all the automated tests have passed. Configure AWS CodeDeploy to use an <code>All-at-once</code> deployment configuration for deployments.</p>",
            "Set up an AWS CodeBuild configuration which automatically starts whenever a new code is pushed. Configure CloudFormation to trigger a pipeline in AWS CodePipeline that deploys the new Lambda version. Specify the percentage of traffic that will initially be routed to your updated Lambda function as well as the interval to deploy the code over time in the CloudFormation template.",
            "<p>Set up a new pipeline in AWS CodePipeline and configure a new source code step that will automatically trigger whenever a new code is pushed to a GitHub repository. Use AWS CodeBuild for the build step to run the tests automatically then set up an AWS CodeDeploy configuration to deploy the updated Lambda function. Select the predefined <code>CodeDeployDefault.LambdaLinear10PercentEvery3Minutes</code> configuration option for deployment.</p>",
            "Develop a custom shell script that utilizes a post-commit hook to upload the latest version of the Lambda function in an S3 bucket. Set up the S3 event trigger which will invoke a Lambda function that deploys the new version. Specify the percentage of traffic that will initially be routed to your updated Lambda as well as the interval to deploy the code over time."
          ],
          "explanation": "<p><strong>AWS CodeBuild</strong> is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don\u2019t need to provision, manage, and scale your own build servers. CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p><p><strong>AWS CodePipeline</strong> is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.</p><p>When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application.</p><p>There are three ways traffic can shift during a deployment:</p><p><strong>Canary</strong>: Traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p><p><strong>Linear</strong>: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment.</p><p><strong>All-at-once</strong>: All traffic is shifted from the original Lambda function to the updated Lambda function version all at once.</p><p><img src=\"https://media.tutorialsdojo.com/public/cicd-taskcat-pipeline.9f41c290e6636774f3c92a1dad55350e14d5cf52.png\">Hence, the correct answer is <strong>Set up a new pipeline in AWS CodePipeline and configure a new source code step that will automatically trigger whenever a new code is pushed to a GitHub repository. Use AWS CodeBuild for the build step to run the tests automatically then set up an AWS CodeDeploy configuration to deploy the updated Lambda function. Select the predefined </strong><code><strong>CodeDeployDefault.LambdaLinear10PercentEvery3Minutes</strong></code><strong> configuration option for deployment.</strong></p><p>The option that says: <strong>Set up a new pipeline in AWS CodePipeline and configure a post-commit hook to start the pipeline after all the automated tests have passed. Configure AWS CodeDeploy to use an </strong><code><strong>All-at-once</strong></code><strong> deployment configuration for deployments</strong> is incorrect because the All-at-once configuration will only cause all traffic to be shifted from the original Lambda function to the updated Lambda function version all at once, just as what its name implies.</p><p>The option that says: <strong>Set up an AWS CodeBuild configuration which automatically starts whenever a new code is pushed. Configure CloudFormation to trigger a pipeline in AWS CodePipeline that deploys the new Lambda version. Specify the percentage of traffic that will initially be routed to your updated Lambda function as well as the interval to deploy the code over time in the CloudFormation template<em> </em></strong>is incorrect because you can just use CodeDeploy for deployments to streamline the workflow instead of using a combination of CodePipeline and CloudFormation.</p><p>The option that says:<strong><em> </em>Develop a custom shell script that utilizes a post-commit hook to upload the latest version of the Lambda function in an S3 bucket. Set up the S3 event trigger which will invoke a Lambda function that deploys the new version. Specify the percentage of traffic that will initially be routed to your updated Lambda as well as the interval to deploy the code over time</strong> is incorrect because developing a custom shell script as well as using S3 event triggers take a lot of time and administrative effort to implement. You should use a combination of GitHub, CodeBuild, CodeDeploy, and CodePipeline instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps-lambda.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 138248241,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A commercial bank has a hybrid cloud architecture in AWS where its online banking platform is hosted. The CTO instructed its Lead DevOps Engineer to implement a suitable deployment set up that minimizes the impact on their production environment. The CI/CD process should be configured as follows: </p><p>- A new fleet of Amazon EC2 instances should be automatically launched first before the actual production deployment. The additional instances will serve traffic during the deployment. </p><p>- All available EC2 instances across various Availability Zones must be load-balanced and must automatically heal if it becomes impaired due to an underlying hardware failure in Amazon EC2. </p><p>- At least half of the incoming traffic must be rerouted to the new application version that is hosted to the new instances. </p><p>- The deployment should be considered successful if traffic is rerouted to at least half of the available EC2 instances. </p><p>- All temporary files must be deleted before routing traffic to the new fleet of instances. Ensure that any other files that were automatically generated during the deployment process are removed. </p><p>- To reduce costs, the EC2 instances that host the old version in the deployment group must be terminated immediately. </p><p>What should the Engineer do to satisfy these requirements?</p>",
          "answers": [
            "<p>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the <code>CodeDeployDefault.OneAtAtime</code> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the <code>AllowTraffic</code> hook within the <code>appspec.yml</code> configuration file to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the <code>BeforeAllowTraffic</code> Traffic hook within <code>appspec.yml</code> to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the <code>CodeDeployDefault.HalfAtAtime</code> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the &lt;code&gt;CodeDeployDefault AllatOnce&lt;/code&gt; as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>"
          ],
          "explanation": "<p>The content in the <code>'hooks'</code> section of the AppSpec file varies, depending on the compute platform for your deployment. The <code>'hooks'</code> section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The <code>'hooks'</code> section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line. Here are descriptions of the hooks available for use in your AppSpec file.</p><p><code><strong>ApplicationStop</strong></code> \u2013 This deployment lifecycle event occurs even before the application revision is downloaded. You can specify scripts for this event to gracefully stop the application or remove currently installed packages in preparation for a deployment. The AppSpec file and scripts used for this deployment lifecycle event are from the previous successfully deployed application revision.</p><p><code><strong>DownloadBundle</strong></code> \u2013 During this deployment lifecycle event, the CodeDeploy agent copies the application revision files to a temporary location:</p><p><code>/opt/codedeploy-agent/deployment-root/<em>deployment-group-id</em>/<em>deployment-id</em>/deployment-archive</code> folder on Amazon Linux, Ubuntu Server, and RHEL Amazon EC2 instances.</p><p><code>C:\\ProgramData\\Amazon\\CodeDeploy\\<em>deployment-group-id</em>\\<em>deployment-id</em>\\deployment-archive</code> folder on Windows Server Amazon EC2 instances.</p><p>This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>BeforeInstall</strong></code> \u2013 You can use this deployment lifecycle event for preinstall tasks, such as decrypting files and creating a backup of the current version.</p><p><code><strong>Install</strong></code> \u2013 During this deployment lifecycle event, the CodeDeploy agent copies the revision files from the temporary location to the final destination folder. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterInstall</strong></code> \u2013 You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions.</p><p><code><strong>ApplicationStart</strong></code> \u2013 You typically use this deployment lifecycle event to restart services that were stopped during <code>ApplicationStop</code>.</p><p><code><strong>ValidateService</strong></code> \u2013 This is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.</p><p><code><strong>BeforeBlockTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances before they are deregistered from a load balancer.</p><p><code><strong>BlockTraffic</strong></code> \u2013 During this deployment lifecycle event, internet traffic is blocked from accessing instances that are currently serving traffic. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterBlockTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><code><strong>BeforeAllowTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances before they are registered with a load balancer.</p><p><code><strong>AllowTraffic</strong></code> \u2013 During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterAllowTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances after they are registered with a load balancer.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2019-12-16_03-04-56-691d69e91f667f1c9bb8b0554c2da933.png\"></p><p>Hence, the correct answer is: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BeforeAllowTraffic</strong></code><strong> Traffic hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files.</strong></p><p>The option that says: <strong>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the </strong><code><strong>CodeDeployDefault.OneAtAtime</strong></code><strong> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the </strong><code><strong>AllowTraffic</strong></code><strong> hook within the </strong><code><strong>appspec.yml</strong></code><strong> configuration file to purge the temporary files</strong> is incorrect because you should use blue/green deployment instead of in-place. In addition, the <code><strong><em>AllowTraffic</em></strong></code> event just allows the incoming traffic to the instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p>The option that says: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.HalfAtAtime</strong></code><strong> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appspec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because the <code>BlockTraffic</code> event is reserved for the CodeDeploy agent and cannot be used to run custom scripts such as deleting the temporary files.</p><p>The option that says: <strong>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.AllatOnce</strong></code><strong> as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because you should use a blue/green deployment instead of in-place. It is also incorrect to use the <code>CodeDeployDefault AllatOnce</code> deployment configuration as this attempts to deploy the application revision to as many instances as possible at once.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 99528229,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.</p><p>Which actions should the engineer take?</p>",
          "answers": [
            "<p>Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL.</p>",
            "<p>Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield.</p>",
            "<p>Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL.</p>",
            "<p>Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty.</p>"
          ],
          "explanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website.</p><p>AWS Web Application Firewall (WAF) can be used to protect the static website from web exploits. To do this, the engineer should create an AWS WAF web ACL.</p><p><strong>CORRECT: </strong>\"Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield\" is incorrect.</p><p>Both EC2 and ElastiCache use EC2 instances so they are not serverless services. AWS Shield protects against DDoS attacks but AWS WAF should be used for web exploits.</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL\" is incorrect.</p><p>ECS with Fargate is serverless but you cannot use a Redis cluster to cache content served by a task running on Amazon ECS. ElastiCache is used for caching database content.</p><p><strong>INCORRECT:</strong> \"Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty\" is incorrect.</p><p>Lambda with a REST API as the frontend is definitely possible, however the static website on S3 would be easier to deploy and manage. AWS GuardDuty does not protect against web exploits.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>"
        }
      },
      {
        "id": 115961513,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>",
          "answers": [
            "<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>",
            "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>",
            "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>",
            "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"
          ],
          "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 134588475,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An organization runs its web application on EC2 instances within an Auto Scaling group. The EC2 instances is behind an Application Load Balancer and is deployed across multiple Availability Zones. The developers of the organization have introduced fresh features to the web application, but require to be tested before implementation to prevent any interruptions. The organization requires that the deployment strategy should:</p><ul><li><p>Deploy a duplicate fleet of instances with an equivalent capacity to the primary fleet.</p></li><li><p>Keep the original fleet unaltered while the secondary fleet is being launched.</p></li><li><p>Shift traffic to the secondary fleet once it is completely deployed.</p></li><li><p>Automatically terminate the original fleet one hour after the transition.</p></li></ul><p>Which of the following is the MOST suitable solution that the DevOps Engineer should implement?</p>",
          "answers": [
            "<p>Utilize AWS CodeDeploy and set up a deployment group that has a blue/green deployment configuration. Set the BlueInstanceTerminationOption <code>action</code> to TERMINATE and <code>terminationWaitTimeInMinutes</code> with a 1-hour waiting period.</p>",
            "<p>Deploy an AWS CloudFormation template that includes a retention policy of 1 hour for the ALB. Then, update the Amazon Route 53 record to reflect the updated ALB.</p>",
            "<p>Create two AWS Elastic Beanstalk environments to execute a blue/green deployment from the original environment to the new one. Configure an application version lifecycle policy to terminate the primary environment in 1 hour</p>",
            "<p>Configure AWS Elastic Beanstalk with an Immutable setting, then create a .ebextension file using the Resources key to establish a deletion policy of 1 hour for the ALB, then deploy the application.</p>"
          ],
          "explanation": "<p><strong>AWS CodeDeploy</strong> is a fully managed deployment service that automates software deployments to various compute services, such as Amazon Elastic Compute Cloud (EC2), Amazon Elastic Container Service (ECS), AWS Lambda, and your on-premises servers.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-codedeploy.png\"></p><p>A <strong>deployment group</strong> is the AWS CodeDeploy entity for grouping EC2 instances or AWS Lambda functions in a CodeDeploy deployment. For EC2 deployments, it is a set of instances associated with an application that you target for a deployment.</p><p><strong>BlueInstanceTerminationOption</strong> contains information about whether instances in the original environment are terminated when a blue/green deployment is successful.</p><p><strong>-action<br></strong></p><p>The action to take on instances in the original environment after a successful blue/green deployment.</p><p><code>TERMINATE</code>: Instances are terminated after a specified wait time.</p><p><code>KEEP_ALIVE</code>: Instances are left running after they are deregistered from the load balancer and removed from the deployment group.</p><p><strong>-terminationWaitTimeInMinutes</strong></p><p>The number of minutes to wait after a successful blue/green deployment before terminating instances from the original environment.</p><p><br></p><p>Hence, the correct answer is the option that says: <strong>Utilize AWS CodeDeploy and set up a deployment group that has a blue/green deployment configuration. Set the BlueInstanceTerminationOption </strong><code><strong>action</strong></code><strong> to TERMINATE and </strong><code><strong>terminationWaitTimeInMinutes</strong></code><strong> with a 1-hour waiting period.</strong></p><p>The option that says: <strong>Deploy an AWS CloudFormation template that includes a retention policy of 1 hour for the ALB. Then, update the Amazon Route 53 record to reflect the updated ALB </strong>is incorrect because you cannot set a retention policy in CloudFormation.</p><p>The option that says: <strong>Create two AWS Elastic Beanstalk environments to execute a blue/green deployment from the original environment to the new one. Configure an application version lifecycle policy to terminate the primary environment in 1 hour </strong>is incorrect because the application version lifecycle policy is not used for EC2 instances and only deletes old .config files. In addition, the minimum age limit is set in days, not hours.</p><p>The option that says: <strong>Configure AWS Elastic Beanstalk with an Immutable setting, then create a .ebextension file using the Resources key to establish a deletion policy of 1 hour for the ALB, then deploy the application </strong>is incorrect because deletion policy is primarily used to preserve, and in some cases, backup a resource when the stack is deleted. In addition, the deletion policy cannot be set to delete a resource after 1 hour.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html\">https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html</a></p><p><a href=\"https://aws.amazon.com/codedeploy/\">https://aws.amazon.com/codedeploy/</a></p><p><a href=\"https://aws.amazon.com/codedeploy/faqs/\">https://aws.amazon.com/codedeploy/faqs/</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 67357098,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has configured AWS Organizations to manage its multiple AWS accounts. The company uses Amazon Elastic File System (Amazon EFS) as a shared storage service, configured in AWS Account A of the company. To implement a serverless architecture, the company has decided to move its applications to AWS Lambda. The Lambda functions will be managed through another AWS account (Account B). All the Lambda functions will be deployed in a VPC. A DevOps team needs help to continue using Amazon EFS in Account A with the Lambda function in Account B.</p>\n\n<p>How will you reconfigure the existing EFS file system for use with AWS Lambda function? (Select two)</p>\n",
          "answers": [
            "<p>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</p>",
            "<p>Update the Lambda execution roles with permission to access the VPC and the EFS file system</p>",
            "<p>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</p>",
            "<p>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</p>",
            "<p>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region, or cross-AZ connectivity between EFS and Lambda</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</strong></p>\n\n<p><strong>Update the Lambda execution roles with permission to access the VPC and the EFS file system</strong></p>\n\n<p>You can configure a function to mount an Amazon Elastic File System (Amazon EFS) file system to a local directory. With Amazon EFS, your function code can access and modify shared resources safely and at high concurrency.</p>\n\n<p>Execution role and user permissions: If the file system doesn't have a user-configured AWS Identity and Access Management (IAM) policy, EFS uses a default policy that grants full access to any client that can connect to the file system using a file system mount target. If the file system has a user-configured IAM policy, your function's execution role must have the correct <code>elasticfilesystem</code> permissions.</p>\n\n<p>Configuring a file system and access point: To connect an EFS file system with a Lambda function, you use an EFS access point, an application-specific entry point into an EFS file system that includes the operating system user and group to use when accessing the file system, file system permissions, and can limit access to a specific path in the file system. This helps keep file system configuration decoupled from the application code.</p>\n\n<p>You can access the same EFS file system from multiple functions, using the same or different access points. For example, using different EFS access points, each Lambda function can access different paths in a file system, or use different file system permissions.</p>\n\n<p>Connecting to a file system: A function connects to a file system over the local network in a VPC. The subnets that your function connects to can be the same subnets that contain mount points for your file system, or subnets in the same Availability Zone that can route NFS traffic (port 2049) to the file system. To mount an EFS file system, your Lambda functions must be connected to an Amazon Virtual Private Cloud (Amazon VPC) that can reach the EFS mount targets.</p>\n\n<p>A Lambda function in one account can mount a file system in a different account. For this scenario, you configure VPC peering between the function VPC and the file system VPC.</p>\n\n<p>An example showcasing the use of EFS with AWS Lambda:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q8-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/\">https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</strong> - A VPC peering connection is needed since EFS and AWS Lambda are in different AWS accounts. SCPs alone are not sufficient to grant permissions to the accounts in your organization. You should note that no permissions are granted by an SCP. The Lambda execution role will need explicit permissions for access to EFS.</p>\n\n<p><strong>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region or cross-AZ connectivity between EFS and Lambda</strong> - This statement is incorrect. AWS does not support cross-region, or cross AZ connectivity between EFS and Lambda.</p>\n\n<p><strong>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</strong> - It is mentioned in the use case that EFS and AWS Lambda will remain in their respective accounts. Hence, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html</a></p>\n"
        }
      },
      {
        "id": 134588383,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A global cloud-based payment processing system is hosted in AWS which accepts credit card payments as well as cryptocurrencies such as Bitcoin. It is deployed in AWS which uses EC2, DynamoDB, S3, and CloudFront to process the payments. Since they are accepting credit card information from the users, they are required to be compliant with the Payment Card Industry Data Security Standard (PCI DSS). It was found that the credit card numbers are not properly encrypted on the recent 3rd-party audit and hence, their system failed the PCI DSS compliance test. You were hired by the company to solve this issue so they can release the product in the market as soon as possible. In addition, you also have to improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content.</p><p>Which of the following is the BEST option to protect and encrypt the sensitive credit card information of the users and improve the cache hit ratio of your CloudFront distribution?</p>",
          "answers": [
            "<p>Use a custom SSL in the CloudFront distribution. Configure your origin to add <code>User-Agent</code> and <code>Host</code> headers to your objects to increase your cache hit ratio.</p>",
            "<p>Modify the CloudFront distribution to use Signed URLs. Configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code> to increase your cache hit ratio.</p>",
            "<p>Set up an origin access identity (OAI) and add it to the CloudFront distribution. Configure your origin to add <code>User-Agent</code> and <code>Host</code> headers to your objects to increase your cache hit ratio.</p>",
            "<p>Secure end-to-end connections to the origin servers in your CloudFront distribution by using HTTPS and field-level encryption. Set up your origin to add a <code>Cache-Control max-age</code> directive to your objects. Apply the longest practical value for <code>max-age</code> to increase your cache hit ratio.</p>"
          ],
          "explanation": "<p>You can already configure CloudFront to help enforce secure end-to-end connections to origin servers by using HTTPS. Field-level encryption adds an additional layer of security along with HTTPS that lets you protect specific data throughout system processing so that only certain applications can see it. Field-level encryption allows you to securely upload user-submitted sensitive information to your web servers. The sensitive information provided by your clients is encrypted at the edge closer to the user and remains encrypted throughout your entire application stack, ensuring that only applications that need the data\u2014and have the credentials to decrypt it\u2014are able to do so.</p><p><img src=\"https://media.tutorialsdojo.com/public/fleoverview_2AUG2023.png\"></p><p>To use field-level encryption, you configure your CloudFront distribution to specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request.</p><p>You can improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content; that is, by improving the cache hit ratio for your distribution. To increase your cache hit ratio, you can configure your origin to add a <code>Cache-Control max-age</code> directive to your objects and specify the longest practical value for <code>max-age</code>. The shorter the cache duration, the more frequently CloudFront forwards another request to your origin to determine whether the object has changed and, if so, to get the latest version.</p><p>Hence, the correct answer is: <strong>Secure end-to-end connections to the origin servers in your CloudFront distribution by using HTTPS and field-level encryption. Set up your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects. Apply the longest practical value for </strong><code><strong>max-age</strong></code><strong> to increase your cache hit ratio.</strong></p><p>The option that says: <strong>Use a custom SSL in the CloudFront distribution. Configure your origin to add </strong><code><strong>User-Agent</strong></code><strong> and </strong><code><strong>Host</strong></code><strong> headers to your objects to increase your cache hit ratio<em> </em></strong>is incorrect. Although it provides secure end-to-end connections to origin servers, it is better to add field-level encryption to protect the credit card information.</p><p>The option that says: <strong>Modify the CloudFront distribution to use Signed URLs. Configure your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects, and specify the longest practical value for </strong><code><strong>max-age</strong></code><strong> to increase your cache hit ratio<em> </em></strong>is incorrect because a Signed URL provides a way to distribute private content but it doesn't encrypt the sensitive credit card information.</p><p>The option that says: <strong>Set up an origin access identity (OAI) and add it to the CloudFront distribution. Configure your origin to add </strong><code><strong>User-Agent</strong></code><strong> and </strong><code><strong>Host</strong></code><strong> headers to your objects to increase your cache hit ratio </strong>is incorrect because OAI is mainly used to restrict access to objects in S3 bucket, but does not provide encryption to specific fields.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-setting-up\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-setting-up</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>"
        }
      },
      {
        "id": 75949146,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
          "answers": [
            "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
            "<p>The new AZ has not been enabled for the ALB.</p>",
            "<p>The AMI has not been updated to include the new AZ.</p>",
            "<p>There are no subnets associated with the new AZ.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 82921428,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An e-commerce company would like to automate the patching of their hybrid fleet and distribute some patches through their internal patch repositories every week. As a DevOps Engineer at the company, you have been tasked to implement this most efficiently.</p>\n\n<p>Which of the following options represents the BEST solution to meet this requirement?</p>\n",
          "answers": [
            "<p>Using SSM, implement a Custom Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>",
            "<p>Using SSM Parameter Store, configure the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>",
            "<p>Manage your instances with AWS OpsWorks. Define a maintenance window and define custom chef cookbooks for the 'configure' lifecycle hook that will patch the instances from the internal patch repositories. Schedule the window with a weekly recurrence</p>",
            "<p>Using SSM, do a RunCommand to install the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Using SSM, implement a Custom Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong></p>\n\n<p>SSM Patch Manager automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p>Patch Manager provides predefined patch baselines for each of the operating systems supported by Patch Manager. You can use these baselines as they are currently configured (you can't customize them) or you can create your own custom patch baselines. Custom patch baselines allow you greater control over which patches are approved or rejected for your environment.</p>\n\n<p>When you use the default repositories configured on an instance for patching operations, Patch Manager scans for or installs security-related patches. This is the default behavior for Patch Manager.</p>\n\n<p>On Linux systems, however, you can also use Patch Manager to install patches that are not related to security, or that are in a different source repository than the default one configured on the instance. You can specify alternative patch source repositories when you create a custom patch baseline. In each custom patch baseline, you can specify patch source configurations for up to 20 versions of a supported Linux operating system. You can then set up a weekly maintenance window and include the Run Command RunPatchBaseline.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q45-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Using SSM Parameter Store, configure the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong> - SSM Parameter Store is used to store parameter values but cannot write configuration files on EC2 instances (the EC2 instances would have to fetch the value from the Parameter Store instead).</p>\n\n<p><strong>Manage your instances with AWS OpsWorks. Define a maintenance window and define custom chef cookbooks for the 'configure' lifecycle hook that will patch the instances from the internal patch repositories. Schedule the window with a weekly recurrence</strong> - Using chef cookbooks via OpsWorks may work for what we need, but the Patch Manager of SSM is a better way of achieving this.</p>\n\n<p><strong>Using SSM, do a RunCommand to install the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong> - Using SSM RunCommand may work for what we need, but the Patch Manager of SSM is a better way of achieving this.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-alt-source-repository.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-alt-source-repository.html</a></p>\n"
        }
      },
      {
        "id": 82921422,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multi-national retail company is operating a multi-account strategy using AWS Organizations. Each account produces logs to CloudWatch Logs and the company would like to aggregate these logs under a single centralized account for archiving purposes. It needs the solution to be secure and centralized. The target destination for the logs should have little to no provisioning on the storage side.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution to meet these requirements?</p>\n",
          "answers": [
            "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3</p>",
            "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift</p>",
            "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon ES</p>",
            "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Lambda function on that log subscription, and implement a script to send the data to Amazon ES</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3</strong></p>\n\n<p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are Base64 encoded and compressed with the gzip format.</p>\n\n<p>For cross-account log data sharing with subscriptions, you can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Kinesis or Kinesis Data Firehose stream to perform custom processing and analysis. Therefore we have to subscribe the log destination to a Kinesis Firehose delivery stream, which in turn has a destination of S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift</strong> - The issue with this option is that the target for Kinesis Firehose is set as Amazon Redshift which is not a serverless service and requires provisioning.</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon ES</strong> - The issue with this option is that the target for Kinesis Firehose is set as Amazon ES which is not a serverless service and requires provisioning.</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Lambda function on that log subscription, and implement a script to send the data to Amazon ES</strong> - If the log destination is a Lambda function, this could work, but it will be a problem as this Lambda function sends the data to Amazon ES, which is not a serverless service and requires provisioning.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p>\n"
        }
      },
      {
        "id": 134588375,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A startup aims to rearchitect its internal web application hosted on Amazon EC2 into serverless architecture. At present, the startup deploys changes to the application by provisioning a new Auto Scaling group of EC2 instances across multiple Availability Zones and is fronted with a new Application Load Balancer. It then shifts the traffic with the use of Amazon Route 53 weighted routing policy. The DevOps Engineer of the startup will need to design a deployment strategy for serverless architecture similar to the current process that retains the ability to test new features with a limited set of users before making the features accessible to the entire user base. The startup plans to use AWS Lambda and Amazon API Gateway for the serverless architecture.</p><p>Which of the following is the MOST suitable solution to meet the requirements?</p>",
          "answers": [
            "<p>Utilize AWS Elastic Beanstalk to deploy Lambda functions and API Gateway. When there are code changes, a new version of both Lambda functions and API should be deployed. Use Elastic Beanstalk's blue/green deployment strategy to shift traffic gradually.</p>",
            "<p>Deploy Lambda functions with versions and API Gateway using AWS CloudFormation. When there are code changes, update the CloudFormation stack with the new Lambda code then a canary release strategy should be used to update the API versions. Once testing is done, promote the new version.</p>",
            "<p>Use AWS CodeDeploy to deploy the Lambda functions and the API Gateway. When there are code changes, use CodeDeploy\u2019s All at once deployment strategy, then redirect all traffic immediately using Amazon Route 53 simple routing policy.</p>",
            "<p>Deploy Lambda functions and API Gateway via AWS CDK. When there are code changes, update the CloudFormation Stack and deploy the new version of the Lambda functions and APIs. Enable canary release strategy by utilizing Amazon Route 53 failover routing policy.</p>"
          ],
          "explanation": "<p>By introducing alias traffic shifting, implementing <strong>canary deployments</strong> of <strong>Lambda functions</strong> has become effortless. The weightings of additional version can be adjusted on an alias to route invocation traffic to new function versions based on the weight specified.</p><p>In<strong> API Gateway</strong>, a <strong>canary release deployment</strong> uses the deployment stage for the production release of the base version of an API, and attaches to the stage a canary release for the new versions, relative to the base version, of the API. The stage is associated with the initial deployment and the canary with subsequent deployments.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-canary-release-strategy.png\"></p><p>Hence, the correct answer is the option that says: <strong>Deploy Lambda functions with versions and API Gateway using AWS CloudFormation. When there are code changes, update the CloudFormation stack with the new Lambda code then a canary release strategy should be used to update the API versions. Once testing is done, promote the new version.</strong></p><p>The option that says: <strong>Deploy Lambda functions and API Gateway via AWS CDK. When there are code changes, update the CloudFormation Stack and deploy the new version of the Lambda functions and APIs. Enable canary release strategy by utilizing Amazon Route 53 failover routing policy </strong>is incorrect because failover routing policy is primarily used for active-passive failover that lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. In addition, Route 53 cannot set Lambda versions as target.</p><p>The option that says: <strong>Utilize AWS Elastic Beanstalk to deploy Lambda functions and API Gateway. When there are code changes, a new version of both Lambda functions and API should be deployed. Use Elastic Beanstalk's blue/green deployment strategy to shift traffic gradually </strong>is incorrect because Elastic Beanstalk cannot deploy Lambda functions and API Gateway.</p><p>The option that says: <strong>Use AWS CodeDeploy to deploy the Lambda functions and the API Gateway. When there are code changes, use CodeDeploy\u2019s All at once deployment strategy, then redirect all traffic immediately using Amazon Route 53 simple routing policy </strong>is incorrect because the All at once deployment strategy updates all instances simultaneously. All instances in your environment are out of service for a short period of time. This strategy doesn\u2019t allow for testing new features with a limited set of users before making the features accessible to the entire user base, which is a requirement in the question. Also, Amazon Route 53 simple routing policy is used when you have a single resource that performs a given function for your domain, it doesn\u2019t allow for gradual traffic shifting which is required in the context of the question.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html#api-gateway-canary-release-deployment-overview\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html#api-gateway-canary-release-deployment-overview</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/\">https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/performing-canary-deployments-for-service-integrations-with-amazon-api-gateway/\">https://aws.amazon.com/blogs/compute/performing-canary-deployments-for-service-integrations-with-amazon-api-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      }
    ],
    "answers": {
      "67357098": [
        "a",
        "b"
      ],
      "75949068": [
        "b",
        "c"
      ],
      "75949146": [
        "b"
      ],
      "75949172": [
        "d"
      ],
      "82921334": [
        "a"
      ],
      "82921350": [
        "a"
      ],
      "82921422": [
        "a"
      ],
      "82921428": [
        "a"
      ],
      "82921460": [
        "a"
      ],
      "99528229": [
        "a"
      ],
      "115961509": [
        "a"
      ],
      "115961513": [
        "b"
      ],
      "134588375": [
        "b"
      ],
      "134588383": [
        "d"
      ],
      "134588475": [
        "a"
      ],
      "138248121": [
        "a"
      ],
      "138248151": [
        "d"
      ],
      "138248165": [
        "c"
      ],
      "138248241": [
        "b"
      ],
      "143860767": [
        "b",
        "d"
      ]
    }
  },
  {
    "id": "1770367218110",
    "date": "2026-02-06T08:40:18.110Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 20,
    "incorrect": 0,
    "unanswered": 0,
    "total": 20,
    "percent": 100,
    "duration": 5581459,
    "questions": [
      {
        "id": 75949130,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 3",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company\u2019s shopping website is hosted in an Auto Scaling group (ASG) of Amazon EC2 instances. There\u2019s a sale coming up and the company anticipate huge traffic on the website. Currently, there\u2019s a dynamic target tracking policy which scales up the instances gradually. However, the EC2 instances are taking a long time to spin up and the load balancer is sending traffic to unhealthy instances.</p><p>How can the issue be resolved with minimal operational overhead and cost?</p>",
          "answers": [
            "<p>Add a lifecycle hook to the Auto Scaling group, and to scale up quickly, utilize a warm pool.</p>",
            "<p>Change the dynamic scaling policy to use a manual scaling policy and increase the pool size.</p>",
            "<p>Put the EC2 instances in standby state to debug the instance spin up time and add lifecycle hooks.</p>",
            "<p>Configure the desired capacity to a large number of EC2 instances before the event so the application can handle the additional load.</p>"
          ],
          "explanation": "<p>Add a lifecycle hook in EC2 instance and to scale up quickly, utilize a warm pool.</p><p>A lifecycle hook can be added on scale out/ scale in events and get signal back from EC2 instances. This is to make the load balancer aware when the instances are ready to serve traffic.</p><p>A warm pool gives the ability to decrease latency for the applications that have exceptionally long boot times, for example, because instances need to write massive amounts of data to disk. With warm pools, Auto Scaling groups don\u2019t need to be over provisioned to manage latency and improve application performance.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to the Auto Scaling group, and to scale up quickly, utilize a warm pool\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Change the dynamic scaling policy to use a manual scaling policy and increase the pool size\" is incorrect.</p><p>This will increase the effort and cost of the solution.</p><p><strong>INCORRECT:</strong> \"Put the EC2 instances in standby state to debug the instance spin up time and add lifecycle hooks\" is incorrect.</p><p>Standby mode is useful when there\u2019s a specific issue within the instance that needs to be debugged and is not a regular practice.</p><p><strong>INCORRECT:</strong> \"Configure the desired capacity to a large number of EC2 instances before the event so the application can handle the additional load\" is incorrect.</p><p>This will increase the overall cost and will result in underutilization of instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/\">https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 143860749,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is deploying an application in four AWS Regions across North America, Europe, and Asia. The application will be used by millions of users. The application must allow users to submit data through the application layer in each Region and have it saved in a low-latency database layer. The company also must ensure that the data can be read through the application layer in each Region.</p><p>Which solution will meet these requirements with the LOWEST latency of reads and writes?</p>",
          "answers": [
            "<p>Create a table in Amazon DynamoDB and enable global tables in each of the four Regions.</p>",
            "<p>Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions.</p>",
            "<p>Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions.</p>",
            "<p>Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions.</p>"
          ],
          "explanation": "<p>Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p><p>This is the only workable solution in the list that provides both reads and writes in each Region that are replicated across the other Regions. This is also the best solution as it provides low latency reads and writes.</p><p><strong>CORRECT: </strong>\"Create a table in Amazon DynamoDB and enable global tables in each of the four Regions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions\" is incorrect.</p><p>This solutions does not provide local writes in each Region as the Read Replicas cannot be written to. Therefore, this solution only offers low latency reads in each Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions\" is incorrect.</p><p>Replication groups in ElastiCache are used within a Region and not across Regions so this solution does not work.</p><p><strong>INCORRECT:</strong> \"Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions\" is incorrect.</p><p>This solution provides a database in each Region, there is no mechanism for replication. You can create replica instances in different Regions but that would only provide low latency reads (as with RDS), and not low latency writes.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>"
        }
      },
      {
        "id": 75949142,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.</p><p>How can the company meet these requirements with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns.</p>",
            "<p>Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket.</p>"
          ],
          "explanation": "<p>The simplest solution for this request is to enable server access logging on the bucket and then analyze the data that is logged using Amazon Athena. Server access logging provides detailed records for the requests that are made to a bucket. This includes the information requested by the company.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p><strong>CORRECT: </strong>\"Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket\" is incorrect $</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/\">https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>"
        }
      },
      {
        "id": 82921394,
        "correct_response": [
          "a",
          "d"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a multi-national financial services company manages hundreds of accounts through AWS Organizations. As part of the security compliance requirements, the team must enforce the use of a security-hardened AMI in each AWS account. When a new AMI is created, the team wants to make sure new EC2 instances cannot be instantiated from the old AMI. Additionally, the team also wants to track and audit compliance of AMI usage across all the accounts.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement. What do you recommend? (Select two)</p>\n",
          "answers": [
            "<p>Create an AWS Automation document to create that AMI in a master account and share the AMI with the other accounts. When a new AMI is created, un-share the previous AMI and share the new one</p>",
            "<p>Create an AWS Automation document to create that AMI in a master account and copy the AMI into the other accounts. When a new AMI is created, copy it as well</p>",
            "<p>Create an AWS Automation document to create that AMI and deploy it to all the accounts using AWS CloudFormation StackSets. Run the Automation in all the accounts to have the AMI created locally</p>",
            "<p>Create an AWS Config Custom Rule in all the accounts using CloudFormation StackSets. Report the rule's result using an AWS Config aggregation</p>",
            "<p>Create an AWS Lambda function in all the accounts using CloudFormation StackSets, which will check the AMI id of all the EC2 instances in the account. Give it an IAM role that allows it to publish messages to an SNS topic in the master account</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an AWS Automation document to create that AMI in a master account and share the AMI with the other accounts. When a new AMI is created, un-share the previous AMI and share the new one</strong></p>\n\n<p>The DevOps team needs to provide approved AMIs that include the latest operating system updates, hardening requirements, and required\nthird-party software agents thereby enabling a repeatable, scalable, and approved application stack factory that increases innovation velocity and reduces effort. This solution uses Amazon EC2 Systems Manager Automation to drive the workflow.</p>\n\n<p>AMI hardening process:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i2.jpg\"></p>\n\n<p>via - <a href=\"https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf\">https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf</a></p>\n\n<p>After you have an approved AMI, you can distribute the AMI across AWS Regions, and then share it with any other AWS accounts. To do this, you use an Amazon EC2 Systems Manager Automation document that uses an AWS Lambda function to copy the AMIs across a specified list of regions, and then another Lambda function to share this copied AMI with the other accounts. The resulting AMI IDs can be stored in the SSM Parameter Store or Amazon DynamoDB for later consumption.</p>\n\n<p>Copying and sharing across AWS Regions and accounts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i3.jpg\">\nvia - <a href=\"https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf\">https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf</a></p>\n\n<p><strong>Create an AWS Config Custom Rule in all the accounts using CloudFormation StackSets. Report the rule's result using an AWS Config aggregation</strong></p>\n\n<p>AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p>An AWS Config rule represents your desired configuration settings for specific AWS resources or an entire AWS account. AWS Config provides customizable, predefined rules to help you get started. If a resource violates a rule, AWS Config flags the resource and the rule as noncompliant, and AWS Config notifies you through Amazon SNS.</p>\n\n<p>For the given use-case, you need to create a Config custom rule to check that only the new AMI is being used and then report the rule's result using an AWS Config aggregation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i5.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n\n<p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p>\n\n<p>Multiple accounts and multiple regions.</p>\n\n<p>Single account and multiple regions.</p>\n\n<p>An organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/config/latest/developerguide/images/Aggregate_Data_Landing_Page_Diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n\n<p>So to summarize, the key is to enforce AMI usage. As such, you don't want the AMI to be created or copied locally onto the other accounts, you want it to be available only in a central account and \"shared\" with other accounts. This way, if you have a new AMI, you unshare the previous one and share the new one. Finally, to monitor the EC2 instances and their AMI ID over time, an AWS Config custom rule is perfect for that.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Automation document to create that AMI in a master account and copy the AMI into the other accounts. When a new AMI is created, copy it as well</strong> - You don't want the AMI to be created in a master account and then copied locally onto the other accounts, you want it to be available only in a central account and \"shared\" with other accounts.</p>\n\n<p><strong>Create an AWS Automation document to create that AMI and deploy it to all the accounts using AWS CloudFormation StackSets. Run the Automation in all the accounts to have the AMI created locally</strong> - You can't create the AMI in a master account using AWS Automation document and then deploy it to all the accounts using AWS CloudFormation StackSets, rather you want it to be available only in a central account and then \"share\" it with other accounts.</p>\n\n<p><strong>Create an AWS Lambda function in all the accounts using CloudFormation StackSets, which will check the AMI id of all the EC2 instances in the account. Give it an IAM role that allows it to publish messages to an SNS topic in the master account</strong> - You could use the Lambda function in all accounts to check the AMI id of all the EC2 instances in the account, but it would not allow you to track as well as audit the compliance of AMI usage across all the accounts.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf\">https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n"
        }
      },
      {
        "id": 138248161,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An insurance firm is using AWS CloudFormation for deploying its applications in AWS. The firm has a multi-tier web application that stores financial data in an Amazon RDS MySQL database in a Multi-AZ deployments configuration. The firm instructed its DevOps Engineer to upgrade the RDS instance to the latest major version of MySQL database. It is of utmost importance to ensure minimal downtime when doing the upgrade to avoid any business disruption.</p><p>Which of the following should the engineer implement to properly upgrade the database while minimizing downtime?</p>",
          "answers": [
            "<p>In the <code>AWS::RDS::DBInstance</code> resource type in the CloudFormation template, update the <code>EngineVersion</code> property to the latest MySQL database version. Create a second application stack and launch a new Read Replica with the same properties as the primary database instance that will be upgraded. Finally, perform an Update Stack operation in CloudFormation.</p>",
            "<p>In the <code>AWS::RDS::DBInstance</code> resource type in the CloudFormation template, update the <code>DBEngineVersion</code> property to the latest MySQL database version. Trigger an Update Stack operation in CloudFormation. Launch a new RDS Read Replica with the same properties as the primary database instance that will be upgraded. Finally, perform a second Update Stack operation.</p>",
            "<p>In the <code>AWS::RDS::DBInstance</code> resource type in the CloudFormation template, update the <code>AutoMinorVersionUpgrade</code> property to the latest MySQL database version. Launch a new RDS Read Replica with the same properties as the primary database instance that will be upgraded. Finally, trigger an Update Stack operation in CloudFormation.</p>",
            "<p>In the <code>AWS::RDS::DBInstance</code> resource type in the CloudFormation template, update the <code>AllowMajorVersionUpgrade</code> property to the latest MySQL database version. Afterward, directly trigger an Update Stack operation in CloudFormation.</p>"
          ],
          "explanation": "<p>If your MySQL DB instance is currently in use with a production application, you can follow a procedure to upgrade the database version for your DB instance that can reduce the amount of downtime for your application.</p><p>Periodically, Amazon RDS performs maintenance on Amazon RDS resources. Maintenance most often involves updates to the DB instance's underlying hardware, underlying operating system (OS), or database engine version. Updates to the operating system most often occur for security issues and should be done as soon as possible.</p><p>Some maintenance items require that Amazon RDS take your DB instance offline for a short time. Maintenance items that require a resource to be offline include the required operating system or database patching. Required patching is automatically scheduled only for patches that are related to security and instance reliability. Such patching occurs infrequently (typically once every few months) and seldom requires more than a fraction of your maintenance window.</p><p><img src=\"https://media.tutorialsdojo.com/public/offlinepatchavailabledetails.png\"></p><p>When you modify the database engine for your DB instance in a Multi-AZ deployment, Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.</p><p>Hence, the correct answer is: <strong>In the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource type in the CloudFormation template, update the </strong><code><strong>EngineVersion</strong></code><strong> property to the latest MySQL database version. Create a second application stack and launch a new Read Replica with the same properties as the primary database instance that will be upgraded. Finally, perform an Update Stack operation in CloudFormation</strong>.</p><p>The option that says: <strong>In the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource type in the CloudFormation template, update the </strong><code><strong>DBEngineVersion</strong></code><strong> property to the latest MySQL database version. Trigger an Update Stack operation in CloudFormation. Launch a new RDS Read Replica with the same properties as the primary database instance that will be upgraded. Finally, perform a second Update Stack operation</strong> is incorrect because this solution may possibly experience downtime since you trigger the Update Stack operation first before creating a Read Replica, which you could have used as a backup instance in the event of update failures. Remember that when you modify the database engine for your RDS Multi-AZ instance, the database engine for the entire Multi-AZ deployment is shut down during the upgrade. In addition, there is no such <em>DBEngineVersion</em> property.</p><p>The option that says: <strong>In the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource type in the CloudFormation template, update the </strong><code><strong>AutoMinorVersionUpgrade</strong></code><strong> property to the latest MySQL database version. Launch a new RDS Read Replica with the same properties as the primary database instance that will be upgraded. Finally, trigger an Update Stack operation in CloudFormation</strong> is incorrect because the AutoMinorVersionUpgrade property is simply a value that indicates whether minor engine upgrades are applied automatically to the DB instance during the maintenance window. By default, minor engine upgrades are applied automatically. You have to use the <em>EngineVersion</em> property instead.</p><p>The option that says: <strong>In the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource type in the CloudFormation template, update the </strong><code><strong>AllowMajorVersionUpgrade</strong></code><strong> property to the latest MySQL database version. Afterward, directly trigger an Update Stack operation in CloudFormation </strong>is incorrect because if the database upgrade fails, your entire system will be unavailable since you have no Read Replicas that you can use as a failover. Remember that when you modify the database engine for your DB instance in a Multi-AZ deployment configuration, Amazon RDS upgrades both the primary and secondary DB instances at the same time, which means that the RDS shuts down the whole database. The <code>AllowMajorVersionUpgrade</code> property is only a value that indicates whether major version upgrades are allowed.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html \">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime</a></p><p><br></p><p><strong>Check out these AWS CloudFormation and Amazon RDS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 138248211,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A CTO of a leading insurance company has recently decided to migrate its online customer portal to AWS. The customers will use the online portal to view the paid insurance premiums and manage accounts. For improved scalability, the application should be hosted in an Auto Scaling group of On-Demand Amazon EC2 instances with a custom Amazon Machine Image (AMI). The same architecture will also be used for the non-production environments (DEV, TEST, and STAGING). The DevOps Engineer is instructed by the CTO to design a deployment strategy that securely stores the credentials of each environment, expedites the startup time for the EC2 instances, and allows the same AMI to work in all environments.</p><p>How should the DevOps Engineer set up the deployment configuration to accomplish this task?</p>",
          "answers": [
            "<p>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Session Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials.</p>",
            "<p>Add a tag to each EC2 instance based on their environment. Use AWS Systems Manager Automation to preconfigure the AMI by installing all of the required applications and software dependencies. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Systems Manager Parameter Store to store the credentials as Secure String parameters.</p>",
            "<p>Add a tag to each EC2 instance based on their environment. Use a preconfigured AMI from AWS Marketplace. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials.</p>",
            "<p>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Patch Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use AWS AppConfig to store the environment-specific configuration and credentials securely.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Parameter Store</strong> provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Highly scalable, available, and durable, Parameter Store is backed by the AWS Cloud.</p><p>Systems Manager Automation simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p>Automation offers one-click automations for simplifying complex tasks such as creating golden Amazon Machines Images (AMIs), and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for a variety of reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img src=\"https://media.tutorialsdojo.com/public/custom_ami_1.gif\"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Add a tag to each EC2 instance based on their environment. Use AWS Systems Manager Automation to preconfigure the AMI by installing all of the required applications and software dependencies. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Systems Manager Parameter Store to store the credentials as Secure String parameters.</strong></p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Session Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials</strong> is incorrect. The Session Manager service is just a fully managed AWS Systems Manager capability that lets you manage your Amazon EC2 instances through an interactive one-click browser-based shell or through the AWS CLI. It is not capable to build a custom AMI, unlike Systems Manager Automation.</p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Use a preconfigured AMI from AWS Marketplace. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials </strong>is incorrect. The company is using a custom AMI and not a public AMI from AWS Marketplace. You have to preconfigure the AMI using the Systems Manager Automation instead.</p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Patch Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use AWS AppConfig to store the environment-specific configuration and credentials securely</strong> is incorrect. The AWS Patch Manager is typically used for patching and managing operating system patches, not for installing applications or preconfiguring the AMI. Additionally, AWS AppConfig is more for managing feature flags or dynamic configurations and is not the best fit for secure storage of environment-specific credentials.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/\">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248103,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A fast-growing company has multiple AWS accounts which are consolidated using AWS Organizations, and the company expects to add new accounts soon. The DevOps engineer was instructed to design a centralized logging solution to deliver all Amazon VPC Flow Logs and Amazon CloudWatch Logs across all sub-accounts to its dedicated Audit account for compliance purposes. The logs should also be properly indexed in order to perform search, retrieval, and analysis.</p><p>Which of the following is the MOST suitable solution that the engineer should implement to meet the above requirements?</p>",
          "answers": [
            "<p>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account.</p>",
            "<p>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</p>",
            "<p>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account.</p>",
            "<p>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account.</p>"
          ],
          "explanation": "<p>You can load streaming data into your Amazon OpenSearch Service domain from many different sources in AWS. Some sources, like Amazon Data Firehose and Amazon CloudWatch Logs, have built-in support for Amazon OpenSearch Service (successor to Amazon ElasticSearch). Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers. The Lambda functions respond to new data by processing it and streaming it to your domain.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-Amazon-OpenSearch-Service-02-04-2025.png\"></p><p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, Amazon Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. To begin subscribing to log events, create the receiving source, such as a Kinesis stream, where the events will be delivered. A subscription filter defines the filter pattern to use for filtering which log events get delivered to your AWS resource, as well as information about where to send matching log events to.</p><p>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Amazon Kinesis stream to perform custom processing and analysis. Custom processing is especially useful when you collaborate and analyze data across many accounts. For example, a company's information security group might want to analyze data for real-time intrusion detection or anomalous behaviors so it could conduct an audit of accounts in all divisions in the company by collecting their federated production logs for central processing.</p><p>A real-time stream of event data across those accounts can be assembled and delivered to the information security groups, who can use Kinesis to attach the data to their existing security analytic systems. Kinesis streams are currently the only resource supported as a destination for cross-account subscriptions.</p><p>Hence, the correct solution is: <strong>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</strong></p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account</strong> is incorrect. While Lambda functions can be used to process logs, relying solely on a Lambda function to handle a large volume of logs might only lead to performance bottlenecks and scalability issues. Lambda has concurrency limits, and processing logs from multiple sub-accounts may exceed these limits, especially as the company grows and more accounts are added.</p><p>The option that says: <strong>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account </strong>is incorrect because the CloudWatch subscription filter doesn't directly support SQS. You should use a Kinesis Data Stream, Kinesis Firehose, or Lambda function.</p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account</strong> is incorrect. Although this approach typically works, using a self-hosted OpenSearch cluster on an EC2 instance introduces unnecessary complexity and overhead regarding management, scaling, and availability. AWS-managed OpenSearch Service would be a more efficient and cost-effective option. Additionally, relying on EC2 for hosting OpenSearch adds management and operational burdens, whereas using AWS-managed services like Kinesis Data Streams or Firehose with OpenSearch is more reliable and easier to scale.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html</a></p><p><a href=\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 99528231,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A law firm is planning to migrate existing applications to AWS. These applications are hosted in an on-premises data center and are complex in nature. The applications could take many months to migrate. While the migration is underway, the application development team implemented a tactical solution using Amazon CloudFront with a custom origin pointing to the SSL endpoint URL for the legacy web application.</p><p>The ad-hoc solution worked for several weeks; however, all browser connections recently began showing an HTTP 502 Bad Gateway error with the header \"X-Cache: Error from CloudFront\". Network monitoring services show that the HTTPS port 443 on the legacy web application is open and responding to requests.</p><p>Which option could be the reason for the error and how can it be solved?</p>",
          "answers": [
            "<p>The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server via the AWS Certificate Manager (ACM) in the hosting region.</p>",
            "<p>The SSL certificate on the CloudFront distribution has expired. Reissue the SSL certificate on the CloudFront distribution via the AWS Certificate Manager (ACM) in the hosting region.</p>",
            "<p>The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server that is signed by a globally recognized certificate authority (CA). Install the full certificate chain onto the legacy web application server.</p>",
            "<p>The SSL certificate on the legacy web application server has expired. Install a new self-signed certificate along with the full certificate chain onto the legacy web application server.</p>"
          ],
          "explanation": "<img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-04-44-3acbb19734df50fc3c67eb1acc619f2b.jpg\"><p><strong>CORRECT: </strong>\"The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server that is signed by a globally recognized certificate authority (CA). Install the full certificate chain onto the legacy web application server\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server via the AWS Certificate Manager (ACM) in the hosting region\" is incorrect.</p><p>You can't export an Amazon Issued ACM public certificate for use on an EC2 instance or another custom web server because ACM manages the private key.</p><p><strong>INCORRECT:</strong> \"The SSL certificate on the CloudFront distribution has expired. Reissue the SSL certificate on the CloudFront distribution via the AWS Certificate Manager (ACM) in the hosting region\" is incorrect.</p><p>If you\u2019re using certificates that you get from a third-party certificate authority (CA), you must monitor certificate expiration dates and renew the certificates that you import into AWS Certificate Manager (ACM) or upload to the AWS Identity and Access Management certificate store before they expire.</p><p><strong>INCORRECT</strong>: \"The SSL certificate on the legacy web application server has expired. Install a new self-signed certificate along with the full certificate chain onto the legacy web application server\" is incorrect.</p><p>A self-signed certificate is a security certificate that is not signed by a certificate authority (CA). You can't use a self-signed certificate for HTTPS communication between CloudFront and your origin, so this option is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html#https-requirements-cert-expiration\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html#https-requirements-cert-expiration</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>"
        }
      },
      {
        "id": 75949080,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A custom application generates events and produces data that must be processed for each event. An event-driven solution is required to process the events and save the output to a serverless key/value store.</p><p>Which actions should a DevOps engineer take?</p>",
          "answers": [
            "<p>Configure the application to submit the event data to an Amazon S3 bucket. Create an Amazon EventBridge rule that reacts to state changes in S3 and triggers Amazon Athena to process the data and save the output to an Amazon DynamoDB table.</p>",
            "<p>Configure the application to submit the event data to an SQS queue. Configure a trigger for an AWS Lambda function and configure the function to process the data and save the output to an Amazon ElastiCache cluster.</p>",
            "<p>Configure the application to submit the event data to an Amazon Kinesis Data Firehose delivery stream with an Amazon S3 destination. Configure an event notification for an AWS Lambda function to process the data and save the output to an Amazon RDS database.</p>",
            "<p>Configure the application to submit the event data to an SNS topic. Subscribe an AWS Lambda function to the topic and configure the function to process the data and save the output to an Amazon DynamoDB table.</p>"
          ],
          "explanation": "<p>To create an event-driven architecture for this requirement the engineer can configure the application to submit the event data to an SNS topic. The Lambda function can be subscribed to the topic and will process the data and then save the results to Amazon DynamoDB which is a key/value database.</p><p><strong>CORRECT: </strong>\"Configure the application to submit the event data to an SNS topic. Subscribe an AWS Lambda function to the topic and configure the function to process the data and save the output to an Amazon DynamoDB table\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the application to submit the event data to an Amazon S3 bucket. Create an Amazon EventBridge rule that reacts to state changes in S3 and triggers Amazon Athena to process the data and save the output to an Amazon DynamoDB table\" is incorrect.</p><p>An event notification rule can be created on S3 rather than using EventBridge. However, you cannot use Athena to process data as it is an analytics service, not a compute service.</p><p><strong>INCORRECT:</strong> \"Configure the application to submit the event data to an Amazon Kinesis Data Firehose delivery stream with an Amazon S3 destination. Configure an event notification for an AWS Lambda function to process the data and save the output to an Amazon RDS database\" is incorrect.</p><p>Amazon RDS is not a serverless key/value store. It is a relational database that uses Amazon EC2 instances so it is not suitable for this requirement.</p><p><strong>INCORRECT:</strong> \"Configure the application to submit the event data to an SQS queue. Configure a trigger for an AWS Lambda function and configure the function to process the data and save the output to an Amazon ElastiCache cluster\" is incorrect.</p><p>Amazon ElastiCache is a key/value store, but it is not a serverless database; it uses Amazon EC2 instances, so it is not suitable for this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>"
        }
      },
      {
        "id": 82921438,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>Your company has adopted a git repository technology to store and have version control on the application code. Your company would like to make sure the production branch of the code is deployed to the production environment, but also would like to enable other versions of the code to be deployed to the development and staging environments for performing various kinds of user acceptance testing.</p>\n\n<p>As a DevOps Engineer, which solution would you implement for the given requirement?</p>\n",
          "answers": [
            "<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</p>",
            "<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</p>",
            "<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</p>",
            "<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Projects/CICD%20Pipeline/setup-cicd-pipeline2.5cefde1406fa6787d9d3c38ae6ba3a53e8df3be8.png\">\nvia - <a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p>Here you only need one git repository and create a production branch for deploys to production. The other key requirement of the given use-case is that two versions of the code need to be deployed to different environments. As such, you will need two CodePipelines. If you had one with a manual approval step at the end, then the code deployed to production would be coming from the master branch instead of the production branch. Here, we specifically need code in the production branch to be deployed to production, so, therefore, we need a second CodePipeline and to merge code from master to production through Pull Requests.</p>\n\n<p>Code Pipeline Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</strong> - As mentioned in the explanation above, a key requirement is that two versions of the code need to be deployed to different environments. If you use a manual approval step after the deployment to staging then the same version of the code from the master branch would also be deployed to the production environment. Instead, you need to maintain a production branch of the code that can be deployed to the production environment.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</strong> - It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to merging through a pull request has been added as a distractor.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</strong> - It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to the manual approval step has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/faqs/\">https://aws.amazon.com/codecommit/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n"
        }
      },
      {
        "id": 138248111,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is planning to deploy a new version of their legacy application in AWS which is deployed to an Auto Scaling group of EC2 instances with an Application Load Balancer in front. To avoid any disruption of their services, they need to implement canary testing first before all of the traffic is shifted to the new application version.</p><p>Which of the following solutions can meet this requirement?</p>",
          "answers": [
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</p>",
            "<p>Do a Canary deployment using CodeDeploy with a <code>CodeDeployDefault.LambdaCanary10Percent30Minutes</code> deployment configuration.</p>",
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers.</p>",
            "<p>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment.</p>"
          ],
          "explanation": "<p>The purpose of a canary deployment is to reduce the risk of deploying a new version that impacts the <a href=\"https://wa.aws.amazon.com/wat.concept.workload.en.html\" title=\"The set of components that together deliver business value.\">workload</a>. The method will incrementally deploy the new version, making it visible to new users in a slow fashion. As you gain confidence in the deployment, you will deploy it to replace the current version in its entirety.</p><p><img src=\"https://media.tutorialsdojo.com/public/Upgrades_Image1.jpeg\"></p><p>To properly implement the canary deployment, you should do the following steps:</p><p>- Use a router or load balancer that allows you to send a small percentage of users to the new version.</p><p>- Use a dimension on your KPIs to indicate which version is reporting the metrics.</p><p>- Use the metric to measure the success of the deployment; this indicates whether the deployment should continue or rollback.</p><p>- Increase the load on the new version until either all users are on the new version or you have fully rolled back.</p><p><br></p><p>Hence, the correct answer is: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</strong></p><p>The option that says: <strong>Do a Canary deployment using CodeDeploy with a </strong><code><strong>CodeDeployDefault.LambdaCanary10Percent30Minutes</strong></code><strong> deployment configuration</strong> is incorrect because this specific configuration type is only applicable for Lambda functions and for the applications hosted in an Auto Scaling group.</p><p>The option that says: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers</strong> is incorrect because you can't use CloudFront to adjust the weight of the incoming traffic to your application. You should use Route 53 instead.</p><p>The option that says: <strong>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment</strong> is incorrect because you can only integrate a Network Load Balancer to your Amazon API Gateway. Moreover, this service is only applicable for APIs, not full-fledged web applications.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html\">https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/\">https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/</a></p>"
        }
      },
      {
        "id": 99528221,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company manages several legacy applications that all generate different log formats. The logs need to be standardized so they can be queried and analyzed. A DevOps engineer needs a solution for standardizing the log formats before writing them to<br>an Amazon S3 bucket.</p><p>How can this requirement be met at the LOWEST cost?</p>",
          "answers": [
            "<p>Configure the application to send its logs to an Amazon OpenSearch cluster and use Lambda to normalize the logs and export to S3.</p>",
            "<p>Configure the application to send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight.</p>",
            "<p>Configure the application to send the log files to an Amazon S3 bucket and use Amazon Redshift Spectrum to normalize the logs in place.</p>",
            "<p>Configure the Amazon Kinesis Agent to upload the logs to Amazon Kinesis Data Firehose and use an AWS Lambda function to normalize the log files before they are loaded to Amazon S3.</p>"
          ],
          "explanation": "<p>Kinesis Data Firehose can invoke an AWS Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Kinesis Data Firehose data transformation when you create your delivery stream.</p><p>Each server can run the Kinesis agent and upload logs to the Kinesis Data Firehose delivery stream. Then the Lambda function will normalize the log files before they are loaded to the S3 bucket.</p><p><strong>CORRECT: </strong>\"Configure the Amazon Kinesis Agent to upload the logs to Amazon Kinesis Data Firehose and use an AWS Lambda function to normalize the log files before they are loaded to Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the application to send its logs to an Amazon OpenSearch cluster and use Lambda to normalize the logs and export to S3\" is incorrect.</p><p>The Amazon OpenSearch service is not a suitable solution for this scenario as it is a service that is used for searching and analyzing data sets. In this case the data simply needs to be transformed (normalized) before loading it to S3 so there is no need to involve OpenSearch.</p><p><strong>INCORRECT:</strong> \"Configure the application to send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight\" is incorrect.</p><p>QuickSight is used for analysis but in this scenario we simply need a solution for normalizing the log files before loading to S3.</p><p><strong>INCORRECT:</strong> \"Configure the application to send the log files to an Amazon S3 bucket and use Amazon Redshift Spectrum to normalize the logs in place\" is incorrect.</p><p>To use RedShift Spectrum you must have a RedShift cluster in place and these run-on Amazon EC2 instances. Therefore, this solution is unlikely to be the lowest cost option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html\">https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>"
        }
      },
      {
        "id": 75949104,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 3",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer is deploying a series of updates to a web application that runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The updates are being deployed using a blue/green strategy with immutable instances.</p><p>An issue has been occurring where users are being logged of the application during deployments. The DevOps engineer needs to ensure that users remain logged in when scaling events occur and application updates are deployed.</p><p>How can these requirements be met with the LOWEST latency?</p>",
          "answers": [
            "<p>Enable session affinity on the load balancer and store authenticated session information on the instance\u2019s attached block volumes.</p>",
            "<p>Configure the application to store authenticated session information in an Amazon ElastiCache cluster.</p>",
            "<p>Enable sticky sessions on the target group and store authenticated session information on the instance\u2019s attached block volumes.</p>",
            "<p>Configure the application to store authenticated session information in an Amazon S3 bucket.</p>"
          ],
          "explanation": "<p>The issue here is that instances are being terminated and replaced and users who were bound to those instances and then being asked to reauthenticate when the load balancer directs them to another instance. This is because the information associated with their authenticated session was stored on the instance that has been terminated.</p><p>The best way to ensure that users are not asked to reauthenticate in this situation is to store the data in a session store. Amazon ElastiCache is ideal for this use case as it stores key/value pairs and offers extremely low latency.</p><p>The diagram below shows how you can use either DynamoDB or ElastiCache as a shared store for session state data:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-26-17-6d066d4bf6cf5e2b25e2c66fc70b4464.jpg\"><p><strong>CORRECT: </strong>\"Configure the application to store authenticated session information in an Amazon ElastiCache cluster\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the application to store authenticated session information in an Amazon S3 bucket\" is incorrect.</p><p>Amazon S3 could be used in this situation but the latency would likely be higher compared to using Amazon ElastiCache.</p><p><strong>INCORRECT:</strong> \"Enable sticky sessions on the target group and store authenticated session information on the instance\u2019s attached block volumes\" is incorrect.</p><p>Storing the data in the attached block volume is a bad idea as this would be deleted when the instance is terminated.</p><p><strong>INCORRECT:</strong> \"Enable session affinity on the load balancer and store authenticated session information on the instance\u2019s attached block volumes\" is incorrect.</p><p>Stick sessions (also known as session affinity) is enabled at the target group, not at the load balancer level. Also, the session state data should not be stored on a block volume.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/\">https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>"
        }
      },
      {
        "id": 138248137,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading software development company has various web applications hosted in an Auto Scaling group of Amazon EC2 instances which are designed for high availability and fault tolerance. The company is using AWS CloudFormation to easily manage its cloud infrastructure as code as well as for deployment. Currently, it has to manually update its CloudFormation templates for every new available AMI of its application. This procedure is prone to human errors and entails a high management overhead on its deployment process.</p><p>Which of the following is the MOST suitable and cost-effective solution that the DevOps engineer should implement to automate this process?</p>",
          "answers": [
            "<p>Configure the CloudFormation template to use AMI mappings. Integrate AWS Lambda and Amazon EventBridge to create a function that regularly runs every hour to detect new AMIs as well as update the mapping in the template. Reference the AMI mappings in the launch template resource block.</p>",
            "<p>Configure the CloudFormation template to use conditional statements to check if new AMIs are available. Fetch the new AMI ID using the <code>cfn-init</code> helper script and reference it in the launch template resource block.</p>",
            "<p>Pull the new AMI IDs using an AWS Lambda-backed custom resource in the CloudFormation template. Reference the AMI ID that the custom resource fetched in the launch template resource block.</p>",
            "<p>Launch an EC2 instance to run a custom shell script every hour to check for new AMIs. The script should update the launch template resource block of the CloudFormation template with the new AMI ID if there are new ones available.</p>"
          ],
          "explanation": "<p>Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. For example, you might want to include resources that aren't available as AWS CloudFormation resource types. You can include those resources by using custom resources. That way, you can still manage all your related resources in a single stack.</p><p>Use the <strong><em>AWS::CloudFormation::CustomResource</em></strong> or, alternatively, the <strong><em>Custom::&lt;User-Defined Resource Name&gt;</em></strong><em> </em>resource type to define custom resources in your templates. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic.</p><p><img src=\"https://media.tutorialsdojo.com/public/CloudFormation%20AMIManager%20Flow%20-%20Create.png\">When you associate a Lambda function with a custom resource, the function is invoked whenever the custom resource is created, updated, or deleted. AWS CloudFormation calls a Lambda API to invoke the function and to pass all the request data (such as the request type and resource properties) to the function. The power and customizability of Lambda functions, in combination with AWS CloudFormation, enable a wide range of scenarios, such as dynamically looking up AMI IDs during stack creation or implementing and using utility functions, such as string reversal functions.</p><p>AWS CloudFormation templates that declare an Amazon Elastic Compute Cloud (Amazon EC2) instance must also specify an Amazon Machine Image (AMI) ID, which includes an operating system and other software and configuration information used to launch the instance. The correct AMI ID depends on the instance type and region in which you're launching your stack. And IDs can change regularly, such as when an AMI is updated with software updates.</p><p>Normally, you might map AMI IDs to specific instance types and regions. To update the IDs, you manually change them in each of your templates. By using custom resources and AWS Lambda (Lambda), you can create a function that gets the IDs of the latest AMIs for the region and instance type that you're using so that you don't have to maintain mappings.</p><p>Hence, the correct answer is:<strong> Pull the new AMI IDs using an AWS Lambda-backed custom resource in the CloudFormation template. Reference the AMI ID that the custom resource fetched in the launch template resource block.</strong></p><p>The option that says: <strong>Configure the CloudFormation template to use AMI mappings. Integrate AWS Lambda and Amazon EventBridge to create a function that regularly runs every hour to detect new AMIs as well as update the mapping in the template. Reference the AMI mappings in the launch template resource block</strong> is incorrect. Although this solution may work, it is not economical to set up a scheduled job that runs every 1 hour just to detect new AMIs and update your CloudFormation templates. This is an inefficient solution since the AMIs are not updated that often to begin with, which means that most of the hourly processing done by the Lambda function will yield no result. A better design would be to use AWS Lambda-backed custom resources instead in CloudFormation, which will fetch the new AMI IDs upon deployment.</p><p>The option that says: <strong>Configure the CloudFormation template to use conditional statements to check if new AMIs are available. Fetch the new AMI ID using the </strong><code><strong>cfn-init</strong></code><strong> helper script and reference it in the launch template resource block<em> </em></strong>is incorrect because a cfn-init helper script is primarily used to fetch metadata, install packages and start/stop services to your EC2 instances that are already running. A better solution to implement here is to use AWS Lambda-backed custom resource in the CloudFormation template to pull the new AMI IDs.</p><p>The option that says: <strong>Launch an EC2 instance to run a custom shell script every hour to check for new AMIs. The script should update the launch template resource block of the CloudFormation template with the new AMI ID if there are new ones available<em> </em></strong>is incorrect. Although this solution may work, it includes the unnecessary cost of running an EC2 instance which is charged 24/7 but only does the actual processing every hour. This can simply be replaced by using an AWS Lambda-backed custom resource in CloudFormation.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/\">https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 134588397,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has a PROD, DEV, and TEST environment in its software development department, each contains hundreds of EC2 instances and other AWS services. There was a series of security patches that have been released on the official Ubuntu operating system for a critical flaw that was recently discovered. Although this is an urgent matter, there is no guarantee that these patches will be bug-free and production-ready. This is why a DevOps engineer was instructed to immediately patch all of their affected EC2 instances in all the environments, except for the PROD environment. The EC2 instances in their PROD environment will only be patched after the initial patches have been verified to work effectively in their non-PROD environments. Each environment also has different baseline patch requirements that you will need to satisfy. </p><p>How should the DevOps engineer perform this task with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Develop various shell scripts for each environment that specifies which patch will serve as its baseline. Tag each instance based on its environment, business unit, and operating system. Add the Amazon EC2 instances into Target Groups using the AWS Systems Manager Run Command and then execute the script corresponding to each Target Group.</p>",
            "<p>Set up a new patch baseline in AWS Systems Manager Patch Manager for each environment. Tag each Amazon EC2 instance based on its operating system. Categorize EC2 instances based on their tags using Patch Groups. Apply the patches specified in their corresponding patch baseline to each Patch Group. Use Patch Compliance to ensure that the patches have been installed correctly. Using AWS Config, record all of the changes to patch and association compliance statuses.</p>",
            "<p>Tag each instance based on its environment, business unit, and operating system. Set up a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize each Amazon EC2 instance based on its tags using Patch Groups. Apply the required patches specified in the corresponding patch baseline to each Patch Group.</p>",
            "<p>Use the AWS Systems Manager Maintenance Windows to set up a scheduled maintenance period for each environment, where the period is after business hours so as not to affect daily operations. The Systems Manager will execute a cron job that will install the required patches for each Amazon EC2 instance in each environment during the maintenance period. Use the Systems Manager Managed Instances to verify that your environments are fully patched and compliant.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type.</p><p><em>Patch Manager</em> uses patch baselines<em>,</em> which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. For each auto-approval rule that you create, you can specify an auto-approval delay. This delay is the number of days to wait after the patch was released before the patch is automatically approved for patching.</p><p><img src=\"https://media.tutorialsdojo.com/public/AWS-Systems-Manager-Patch-Manager_3AUG2023.png\"></p><p>A patch group is an optional means of organizing instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested.</p><p><img src=\"https://media.tutorialsdojo.com/public/patch-groups-how-it-works_3AUG2023.png\"></p><p>You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: <code><strong>Patch Group</strong></code>. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution.</p><p>Hence, the correct answer is: <strong>Tag each instance based on its environment, business unit, and operating system. Set up a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize each Amazon EC2 instance based on its tags using Patch Groups. Apply the required patches specified in the corresponding patch baseline to each Patch Group.</strong></p><p>The option that says: <strong>Develop various shell scripts for each environment that specifies which patch will serve as its baseline. Tag each instance based on its environment, business unit, and operating system. Add the Amazon EC2 instances into Target Groups using the AWS Systems Manager Run Command and then execute the script corresponding to each Target Group</strong> is incorrect as this option takes more effort to perform because you are using Systems Manager Run Command instead of Patch Manager. The Run Command service enables you to automate common administrative tasks and perform ad hoc configuration changes at scale, however, it takes a lot of effort to implement this solution. You can use Patch Manager instead to perform the task required by the scenario since you need to perform this task with the least amount of effort.</p><p>The option that says: <strong>Set up a new patch baseline in AWS Systems Manager Patch Manager for each environment. Tag each Amazon EC2 instance based on its operating system. Categorize EC2 instances based on their tags using Patch Groups. Apply the patches specified in their corresponding patch baseline to each Patch Group. Use Patch Compliance to ensure that the patches have been installed correctly. Using AWS Config, record all of the changes to patch and association compliance statuses </strong>is incorrect because you should be tagging instances based on the environment and its OS type in which they belong and not just its OS type. This is because the type of patches that will be applied varies between the different environments. With this option, the Ubuntu EC2 instances in <strong>all</strong> of your environments, including in production, will automatically be patched.</p><p>The option that says: <strong>Use the AWS Systems Manager Maintenance Windows to set up a scheduled maintenance period for each environment, where the period is after business hours so as not to affect daily operations. The Systems Manager will execute a cron job that will install the required patches for each Amazon EC2 instance in each environment during the maintenance period. Use the Systems Manager Managed Instances to verify that your environments are fully patched and compliant </strong>is incorrect because this is not the simplest way to address the issue using AWS Systems Manager. The AWS Systems Manager Maintenance Windows feature lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Each Maintenance Window has a schedule, a maximum duration, a set of registered targets (the instances that are acted upon), and a set of registered tasks. Although this solution may work, it entails a lot of configuration and effort to implement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.htmll</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248139,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A global IT consulting company has a multi-tier enterprise resource planning application which is hosted in AWS. It runs on an Auto Scaling group of Amazon EC2 instances across multiple Availability Zones behind an Application Load Balancer. For its database tier, all of its data is persisted in an Amazon RDS MySQL database running in a Multi-AZ deployments configuration. All of the static content of the application is durably stored in Amazon S3. The company is already using AWS CloudFormation templates for managing and deploying its AWS resources. Few weeks ago, the company failed an IT audit due to its application\u2019s long recovery time and excessive data loss in a simulated disaster recovery scenario drill.</p><p>How should the DevOps Engineer implement a multi-region disaster recovery plan which has the LOWEST recovery time and the LEAST data loss?</p>",
          "answers": [
            "<p>Launch the application stack in another AWS region using the CloudFormation template. Create an RDS Read Replica in the other region then enable cross-region replication between the original Amazon S3 bucket and a new S3 bucket. Promote the RDS Read Replica as the master in the event of application failover. Increase the capacity of the Auto Scaling group using the CloudFormation stack template to improve the scalability of the application.</p>",
            "<p>Launch the application stack in another AWS region using the CloudFormation template. Take a daily RDS cross-region snapshot to the other region using a scheduled job running in AWS Lambda and Amazon EventBridge. Enable cross-region replication between the original S3 bucket and Amazon Glacier. In the event of application outages, launch a new application stack in the other AWS region and restore the database from the most recent snapshot.</p>",
            "<p>Launch the application stack in another AWS region using the CloudFormation template. Create another RDS standby DB instance in the other region then enable cross-region replication between the original S3 bucket and a new S3 bucket. The Standby DB instance will automatically be the master DB in the event of an application fail over. Increase the capacity of the Auto Scaling group using the CloudFormation stack template to improve the scalability of the application.</p>",
            "<p>Launch the application stack in another AWS region using the CloudFormation template. Enable cross-region replication between the original S3 bucket and a new S3 bucket. Set up an Application Load Balancer which will distribute the traffic to the other AWS region in the event of an outage. Maintain the Multi-AZ deployments configuration of the RDS database which can ensure the availability of your data even in the event of a regional AWS outage in the primary site.</p>"
          ],
          "explanation": "<p><strong>Amazon RDS Read Replicas</strong> provide enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle as well as Amazon Aurora.</p><p>Read replicas in Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle provide a complementary availability mechanism to Amazon RDS Multi-AZ Deployments. You can promote a read replica if the source DB instance fails. You can also replicate DB instances across AWS Regions as part of your disaster recovery strategy, which is not available with Multi-AZ Deployments since this is only applicable in a single AWS Region. This functionality complements the synchronous replication, automatic failure detection, and failover provided with Multi-AZ deployments.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-11-07_22-42-29-e6f64cabd382181b6d81c7d67fead25c.png\"></p><p>When you copy a snapshot to an AWS Region that is different from the source snapshot's AWS Region, the first copy is a full snapshot copy, even if you copy an incremental snapshot. A full snapshot copy contains all of the data and metadata required to restore the DB instance. After the first snapshot copy, you can copy incremental snapshots of the same DB instance to the same destination region within the same AWS account.</p><p>Depending on the AWS Regions involved and the amount of data to be copied, <em>a cross-region snapshot copy can take hours to complete.</em> In some cases, there might be a large number of cross-region snapshot copy requests from a given source AWS Region. In these cases, Amazon RDS might put new cross-region copy requests from that source AWS Region into a queue until some in-progress copies complete. No progress information is displayed about copy requests while they are in the queue. Progress information is displayed when the copy starts.</p><p>This means that a cross-region snapshot doesn't provide a high RPO compared with a Read Replica since the snapshot takes significant time to complete. Although this is better than Multi-AZ deployments since you can replicate your database across AWS Regions, using a Read Replica is still the best choice for providing a high RTO and RPO for disaster recovery.</p><p>Hence, the correct answer is: <strong>Launch the application stack in another AWS region using the CloudFormation template. Create an RDS Read Replica in the other region then enable cross-region replication between the original Amazon S3 bucket and a new S3 bucket. Promote the RDS Read Replica as the master in the event of application failover. Increase the capacity of the Auto Scaling group using the CloudFormation stack template to improve the scalability of the application.</strong></p><p>The option that says: <strong>Launch the application stack in another AWS region using the CloudFormation template. Take a daily RDS cross-region snapshot to the other region using a scheduled job running in AWS Lambda and Amazon EventBridge. Enable cross-region replication between the original S3 bucket and Amazon Glacier. In the event of application outages, launch a new application stack in the other AWS region and restore the database from the most recent snapshot</strong> is incorrect. Although this solution may work, the use of cross-region snapshot doesn't provide the LOWEST recovery time and the LEAST data loss because a snapshot can take several hours to complete. The best solution to use here is to simply launch a Read Replica to another AWS Region, which asynchronously replicates the data from the source database to the other AWS Region.</p><p>The option that says:<strong> Launch the application stack in another AWS region using the CloudFormation template. Create another RDS standby DB instance in the other region then enable cross-region replication between the original S3 bucket and a new S3 bucket. The Standby DB instance will automatically be the master DB in the event of an application fail over. Increase the capacity of the Auto Scaling group using the CloudFormation stack template to improve the scalability of the application </strong>is incorrect because the scope of the Multi-AZ deployments is bound to a single AWS Region only. You cannot host your standby DB instance to another AWS Region. You should either use Read Replicas or cross-region snapshots instead.</p><p>The option that says: <strong>Launch the application stack in another AWS region using the CloudFormation template. Enable cross-region replication between the original S3 bucket and a new S3 bucket. Set up an Application Load Balancer which will distribute the traffic to the other AWS region in the event of an outage. Maintain the Multi-AZ deployments configuration of the RDS database which can ensure the availability of your data even in the event of a regional AWS outage in the primary site<em> </em></strong>is incorrect because an ELB can't distribute traffic to different AWS Regions, unlike Route 53. Moreover, a Multi-AZ deployments configuration can only handle an outage of one or more Availability Zones and not the entire AWS Region.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/ \">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions</a></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>"
        }
      },
      {
        "id": 75949160,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 4",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer deployed an application to AWS which makes use of Amazon EC2 Auto Scaling to launch new instances. The engineer needs to modify the instance type used for all new instances that are launched through automatic scaling. The Auto Scaling group is configured to use a launch template.</p><p>Which of the following actions should be taken to achieve this requirement?</p>",
          "answers": [
            "<p>Create an AWS Elastic Beanstalk environment to deploy the new instance type for all scaling events.</p>",
            "<p>Use the Overrides structure to define a new launch template for individual instance types using the existing Auto Scaling group.</p>",
            "<p>Launch new EC2 instances with the new instance type using the AWS CLI and attach them to the Auto Scaling group.</p>",
            "<p>Modify the existing launch template to create a new version that uses the new instance type and modify the Auto Scaling group to use the new template version.</p>"
          ],
          "explanation": "<p>When you make a change to an existing launch template it creates a new version of the template. It is then easy to modify the Auto Scaling group to use the new version. As you can see in the diagram below you can modify the launch template used or the version of the launch template used.</p><p>In this case the engineer can simply save a new version of the template and then update the version of template used by the Auto Scaling group. All subsequent scaling events will launch using the new version of the template.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-06-22-87f49932ae89a3d9a078c0a894e0c1f5.jpg\"><p><strong>CORRECT: </strong>\"Modify the existing launch template to create a new version that uses the new instance type and modify the Auto Scaling group to use the new template version\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the Overrides structure to define a new launch template for individual instance types using the existing Auto Scaling group\" is incorrect.</p><p>The Overrides structure is used to define a different launch template for specific instance types. The best solution for this scenario is to simply update the launch template.</p><p><strong>INCORRECT:</strong> \"Create an AWS Elastic Beanstalk environment to deploy the new instance type for all scaling events\" is incorrect.</p><p>There is no need to move the solution to Elastic Beanstalk as the launch template can simply be updated.</p><p><strong>INCORRECT:</strong> \"Launch new EC2 instances with the new instance type using the AWS CLI and attach them to the Auto Scaling group\" is incorrect.</p><p>This is not a good solution as it is highly manual. The solution should be automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/examples-launch-templates-aws-cli.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/examples-launch-templates-aws-cli.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 138248191,
        "correct_response": [
          "c",
          "e"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has an internal application that is hosted on an Amazon EC2 instance running in a VPC. As part of the setup, the application needed to download an object from the S3 bucket. However, the application received an AccessDenied error in logs when trying to download the object from the restricted S3 bucket.</p><p>Which of the following should the DevOps Engineer investigate to identify the cause of the issue? (Select TWO.)</p>",
          "answers": [
            "<p>The object has been moved to a different storage class.</p>",
            "<p>The S3 Object Lock is enabled.</p>",
            "<p>The S3 bucket policy has permission errors.</p>",
            "<p>The S3 bucket has default encryption enabled.</p>",
            "<p>The IAM role attached to the EC2 instance has configuration errors.</p>"
          ],
          "explanation": "<p><strong>S3 bucket policy</strong> allows for securing access to objects in buckets so that only users with the necessary permissions can access them. Even authenticated users can be restricted from accessing Amazon S3 resources if they lack the appropriate permissions.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-s3.png\"></p><p>An <strong>IAM role</strong> is a type of IAM identity that can be created in an AWS account with defined permissions. It shares similarities with IAM users, as it also has permission policies that determine its actions in AWS. However, unlike an IAM user, an IAM role is not uniquely associated with a specific individual and can be assumed by anyone who requires its permission. Moreover, an IAM role does not have standard long-term credentials such as a password or access keys; instead, it provides temporary security credentials for the duration of the role session.</p><p>Hence, the correct answers are the option that says:</p><p>- <strong>The S3 bucket policy has permission errors.</strong></p><p><strong>- The IAM role attached to the EC2 instance has configuration errors.</strong></p><p>The following options will not cause the error:</p><p><strong>- The object has been moved to a different storage class</strong></p><p><strong>- The S3 Object Lock is enabled</strong></p><p><strong>- The S3 bucket has default encryption enabled</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://repost.aws/knowledge-center/ec2-instance-access-s3-bucket\">https://repost.aws/knowledge-center/ec2-instance-access-s3-bucket</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><a href=\"https://aws.amazon.com/s3/\">https://aws.amazon.com/s3/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 134588471,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading digital consultancy company has two teams in its IT department: the DevOps team and the Security team, that are working together on different components of its cloud architecture. AWS CloudFormation is used to manage its resources across all of its AWS accounts, including AWS Config for configuration management. The Security team applies the operating system-level updates and patches while the DevOps team manages application-level dependencies and updates. The DevOps team must use the latest AMI when launching new EC2 instances and deploying its flagship application. </p><p>Which of the following options is the MOST scalable method for integrating the two processes and teams?</p>",
          "answers": [
            "<p>Instruct the Security team to set up an AWS CloudFormation template that creates new versions of their AMIs and lists the Amazon Resource names (ARNs) of the AMIs in an encrypted S3 object as part of the stack output section. Direct the DevOps team to use the cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs.</p>",
            "<p>Instruct the Security team to set up an AWS CloudFormation stack that creates an AWS CodePipeline pipeline that builds new Amazon Machine Images. Then, store the AMI ARNs as parameters in AWS Systems Manager Parameter Store as part of the pipeline output. Order the DevOps team to use the <code>AWS::SSM::Parameter</code> section in their CloudFormation stack to obtain the most recent AMI ARN from the Parameter Store.</p>",
            "<p>Instruct the Security team to maintain a nested stack in AWS CloudFormation that includes both the OS and the templates from the DevOps team. Order the Security team to use the stack update action to deploy updates to the application stack whenever the DevOps team changes the application code.</p>",
            "<p>Instruct the Security team to use a CloudFormation stack that launches an AWS CodePipeline pipeline that builds new AMIs then store the latest AMI ARNs in an encrypted S3 object as part of the pipeline output. Order the DevOps team to use a cross-stack reference within their own CloudFormation template to get that S3 object location and obtain the most recent AMI ARNs to use when deploying their application.</p>"
          ],
          "explanation": "<p><strong>Dynamic references</strong> provide a compact, powerful way for you to specify external values that are stored and managed in other services, such as the Systems Manager Parameter Store, in your stack templates. When you use a dynamic reference, CloudFormation retrieves the value of the specified reference when necessary during stack and change set operations.</p><p>CloudFormation currently supports the following dynamic reference patterns:</p><p>- ssm, for plaintext values stored in AWS Systems Manager Parameter Store</p><p>- ssm-secure, for secure strings stored in AWS Systems Manager Parameter Store</p><p>- secretsmanager, for entire secrets or specific secret values that are stored in AWS Secrets Manager</p><p><img src=\"https://media.tutorialsdojo.com/public/AWS-Systems-Manager-Parameter-Store_6AUG2023.png\"></p><p>Some considerations when using dynamic references:</p><p>- You can include up to 60 dynamic references in a stack template.</p><p>- For transforms, such as AWS::Include and AWS::Serverless, AWS CloudFormation does not resolve dynamic references prior to invoking any transforms. Rather, AWS CloudFormation passes the literal string of the dynamic reference to the transform. Dynamic references (including those inserted into the processed template as the result of a transform) are resolved when you execute the change set using the template.</p><p>- Dynamic references for secure values, such as <code>ssm-secure</code> and <code>secretsmanager</code>, are not currently supported in custom resources.</p><p>Hence, the correct answer is: <strong>Instruct the Security team to set up an AWS CloudFormation stack that creates an AWS CodePipeline pipeline that builds new Amazon Machine Images. Then, store the AMI ARNs as parameters in AWS Systems Manager Parameter Store as part of the pipeline output. Order the DevOps team to use the </strong><code><strong>AWS::SSM::Parameter</strong></code><strong> section in their CloudFormation stack to obtain the most recent AMI ARN from the Parameter Store.</strong></p><p>The option that says: <strong>Instruct the Security team to set up an AWS CloudFormation template that creates new versions of their AMIs and lists the Amazon Resource names (ARNs) of the AMIs in an encrypted S3 object as part of the stack output section. Direct the DevOps team to use the cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs </strong>is incorrect because it is better to store the parameters in AWS Systems Manager Parameter Store.</p><p>The option that says: <strong>Instruct the Security team to maintain a nested stack in AWS CloudFormation that includes both the OS and the templates from the DevOps team. Order the Security team to use the stack update action to deploy updates to the application stack whenever the DevOps team changes the application code</strong> is incorrect because using a nested stack will not decouple the responsibility of the two teams. Integrating AWS Systems Manager Parameter Store to store the ARN of the AMIs is a better solution.</p><p>The option that says: <strong>Instruct the Security team to use a CloudFormation stack that launches an AWS CodePipeline pipeline that builds new AMIs, then store the latest AMI ARNs in an encrypted S3 object as part of the pipeline output. Order the DevOps team to use a cross-stack reference within their own CloudFormation template to get that S3 object location and obtain the most recent AMI ARNs to use when deploying their application</strong> is incorrect. Although this is a valid solution, it entails a lot of effort to set up a cross-stack reference within the DevOps team's own CloudFormation template to get that S3 object location and obtain the most recent AMI ARNs. You also have to ensure that the AMI ARN on Amazon S3 is the latest one.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 82921452,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multi-national retail company is in the process of capturing all of its infrastructure as code using CloudFormation. The infrastructure inventory is huge and will contain a networking stack, an application stack, a data stack, and so on. Some teams are ready to move ahead with the process while others are lagging, and there is a desire to keep all the infrastructure version controlled.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?</p>\n",
          "answers": [
            "<p>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</p>",
            "<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</p>",
            "<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</p>",
            "<p>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</strong></p>\n\n<p>While using CloudFormation, you work with templates and stacks. You create templates to describe your AWS resources and their properties. When you use AWS CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's AWS CloudFormation template.</p>\n\n<p>In CloudFormation the best practice is to separate stacks into individual, separate logical components that have dependencies on each other. To link through these dependencies, the best is to use Exports and Imports. Each individual CloudFormation template must be a separate file.</p>\n\n<p>CloudFormation best practices:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</strong></p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</strong></p>\n\n<p>The issue with both these options is that different teams are working on different pieces of the infrastructure with their own timelines, so it's difficult to combine all elements of the infrastructure into a single master template. It's much better to have one template per logical element of the infrastructure that is owned by the respective team and then use outputs and exports to reference values in the stacks. Nested Stacks can be helpful if a component configuration (such as a Load Balancer) can be reused across many stacks.</p>\n\n<p><strong>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</strong> - Using outputs and exports for individual templates is much better than collaborating via pull requests at code repository level. Using individual templates gives ownership to the contributing team to make sure that the CloudFormation templates are always functional and ready to be referenced in other stacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n"
        }
      }
    ],
    "answers": {
      "75949080": [
        "d"
      ],
      "75949104": [
        "b"
      ],
      "75949130": [
        "a"
      ],
      "75949142": [
        "c"
      ],
      "75949160": [
        "d"
      ],
      "82921394": [
        "a",
        "d"
      ],
      "82921438": [
        "a"
      ],
      "82921452": [
        "a"
      ],
      "99528221": [
        "d"
      ],
      "99528231": [
        "c"
      ],
      "134588397": [
        "c"
      ],
      "134588471": [
        "b"
      ],
      "138248103": [
        "b"
      ],
      "138248111": [
        "a"
      ],
      "138248137": [
        "c"
      ],
      "138248139": [
        "a"
      ],
      "138248161": [
        "a"
      ],
      "138248191": [
        "c",
        "e"
      ],
      "138248211": [
        "b"
      ],
      "143860749": [
        "a"
      ]
    }
  },
  {
    "id": "1770349234918",
    "date": "2026-02-06T03:40:34.918Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 18,
    "incorrect": 2,
    "unanswered": 0,
    "total": 20,
    "percent": 90,
    "duration": 6038187,
    "questions": [
      {
        "id": 115961499,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 5",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A development team use a staging deployment of an application to test updates. The application includes an Amazon RDS database instances and Amazon EC2 instances. The resources only need to run when testing deployments are run using AWS CodePipeline. The testing usually runs for just a few hours a couple of times each week. A DevOps engineer wants cost-effective automating the instantiation and shutdown of the resources without changing the architecture of the application</p><p>Which solution best meets the requirements?</p>",
          "answers": [
            "<p>Configure CodePipeline to subscribe to an event in Amazon EventBridge that triggers an AWS Systems Manager automation document that starts and stops the EC2 and RDS instances before and after deployment tests.</p>",
            "<p>Convert the RDS database to an Amazon Aurora Serverless database and create an Application Load Balancer for EC2. Use an AWS Lambda function to start and stop the EC2 and RDS instances before and after tests.</p>",
            "<p>Put the EC2 instances into an Auto Scaling group. Use Application Auto Scaling to configure a scheduled scaling event that runs at the start of the deployment tests.</p>",
            "<p>Replace the EC2 instances with EC2 Spot Instances and the RDS database with an RDS Reserved Instance. Use AWS CLI commands to start and stop EC2 and RDS instances before and after tests.</p>"
          ],
          "explanation": "<p>You can monitor CodePipeline events in EventBridge and then trigger another service to run using the event data. In this case, the DevOps engineer can create an event pattern that triggers the Systems Manager automation document to run when a CodePipeline execution has been initiated.</p><p><strong>CORRECT: </strong>\"Configure CodePipeline to subscribe to an event in Amazon EventBridge that triggers an AWS Systems Manager automation document that starts and stops the EC2 and RDS instances before and after deployment tests\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Convert the RDS database to an Amazon Aurora Serverless database and create an Application Load Balancer for EC2. Use an AWS Lambda function to start and stop the EC2 and RDS instances before and after tests\" is incorrect.</p><p>This requires more of an architectural change to the application and there is no automated solution for triggering the execution of the Lambda function.</p><p><strong>INCORRECT:</strong> \"Put the EC2 instances into an Auto Scaling group. Use Application Auto Scaling to configure a scheduled scaling event that runs at the start of the deployment tests\" is incorrect.</p><p>There is no solution here for the RDS database, this answer only provides a partial solution for EC2 instances.</p><p><strong>INCORRECT:</strong> \"Replace the EC2 instances with EC2 Spot Instances and the RDS database with an RDS Reserved Instance. Use AWS CLI commands to start and stop EC2 and RDS instances before and after tests\" is incorrect.</p><p>Changing the pricing structure does not automate starting and stopping the resources. Also, a reserved instance is not a good pricing option for this solution as it means you have paid regardless of whether the resource is running. The AWS CLI commands are not automated in this answer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/systems-manager-automation-documents-manage-instances-cut-costs-off-hours/\">https://aws.amazon.com/blogs/mt/systems-manager-automation-documents-manage-instances-cut-costs-off-hours/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 138248201,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps Engineer in a leading aerospace engineering company has a hybrid cloud architecture that connects its on-premises data center with AWS via Direct Connect Gateway. There is a new requirement to implement an automated OS patching solution for all of the Windows servers hosted on-premises as well as in AWS Cloud. The AWS Systems Manager service should be utilized to automate the patching of the servers.</p><p>Which combination of steps should be set up to satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS <code>AssumeRoleWithSAML</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
            "<p>Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS <code>AssumeRole</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>mi-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager State Manager.</p>"
          ],
          "explanation": "<p>A hybrid environment includes on-premises servers and virtual machines (VMs) that have been configured for use with Systems Manager, including VMs in other cloud environments. After following the steps below, the users who have been granted permissions by the AWS account administrator can use AWS Systems Manager to configure and manage their organization's on-premises servers and virtual machines (VMs).</p><p>To configure your hybrid servers and VMs for AWS Systems Manager, just follow these provided steps:</p><p>1. Complete General Systems Manager Setup Steps<br>2. Create an IAM Service Role for a Hybrid Environment<br>3. Install a TLS certificate on On-Premises Servers and VMs<br>4. Create a Managed-Instance Activation for a Hybrid Environment<br>5. Install SSM Agent for a Hybrid Environment (Windows)<br>6. Install SSM Agent for a Hybrid Environment (Linux)<br>7. (Optional) Enable the Advanced-Instances Tier</p><p><br></p><p>Configuring your hybrid environment for Systems Manager enables you to do the following:</p><p>- Create a consistent and secure way to remotely manage your hybrid workloads from one location using the same tools or scripts.</p><p>- Centralize access control for actions that can be performed on your servers and VMs by using AWS Identity and Access Management (IAM).</p><p>- Centralize auditing and your view into the actions performed on your servers and VMs by recording all actions in AWS CloudTrail.</p><p>- Centralize monitoring by configuring CloudWatch Events and Amazon SNS to send notifications about service execution success.</p><p><img src=\"https://media.tutorialsdojo.com/public/how-it-works.png\">After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as <em>managed instances</em>. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix \"mi-\". Amazon EC2 instance IDs use the prefix \"i-\".</p><p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants <code>AssumeRole</code> trust to the Systems Manager service. You only need to create the service role for a hybrid environment once for each AWS account.</p><p>Hence, the correct answers are:</p><p><strong>- Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRole</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</strong></p><p><strong>- Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>mi-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager.</strong></p><p>The option that says: <strong>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRoleWithSAML</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation </strong>is incorrect because you have to execute the <code><em>AssumeRole </em></code>operation instead and not the <code><em>AssumeRoleWithSAML</em></code><em> </em>operation<em>. </em>Moreover, you only need to set up a single IAM service role.</p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager </strong>is incorrect because the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix in the SSM console and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix<strong><em>.</em></strong></p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager State Manager</strong> is incorrect because the AWS Systems Manager State Manager is just a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. You have to apply the patches using the Systems Manager Patch Manager instead. In addition, the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 82921440,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.</p>\n\n<p>Which of the following options would you suggest? (Select two)</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</p>",
            "<p>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</p>",
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>Create a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p><strong>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>Understand how a pipeline execution state change rule works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Here we are only interested in pipeline failures, so we need to choose <code>CodePipeline Pipeline Execution State Change</code>.\nFinally, CloudWatch Event rules do not support Slack as a target, therefore we must create a Lambda function for it.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i2.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</strong> - CloudWatch Event rules do not support Slack as a target, so this option is incorrect.</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>**\nCreate a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>Here we are only interested in pipeline failures, so we just need to choose <code>CodePipeline Pipeline Execution State Change</code>. Therefore both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n"
        }
      },
      {
        "id": 134588437,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A financial company has a total of over a hundred Amazon EC2 instances running across their development, testing, and production environments in AWS. Based on a recent IT review, the company initiated a new compliance rule that mandates a monthly audit of every Linux and Windows EC2 instances check for system performance issues. Each instance must have a logging function that collects various system details and retrieve custom metrics from installed applications or services. The DevOps team will periodically review these logs and analyze their contents using AWS Analytics tools, and the result will be stored in an S3 bucket. </p><p>Which is the MOST recommended way to collect and analyze logs from the instances with MINIMAL effort?</p>",
          "answers": [
            "<p>Configure and install the unified CloudWatch Logs agent in each Amazon EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
            "<p>Configure and install AWS SDK in each Amazon EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and analyze the log data of all instances using CloudWatch Logs Insights.</p>",
            "<p>Configure and install the AWS Systems Manager Agent (SSM Agent) in each EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
            "<p>Configure and install AWS Inspector Agent in each Amazon EC2 instance that will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all EC2 instances.</p>"
          ],
          "explanation": "<p>To collect logs from your Amazon EC2 instances and on-premises servers into CloudWatch Logs, AWS offers both a new unified CloudWatch agent, and an older CloudWatch Logs agent. It is recommended to use the unified CloudWatch agent, which has the following advantages:</p><p>- You can collect both logs and advanced metrics with the installation and configuration of just one agent.</p><p>- The unified agent enables the collection of logs from servers running Windows Server.</p><p>- If you are using the agent to collect CloudWatch metrics, the unified agent also enables the collection of additional system metrics, for in-guest visibility.</p><p>- The unified agent provides better performance.</p><p><img src=\"https://media.tutorialsdojo.com/public/LogsInsights-workflow_6AUG2023.png\"></p><p>CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you quickly and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p><p>CloudWatch Logs Insights includes a purpose-built query language with a few simple but powerful commands. CloudWatch Logs Insights provides sample queries, command descriptions, query autocompletion, and log field discovery to help you get started quickly. Sample queries are included for several types of AWS service logs.</p><p>Hence, the correct answer is: <strong>Configure and install the unified CloudWatch Logs agent in each Amazon EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</strong></p><p>The option that says: <strong>Configure and install AWS SDK in each Amazon EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and analyze the log data of all instances using CloudWatch Logs Insights</strong> is incorrect. Although this is a valid solution, this entails a lot of effort to implement as you have to allocate time to install the AWS SDK to each instance and develop a custom monitoring solution. Remember that the question is specifically looking for a solution that can be implemented with minimal effort. In addition, it is unnecessary and not cost-efficient to enable detailed monitoring in CloudWatch in order to meet the requirements of this scenario since this can be done using CloudWatch Logs.</p><p>The option that says: <strong>Configure and install the AWS Systems Manager Agent (SSM Agent) in each EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights</strong> is incorrect. Although this is also a valid solution, it is more efficient to use CloudWatch agent than an SSM agent. Manually connecting to an instance to view log files and troubleshoot an issue with SSM Agent is time-consuming hence, for more efficient instance monitoring, you can use the CloudWatch Agent instead to send the log data to Amazon CloudWatch Logs.</p><p>The option that says: <strong>Configure and install AWS Inspector Agent in each Amazon EC2 instance that will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all EC2 instances </strong>is incorrect because AWS Inspector is simply a security assessment service that only helps you in checking for unintended network accessibility of your EC2 instances and for vulnerabilities on those EC2 instances. Furthermore, setting up an Amazon CloudWatch dashboard is not suitable since it's primarily used for scenarios where you have to monitor your resources in a single view, even those resources that are spread across different AWS Regions. It is better to use CloudWatch Logs Insights instead since it enables you to interactively search and analyze your log data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html \">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>CloudWatch Agent vs SSM Agent vs Custom Daemon Scripts:</strong></p><p><a href=\"https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/?src=udemy\">https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/</a></p>"
        }
      },
      {
        "id": 138248105,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A startup prioritizes a serverless approach, using AWS Lambda for new workloads to analyze performance and identify bottlenecks. The startup aims to transition to self-managed services on top of Amazon EC2 later if it is more cost-effective. To do this, a solution for granular monitoring of every component of the call graph, including services and internal functions, for all requests, is required. In addition, the startup wants engineers and other stakeholders to be notified of performance irregularities as soon as such irregularities arise.</p><p>Which option will meet these requirements?</p>",
          "answers": [
            "<p>Create an internal extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms.</p>",
            "<p>Consolidate workflows spanning multiple Lambda functions into 1 function per workflow. Create an external extension and enable AWS X-Ray active tracing to instrument functions into segments. Assign an execution role allowing X-Ray actions. Enable X-Ray insights and set up appropriate Amazon EventBridge rules and Amazon CloudWatch alarms.</p>",
            "<p>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable Amazon CloudWatch Logs insights. Configure relevant Amazon EventBridge rules and CloudWatch alarms.</p>",
            "<p>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms.</p>"
          ],
          "explanation": "<p><strong>AWS X-Ray</strong> is a service that analyzes the execution and performance behavior of distributed applications. Traditional debugging methods don\u2019t work so well for microservice-based applications, in which there are multiple, independent components running on different services. X-Ray enables rapid diagnosis of errors, slowdowns, and timeouts by breaking down application latency.</p><p><img alt=\"AWSXRay_LambdaServiceMap\" height=\"535\" src=\"https://media.tutorialsdojo.com/public/AWSXRay_LambdaServiceMap_22mar2024.png\" width=\"1000\"></p><p><em>Insights</em> is a feature of X-Ray that records performance outliers and tracks their impact until resolved. With insights, issues can be identified where they are occurring and what is causing them, and be triaged with the appropriate severity. Insights notifications are sent as the issue changes over time and can be integrated with your monitoring and alerting solution using Amazon EventBridge.</p><p>With an external <strong>AWS Lambda Extension</strong> using the telemetry API and X-Ray active tracing enabled, workflows are broken down into segments corresponding to the unit of work each Lambda function does. This can even be further broken down into subsegments by instrumenting calls to dependencies and related work, such as when a Lambda function requires data from DynamoDB and additional logic to process the response.</p><p>Lambda extensions come in two flavors: external and internal. The main difference is that an external extension runs in a separate process and can run longer to clean up after the Lambda function terminates, whereas an internal one runs in-process.<br><img alt=\"AWSLambdaExtension_overview_full_sequence\" height=\"670\" src=\"https://media.tutorialsdojo.com/public/AWSLambdaExtension_overview_full_sequence.png\" width=\"1000\"></p><p>Hence, the correct answer is: <strong>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms.</strong></p><p>The option that says: <strong>Create an internal extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable X-Ray insights. Configure relevant Amazon EventBridge rules and Amazon CloudWatch alarms</strong> is incorrect. An internal Lambda extension only works in-process. In the scenario, since X-Ray is the solution chosen for tracing and the X-Ray daemon runs as a separate process, an implementation based on an internal Lambda extension will not work.</p><p>The option that says: <strong>Consolidate workflows spanning multiple Lambda functions into 1 function per workflow. Create an external extension and enable AWS X-Ray active tracing to instrument functions into segments. Assign an execution role allowing X-Ray actions. Enable X-Ray insights and set up appropriate Amazon EventBridge rules and Amazon CloudWatch alarms</strong> is incorrect. Aside from adding unnecessary engineering work, this primarily prevents the reuse of functions in different workflows and increases the chance of undesirable duplication. Use X-Ray groups instead to group traces from individual workflows.</p><p>The option that says: <strong>Create an external extension and instrument Lambda workloads into AWS X-Ray segments and subsegments. Enable X-Ray active tracing for Lambda. Assign an execution role allowing X-Ray actions. Set up X-Ray groups around workflows and enable Amazon CloudWatch Logs insights. Configure relevant Amazon EventBridge rules and CloudWatch alarms</strong> is incorrect. Although Cloudwatch Logs insights and X-Ray insights both analyze and surface emergent issues from data, they do it on very different types of data -- logs and traces, respectively. As logs do not have graph-like relationships of trace segments/spans, they may require more work or data to surface the same issues.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html</a></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-groups.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-console-groups.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-extensions.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-extensions.html</a></p><p><br></p><p><strong>Check out this AWS X-Ray Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-x-ray/?src=udemy\">https://tutorialsdojo.com/aws-x-ray/</a></p>"
        }
      },
      {
        "id": 67357166,
        "correct_response": [
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The flagship application at a company is deployed on Amazon EC2 instances running behind an Application Load Balancer (ALB) within an Auto Scaling group. A DevOps Engineer wants to configure a Blue/Green deployment for this application and has already created launch templates and Auto Scaling groups for both blue and green environments, each deploying to their respective target groups. The ALB can direct traffic to either environment's target group, and an Amazon Route 53 record points to the ALB. The goal is to enable an all-at-once transition of traffic from the software running on the blue environment's EC2 instances to the newly deployed software on the green environment's EC2 instances.</p>\n\n<p>What steps should the DevOps Engineer take to fulfill these requirements?</p>\n",
          "answers": [
            "<p>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Perform a Route 53 DNS update to point to the green environment's endpoint on the ALB</p>",
            "<p>Leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group. Then initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances</p>",
            "<p>Set up an all-at-once deployment to the blue environment's EC2 instances. Perform a Route 53 DNS update to point to the green environment's endpoint on the ALB</p>",
            "<p>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group</strong></p>\n\n<p>A Blue/Green deployment is a deployment strategy in which you create two separate, but identical environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. Using a Blue/Green deployment strategy increases application availability and reduces deployment risk by simplifying the rollback process if a deployment fails. Once testing has been completed on the green environment, live application traffic is directed to the green environment and the blue environment is deprecated.</p>\n\n<p>Several AWS deployment services support Blue/Green deployment strategies including Elastic Beanstalk, OpsWorks, CloudFormation, CodeDeploy, and Amazon ECS.</p>\n\n<p>For the given use case, the blue group carries the production load while the green group is staged and deployed with the new code. When it\u2019s time to deploy, you simply attach the green group to the existing load balancer to introduce traffic to the new environment. As you scale up the green Auto Scaling group, you can take the blue Auto Scaling group instances out of service by either terminating them or putting them in a Standby state.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q36-i1.jpg\">\nvia - <a href=\"https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\">https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an all-at-once deployment to the blue environment's EC2 instances. Perform a Route 53 DNS swap to the green environment's endpoint on the ALB</strong></p>\n\n<p><strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Perform a Route 53 DNS swap to the green environment's endpoint on the ALB</strong></p>\n\n<p>Both these options have been added as distractors. Since there is only a single ALB per the given use case, so there is no alternate endpoint available for a Route 53 DNS update.</p>\n\n<p><strong>Leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group. Then initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances</strong> - The order of execution for this option is incorrect as it points the ALB to the green environment's target group before deploying the new software on the green environment's EC2 instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\">https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n"
        }
      },
      {
        "id": 138248103,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A fast-growing company has multiple AWS accounts which are consolidated using AWS Organizations, and the company expects to add new accounts soon. The DevOps engineer was instructed to design a centralized logging solution to deliver all Amazon VPC Flow Logs and Amazon CloudWatch Logs across all sub-accounts to its dedicated Audit account for compliance purposes. The logs should also be properly indexed in order to perform search, retrieval, and analysis.</p><p>Which of the following is the MOST suitable solution that the engineer should implement to meet the above requirements?</p>",
          "answers": [
            "<p>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account.</p>",
            "<p>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</p>",
            "<p>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account.</p>",
            "<p>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account.</p>"
          ],
          "explanation": "<p>You can load streaming data into your Amazon OpenSearch Service domain from many different sources in AWS. Some sources, like Amazon Data Firehose and Amazon CloudWatch Logs, have built-in support for Amazon OpenSearch Service (successor to Amazon ElasticSearch). Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers. The Lambda functions respond to new data by processing it and streaming it to your domain.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-Amazon-OpenSearch-Service-02-04-2025.png\"></p><p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, Amazon Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. To begin subscribing to log events, create the receiving source, such as a Kinesis stream, where the events will be delivered. A subscription filter defines the filter pattern to use for filtering which log events get delivered to your AWS resource, as well as information about where to send matching log events to.</p><p>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Amazon Kinesis stream to perform custom processing and analysis. Custom processing is especially useful when you collaborate and analyze data across many accounts. For example, a company's information security group might want to analyze data for real-time intrusion detection or anomalous behaviors so it could conduct an audit of accounts in all divisions in the company by collecting their federated production logs for central processing.</p><p>A real-time stream of event data across those accounts can be assembled and delivered to the information security groups, who can use Kinesis to attach the data to their existing security analytic systems. Kinesis streams are currently the only resource supported as a destination for cross-account subscriptions.</p><p>Hence, the correct solution is: <strong>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</strong></p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account</strong> is incorrect. While Lambda functions can be used to process logs, relying solely on a Lambda function to handle a large volume of logs might only lead to performance bottlenecks and scalability issues. Lambda has concurrency limits, and processing logs from multiple sub-accounts may exceed these limits, especially as the company grows and more accounts are added.</p><p>The option that says: <strong>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account </strong>is incorrect because the CloudWatch subscription filter doesn't directly support SQS. You should use a Kinesis Data Stream, Kinesis Firehose, or Lambda function.</p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account</strong> is incorrect. Although this approach typically works, using a self-hosted OpenSearch cluster on an EC2 instance introduces unnecessary complexity and overhead regarding management, scaling, and availability. AWS-managed OpenSearch Service would be a more efficient and cost-effective option. Additionally, relying on EC2 for hosting OpenSearch adds management and operational burdens, whereas using AWS-managed services like Kinesis Data Streams or Firehose with OpenSearch is more reliable and easier to scale.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html</a></p><p><a href=\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 138248141,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A government-sponsored health service is running its web application containing information about the clinics, hospitals, medical specialists, and other medical services in the country. The organization also has a set of public web services which enable third-party companies to search medical data for its respective applications and clients. AWS Lambda functions are used for the public APIs. For its database-tier, an Amazon DynamoDB table stores all of the data with an Amazon OpenSearch Service domain, which supports the search feature and stores the indexes. A DevOps engineer has been instructed to ensure that in the event of a failed deployment, there should be no downtime and a system should be in place to prevent subsequent deployments. The service must strictly maintain full capacity during API deployment without any reduced capacity to avoid degradation of service.</p><p>How can the engineer meet the above requirements in the MOST efficient way?</p>",
          "answers": [
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>All at Once</code>.</p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Rolling</code>.</p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Immutable</code></p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication.</p>"
          ],
          "explanation": "<p><strong>AWS Elastic Beanstalk</strong> provides several options for how <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">deployments</a> are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's an automatically scaling environment (you didn't specify the <code>--single</code> option), it uses rolling deployments.</p><p>With rolling deployments, Elastic Beanstalk splits the environment's EC2 instances into batches and deploys the new version of the application to one batch at a time, leaving the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p><p><img src=\"https://media.tutorialsdojo.com/public/environments-mgmt-updates-immutable.png\"></p><p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.''</p><p>Immutable deployments perform an <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">immutable update</a> to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.</p><p>Hence, the correct answer is: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>Immutable.</strong></code></p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>All at Once</strong></code><strong><em> </em></strong>is incorrect because this policy only deploys the new version to all instances simultaneously, which means that the instances in your environment are out of service for a short time while the deployment occurs.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to</strong> <code><strong>Rolling</strong></code><em> </em>is incorrect because this policy will just deploy the new version in batches where each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication</strong> is incorrect because you can't host a dynamic web application in Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 67357122,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company uses multiple AWS accounts to help isolate and manage business applications. This multi-account environment consists of an AWS Transit Gateway to route all outbound traffic through a common network account. A firewall appliance inspects all traffic before it is forwarded to an internet gateway. The firewall appliance is configured to send logs to Amazon CloudWatch Logs for all events generated.</p>\n\n<p>Recently, the security team has advised about probable illegal access of resources. As DevOps Engineer, you have been advised to configure an alert to the security team if the firewall appliance generates an event of Critical severity.</p>\n\n<p>How should a DevOps engineer configure this requirement?</p>\n",
          "answers": [
            "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>",
            "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send notification to an Amazon Simple Notification Service (Amazon SNS) topic if Critical event is detected. Subscribe the email address of the security team to the SNS topic</p>",
            "<p>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>",
            "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong></p>\n\n<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p>\n\n<p>A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager.</p>\n\n<p>Amazon CloudWatch metrics and alarms:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send a notification to an Amazon Simple Notification Service (Amazon SNS) topic if a Critical event is detected. Subscribe the email address of the security team to the SNS topic</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations. Kinesis Data Firehose is not supported as a destination for the metric streams.</p>\n\n<p><strong>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - Transit Gateway Flow Logs is a feature that enables you to capture information about the IP traffic going to and from your transit gateways. This does not meet our objective of analyzing firewall log data.</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications running on AWS Lambda. The solution collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network. This option acts as a distractor for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html</a></p>\n"
        }
      },
      {
        "id": 82921344,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An industrial appliances company would like to take advantage of AWS Systems Manager to manage their on-premise instances and their EC2 instances. This will allow them to run some SSM RunCommand across their hybrid fleet. The company would also like to effectively manage the size of the fleet. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>How would you set up the on-premise server to achieve this objective?</p>\n",
          "answers": [
            "<p>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>",
            "<p>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>",
            "<p>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>",
            "<p>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p>AWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. With Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status.</p>\n\n<p>How Systems Manager Works:\n<img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\">\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants AssumeRole trust to the Systems Manager service. You only need to create a service role for a hybrid environment once for each AWS account.</p>\n\n<p>To set up servers and virtual machines (VMs) in your hybrid environment as managed instances, you need to create a managed-instance activation. After you complete the activation, you immediately receive an Activation Code and Activation ID. You specify this Code/ID combination when you install SSM agents on servers and VMs in your hybrid environment. The Code/ID provides secure access to the Systems Manager service from your managed instances.</p>\n\n<p>In the Instance limit field, specify the total number of on-premises servers or VMs that you want to register with AWS as part of the activation. This means you don't need to create a unique activation Code/ID for each managed instance.</p>\n\n<p>After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as managed instances. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix \"mi-\". Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong> - As mentioned in the explanation earlier, the on-premise instances use the prefix \"mi-\" whereas the Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><strong>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p><strong>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong></p>\n\n<p>Both these options suggest using the Access Key ID and the Secret Access Key ID to register your on-premise servers which is considered a bad practice from a security perspective. Instead, you should use an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html</a></p>\n\n<p><a href=\"#\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p>\n"
        }
      },
      {
        "id": 82921340,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A financial services company has a solution in place to track all the API calls made by users, applications, and SDK within the AWS account. Recently, it has experienced a hack and could find a user amongst the logs that did some compromising API calls. The company wants to know with 100% certainty that the log files represent the correct sequence of events and have not been altered. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following would you suggest as the most effective solution?</p>\n",
          "answers": [
            "<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</p>",
            "<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</p>",
            "<p>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</p>",
            "<p>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p>How CloudTrail Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation. This feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p>For the given use-case, to track API calls made within your account, you need to use AWS CloudTrail. Then the right way to verify log integrity would be to use the CloudTrail <code>validate-logs</code> command.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</strong> -  S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as \u201cwrite once read many\u201d (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn't provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won't give us the information on who made the API calls.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won't give us the information of who made the API calls. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn't provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n"
        }
      },
      {
        "id": 134588461,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading e-commerce company has a payment portal that handles the payment and refund transactions of its online platform. The portal is hosted in an Auto Scaling group of On-Demand Amazon EC2 instances across three multiple Availability Zones in the US West (N California) region. There is a new requirement to improve the system monitoring of the application as well to track the number of payment and refund transactions being done every minute. The DevOps team should also be notified if this metric breaches the specified threshold.</p><p>Which of the following options provides the MOST cost-effective and automated solution that will satisfy the above requirement?</p>",
          "answers": [
            "<p>Set up an ELK Stack in AWS using Amazon OpenSearch service for log processing. Store the payments and refund transactions in each instance and configure Logstash to send the logs to OpenSearch. Set up a Kibana dashboard to view the data and the metric graphs.</p>",
            "<p>Configure the instances to push the number of payments and refund transactions to Amazon CloudWatch as a custom metric. Set up a CloudWatch alarm to notify the DevOps team using Amazon SNS when the threshold is breached. View statistical graphs of your published metrics with the AWS Management Console.</p>",
            "<p>Configure the instances to push the number of payments and refund transactions to Amazon CloudWatch Logs as a custom metric. Develop a custom monitoring application using a Python-based Flask web application to view the metrics and host it in an EC2 instance.</p>",
            "<p>Configure the instances to push the entire log of each payment and refund transactions to Amazon EventBridge as a custom metric. Set up a CloudWatch alarm to notify the DevOps team using Amazon SNS when the threshold is breached. View statistical graphs of your published metrics with the AWS Management Console.</p>"
          ],
          "explanation": "<p>Metrics are data about the performance of your systems. By default, several services provide free metrics for resources (such as Amazon EC2 instances, Amazon EBS volumes, and Amazon RDS DB instances). You can also enable detailed monitoring for some resources, such as your Amazon EC2 instances, or publish your own application metrics. Amazon CloudWatch can load all the metrics in your account (both AWS resource metrics and application metrics that you provide) for search, graphing, and alarms.</p><p>Metric data is kept for 15 months, enabling you to view both up-to-the-minute data and historical data. You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console.</p><p>CloudWatch stores data about a metric as a series of data points. Each data point has an associated timestamp. You can even publish an aggregated set of data points called a <em>statistic set</em>.</p><p><img src=\"https://media.tutorialsdojo.com/public/CW-Overview_6AUG2023.png\">You can aggregate your data before you publish it to CloudWatch. When you have multiple data points per minute, aggregating data minimizes the number of calls to <strong>put-metric-data</strong>. For example, instead of calling <strong>put-metric-data</strong> multiple times for three data points that are within 3 seconds of each other, you can aggregate the data into a statistic set that you publish with one call, using the <code>--statistic-values</code> parameter.</p><p>Hence, the correct answer is: <strong>Configure the instances to push the number of payments and refund transactions to Amazon CloudWatch as a custom metric. Set up a CloudWatch alarm to notify the DevOps team using Amazon SNS when the threshold is breached. View statistical graphs of your published metrics with the AWS Management Console.</strong></p><p>The option that says: <strong>Set up an ELK Stack in AWS using Amazon OpenSearch service for log processing. Store the payments and refund transactions in each instance and configure Logstash to send the logs to OpenSearch. Set up a Kibana dashboard to view the data and the metric graphs</strong> is incorrect. Although this solution may work, it entails a lot of effort to set up and costs more to maintain. A more cost-effective solution is to primarily use custom metrics in CloudWatch.</p><p>The option that says: <strong>Configure the instances to push the number of payments and refund transactions to Amazon CloudWatch Logs as a custom metric. Develop a custom monitoring application using a Python-based Flask web application to view the metrics and host it in an EC2 instance</strong> is incorrect because it will typically take you a considerable amount of time to set up the Flask web app. Running this solution also entails an added monthly cost.</p><p>The option that says: <strong>Configure the instances to push the entire log of each payment and refund transactions to Amazon EventBridge as a custom metric. Set up a CloudWatch alarm to notify the DevOps team using Amazon SNS when the threshold is breached. View statistical graphs of your published metrics with the AWS Management Console</strong> is incorrect because you can't push a custom metric using Amazon EventBridge.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 138248211,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A CTO of a leading insurance company has recently decided to migrate its online customer portal to AWS. The customers will use the online portal to view the paid insurance premiums and manage accounts. For improved scalability, the application should be hosted in an Auto Scaling group of On-Demand Amazon EC2 instances with a custom Amazon Machine Image (AMI). The same architecture will also be used for the non-production environments (DEV, TEST, and STAGING). The DevOps Engineer is instructed by the CTO to design a deployment strategy that securely stores the credentials of each environment, expedites the startup time for the EC2 instances, and allows the same AMI to work in all environments.</p><p>How should the DevOps Engineer set up the deployment configuration to accomplish this task?</p>",
          "answers": [
            "<p>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Session Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials.</p>",
            "<p>Add a tag to each EC2 instance based on their environment. Use AWS Systems Manager Automation to preconfigure the AMI by installing all of the required applications and software dependencies. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Systems Manager Parameter Store to store the credentials as Secure String parameters.</p>",
            "<p>Add a tag to each EC2 instance based on their environment. Use a preconfigured AMI from AWS Marketplace. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials.</p>",
            "<p>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Patch Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use AWS AppConfig to store the environment-specific configuration and credentials securely.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Parameter Store</strong> provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Highly scalable, available, and durable, Parameter Store is backed by the AWS Cloud.</p><p>Systems Manager Automation simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p>Automation offers one-click automations for simplifying complex tasks such as creating golden Amazon Machines Images (AMIs), and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for a variety of reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img src=\"https://media.tutorialsdojo.com/public/custom_ami_1.gif\"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Add a tag to each EC2 instance based on their environment. Use AWS Systems Manager Automation to preconfigure the AMI by installing all of the required applications and software dependencies. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Systems Manager Parameter Store to store the credentials as Secure String parameters.</strong></p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Session Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials</strong> is incorrect. The Session Manager service is just a fully managed AWS Systems Manager capability that lets you manage your Amazon EC2 instances through an interactive one-click browser-based shell or through the AWS CLI. It is not capable to build a custom AMI, unlike Systems Manager Automation.</p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Use a preconfigured AMI from AWS Marketplace. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials </strong>is incorrect. The company is using a custom AMI and not a public AMI from AWS Marketplace. You have to preconfigure the AMI using the Systems Manager Automation instead.</p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Patch Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use AWS AppConfig to store the environment-specific configuration and credentials securely</strong> is incorrect. The AWS Patch Manager is typically used for patching and managing operating system patches, not for installing applications or preconfiguring the AMI. Additionally, AWS AppConfig is more for managing feature flags or dynamic configurations and is not the best fit for secure storage of environment-specific credentials.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/\">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 67357174,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company wants to create an automated monitoring solution to generate real-time customized notifications regarding unrestricted security groups in the company's production AWS account. The notification must contain the name and ID of the noncompliant security group. The DevOps team at the company has already activated the restricted-ssh AWS Config managed rule. The team has also set up an Amazon Simple Notification Service (Amazon SNS) topic and subscribed relevant personnel to it.</p>\n\n<p>Which of the following options represents the BEST solution for the given scenario?</p>\n",
          "answers": [
            "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>",
            "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>",
            "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>",
            "<p>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong></p>\n\n<p>You can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes do not comply with the conditions in your rules.</p>\n\n<p>The restricted-ssh rule checks if the incoming SSH traffic for the security groups is accessible. The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0). This rule applies only to IPv4.</p>\n\n<p>For the given use case, you need to monitor for the NON_COMPLIANT evaluation result of the rule, which implies that the rule has failed the conditions of the compliance check. You can then create an Amazon EventBridge rule (with AWS Config configured as a source) that is put in action when it matches the NON_COMPLIANT evaluation result of the restricted-ssh rule. The EventBridge rule, in turn, publishes a notification to the SNS topic.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p>You should note that you can only set up a filter policy on an SNS subscription and NOT on the SNS topic itself. In addition, it is wasteful to set up Amazon EventBridge rule on all AWS Config managed rules, rather than only the restricted-ssh rule. Therefore, both these options are incorrect.</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong> - You get an ERROR evaluation result when one of the required/optional parameters is not valid, or not of the correct type, or is formatted incorrectly. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\">https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html</a></p>\n"
        }
      },
      {
        "id": 82921342,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>As a DevOps Engineer at a social media company, you have deployed your application in an Auto Scaling group (ASG) using CloudFormation. You would like to update the Auto Scaling Group to have all the instances reference the newly created launch configuration, which upgrades the instance type. Your ASG currently contains 6 instances and you need at least 4 instances to be up at all times.</p>\n\n<p>Which configuration should you use in the CloudFormation template?</p>\n",
          "answers": [
            "<p>AutoScalingRollingUpdate</p>",
            "<p>AutoScalingReplacingUpdate</p>",
            "<p>AutoScalingLaunchTemplateUpdate</p>",
            "<p>AutoScalingLaunchConfigurationUpdate</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>AutoScalingRollingUpdate</strong></p>\n\n<p>To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the AutoScalingRollingUpdate policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. AutoScalingRollingUpdate is perfect for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q72-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AutoScalingReplacingUpdate</strong> - To specify how AWS CloudFormation handles replacement updates for an Auto Scaling group, you should use the AutoScalingReplacingUpdate policy. This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group. This option will create a new ASG entirely, so this is ruled out.</p>\n\n<p><strong>AutoScalingLaunchTemplateUpdate</strong></p>\n\n<p><strong>AutoScalingLaunchConfigurationUpdate</strong></p>\n\n<p>AutoScalingLaunchTemplateUpdate and AutoScalingLaunchConfigurationUpdate do not exist, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n"
        }
      },
      {
        "id": 134588397,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has a PROD, DEV, and TEST environment in its software development department, each contains hundreds of EC2 instances and other AWS services. There was a series of security patches that have been released on the official Ubuntu operating system for a critical flaw that was recently discovered. Although this is an urgent matter, there is no guarantee that these patches will be bug-free and production-ready. This is why a DevOps engineer was instructed to immediately patch all of their affected EC2 instances in all the environments, except for the PROD environment. The EC2 instances in their PROD environment will only be patched after the initial patches have been verified to work effectively in their non-PROD environments. Each environment also has different baseline patch requirements that you will need to satisfy. </p><p>How should the DevOps engineer perform this task with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Develop various shell scripts for each environment that specifies which patch will serve as its baseline. Tag each instance based on its environment, business unit, and operating system. Add the Amazon EC2 instances into Target Groups using the AWS Systems Manager Run Command and then execute the script corresponding to each Target Group.</p>",
            "<p>Set up a new patch baseline in AWS Systems Manager Patch Manager for each environment. Tag each Amazon EC2 instance based on its operating system. Categorize EC2 instances based on their tags using Patch Groups. Apply the patches specified in their corresponding patch baseline to each Patch Group. Use Patch Compliance to ensure that the patches have been installed correctly. Using AWS Config, record all of the changes to patch and association compliance statuses.</p>",
            "<p>Tag each instance based on its environment, business unit, and operating system. Set up a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize each Amazon EC2 instance based on its tags using Patch Groups. Apply the required patches specified in the corresponding patch baseline to each Patch Group.</p>",
            "<p>Use the AWS Systems Manager Maintenance Windows to set up a scheduled maintenance period for each environment, where the period is after business hours so as not to affect daily operations. The Systems Manager will execute a cron job that will install the required patches for each Amazon EC2 instance in each environment during the maintenance period. Use the Systems Manager Managed Instances to verify that your environments are fully patched and compliant.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type.</p><p><em>Patch Manager</em> uses patch baselines<em>,</em> which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. For each auto-approval rule that you create, you can specify an auto-approval delay. This delay is the number of days to wait after the patch was released before the patch is automatically approved for patching.</p><p><img src=\"https://media.tutorialsdojo.com/public/AWS-Systems-Manager-Patch-Manager_3AUG2023.png\"></p><p>A patch group is an optional means of organizing instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested.</p><p><img src=\"https://media.tutorialsdojo.com/public/patch-groups-how-it-works_3AUG2023.png\"></p><p>You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: <code><strong>Patch Group</strong></code>. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution.</p><p>Hence, the correct answer is: <strong>Tag each instance based on its environment, business unit, and operating system. Set up a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize each Amazon EC2 instance based on its tags using Patch Groups. Apply the required patches specified in the corresponding patch baseline to each Patch Group.</strong></p><p>The option that says: <strong>Develop various shell scripts for each environment that specifies which patch will serve as its baseline. Tag each instance based on its environment, business unit, and operating system. Add the Amazon EC2 instances into Target Groups using the AWS Systems Manager Run Command and then execute the script corresponding to each Target Group</strong> is incorrect as this option takes more effort to perform because you are using Systems Manager Run Command instead of Patch Manager. The Run Command service enables you to automate common administrative tasks and perform ad hoc configuration changes at scale, however, it takes a lot of effort to implement this solution. You can use Patch Manager instead to perform the task required by the scenario since you need to perform this task with the least amount of effort.</p><p>The option that says: <strong>Set up a new patch baseline in AWS Systems Manager Patch Manager for each environment. Tag each Amazon EC2 instance based on its operating system. Categorize EC2 instances based on their tags using Patch Groups. Apply the patches specified in their corresponding patch baseline to each Patch Group. Use Patch Compliance to ensure that the patches have been installed correctly. Using AWS Config, record all of the changes to patch and association compliance statuses </strong>is incorrect because you should be tagging instances based on the environment and its OS type in which they belong and not just its OS type. This is because the type of patches that will be applied varies between the different environments. With this option, the Ubuntu EC2 instances in <strong>all</strong> of your environments, including in production, will automatically be patched.</p><p>The option that says: <strong>Use the AWS Systems Manager Maintenance Windows to set up a scheduled maintenance period for each environment, where the period is after business hours so as not to affect daily operations. The Systems Manager will execute a cron job that will install the required patches for each Amazon EC2 instance in each environment during the maintenance period. Use the Systems Manager Managed Instances to verify that your environments are fully patched and compliant </strong>is incorrect because this is not the simplest way to address the issue using AWS Systems Manager. The AWS Systems Manager Maintenance Windows feature lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Each Maintenance Window has a schedule, a maximum duration, a set of registered targets (the instances that are acted upon), and a set of registered tasks. Although this solution may work, it entails a lot of configuration and effort to implement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.htmll</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248169,
        "correct_response": [
          "d"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has a web application that runs on an Auto Scaling group of Amazon EC2 instances distributed across multiple Availability Zones, with traffic managed by an Application Load Balancer. Amazon RDS MySQL is used for the database tier, and incoming traffic is routed to the load balancer through Amazon Route 53. The Application Load Balancer has a health check that monitors the status of the web servers and verifies that the servers can properly access the database. For compliance purposes, management instructed the Operations team to implement a geographically isolated disaster recovery site to ensure business continuity. The required RPO is 5 minutes, while the RTO should be 2 hours.</p><p>Which of the following options requires the LEAST amount of changes to the application stack?</p>",
          "answers": [
            "Clone the entire application stack except for its RDS database in a different Availability Zone. Create Read Replicas in another Availability Zone and configure the new stack to point to the local RDS instance. Set up a failover routing policy in Route 53 that will automatically route traffic to the new stack in the event of an outage.",
            "<p>Clone the application stack except for RDS in a different AWS Region. Enable RDS Multi-AZ deployments configuration and deploy the standby database instance in the new region. Configure the new application stack to point to the local RDS database instance. Set up a latency routing policy in Route 53 that will automatically route traffic to the new stack in the event of an outage.</p>",
            "<p>Configure the RDS to use Multi-AZ deployments configuration and create Read Replicas. Increase the number of application servers of the stack. Set up a latency routing policy in Route 53 that will automatically route traffic to the application servers.</p>",
            "<p>Clone the application stack except for RDS in a different AWS Region. Create Read Replicas in the new region and configure the new application stack to point to the local RDS database instance. Set up a failover routing policy in Route 53 that will automatically route traffic to the new application stack in the event of an outage.</p>"
          ],
          "explanation": "<p>When you have more than one resource performing the same function \u2014 for example, more than one HTTP server or mail server \u2014 you can configure Amazon Route 53 to check the health of your resources and respond to DNS queries using only the healthy resources. Suppose you have a website with a domain name of tutorialsdojo.com, which is hosted on six servers, two each in three data centers around the world. You can configure Amazon Route 53 to check the health of those servers and to respond to DNS queries for tutorialsdojo.com using only the servers that are currently healthy.</p><p><strong>Route 53</strong> can check the health of your resources in both simple and complex configurations:</p><p>- In simple configurations, you create a group of records that all have the same name and type, such as a group of weighted records with a type of A for tutorialsdojo.com. You then configure Route 53 to check the health of the corresponding resources. Route 53 responds to DNS queries based on the health of your resources.</p><p>- In more complex configurations, you create a tree of records that route traffic based on multiple criteria. For example, if latency for your users is your most important criterion, then you might use latency alias records to route traffic to the region that provides the best latency. The latency alias records might have weighted records in each region as the alias target. The weighted records might route traffic to EC2 instances based on the instance type. As with a simple configuration, you can configure Route 53 to route traffic based on the health of your resources.</p><p><img alt=\"Amazon Route 53 Evaluate Target Health\" height=\"471\" src=\"https://media.tutorialsdojo.com/public/td-amazon-route53-evaluate-target-health.png\" width=\"1000\"></p><p>This approach provides a disaster recovery site in a different AWS region as required. Read Replicas copy data asynchronously to the new region and can be promoted to the primary database when needed. Although promotion is manual or needs automation, it supports the required recovery point objective (RPO) of 5 minutes and recovery time objective (RTO) of 2 hours. Route 53 failover routing automatically redirects traffic to the disaster recovery site during outages with minimal changes to the application stack.</p><p>Hence, the correct answer is: <strong>Clone the application stack except for RDS in a different AWS Region. Create Read Replicas in the new region and configure the new application stack to point to the local RDS database instance. Set up a failover routing policy in Route 53 that will automatically route traffic to the new application stack in the event of an outage.</strong></p><p>The option that says: <strong>Clone the entire application stack except for its RDS database in a different Availability Zone. Create Read Replicas in another Availability Zone and configure the new stack to point to the local RDS instance. Set up a failover routing policy in Route 53 that will automatically route traffic to the new stack in the event of an outage </strong>is incorrect because this is only deployed in another Availability Zone which could also be affected by an AWS Region outage. The new stack should be deployed on a totally separate AWS Region instead.</p><p>The option that says:<strong> Clone the application stack except for RDS in a different AWS Region. Enable RDS Multi-AZ deployments configuration and deploy the standby database instance in the new region. Configure the new application stack to point to the local RDS database instance. Set up a latency routing policy in Route 53 that will automatically route traffic to the new stack in the event of an outage</strong> is incorrect because a Multi-AZ RDS database spans to several Availability Zones within a single Region only, and not to an entirely new region. You cannot deploy the standby database instance in the new AWS region.</p><p>The option that says: <strong>Configure the RDS to use Multi-AZ deployments configuration and create Read Replicas. Increase the number of application servers of the stack. Set up a latency routing policy in Route 53 that will automatically route traffic to the application servers</strong> is incorrect. Although this architecture can cope with an individual AZ outage, the systems will still be unavailable in the event of an AWS Region-wide unavailability.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><br></p><p><strong>Check out these AWS Elastic Beanstalk and Amazon Route 53 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 82921358,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company uses the open-source tool Jenkins on its on-premise infrastructure to perform CICD. It has decided to move to AWS and take advantage of the elasticity properties of the cloud provider to have more efficient workloads. It needs to ensure the Jenkins setup is highly available, fault-tolerant and also elastic to perform builds. The company has hired you as an AWS Certified DevOps Engineer Professional to build the most cost-effective solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend?</p>\n",
          "answers": [
            "<p>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>",
            "<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>",
            "<p>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</p>",
            "<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p>In the AWS Cloud, a web-accessible application like Jenkins is typically designed for high availability and fault tolerance by spreading instances across multiple AZs and fronting them with an Elastic Load Balancing (ELB) load balancer. Elastic Load Balancing automatically distributes incoming application traffic across multiple Amazon EC2 instances in the cloud. It enables you to achieve greater levels of fault tolerance in your applications and seamlessly provides the required amount of load balancing capacity needed to distribute application traffic. If your business requirements demand a fault-tolerant Jenkins environment, your preferred setup might be a scenario in which multiple masters with their own workers are placed in separate Availability Zones.</p>\n\n<p>You can use the Jenkins plugin for AWS CodeBuild to integrate CodeBuild with your Jenkins build jobs. Instead of sending your build jobs to Jenkins build nodes, you use the plugin to send your build jobs to CodeBuild. This eliminates the need for you to provision, configure, and manage Jenkins build nodes.</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2019/10/20/Diagram2.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/\">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p>For the given use-case, Jenkins must be deployed as a multi-master across multi-AZ to be highly available and fault-tolerant. The Jenkins CodeBuild plugin allows to elastically start CodeBuild builds that run a special docker image that works as a Jenkins slave. It allows you to be fully elastic in the cloud with Jenkins, and only pay exactly for the resources you have used.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p>As mentioned in the explanation above, if configured with EC2 instances in an Auto Scaling Group, the setup will be elastic in some ways, but probably expensive if the EC2 instances are not fully utilized at capacity. So these three options are not the best fit for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/\">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf\">https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf</a></p>\n"
        }
      },
      {
        "id": 115961501,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application is being deployed using an AWS CodePipeline pipeline. The pipeline includes an AWS CodeBuild stage which downloads source code from AWS CodeCommit, pulls data from an S3 bucket, and builds and tests the application before deployment.</p><p>A DevOps engineer has discovered that the S3 data is not being successfully downloaded due to a permissions issue.</p><p>How can the permissions be assigned to CodeBuild in the MOST secure manner?</p>",
          "answers": [
            "<p>Use an aws:Referer condition key in the CodeBuild project settings. Update the buildspec to use the AWS CLI to download the data.</p>",
            "<p>Modify the service role for the CodeBuild project to include permissions for S3. Use the AWS CLI to download the data.</p>",
            "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the buildspec to use cURL to pass the token and download the data.</p>",
            "<p>Configure an IAM access key and a secret access key in the application code and use the AWS CLI to download the data.</p>"
          ],
          "explanation": "<p>The most likely issue is that the service role used by AWS CodeBuild does not have the correct permissions to download the data securely from the Amazon S3 bucket. CodeBuild uses the service role for all operations that are performed on your behalf. Therefore, the role must have the permissions needed during the build stage.</p><p>In this case, simply adding the correct permissions statements to the policy attached to the service role should resolve the permission issue. The data can then be downloaded from S3 using the AWS CLI by specifying commands in the buildspec document.</p><p><strong>CORRECT: </strong>\"Modify the service role for the CodeBuild project to include permissions for S3. Use the AWS CLI to download the data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure an IAM access key and a secret access key in the application code and use the AWS CLI to download the data\" is incorrect.</p><p>This is an insecure method of using credentials and should be avoided. It would also not provide the permissions needed by CodeBuild as the service gets those permissions from the service role.</p><p><strong>INCORRECT:</strong> \"Use an aws:Referer condition key in the CodeBuild project settings. Update the buildspec to use the AWS CLI to download the data\" is incorrect.</p><p>The condition key referenced is used in policies to restrict access to specific HTTP referers. This is not useful here as it does not provide any permissions to CodeBuild.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the buildspec to use cURL to pass the token and download the data\" is incorrect.</p><p>You cannot configure different authentication options on S3 as it is a managed service. You can only limit who can access the bucket and objects and under what conditions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949068,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>To increase security, a company plans to use AWS Systems Manager Session Manager to managed Amazon EC2 instances rather than SSH. The connectivity to Session Manager should also use a private network connection.</p><p>Which configuration actions should be taken to implement this? (Select TWO.)</p>",
          "answers": [
            "<p>Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range.</p>",
            "<p>Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager.</p>",
            "<p>Create a VPC endpoint for Systems Manager in the Region where the instances are running.</p>",
            "<p>Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables.</p>",
            "<p>Establish private Session Manager connectivity using the instance IDs of EC2 instances.</p>"
          ],
          "explanation": "<p>Systems Manager Session Manager enables secure remote access to EC2 instances without the need to open ports for SSH or create bastion hosts. You can connect to instances through Session Manager privately by establishing a VPC endpoint in your VPC. This ensures that all connectivity takes place using private addresses.</p><p>The EC2 instances must have the Systems Manager agent running and they must have permissions to be able to communicate with the Systems Manager service. An instance profile can be easily attached to provide these permissions to instances.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-59-28-dac1a7fafdbe3507d5ca4b1fe07ed213.jpg\"><p><strong>CORRECT: </strong>\"Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Systems Manager in the Region where the instances are running\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range\" is incorrect.</p><p>Port 22 is used by the SSH protocol and is not required by Systems Manager Session Manager.</p><p><strong>INCORRECT:</strong> \"Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables\" is incorrect.</p><p>You cannot connect to EC2 instances via a NAT gateway. These gateways are used for outbound (internet) connectivity from the EC2 instances in private subnets.</p><p><strong>INCORRECT:</strong> \"Establish private Session Manager connectivity using the instance IDs of EC2 instances\" is incorrect.</p><p>You cannot connect privately to instances simply by connecting via the instance ID. You must establish a VPC endpoint and attach the necessary permissions to instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      }
    ],
    "answers": {
      "67357122": [
        "a"
      ],
      "67357166": [
        "d"
      ],
      "67357174": [
        "b"
      ],
      "75949068": [
        "b",
        "c"
      ],
      "82921340": [
        "a"
      ],
      "82921342": [
        "a"
      ],
      "82921344": [
        "a"
      ],
      "82921358": [
        "b"
      ],
      "82921440": [
        "a"
      ],
      "115961499": [
        "a"
      ],
      "115961501": [
        "b"
      ],
      "134588397": [
        "c"
      ],
      "134588437": [
        "a"
      ],
      "134588461": [
        "b"
      ],
      "138248103": [
        "b"
      ],
      "138248105": [
        "d"
      ],
      "138248141": [
        "c"
      ],
      "138248169": [
        "d"
      ],
      "138248201": [
        "b",
        "c"
      ],
      "138248211": [
        "b"
      ]
    }
  },
  {
    "id": "1770326213331",
    "date": "2026-02-05T21:16:53.331Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 18,
    "incorrect": 2,
    "unanswered": 0,
    "total": 20,
    "percent": 90,
    "duration": 4136170,
    "questions": [
      {
        "id": 75949164,
        "correct_response": [
          "b",
          "d"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is deploying a new application that uses Amazon EC2 instances for the web tier and a MySQL database for the database tier. An Application Load Balancer (ALB) will be used in front of the web tier. The company requires an RPO of 2 hours and an RTO of 10 minutes for the solution.</p><p>Which combination of deployment strategies will meet these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Create two Amazon Aurora clusters spread across two Regions. Use AWS Database Migration Service (AWS DMS) to synchronize changes.</p>",
            "<p>Create an Amazon Aurora global database in two Regions for the database tier. In the event of a failure, promote the secondary Region to take on read/write responsibilities.</p>",
            "<p>Create an Amazon Aurora multi-master cluster across multiple Regions for the database tier. Configure the database in an active-active mode across the Regions.</p>",
            "<p>Deploy the application in two Regions and create Amazon Route 53 failover-based routing records pointing to the ALB in each Regions. Enable health checks for the records.</p>",
            "<p>Deploy the application in two Regions and create Amazon Route 53 latency-based routing records pointing to the ALB in each Regions. Enable health checks for the records.</p>"
          ],
          "explanation": "<p>To meet the RTO and RPO requirements the best solution for the database tier is to use Amazon Aurora global database. This solution provides replication from the Aurora storage layer across Regions. The reader endpoint in the secondary endpoint can be promoted in the event of a DR scenario to be the main database which takes on read/write responsibilities.</p><p>For the web tier this can be placed behind ALBs in each Region. Amazon Route 53 failover-based routing policies with health checks should be created. These records will point to the ALBs in each Region and if the health checks fail in the primary Region, automatic failover to the secondary Region will occur.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-10-42-d7b7624f7e1003a9bd3098d979fb2910.jpg\"></p><p><strong>CORRECT: </strong>\"Create an Amazon Aurora global database in two Regions for the database tier. In the event of a failure, promote the secondary Region to take on read/write responsibilities\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Deploy the application in two Regions and create Amazon Route 53 failover-based routing records pointing to the ALB in each Regions. Enable health checks for the records\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create two Amazon Aurora clusters spread across two Regions. Use AWS Database Migration Service (AWS DMS) to synchronize changes\" is incorrect.</p><p>The better solution is to use Aurora Global Database as this will simplify the failover process which can be instantiated through a few API calls. Also, this replication uses Aurora replication with low latency and the Aurora reader endpoint can also be utilized in the second Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Aurora multi-master cluster across multiple Regions for the database tier. Configure the database in an active-active mode across the Regions\" is incorrect.</p><p>It is not possible to create multi-master clusters across Regions, they work within a Region only.</p><p><strong>INCORRECT:</strong> \"Deploy the application in two Regions and create Amazon Route 53 latency-based routing records pointing to the ALB in each Regions. Enable health checks for the records\" is incorrect.</p><p>Failover routing policy records should be created, not latency routing records. With latency records users closer to the secondary Region will be directed there but the DB layer is not in read/write mode except in a DR scenario.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>"
        }
      },
      {
        "id": 134588397,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has a PROD, DEV, and TEST environment in its software development department, each contains hundreds of EC2 instances and other AWS services. There was a series of security patches that have been released on the official Ubuntu operating system for a critical flaw that was recently discovered. Although this is an urgent matter, there is no guarantee that these patches will be bug-free and production-ready. This is why a DevOps engineer was instructed to immediately patch all of their affected EC2 instances in all the environments, except for the PROD environment. The EC2 instances in their PROD environment will only be patched after the initial patches have been verified to work effectively in their non-PROD environments. Each environment also has different baseline patch requirements that you will need to satisfy. </p><p>How should the DevOps engineer perform this task with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Develop various shell scripts for each environment that specifies which patch will serve as its baseline. Tag each instance based on its environment, business unit, and operating system. Add the Amazon EC2 instances into Target Groups using the AWS Systems Manager Run Command and then execute the script corresponding to each Target Group.</p>",
            "<p>Set up a new patch baseline in AWS Systems Manager Patch Manager for each environment. Tag each Amazon EC2 instance based on its operating system. Categorize EC2 instances based on their tags using Patch Groups. Apply the patches specified in their corresponding patch baseline to each Patch Group. Use Patch Compliance to ensure that the patches have been installed correctly. Using AWS Config, record all of the changes to patch and association compliance statuses.</p>",
            "<p>Tag each instance based on its environment, business unit, and operating system. Set up a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize each Amazon EC2 instance based on its tags using Patch Groups. Apply the required patches specified in the corresponding patch baseline to each Patch Group.</p>",
            "<p>Use the AWS Systems Manager Maintenance Windows to set up a scheduled maintenance period for each environment, where the period is after business hours so as not to affect daily operations. The Systems Manager will execute a cron job that will install the required patches for each Amazon EC2 instance in each environment during the maintenance period. Use the Systems Manager Managed Instances to verify that your environments are fully patched and compliant.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type.</p><p><em>Patch Manager</em> uses patch baselines<em>,</em> which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. For each auto-approval rule that you create, you can specify an auto-approval delay. This delay is the number of days to wait after the patch was released before the patch is automatically approved for patching.</p><p><img src=\"https://media.tutorialsdojo.com/public/AWS-Systems-Manager-Patch-Manager_3AUG2023.png\"></p><p>A patch group is an optional means of organizing instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested.</p><p><img src=\"https://media.tutorialsdojo.com/public/patch-groups-how-it-works_3AUG2023.png\"></p><p>You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: <code><strong>Patch Group</strong></code>. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution.</p><p>Hence, the correct answer is: <strong>Tag each instance based on its environment, business unit, and operating system. Set up a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize each Amazon EC2 instance based on its tags using Patch Groups. Apply the required patches specified in the corresponding patch baseline to each Patch Group.</strong></p><p>The option that says: <strong>Develop various shell scripts for each environment that specifies which patch will serve as its baseline. Tag each instance based on its environment, business unit, and operating system. Add the Amazon EC2 instances into Target Groups using the AWS Systems Manager Run Command and then execute the script corresponding to each Target Group</strong> is incorrect as this option takes more effort to perform because you are using Systems Manager Run Command instead of Patch Manager. The Run Command service enables you to automate common administrative tasks and perform ad hoc configuration changes at scale, however, it takes a lot of effort to implement this solution. You can use Patch Manager instead to perform the task required by the scenario since you need to perform this task with the least amount of effort.</p><p>The option that says: <strong>Set up a new patch baseline in AWS Systems Manager Patch Manager for each environment. Tag each Amazon EC2 instance based on its operating system. Categorize EC2 instances based on their tags using Patch Groups. Apply the patches specified in their corresponding patch baseline to each Patch Group. Use Patch Compliance to ensure that the patches have been installed correctly. Using AWS Config, record all of the changes to patch and association compliance statuses </strong>is incorrect because you should be tagging instances based on the environment and its OS type in which they belong and not just its OS type. This is because the type of patches that will be applied varies between the different environments. With this option, the Ubuntu EC2 instances in <strong>all</strong> of your environments, including in production, will automatically be patched.</p><p>The option that says: <strong>Use the AWS Systems Manager Maintenance Windows to set up a scheduled maintenance period for each environment, where the period is after business hours so as not to affect daily operations. The Systems Manager will execute a cron job that will install the required patches for each Amazon EC2 instance in each environment during the maintenance period. Use the Systems Manager Managed Instances to verify that your environments are fully patched and compliant </strong>is incorrect because this is not the simplest way to address the issue using AWS Systems Manager. The AWS Systems Manager Maintenance Windows feature lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Each Maintenance Window has a schedule, a maximum duration, a set of registered targets (the instances that are acted upon), and a set of registered tasks. Although this solution may work, it entails a lot of configuration and effort to implement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.htmll</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248121,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company uses a fleet of Linux and Windows servers for its enterprise applications. An automated daily check of each golden AMI owned is needed to monitor the latest Common Vulnerabilities and Exposures (CVE) using Amazon Inspector.</p><p>Which among the options below is the MOST suitable solution that should be implemented?</p>",
          "answers": [
            "<p>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI, install the Inspector agent, and add a custom tag for tracking. Configure the Step Functions to trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the Step Functions every day using an Amazon EventBridge rule.</p>",
            "<p>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will call the Inspector API action <code>StartAssessmentRun</code> after the EC2 instances have booted up, which will run the assessment against all instances with the custom tag you added. Trigger the function every day using an Amazon CloudWatch Alarms.</p>",
            "<p>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the function every day using an Amazon EventBridge rule.</p>",
            "<p>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI and install the Inspector agent. Configure the Step Functions to trigger the Inspector assessment for all instances right after the EC2 instances have booted up. Configure the Step Functions to run daily using the Event Bus in Amazon EventBridge.</p>"
          ],
          "explanation": "<p><strong>Amazon Inspector</strong> tests the network accessibility of your Amazon EC2 instances and the security state of your applications that run on those instances. Amazon Inspector assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings that is organized by level of severity.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-11-07_05-00-58-e155c91caae56656552ec8b5ffed377e.png\"></p><p>With Amazon Inspector, you can automate security vulnerability assessments throughout your development and deployment pipelines or for static production systems. This allows you to make security testing a regular part of development and IT operations.</p><p>Amazon Inspector also offers predefined software called an <em>agent</em> that you can optionally install in the operating system of the EC2 instances that you want to assess. The agent monitors the behavior of the EC2 instances, including network, file system, and process activity. It also collects a wide set of behavior and configuration data (telemetry).</p><p>If you want to set up a recurring schedule for your assessment, you can configure your assessment template to run automatically by creating a Lambda function using the AWS Lambda console. Alternatively, you can select the <em>\"Set up recurring assessment runs once every &lt;number_of_days&gt;, starting now\"</em> checkbox and specify the recurrence pattern (number of days) using the up and down arrows.</p><p>Hence, the correct answer is: <strong>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI, install the Inspector agent, and add a custom tag for tracking. Configure the Step Functions to trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the Step Functions every day using an Amazon EventBridge rule.</strong></p><p>The option that says: <strong>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will call the Inspector API action </strong><code><strong>StartAssessmentRun</strong></code><strong> after the EC2 instances have booted up, which will run the assessment against all instances with the custom tag you added. Trigger the function every day using an Amazon CloudWatch Alarms</strong><em> </em>is incorrect because you can't trigger a Lambda function on a regular basis using CloudWatch Alarms. You have to use Amazon EventBridge instead. Moreover, you typically have to install the Amazon Inspector agent to the EC2 instance in order to run the security assessments.</p><p>The option that says: <strong>Use an AWS Lambda function to launch an Amazon EC2 instance for each operating system from the golden AMI and add a custom tag for tracking. Create another Lambda function that will trigger the Inspector assessment for all instances with the custom tag you added right after the EC2 instances have booted up. Trigger the function every day using an Amazon EventBridge rule<em> </em></strong>is incorrect because, in order for this solution to work, you have to install the Amazon Inspector agent first to the EC2 instance before you can run the security assessments.</p><p>The option that says: <strong>Use AWS Step Functions to launch an Amazon EC2 instance for each operating system from the golden AMI and install the Inspector agent. Configure the Step Functions to trigger the Inspector assessment for all instances right after the EC2 instances have booted up. Configure the Step Functions to run daily using the Event Bus in Amazon EventBridge</strong> is incorrect because the Event bus is primarily used to accept events from AWS services, other AWS accounts, and PutEvents API calls. You should also add a custom tag to the EC2 instance in order to run the Amazon Inspector assessments.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/set-up-amazon-inspector/\">https://aws.amazon.com/premiumsupport/knowledge-center/set-up-amazon-inspector/</a></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_assessments.html#assessment_runs-schedule\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_assessments.html#assessment_runs-schedule</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/\">https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/</a></p><p><br></p><p><strong>Check out this Amazon Inspector Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-inspector/?src=udemy\">https://tutorialsdojo.com/amazon-inspector/</a></p>"
        }
      },
      {
        "id": 99528223,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A data intelligence and analytics company has implemented a CI/CD pipeline using AWS CodePipeline which takes code from an AWS CodeCommit repository and then builds it using AWS CodeBuild. During the deploy stage, the application is deployed onto an Amazon ECS cluster. During deployment, the application is only partly updated on some ECS tasks which are running an older version of the image.</p><p>A DevOps engineer investigated and found that terminating the task or clearing the local Docker cache fixes the issue, but a more robust solution is required that provides visibility and identification to track where container images are deployed. Also, the start-up time of the containers needs to be optimized.</p><p>Which actions should the DevOps engineer take to achieve these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Move all the dependencies into a single image and pull them from a single container registry.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task.</p>",
            "<p>Move secondary dependencies to be downloaded at application startup rather than including them within a static container image.</p>"
          ],
          "explanation": "<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. The IMAGEID property is the SHA digest for the Docker image used to start the container.</p><p>Ideally, a container image is intended to be a complete snapshot of everything that the application requires to function. With a complete container image, the application could be run by downloading one container image from one place.</p><p>There is no need to download several separate pieces from different locations. Therefore, as a best practice, store all application dependencies as static files inside the container image. This will improve performance and start up time.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-00-17-d1b2e063cff66b89f7caee81435b0971.jpg\"><p><strong>CORRECT: </strong>\"Move all the dependencies into a single image and pull them from a single container registry\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time\" is incorrect.</p><p>The exact image tag needs to be pulled because the latest tag will only pull the image that was pushed last.</p><p><strong>INCORRECT:</strong> \"After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task\" is incorrect.</p><p>Using Lambda to SSH onto instances is not a workable solution. SSM Run Command may be better but it\u2019s also not ideal.</p><p><strong>INCORRECT:</strong> \"Move secondary dependencies to be downloaded at application startup rather than including them within a static container image\" is incorrect.</p><p>This would delay the image spin up time rather than improving it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html\">https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949176,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 4",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A global multi-player gaming application that was deployed in Europe now needs to be extended globally. Availability of the application must be extremely high, and latency should be low. The application is hosted on Amazon EC2 instances. It is anticipated that the traffic will be unevenly distributed with a few locations having much more traffic than others.</p><p>Which of the below routing strategies would be best fit considering above parameters?</p>",
          "answers": [
            "<p>Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geoproximity-based routing record. Point the record to each of your load balancers.</p>",
            "<p>Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geolocation-based routing record. Point the record to each of your load balancers.</p>",
            "<p>Deploy Amazon CloudFront in front of the instances to cache requests and deliver content from Edge Locations globally.</p>",
            "<p>Utilize Route 53 latency-based routing and deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions.</p>"
          ],
          "explanation": "<p>Geo-proximity Routing - Lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources.</p><p>You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource.</p><p>The catch in the question is a few regions have heavier traffic load than the others so a bias needs to be configured and geoproximity routing is a better fit for this solution.</p><p><strong>CORRECT: </strong>\"Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geoproximity-based routing record. Point the record to each of your load balancers\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geolocation-based routing record. Point the record to each of your load balancers\" is incorrect.</p><p>If the discrepancy between traffic volume was unknown, this would also been a correct option.</p><p><strong>INCORRECT:</strong> \"Utilize Route 53 latency-based routing and deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions\" is incorrect.</p><p>Latency-based routing is based on latency measurements performed over a period, and the measurements reflect changes in network connectivity and routing.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon CloudFront in front of the instances to cache requests and deliver content from Edge Locations globally\" is incorrect.</p><p>It is unlikely that a CDN can be used to cache a dynamic gaming application. A load balanced deployment of auto scaling EC2 instances is needed to run the application and ensure availability and performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>"
        }
      },
      {
        "id": 134588419,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A rental payment startup has developed a web portal that enables users to pay for their rent using both their credit and debit cards online. They are using a third-party payment service to handle and process credit card payments on their platform since the portal is not fully compliant with the Payment Card Industry Data Security Standard (PCI DSS). The application is hosted in an Auto Scaling group of Amazon EC2 instances, which are launched in private subnets behind an internal-facing Application Load Balancer. The system must connect to an external payment service over the Internet to complete the transaction for every user payment. </p><p>As a DevOps Engineer, what would be the MOST suitable option to implement to satisfy the above requirement?</p>",
          "answers": [
            "<p>Using a NAT Gateway, route credit card payment requests from the EC2 instances to the external payment service. Associate an Elastic IP address to the NAT Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway.</p>",
            "<p>In the Security Group, whitelist the Public IP of the Internet Gateway. Route the user payment requests through the Internet Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the Internet Gateway.</p>",
            "<p>Use the Application Load Balancer to route payment requests from the application servers through the Customer Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the Customer Gateway.</p>",
            "<p>Develop a shell script to automatically assign Elastic IP addresses to the Amazon EC2 instances. Add the script in the User Data of the EC2 instances, which automatically adds the Elastic IP address to the Network Access List upon launch. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to a VPC Endpoint.</p>"
          ],
          "explanation": "<p>You can use a <strong>network address translation (NAT) gateway</strong> to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.</p><p>To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed once you associate it with the NAT Gateway. After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet.</p><p><img src=\"https://media.tutorialsdojo.com/public/nat-gateway-diagram_6AUG2023.png\"></p><p>Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone. You have a limit on the number of NAT gateways you can create in an Availability Zone.</p><p>Remember the difference between NAT Instance and NAT Gateways. A NAT Instance needs to use a script to manage failover between instances while this is done automatically in NAT gateways.</p><p>Hence, the correct answer is: <strong>Using a NAT Gateway, route credit card payment requests from the EC2 instances to the external payment service. Associate an Elastic IP address to the NAT Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway.</strong></p><p>The option that says: <strong>In the Security Group, whitelist the Public IP of the Internet Gateway. Route the user payment requests through the Internet Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the Internet Gateway</strong> is incorrect because you cannot whitelist an IP address using a Security Group. You should use a NAT Gateway instead to enable instances in a private subnet to connect to the Internet.</p><p>The option that says: <strong>Use the Application Load Balancer to route payment requests from the application servers through the Customer Gateway. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to the Customer Gateway</strong> is incorrect because what you need here is a NAT Gateway and not a Customer Gateway. The use of an ELB is also not suitable for this scenario.</p><p>The option that says: <strong>Develop a shell script to automatically assign Elastic IP addresses to the Amazon EC2 instances. Add the script in the User Data of the EC2 instances, which automatically adds the Elastic IP address to the Network Access List upon launch. Update the route table associated with one or more of your private subnets to point Internet-bound traffic to a VPC Endpoint</strong> is incorrect because neither the required NAT Gateway nor the NAT instance is mentioned in this option. Moreover, a VPC endpoint simply enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink. This doesn't allow you to connect to the public Internet.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-comparison.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>"
        }
      },
      {
        "id": 143860749,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is deploying an application in four AWS Regions across North America, Europe, and Asia. The application will be used by millions of users. The application must allow users to submit data through the application layer in each Region and have it saved in a low-latency database layer. The company also must ensure that the data can be read through the application layer in each Region.</p><p>Which solution will meet these requirements with the LOWEST latency of reads and writes?</p>",
          "answers": [
            "<p>Create a table in Amazon DynamoDB and enable global tables in each of the four Regions.</p>",
            "<p>Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions.</p>",
            "<p>Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions.</p>",
            "<p>Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions.</p>"
          ],
          "explanation": "<p>Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p><p>This is the only workable solution in the list that provides both reads and writes in each Region that are replicated across the other Regions. This is also the best solution as it provides low latency reads and writes.</p><p><strong>CORRECT: </strong>\"Create a table in Amazon DynamoDB and enable global tables in each of the four Regions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions\" is incorrect.</p><p>This solutions does not provide local writes in each Region as the Read Replicas cannot be written to. Therefore, this solution only offers low latency reads in each Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions\" is incorrect.</p><p>Replication groups in ElastiCache are used within a Region and not across Regions so this solution does not work.</p><p><strong>INCORRECT:</strong> \"Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions\" is incorrect.</p><p>This solution provides a database in each Region, there is no mechanism for replication. You can create replica instances in different Regions but that would only provide low latency reads (as with RDS), and not low latency writes.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>"
        }
      },
      {
        "id": 99528229,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.</p><p>Which actions should the engineer take?</p>",
          "answers": [
            "<p>Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL.</p>",
            "<p>Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield.</p>",
            "<p>Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL.</p>",
            "<p>Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty.</p>"
          ],
          "explanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website.</p><p>AWS Web Application Firewall (WAF) can be used to protect the static website from web exploits. To do this, the engineer should create an AWS WAF web ACL.</p><p><strong>CORRECT: </strong>\"Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield\" is incorrect.</p><p>Both EC2 and ElastiCache use EC2 instances so they are not serverless services. AWS Shield protects against DDoS attacks but AWS WAF should be used for web exploits.</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL\" is incorrect.</p><p>ECS with Fargate is serverless but you cannot use a Redis cluster to cache content served by a task running on Amazon ECS. ElastiCache is used for caching database content.</p><p><strong>INCORRECT:</strong> \"Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty\" is incorrect.</p><p>Lambda with a REST API as the frontend is definitely possible, however the static website on S3 would be easier to deploy and manage. AWS GuardDuty does not protect against web exploits.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>"
        }
      },
      {
        "id": 99528227,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application is being deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The security team requires that the traffic is secured with SSL/TLS certificates. Protection against common web exploits must also be implemented. The solution should not have a performance impact on the EC2 instances.</p><p>What steps should be taken to secure the web application? (Select TWO.)</p>",
          "answers": [
            "<p>Create an AWS WAF web ACL and attach it to the ALB.</p>",
            "<p>Add an SSL/TLS certificate to a secure listener on the ALB.</p>",
            "<p>Install SSL/TLS certificates on the EC2 instances.</p>",
            "<p>Configure Server-Side Encryption with KMS managed keys.</p>",
            "<p>Enable EBS encryption for the EC2 volumes to encrypt all traffic.</p>"
          ],
          "explanation": "<p>To secure the traffic in transit an SSL/TLS certificate should be attached to a secure listener on the ALB. This will not affect the performance of the EC2 instances as the encryption takes place only between the client and the ALB. The certificate can be issued through AWS Certificate Manager (ACM).</p><p>The AWS Web Application Firewall (AWS WAF) protects against common web exploits. The company can create a web ACL with a rule and action and then attach it to the ALB. This will protect against web exploits.</p><p><strong>CORRECT: </strong>\"Add an SSL/TLS certificate to a secure listener on the ALB\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL and attach it to the ALB\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install SSL/TLS certificates on the EC2 instances\" is incorrect.</p><p>Encryption on the EC2 instances would impact the performance of those instances.</p><p><strong>INCORRECT:</strong> \"Configure Server-Side Encryption with KMS managed keys\" is incorrect.</p><p>This is not relevant to in transit encryption, this is used to encrypt data at rest on services such as Amazon S3.</p><p><strong>INCORRECT:</strong> \"Enable EBS encryption for the EC2 volumes to encrypt all traffic\" is incorrect.</p><p>EBS encryption is used for encrypting data at rest. The question requires encryption using SSL/TLS certificates which is encryption in transit.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\">https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\">https://aws.amazon.com/elasticloadbalancing/application-load-balancer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 138248237,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The Development team of a leading IT consultancy company would like to add a manual approval action before their new application versions are deployed to their production environment. The approval action must be strictly enforced even if the unit and integration tests are all successful. They have set up a pipeline using CodePipeline to orchestrate the workflow of their continuous integration and continuous delivery processes. The new versions of the application are built using CodeBuild and are deployed to a fleet of Amazon EC2 instances using CodeDeploy. </p><p>Which of the following provides the SIMPLEST and the MOST cost-effective solution?</p>",
          "answers": [
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</p>",
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>"
          ],
          "explanation": "<p>You can automate your release process by using AWS CodePipeline to test your code and run your builds with CodeBuild. You can create reports in CodeBuild that contain details about tests that are run during builds.</p><p>You can create tests such as unit tests, configuration tests, and functional tests. The test file format can be JUnit XML or Cucumber JSON. Create your test cases with any test framework that can create files in one of those formats (for example, Surefire JUnit plugin, TestNG, and Cucumber). To create a test report, you add a report group name to the buildspec file of a build project with information about your test cases. When you run the build project, the test cases are run and a test report is created. You do not need to create a report group before you run your tests. If you specify a report group name, CodeBuild creates a report group for you when you run your reports. If you want to use a report group that already exists, you specify its ARN in the buildspec file.</p><p>In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p><img src=\"https://media.tutorialsdojo.com/public/pipeline.png\"></p><p>If the action is approved, the pipeline execution resumes. If the action is rejected\u2014or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping\u2014the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p>Hence, the correct answer is: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</strong></p><p>The option that says: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. You can just simply set up a manual approval action instead of creating a custom action. That takes a lot of effort to configure including the development of a custom job worker.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. It is tedious to automatically perform the unit and integration tests using AWS Step Functions. You can just use CodeBuild to handle all of the tests.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. This solution entails an additional burden to install, configure and launch a third-party CI/CD tool in Amazon EC2. A more simple solution is to just use CodeBuild for tests.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p>"
        }
      },
      {
        "id": 99528219,
        "correct_response": [
          "a",
          "e"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company runs many different workloads across hundreds of Amazon EC2 instances. The DevOps team requires that all instances have standard configurations. These configurations include standard logging, metrics, security assessments, and weekly patching.</p><p>Which combination of actions meets these requirements with the most operational efficiency? (Select TWO.)</p>",
          "answers": [
            "<p>Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances.</p>",
            "<p>Use Amazon Inspector to install and manage AWS Systems Manager, AWS OpsWorks, and the Amazon CloudWatch agent on all instances.</p>",
            "<p>Use AWS OpsWorks and execute custom recipes to patch the EC2 instances. Use AWS Systems Manager Run Command to initiate regular Amazon Inspector assessment runs.</p>",
            "<p>Use AWS CloudFormation with change sets to deploy AMIs patched by Systems Manager Patch Manager to existing EC2 instances. Use AWS Config to require regular Amazon Inspector assessment runs.</p>",
            "<p>Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon EventBridge to schedule Amazon Inspector assessment runs.</p>"
          ],
          "explanation": "<p>The most operationally efficient solution is to use AWS Systems Manager patch manager for patching and Amazon Inspector for assessing the security status of the instances. The Amazon CloudWatch agent can also be installed on the instances to get enhanced metrics and logging.</p><p>Systems Manager can be used to install and manage the other components. You must have the Systems Manager agent installed on the instances and the instance profile attached must have permissions to Systems Manager.</p><p>The patching process can be implemented by using Systems Manager Run Command to schedule tasks that deploy the updates using Systems Manager Patch Manager. EventBridge is ideal for scheduling the Amazon Inspector assessment runs.</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon EventBridge to schedule Amazon Inspector assessment runs\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector to install and manage AWS Systems Manager, AWS OpsWorks, and the Amazon CloudWatch agent on all instances\" is incorrect.</p><p>Inspector is not a service that can be used to install anything, it simply runs assessments against your instances and lets you know if there are any security concerns.</p><p><strong>INCORRECT:</strong> \"Use AWS OpsWorks and execute custom recipes to patch the EC2 instances. Use AWS Systems Manager Run Command to initiate regular Amazon Inspector assessment runs\" is incorrect.</p><p>OpsWorks would be less operationally efficient for this purpose. Systems Manager Patch Manager is a better way to deploy patches as it is designed for this purpose and provides many features to simplify and optimize the patching process.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation with change sets to deploy AMIs patched by Systems Manager Patch Manager to existing EC2 instances. Use AWS Config to require regular Amazon Inspector assessment runs\" is incorrect.</p><p>You cannot deploy patched AMIs to existing instances. AMIs are used to deploy the instance initially but you cannot redeploy the AMI without wiping the instance state. Config also cannot be used to enforce an Amazon Inspector assessment run.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p><p><a href=\"https://digitalcloud.training/amazon-inspector/\">https://digitalcloud.training/amazon-inspector/</a></p>"
        }
      },
      {
        "id": 75949082,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A financial company has applications hosted in multiple AWS accounts which are managed by AWS organizations. A security audit was conducted to address any potential security breaches and implement best practices. The security audit report recommended that all security related logging and security findings are collect in a centralized security account.</p><p>How can this be achieved?</p>",
          "answers": [
            "<p>Use Amazon GuardDuty in each organization to detect the attacks on EC2 instances. Specify an organization as the GuardDuty administrator. Create a CloudWatch rule in the GuardDuty administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Firehose to send findings to a designated S3 bucket.</p>",
            "<p>Use Amazon Macie in each organization to detect the attacks on EC2 instances. Specify an organization as the Macie administrator. Create a CloudWatch rule in the Macie administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Firehose to send findings to a designated S3 bucket.</p>",
            "<p>Use Amazon GuardDuty in each account to detect the attacks on EC2 instances. Create a CloudWatch rule in the GuardDuty administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Streams to send findings to a designated S3 bucket.</p>",
            "<p>Use Amazon Inspector in each organization to detect the attacks on EC2 instances. Specify an organization as the Inspector administrator. Create a CloudWatch rule in the Inspector administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Firehose to send findings to a designated S3 bucket.</p>"
          ],
          "explanation": "<p>GuardDuty analyzes and processes VPC flow log, AWS CloudTrail event log, and DNS log data sources. You don\u2019t need to manually manage these data sources because the data is automatically leveraged and analyzed when you activate GuardDuty.</p><p>For example, GuardDuty consumes VPC Flow Log events directly from the VPC Flow Logs feature through an independent and duplicative stream of flow logs. As a result, you don\u2019t incur any operational burden on existing workloads.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-10-01-c38c34b5c636f2d9f839cf769fa2e58c.jpg\">"
        }
      },
      {
        "id": 138248125,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company wants to implement a continuous delivery workflow that will facilitate the process of source code promotion in the development, staging, and production environments in AWS. In the event of system degradation or failure, the company should also have the ability to roll back the recent deployment of the application.</p><p>Which of the following CI/CD designs is the MOST suitable one to implement and will incur the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production.</p>",
            "Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy."
          ],
          "explanation": "<p>A repository is the fundamental version control object in GitHub or GitLab. It\u2019s where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. In GitHub and GitLab, you can set up notifications to configure notifications so that repository users receive emails about events (for example, another user commenting on code). You can change default settings to customize the default settings for your repository. You can browse contents to easily navigate and browse the contents of your repository. You can create triggers to set up triggers so that code pushes or other events trigger actions, such as emails or code functions. You can also configure a repository on your local computer (a local repo) to push your changes to more than one repository.</p><p><img src=\"https://media.tutorialsdojo.com/public/setup-cicd-pipeline-100324.png\"></p><p>In designing your CI/CD process in AWS, you can use a single repository in GitHub (or GitLab) and create different branches for development, master, and release. You can use CodeBuild to build your application and run tests to verify that all of the core features of your application are working. For deployment, you can either select an in-place or blue/green deployment using CodeDeploy.</p><p>Hence, the correct answer is: <strong>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</strong></p><p>The option that says: <strong>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy </strong>is incorrect because creating a separate repository for each developer is absurd since they can simply clone the code instead. A single repository will suffice in this scenario which can have several branches for development and production deployment purposes.</p><p>The option that says: <strong>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production</strong> is incorrect because you don't need to create two repositories for one application. Instead, you can just create at least two different branches to separate your development and production code.</p><p>The option that says: <strong>Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy</strong> is incorrect because Amazon ECR is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. This is not a suitable service to be used to store your application code.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/devops/continuous-integration/\">https://aws.amazon.com/devops/continuous-integration/</a></p><p><a href=\"https://aws.amazon.com/devops/continuous-delivery/\">https://aws.amazon.com/devops/continuous-delivery/</a></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html</a></p><p><br></p><p><strong>Check out this AWS CodePipelin Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 99528239,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company runs a Java application in an on-premises data center. The application is not currently containerized but the company plans to migrate it to the AWS Cloud and modernize the application into a containerized deployment. A CI/CD pipeline should also be created to automate application updates.</p><p>Which solution can a DevOps engineer use to meet all these requirements?</p>",
          "answers": [
            "<p>Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates.</p>",
            "<p>Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy.</p>",
            "<p>Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository.</p>",
            "<p>Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy.</p>"
          ],
          "explanation": "<p>App2Container (A2C) is a command-line tool for modernizing .NET and Java applications into containerized applications. A2C analyzes and builds an inventory of all applications running in virtual machines on-premises or in the cloud.</p><p>With A2C you can simply select the application you want to containerize, and A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the ECS task and Kubernetes pod definitions.</p><p>App2Container creates CI/CD pipelines for Amazon DevOps services such as CodeBuild and CodeDeploy to build and deploy containers. If you have existing CI/CD tooling (for example, Azure DevOps and Jenkins), then you can integrate A2C provided artifacts \u2013 dockerfile, ECS task definition, EKS deployment YAML - into your existing CI/CD workflows.</p><p><strong>CORRECT: </strong>\"Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates\" is incorrect.</p><p>AWS Copilot is a command line interface (CLI) that you can use to quickly launch and manage containerized applications on AWS. It simplifies running applications on Amazon Elastic Container Service (ECS), AWS Fargate, and AWS App Runner. It is not able to inventory and analyze existing applications or migrate them to containers.</p><p><strong>INCORRECT:</strong> \"Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository\" is incorrect.</p><p>AWS Proton is used to create application stack templates and build infrastructure for applications using the CLI, API, or UI. It is not used for refactoring applications or migrating them to containerized deployments.</p><p><strong>INCORRECT:</strong> \"Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy\" is incorrect.</p><p>Amazon AppFlow is used to automate bi-directional data flows between SaaS applications and AWS services. It is not used to migrate applications between on-premises deployments and Amazon ECS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/app2container/features/\">https://aws.amazon.com/app2container/features/</a></p>"
        }
      },
      {
        "id": 82921450,
        "correct_response": [
          "a",
          "c",
          "e"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>As the Lead DevOps Engineer at a retail company, you have a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. You must collect the logs before an instance is terminated to perform log analytics later on. It's also necessary to collect all the access logs. The analysis of these logs should be performed at a minimal cost, and only need to be run from time to time.</p>\n\n<p>Which of the following options would you suggest to implement the MOST cost-optimal solution for this requirement? (Select three)</p>\n",
          "answers": [
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</p>",
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</p>",
            "<p>Enable Access Logs at the Application Load Balancer level</p>",
            "<p>Enable Access Logs at the Target Group level</p>",
            "<p>Analyze the logs using AWS Athena</p>",
            "<p>Analyze the logs using an EMR cluster</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</strong></p>\n\n<p>Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. When a scale-in event occurs, the terminating instance is first deregistered from the load balancer and while the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q71-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p>For the given use-case, you can configure a lifecycle hook to invoke the CloudWatch Event rule to trigger a Lambda function that launches an SSM Run Command to extract the application logs and store them in S3.</p>\n\n<p><strong>Enable Access Logs at the Application Load Balancer level</strong></p>\n\n<p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p>\n\n<p><strong>Analyze the logs using AWS Athena</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don\u2019t even need to load your data into Athena, it works directly with data stored in S3. You can analyze the access logs stored in S3 via Athena.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</strong> - CloudWatch Logs agent can only be used for continuous log streaming, and NOT for a one-time log extract to S3.</p>\n\n<p><strong>Enable Access Logs at the Target Group level</strong> - Please note that access logs are enabled at the ALB level and NOT at the target group level.</p>\n\n<p><strong>Analyze the logs using an EMR cluster</strong> - Analyzing logs at a low cost and in a serverless fashion should be done using AWS Athena. EMR clusters are usually long-running and cost a lot of money, and don't have serverless scaling capabilities.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n"
        }
      },
      {
        "id": 82921440,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.</p>\n\n<p>Which of the following options would you suggest? (Select two)</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</p>",
            "<p>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</p>",
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>Create a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p><strong>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>Understand how a pipeline execution state change rule works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Here we are only interested in pipeline failures, so we need to choose <code>CodePipeline Pipeline Execution State Change</code>.\nFinally, CloudWatch Event rules do not support Slack as a target, therefore we must create a Lambda function for it.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i2.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</strong> - CloudWatch Event rules do not support Slack as a target, so this option is incorrect.</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>**\nCreate a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>Here we are only interested in pipeline failures, so we just need to choose <code>CodePipeline Pipeline Execution State Change</code>. Therefore both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n"
        }
      },
      {
        "id": 138248243,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An online data analytics application is launched to 12 On-Demand EC2 instances across three Availability Zones using a golden AMI in AWS. Each instance has only 10% utilization after business hours but increases to 30% utilization during peak hours. There are also some third-party applications that use the application from all over the globe with no specific schedule. In the morning, there is always a sudden CPU utilization increase on the EC2 instances due to the number of users logging in to use the application. However, its CPU utilization usually stabilizes after a few hours. A DevOps Engineer has been instructed to reduce costs and improve the overall reliability of the system.</p><p>Which among the following options provides the MOST suitable solution in this scenario?</p>",
          "answers": [
            "<p>Set up two Amazon Eventbridge rules and two Lambda functions. Configure each Amazon Eventbridge rule to invoke a Lambda function and regularly run before and after the peak hours. The first function should stop nine instances after the peak hours end while the second function should restart the nine instances before the business day begins</p>",
            "<p>Set up an Auto Scaling group using the golden AMI with a scaling action based on the CPU Utilization average. Configure a scheduled action for the group to adjust the minimum number of Amazon EC2 instances to three after business hours end, and reset to six before business hours begin.</p>",
            "<p>Set up two AWS Config rules and two Lambda functions. Configure each rule to invoke a Lambda function and regularly run before and after the peak hours. The first function should stop nine instances after the peak hours end while the second function should restart the nine instances before the business day begins.</p>",
            "<p>Launch a group of Scheduled Reserved Instances that regularly run before and after peak hours. Integrate Amazon EventBridge and AWS Lambda to regularly stop nine instances after the peak hours every day and restart the nine instances before the business day begins.</p>"
          ],
          "explanation": "<p>Scaling based on a schedule allows you to set your own scaling schedule for predictable load changes. For example, every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date.</p><p>To configure your Auto Scaling group to scale based on a schedule, you create a scheduled action. The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size specified by the scaling action.</p><p>You can create scheduled actions for scaling one time only or for scaling on a recurring schedule.</p><p><img src=\"https://media.tutorialsdojo.com/public/as-basic-diagram.png\"></p><p>Hence, the correct answer is: <strong>Set up an Auto Scaling group using the golden AMI with a scaling action based on the CPU Utilization average. Configure a scheduled action for the group to adjust the minimum number of Amazon EC2 instances to three after business hours end, and reset to six before business hours begin.</strong></p><p>The option that says: <strong>Set up two Amazon Eventbridge rules and two Lambda functions. Configure each Amazon Eventbridge rule to invoke a Lambda function and regularly run before and after the peak hours. The first function should stop nine instances after the peak hours end while the second function should restart the nine instances before the business day begins</strong> is incorrect because you can simply configure a scheduled action for the Auto Scaling group to adjust the minimum number of the available EC2 instances without using Amazon EventBridge or a Lambda function.</p><p>The option that says: <strong>Set up two AWS Config rules and two Lambda functions. Configure each rule to invoke a Lambda function and regularly run before and after the peak hours. The first function should stop nine instances after the peak hours end while the second function should restart the nine instances before the business day begins </strong>is incorrect because using AWS Config is typically not an appropriate service to use in this scenario. A better solution is to configure a scheduled action in the Auto Scaling group.</p><p>The option that says: <strong>Launch a group of Scheduled Reserved Instances that regularly run before and after peak hours. Integrate Amazon EventBridge and AWS Lambda to regularly stop nine instances after the peak hours every day and restart the nine instances before the business day begins</strong> is incorrect. Although your operating costs will be decreased by using Scheduled Reserved Instances, this setup is still not appropriate since the traffic to the portal is not entirely predictable. Take note that there are third-party applications that use the application from all over the globe with no specific schedule. Hence, the use of Scheduled Reserved Instances is not recommended.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p>"
        }
      },
      {
        "id": 138248221,
        "correct_response": [
          "b",
          "e"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is re-architecting its monolithic system to a serverless application in AWS to save on cost. The deployment of the succeeding new version of the application must be initially rolled out to a small number of users first for testing before the full release. If the post-hook tests fail, there should be an easy way to roll back the deployment. The DevOps Engineer was assigned to design an efficient deployment setup that mitigates any unnecessary outage that impacts their production environment.</p><p>As a DevOps Engineer, how should you satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Launch an Application Load Balancer with an Amazon API Gateway private integration. Attach a single target group to the load balancer and select the \"Canary\" routing option which will automatically route incoming traffic to the new version.</p>",
            "<p>Set up one AWS Lambda Function Alias that points to both the current and new versions. Route 20% of incoming traffic to the new version and once it is considered stable, update the alias to route all traffic to the new version.</p>",
            "<p>Create a new record in Route 53 with a Failover routing policy. Configure the primary record to route 20% of incoming traffic to the new version and set the secondary record to route the rest of the traffic to the current version. Once the new version stabilizes, update the primary record to route all traffic to the new version.</p>",
            "<p>Launch a Network Load Balancer with an Amazon API Gateway private integration. Attach two target groups to the load balancer. Configure the first target group with the current version and the second target group with the new version. Configure the load balancer to route 20% of the incoming traffic to the new version and once it becomes stable, detach the first target group from the load balancer.</p>",
            "<p>Set up a canary deployment in Amazon API Gateway that routes 20% of the incoming traffic to the canary release. Promote the canary release to production once the initial tests have passed.</p>"
          ],
          "explanation": "<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Each alias has a unique ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. Event sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. In a resource policy, you can grant permissions for event sources to use your Lambda function. If you specify an alias ARN in the policy, you don't need to update the policy when the function version changes.</p><p>Use routing configuration on an alias to send a portion of traffic to a second function version. For example, you can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version. You can point an alias to a maximum of two Lambda function versions.</p><p><img src=\"https://media.tutorialsdojo.com/public/API_gateway.png\"></p><p>In API Gateway, you create a canary release deployment when deploying the API with <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/link-relation/deployment-create/#canarySettings\">canary settings</a> as an additional input to the <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/link-relation/deployment-create/\">deployment creation</a> operation.</p><p>You can also create a canary release deployment from an existing non-canary deployment by making a <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/link-relation/stage-update/\"><code>stage:update</code></a> request to add the canary settings on the stage.</p><p>When creating a non-canary release deployment, you can specify a non-existing stage name. API Gateway creates one if the specified stage does not exist. However, you cannot specify any non-existing stage name when creating a canary release deployment. You will get an error and API Gateway will not create any canary release deployment.</p><p>Hence, the correct answers are:</p><p><strong>- Set up one AWS Lambda Function Alias that points to both the current and new versions. Route 20% of incoming traffic to the new version and once it is considered stable, update the alias to route all traffic to the new version.</strong></p><p>- <strong>Set up a canary deployment in Amazon API Gateway that routes 20% of the incoming traffic to the canary release. Promote the canary release to production once the initial tests have passed.</strong></p><p>The option that says:<strong><em> </em>Launch an Application Load Balancer with an Amazon API Gateway private integration. Attach a single target group to the load balancer and select the \"Canary\" routing option which will automatically route incoming traffic to the new version</strong> is incorrect because there is no Canary routing option in an Application Load Balancer.</p><p>The option that says: <strong>Launch a Network Load Balancer with an Amazon API Gateway private integration. Attach two target groups to the load balancer. Configure the first target group with the current version and the second target group with the new version. Configure the load balancer to route 20% of the incoming traffic to the new version and once it becomes stable, detach the first target group from the load balancer </strong>is incorrect because the Network Load Balancer does not support weighted target groups, unlike the Application Load Balancer.</p><p>The option that says: <strong>Create a new record in Route 53 with a Failover routing policy. Configure the primary record to route 20% of incoming traffic to the new version and set the secondary record to route the rest of the traffic to the current version. Once the new version stabilizes, update the primary record to route all traffic to the new version </strong>is incorrect because the failover routing policy simply lets you route traffic to a resource when the resource is healthy, or to a different resource when the first resource is unhealthy. This type of routing is not an appropriate setup. A better solution is to use Canary deployment release in API Gateway to deploy the serverless application.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p><p><br></p><p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 82921442,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a financial services company is deploying the flagship application in highly available mode using Elastic Beanstalk which has created an ASG and an ALB. The team has also specified a <code>.ebextensions</code> file to create an associated DynamoDB table. As a DevOps Engineer in the team, you would like to perform an update to the application but you need to make sure the DNS name won't change and that no new resources will be created. The application needs to remain available during the update.</p>\n\n<p>Which of the following options would you suggest to address the given requirements?</p>\n",
          "answers": [
            "<p>Use a rolling update with 20% at a time</p>",
            "<p>Use a blue/green deployment and swap CNAMEs</p>",
            "<p>Use immutable</p>",
            "<p>Use in-place</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a rolling update with 20% at a time</strong></p>\n\n<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, Immutable, and Traffic splitting) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's a scalable environment (you didn't specify the --single option), it uses rolling deployments.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Comparison of deployment method properties:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches (for this requirement, we shall use a batch with 20% of the instances) and deploys the new version of the application to one batch at a time. It leaves the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version. Therefore, for the given use-case, we should use a rolling update, which will keep our ASG, our instances, and ensure our application can still serve traffic.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a blue/green deployment and swap CNAMEs</strong> - In a blue/green deployment, you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. A blue/green deployment would create a new load balancer and ASG, but the CNAME swap would allow us to keep the same DNS name. So it does not meet the requirements for the given use-case.</p>\n\n<p><strong>Use immutable</strong> - Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. So this option does not meet the requirements for the given use-case.</p>\n\n<p><strong>Use in-place</strong> - In-place would not work even though it doesn't create any new resources because your application will be unavailable as all your instances will be updated at the same time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n"
        }
      },
      {
        "id": 138248211,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A CTO of a leading insurance company has recently decided to migrate its online customer portal to AWS. The customers will use the online portal to view the paid insurance premiums and manage accounts. For improved scalability, the application should be hosted in an Auto Scaling group of On-Demand Amazon EC2 instances with a custom Amazon Machine Image (AMI). The same architecture will also be used for the non-production environments (DEV, TEST, and STAGING). The DevOps Engineer is instructed by the CTO to design a deployment strategy that securely stores the credentials of each environment, expedites the startup time for the EC2 instances, and allows the same AMI to work in all environments.</p><p>How should the DevOps Engineer set up the deployment configuration to accomplish this task?</p>",
          "answers": [
            "<p>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Session Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials.</p>",
            "<p>Add a tag to each EC2 instance based on their environment. Use AWS Systems Manager Automation to preconfigure the AMI by installing all of the required applications and software dependencies. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Systems Manager Parameter Store to store the credentials as Secure String parameters.</p>",
            "<p>Add a tag to each EC2 instance based on their environment. Use a preconfigured AMI from AWS Marketplace. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials.</p>",
            "<p>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Patch Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use AWS AppConfig to store the environment-specific configuration and credentials securely.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Parameter Store</strong> provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Highly scalable, available, and durable, Parameter Store is backed by the AWS Cloud.</p><p>Systems Manager Automation simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p>Automation offers one-click automations for simplifying complex tasks such as creating golden Amazon Machines Images (AMIs), and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for a variety of reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img src=\"https://media.tutorialsdojo.com/public/custom_ami_1.gif\"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Add a tag to each EC2 instance based on their environment. Use AWS Systems Manager Automation to preconfigure the AMI by installing all of the required applications and software dependencies. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Systems Manager Parameter Store to store the credentials as Secure String parameters.</strong></p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Session Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials</strong> is incorrect. The Session Manager service is just a fully managed AWS Systems Manager capability that lets you manage your Amazon EC2 instances through an interactive one-click browser-based shell or through the AWS CLI. It is not capable to build a custom AMI, unlike Systems Manager Automation.</p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Use a preconfigured AMI from AWS Marketplace. Write a bootstrap script in the User Data to analyze the tag and set the environment configuration accordingly. Use the AWS Secrets Manager to store the credentials </strong>is incorrect. The company is using a custom AMI and not a public AMI from AWS Marketplace. You have to preconfigure the AMI using the Systems Manager Automation instead.</p><p>The option that says: <strong>Add a tag to each EC2 instance based on their environment. Preconfigure the AMI by installing all of the required applications and software dependencies using the AWS Systems Manager Patch Manager. Set up an AWS Lambda function that will be invoked by the User Data to analyze the associated tag and set the environment configuration accordingly. Use AWS AppConfig to store the environment-specific configuration and credentials securely</strong> is incorrect. The AWS Patch Manager is typically used for patching and managing operating system patches, not for installing applications or preconfiguring the AMI. Additionally, AWS AppConfig is more for managing feature flags or dynamic configurations and is not the best fit for secure storage of environment-specific credentials.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/\">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      }
    ],
    "answers": {
      "75949082": [
        "a"
      ],
      "75949164": [
        "b",
        "d"
      ],
      "75949176": [
        "a"
      ],
      "82921440": [
        "a",
        "b"
      ],
      "82921442": [
        "c"
      ],
      "82921450": [
        "a",
        "c",
        "d"
      ],
      "99528219": [
        "a",
        "e"
      ],
      "99528223": [
        "a",
        "b"
      ],
      "99528227": [
        "a",
        "b"
      ],
      "99528229": [
        "a"
      ],
      "99528239": [
        "b"
      ],
      "134588397": [
        "c"
      ],
      "134588419": [
        "a"
      ],
      "138248121": [
        "a"
      ],
      "138248125": [
        "a"
      ],
      "138248211": [
        "b"
      ],
      "138248221": [
        "b",
        "e"
      ],
      "138248237": [
        "a"
      ],
      "138248243": [
        "b"
      ],
      "143860749": [
        "a"
      ]
    }
  },
  {
    "id": "1770289500417",
    "date": "2026-02-05T11:05:00.417Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 10,
    "incorrect": 0,
    "unanswered": 0,
    "total": 10,
    "percent": 100,
    "duration": 2145483,
    "questions": [
      {
        "id": 138248227,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A startup software company has several application teams that develop API services for its business. Each application team is responsible for services separated on different AWS accounts. The VPC of each AWS account was initially provisioned with a <code>192.168.0.0/24</code> CIDR block. The services are deployed on Amazon EC2 instances accessed on a secure HTTPS public endpoint of an Application Load Balancer. Integration between the services routes externally to the public internet.</p><p>As part of a security audit, there is a recommendation from the security team to re-architect the integration between services to communicate on HTTPS on the private network only. A solutions architect is asked to suggest a long-term solution considering the possibility of adding more VPCs in the future.</p><p>What should the solutions architect recommend?</p>",
          "answers": [
            "<p>Create peering connections between each VPC of the different AWS accounts. Add a new route to the route table for each peering connection, with the destination set to the CIDR block of the VPC and the target set to the ID of the peering connection. Create Network Load Balancers (NLB) in each VPC and use the NLB DNS names for services integration.</p>",
            "<p>Fix the overlapping IP address ranges by renumbering the IP networks. Create a transit gateway in a new AWS account in AWS Organizations, and configure the VPC attachments. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway across the organization. Create transit gateway attachments to each VPC of the AWS accounts. Add new routes with the destination set to each VPC CIDR block, and the target set to the transit gateway attachment. Create a Network Load Balancer (NLB) in each VPC and use this for communication between services.</p>",
            "<p>Create a new AWS account and set up a new VPC. Create resource shares in AWS Resource Access Manager (AWS RAM) to share the private subnets to specified accounts. Advise the application teams to host the services in EC2 instances deployed in the shared private subnets. Create a Network Load Balancer (NLB) for each account and share the DNS names for the services integration.</p>",
            "<p>Create a Network Load Balancer (NLB) in each of the account VPCs. Create an interface type VPC endpoint for <code>com.amazonaws.us-east-1.elasticloadbalancing</code> service and choose the VPC and subnets. In the AWS PrivateLink console, create a subscription for the VPC endpoint and choose the AWS account that will be allowed access to the NLB. Use the VPC endpoint DNS names for communication between services.</p>"
          ],
          "explanation": "<p>A transit gateway acts as a Regional virtual router for traffic flowing between your virtual private clouds (VPCs) and on-premises networks. A transit gateway scales elastically based on the volume of network traffic. It is a best practice to use a separate subnet for each transit gateway VPC attachment.</p><p>A transit gateway enables you to attach VPCs and VPN connections and route traffic between them. A transit gateway works across AWS accounts, and you can use AWS RAM to share your transit gateway with other accounts. After you share a transit gateway with another AWS account, the account owner can attach their VPCs to your transit gateway. A user from either account can delete the attachment at any time.</p><p><img alt=\"Transit Gateway with AWS RAM\" src=\"https://media.tutorialsdojo.com/public/TransitGatewayWithRAM.png\" width=\"1000\"></p><p>It is a high recommendation and the best option to renumber IP networks when possible, based on two reasons: cost, and simplicity. Changing network configurations is not easy, but it is beneficial in the long term because it removes the ongoing cost of running required components when connecting overlapping networks. Having non-overlapping IPs also makes troubleshooting easier when things go wrong, as resources can easily be identified to the network they are deployed to. This also removes the complexity of managing firewall rules across the organization.</p><p>Thus, the correct answer is: <strong>Fix the overlapping IP address ranges by renumbering the IP networks. Create a transit gateway in a new AWS account in AWS Organizations, and configure the VPC attachments. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway across the organization. Create transit gateway attachments to each VPC of the AWS accounts. Add new routes with the destination set to each VPC CIDR block, and the target set to the transit gateway attachment. Create a Network Load Balancer (NLB) in each VPC and use this for communication between services.</strong></p><p>The option that says: <strong>Create peering connections between each VPC of the different AWS accounts. Add a new route to the route table for each peering connection, with the destination set to the CIDR block of the VPC and the target set to the ID of the peering connection. Create Network Load Balancers (NLB) in each VPC and use the NLB DNS names for services integration</strong> is incorrect. Although VPC peering will only work if the overlapping IP ranges are fixed, managing peering connections between multiple VPCs can be very complex and difficult to manage as the number of VPCs increases. For each <code>x</code> number of VPCs, <code>x*(x-1)/2</code> number of peering connections needs to be created to establish connectivity across each VPC.</p><p>The option that says: <strong>Create a new AWS account and set up a new VPC. Create resource shares in AWS Resource Access Manager (AWS RAM) to share the private subnets to specified accounts. Advise the application teams to host the services in EC2 instances deployed in the shared private subnets. Create a Network Load Balancer (NLB) for each account and share the DNS names for the services integration</strong> is incorrect. Sharing private subnets across accounts using AWS Resource Access Manager (AWS RAM) can simply add to the management overhead. Each time a new subnet is created, it needs to be shared manually with the specified AWS accounts.</p><p>The option that says: <strong>Create a Network Load Balancer (NLB) in each of the account VPCs. Create an interface type VPC endpoint for </strong><code><strong>com.amazonaws.us-east-1.elasticloadbalancing</strong></code><strong> service and choose the VPC and subnets. In the AWS PrivateLink console, create a subscription for the VPC endpoint and choose the AWS account that will be allowed access to the NLB. Use the VPC endpoint DNS names for communication between services</strong> is incorrect. Although using AWS PrivateLink and VPC endpoints can provide connectivity between VPCs, it would require managing individual VPC endpoints and subscriptions, making management complex. It is better to implement a Transit Gateway solution for the long term.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/\">https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/</a><br></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html\">https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html\">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-share.html\">https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-share.html</a></p><p><br></p><p>Check out this AWS Transit Gateway Cheat Sheet:</p><p><a href=\"https://tutorialsdojo.com/aws-transit-gateway/?src=udemy\">https://tutorialsdojo.com/aws-transit-gateway/</a></p>"
        }
      },
      {
        "id": 75949138,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
            "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
            "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
            "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
            "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
            "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"
          ],
          "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 82921382,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A 3D modeling company would like to deploy applications on Elastic Beanstalk with support for various programming languages with predictable and standardized deployment strategies. Some of these languages are supported (such as Node.js, Java, Golang) but others such as Rust are not supported. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following options would you recommend as the MOST efficient solution for this use-case?</p>\n",
          "answers": [
            "<p>Deploy to Elastic Beanstalk using a Multi-Docker container configuration. Package each application as a Docker container in ECR</p>",
            "<p>Create a custom platform for each language that is not supported. Package each application in S3 before deploying to Elastic Beanstalk</p>",
            "<p>Package each application as a standalone AMI that contains the OS, the application runtime and the application itself. To update a Beanstalk environment, provide a new AMI</p>",
            "<p>Run Opsworks on top of Elastic Beanstalk to bring the missing compatibility layer</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy to Elastic Beanstalk using a Multi-Docker container configuration. Package each application as a Docker container in ECR</strong></p>\n\n<p>Elastic Beanstalk supports the deployment of web applications from Docker containers. With Docker containers, you can define your own runtime environment. You can also choose your own platform, programming language, and any application dependencies (such as package managers or tools), which typically aren't supported by other platforms. Elastic Beanstalk can deploy a Docker image and source code to EC2 instances running the Elastic Beanstalk Docker platform. The platform offers multi-container (and single-container) support.</p>\n\n<p>A Dockerrun.aws.json file is an Elastic Beanstalk\u2013specific JSON file that describes how to deploy a set of Docker containers as an Elastic Beanstalk application. You can use a Dockerrun.aws.json file for a multi-container Docker environment. Dockerrun.aws.json describes the containers to deploy to each container instance (Amazon EC2 instance that hosts Docker containers) in the environment as well as the data volumes to create on the host instance for the containers to mount.</p>\n\n<p>Here, the most simple solution is to create a Docker container for each application. By using a Multi-Docker container configuration, we will be able to have a standardized deployment system across all the languages that we want to support.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a custom platform for each language that is not supported. Package each application in S3 before deploying to Elastic Beanstalk</strong> - Creating custom platforms and packaging applications in S3 will be cumbersome across a wide array of platforms. Using a multi-Docker container configuration is more efficient.</p>\n\n<p><strong>Package each application as a standalone AMI that contains the OS, the application runtime and the application itself. To update a Beanstalk environment, provide a new AMI</strong> - Packaging each application as an AMI might work but it's not going to help you standardize the way applications are deployed.</p>\n\n<p><strong>Run Opsworks on top of Elastic Beanstalk to bring the missing compatibility layer</strong> - AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks is a distractor in the question and doesn't have integration with Elastic Beanstalk.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.html</a></p>\n"
        }
      },
      {
        "id": 115961525,
        "correct_response": [
          "c",
          "d"
        ],
        "source": "Neal Set 5",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer needs a managed environment for running a Node.js application. The infrastructure should support load balancing and auto scaling. The application will require a managed relational database, and data should be stored persistently and protected from accidental deletion. The solution should minimize ongoing operational effort.</p><p>Which actions should the engineer take to meet these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Create an auto scaling group of Amazon EC2 instances managed by AWS Systems Manager.</p>",
            "<p>Create multiple AWS Lambda functions and associated Amazon Route 53 multivalue records.</p>",
            "<p>Create an AWS Elastic Beanstalk environment with load balancing and auto scaling enabled.</p>",
            "<p>Create an independent Amazon RDS database in an Amazon VPC with automatic backups and deletion protection enabled.</p>",
            "<p>Create an Amazon DynamoDB table in an Amazon VPC with automatic backups and deletion protection enabled.</p>"
          ],
          "explanation": "<p>AWS Elastic Beanstalk is a service that provides managed infrastructure onto which developers and DevOps engineers can simply add their code. Node.js is supported along with many other popular programming languages. Elastic Beanstalk supports load balancing and auto scaling for the underlying infrastructure.</p><p>The question calls for a relational database that is managed. This requirement can be satisfied by deploying an Amazon RDS database. To ensure the database is protected from accidental deletion it should be created independently of the Elastic Beanstalk environment. The engineer may also want to enable deletion protection and automatic backups.</p><p><strong>CORRECT: </strong>\"Create an AWS Elastic Beanstalk environment with load balancing and auto scaling enabled\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an independent Amazon RDS database in an Amazon VPC with automatic backups and deletion protection enabled\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an auto scaling group of Amazon EC2 instances managed by AWS Systems Manager\" is incorrect.</p><p>This does not provide the managed infrastructure platform the question requires. Elastic Beanstalk is a better solution as it takes care of the management of the underlying infrastructure. Systems Manager can help with management of EC2, but you are still responsible.</p><p><strong>INCORRECT:</strong> \"Create multiple AWS Lambda functions and associated Amazon Route 53 multivalue records\" is incorrect.</p><p>This is not a good solution for running highly available and managed code. Lambda scales concurrently and therefore using Route 53 to load balance via DNS resolution is not necessary.</p><p><strong>INCORRECT:</strong> \"Create an Amazon DynamoDB table in an Amazon VPC with automatic backups and deletion protection enabled\" is incorrect.</p><p>DynamoDB is a NoSQL database, and the question requires that a relational database is deployed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.as.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.as.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/security.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/security.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>"
        }
      },
      {
        "id": 75949088,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An organization is running containerized applications across Amazon EKS, Amazon ECS, and on-premises clusters. Due to some recent issues that caused outages, a solution is required to track container performance and system health, detect errors. The solution should enable collection and aggregation of time-series metrics from all container services for monitoring and analytics.</p><p>Which combination of services can the organization use?</p>",
          "answers": [
            "<p>Use the unified Amazon CloudWatch agent to collect the metrics, Amazon Athena to run SQL queries, and AWS Glue for visualization.</p>",
            "<p>Use Amazon Managed Service for Prometheus for collection of metrics and use Amazon Managed Grafana for visualization and analytics.</p>",
            "<p>Use the AWS Systems Manager agent to collect the metrics and use Amazon Managed Service for Prometheus for visualization and analytics.</p>",
            "<p>Use AWS AppConfig to collect application metrics from the containers and use Amazon OpenSearch Service for visualization and analytics.</p>"
          ],
          "explanation": "<p>Amazon Managed Service for Prometheus is a Prometheus-compatible service that monitors and provides alerts on containerized applications and infrastructure at scale. The service is integrated with Amazon Elastic Kubernetes Service (EKS), Amazon Elastic Container Service (ECS), and AWS Distro for OpenTelemetry.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-05_23-21-58-4a441982fe750d346208169ad7754e86.jpg\"><p>The company can use Amazon Managed Grafana, a fully managed AWS service that makes it easy to use Grafana to monitor operational data with interactive data visualizations in a single console across multiple data sources, without needing to deploy, manage, and operate Grafana servers.</p><p><strong>CORRECT: </strong>\"Use Amazon Managed Service for Prometheus for collection of metrics and use Amazon Managed Grafana for visualization and analytics\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the unified Amazon CloudWatch agent to collect the metrics, Amazon Athena to run SQL queries, and AWS Glue for visualization\" is incorrect.</p><p>The CloudWatch agent is used with Amazon EC2 instances and cannot be used with containers running on all these platforms. The Container Insights tool which is part of CloudWatch can be used for containers running in the AWS Cloud.</p><p><strong>INCORRECT:</strong> \"Use the AWS Systems Manager agent to collect the metrics and use Amazon Managed Service for Prometheus for visualization and analytics\" is incorrect.</p><p>Systems Manager cannot collect the time-series metrics from containerized applications running on the specified platforms.</p><p><strong>INCORRECT:</strong> \"Use AWS AppConfig to collect application metrics from the containers and use Amazon OpenSearch Service for visualization and analytics\" is incorrect.</p><p>AWS AppConfig is a capability of Systems Manager that is used to create, manage, and quickly deploy application configurations. It is not used for collecting metrics.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/prometheus/features/\">https://aws.amazon.com/prometheus/features/</a></p>"
        }
      },
      {
        "id": 134588423,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company runs a popular e-commerce website that provides discounts and weekly sales promotions on various products. It was recently moved from its previous cloud hosting provider to AWS. For its architecture, it uses an Auto Scaling group of Reserved EC2 instances with an Application Load Balancer in front. Their DevOps team needs to set up a CloudFront web distribution that uses a custom domain name where the origin is set to point to the ALB. </p><p>How can the DevOps Engineers properly implement an end-to-end HTTPS connection from the origin to the CloudFront viewers?</p>",
          "answers": [
            "<p>Generate an SSL certificate that is signed by a trusted third-party certificate authority. Import the certificate into AWS Certificate Manager and use it for the Application Load balancer. Set the <code>Viewer Protocol Policy</code> to <code>HTTPS Only</code> in CloudFront and then use an SSL/TLS certificate from a 3rd party certificate authority which was imported to either AWS Certificate Manager or the IAM certificate store.</p>",
            "<p>Generate an SSL self-signed certificate for the Application Load Balancer. Configure the Viewer Protocol Policy setting to <code>HTTPS Only</code> in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority that was imported to either AWS Certificate Manager or the IAM certificate store.</p>",
            "<p>Use a certificate that is signed by a trusted 3rd party certificate authority for the Application Load Balancer, which is then imported into AWS Certificate Manager. Choose the <code>Match Viewer</code> setting for the Viewer Protocol Policy to support both HTTP or HTTPS in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority that was imported to either AWS Certificate Manager or the IAM certificate store.</p>",
            "<p>Utilize an SSL certificate that is signed by a trusted 3rd party certificate authority for the ALB, which is then imported into AWS Certificate Manager. Set the Viewer Protocol Policy to <code>HTTPS Only</code> in CloudFront. Use an SSL/TLS certificate from 3rd party certificate authority which was imported to an Amazon S3 bucket.</p>"
          ],
          "explanation": "<p>For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so that connections are encrypted when CloudFront communicates with viewers. You also can configure CloudFront to use HTTPS to get objects from your origin, so that connections are encrypted when CloudFront communicates with your origin.</p><p>Remember that there are rules on which type of SSL Certificate to use if you are using an EC2 or an ELB as your origin. This question is about setting up an end-to-end HTTPS connection between the Viewers, CloudFront, and your custom origin, which is an ALB instance.</p><p><img src=\"https://media.tutorialsdojo.com/end-to-end-application-load-balancer-to-cloudfront-encryption.png\"></p><p>The certificate issuer you must use depends on whether you want to require HTTPS between viewers and CloudFront or between CloudFront and your origin:</p><p><strong>HTTPS between viewers and CloudFront</strong></p><p>- You can use a certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, Symantec, or other third-party providers.</p><p>- You can use a certificate provided by AWS Certificate Manager (ACM)</p><p><strong>HTTPS between CloudFront and a custom origin</strong></p><p>- If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- If your origin is an ELB load balancer, you can also use a certificate provided by ACM.</p><p>If you're using your own domain name, such as tutorialsdojo.com, you need to change several CloudFront settings. You also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store. Lastly, you should set the Viewer Protocol Policy to <strong>HTTPS Only</strong> in CloudFront.</p><p>Hence, the correct answer is: <strong>Generate an SSL certificate that is signed by a trusted third-party certificate authority. Import the certificate into AWS Certificate Manager and use it for the Application Load balancer. Set the </strong><code><strong>Viewer Protocol Policy</strong></code><strong> to </strong><code><strong>HTTPS Only</strong></code><strong> in CloudFront and then use an SSL/TLS certificate from a 3rd party certificate authority which was imported to either AWS Certificate Manager or the IAM certificate store.</strong></p><p>The option says:<strong> Generate an SSL self-signed certificate for the Application Load Balancer. Configure the Viewer Protocol Policy setting to </strong><code><strong>HTTPS Only</strong></code><strong> in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority that was imported to either AWS Certificate Manager or the IAM certificate store</strong> is incorrect because you cannot directly upload a self-signed certificate in your ALB.</p><p>The option that says: <strong>Use a certificate that is signed by a trusted 3rd party certificate authority for the Application Load Balancer, which is then imported into AWS Certificate Manager. Choose the </strong><code><strong>Match Viewer</strong></code><strong> setting for the Viewer Protocol Policy to support both HTTP or HTTPS in CloudFront. Use an SSL/TLS certificate from a 3rd party certificate authority which was imported to either AWS Certificate Manager or the IAM certificate store</strong> is incorrect because you have to set the Viewer Protocol Policy to <code>HTTPS Only</code>. With<code><strong><em> Match Viewer</em></strong></code><strong><em>, </em></strong>CloudFront communicates with your custom origin using HTTP or HTTPS, depending on the protocol of the viewer request. For example, if you choose Match Viewer for Origin Protocol Policy and the viewer uses HTTP to request an object from CloudFront, CloudFront also uses the same protocol (HTTP) to forward the request to your origin.</p><p>The option that says: <strong>Utilize an SSL certificate that is signed by a trusted 3rd party certificate authority for the ALB, which is then imported into AWS Certificate Manager. Set the Viewer Protocol Policy to </strong><code><strong>HTTPS Only</strong></code><strong> in CloudFront. Use an SSL/TLS certificate from 3rd party certificate authority which was imported to an Amazon S3 bucket</strong> is incorrect because you cannot use an SSL/TLS certificate from a third-party certificate authority which was imported to S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html \">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p>"
        }
      },
      {
        "id": 134588489,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading technology company with a hybrid cloud architecture has a suite of web applications that is composed of 50 modules. Each of the module is a multi-tiered application hosted in an Auto Scaling group of On-Demand Amazon EC2 instances behind an ALB with an external Amazon RDS. The Application Security team is mandated to block access from external IP addresses and only allow access to the 50 applications from the corporate data center. A group of 10 proxy servers with an associated IP address each are used for the corporate network to connect to the Internet. The 10 proxy IP addresses are being refreshed twice a month. The Network team uploads a CSV file that contains the latest proxy IP addresses into a private Amazon S3 bucket. The DevOps Engineer must build a solution to ensure that the applications are accessible from the corporate network in the most cost-effective way and with minimal operational effort.</p><p>Which of the following options will meet the above requirements</p>",
          "answers": [
            "<p>Launch an AWS Lambda function to read the list of proxy IP addresses from the S3 bucket. Configure the function to update the ELB security groups to allow HTTPS requests only from the given IP addresses. Use the S3 Event Notification to automatically invoke the Lambda function when the CSV file is updated.</p>",
            "<p>Host all of the applications and modules in the same Virtual Private Cloud (Amazon VPC). Set up a Direct Connect connection with an active/standby configuration. Update the ELB security groups to allow only inbound HTTPS connections from the corporate network IP addresses.</p>",
            "<p>Develop a custom Python-based Bolo script using the AWS SDK for Python. Configure the script to download the CSV file that contains the proxy IP addresses and update the ELB security groups to allow only HTTPS inbound from the given IP addresses. Host the script in an AWS Lambda function and run it every minute using Amazon EventBridge.</p>",
            "<p>Configure the ELB security groups to allow HTTPS inbound access from the Internet. Set up Amazon Cognito to integrate the company's Active Directory as the identity provider. Integrate all of the 50 modules with Cognito to ensure that only the company employees can log into the application. Store the user access logs to Amazon CloudWatch Logs to record user access activities. Use AWS Config for configuration management that runs twice a month to update the settings accordingly.</p>"
          ],
          "explanation": "<p><strong>AWS Lambda</strong> is a <a href=\"https://aws.amazon.com/serverless/\">serverless compute</a> service that runs your code in response to events and automatically manages the underlying compute resources for you. You can use AWS Lambda to extend other AWS services with custom logic, or create your own back-end services that operate at AWS scale, performance, and security. AWS Lambda can automatically run code in response to <a href=\"http://docs.aws.amazon.com/lambda/latest/dg/intro-core-components.html#intro-core-components-event-sources\">multiple events</a>, such as HTTP requests via <a href=\"https://aws.amazon.com/api-gateway/\">Amazon API Gateway</a>, modifications to objects in <a href=\"https://aws.amazon.com/s3/\">Amazon S3</a> buckets, table updates in <a href=\"https://aws.amazon.com/dynamodb/\">Amazon DynamoDB</a>, and state transitions in <a href=\"https://aws.amazon.com/step-functions/\">AWS Step Functions</a>.</p><p>Lambda runs your code on high-availability compute infrastructure and performs all the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, code and security patch deployment, and code monitoring and logging. All you need to do is supply the code.</p><p>Lambda does not enforce any restrictions on your function logic \u2013 if you can code for it, you can run it within a Lambda function. As part of your function, you may need to call other APIs, or access other AWS services like databases.</p><p><img src=\"https://media.tutorialsdojo.com/public/fig1-add-DNS_filtering_NAT_instance_with_Squid_9AUG2023.png\"></p><p>By default, your service or API must be accessible over the public internet for AWS Lambda to access it. However, you may have APIs or services that are not exposed this way. Typically, you create these resources inside Amazon Virtual Private Cloud (Amazon VPC) so that they cannot be accessed over the public Internet. These resources could be AWS service resources, such as Amazon Redshift data warehouses, Amazon ElastiCache clusters, or Amazon RDS instances. They could also be your own services running on your own EC2 instances. By default, resources within a VPC are not accessible from within a Lambda function.</p><p>AWS Lambda runs your function code securely within a VPC by default. However, to enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within your private VPC.</p><p>Hence, the correct answer is: <strong>Launch an AWS Lambda function to read the list of proxy IP addresses from the S3 bucket. Configure the function to update the ELB security groups to allow HTTPS requests only from the given IP addresses. Use the S3 Event Notification to automatically invoke the Lambda function when the CSV file is updated.</strong></p><p>The option that says: <strong>Host all of the applications and modules in the same Virtual Private Cloud (Amazon VPC). Set up a Direct Connect connection with an active/standby configuration. Update the ELB security groups to allow only inbound HTTPS connections from the corporate network IP addresses</strong> is incorrect because setting up a Direct Connect connection costs a significant amount of money. Remember that the scenario says that you have to ensure that the applications are accessible from the corporate network in the most cost-effective way and with minimal operational effort.</p><p>The option that says: <strong>Develop a custom Python-based Bolo script using the AWS SDK for Python. Configure the script to download the CSV file that contains the proxy IP addresses and update the ELB security groups to allow only HTTPS inbound from the given IP addresses. Host the script in an AWS Lambda function and run it every minute using Amazon EventBridge</strong> is incorrect because running the Lambda function every minute will increase your compute costs. A better solution is to use Amazon S3 Event Notification to automatically invoke the Lambda function when the CSV file is updated.</p><p>The option that says: <strong>Configure the ELB security groups to allow HTTPS inbound access from the Internet. Set up Amazon Cognito to integrate the company's Active Directory as the identity provider. Integrate all of the 50 modules with Cognito to ensure that only the company employees can log into the application. Store the user access logs to Amazon CloudWatch Logs to record user access activities. Use AWS Config for configuration management that runs twice a month to update the settings accordingly</strong> is incorrect because using Amazon Cognito in this scenario is not warranted and is unnecessary as well as the use of AWS Config. Using AWS Lambda can fulfill the requirements in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-set-up-an-outbound-vpc-proxy-with-domain-whitelisting-and-content-filtering/\">https://aws.amazon.com/blogs/security/how-to-set-up-an-outbound-vpc-proxy-with-domain-whitelisting-and-content-filtering/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/\">https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><p><br></p><p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p>"
        }
      },
      {
        "id": 99528205,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team has prohibited unauthenticated requests to S3 buckets for this project.</p><p>How can this issue be resolved in the <strong>MOST secure</strong> manner?</p>",
          "answers": [
            "<p>Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.</p>",
            "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</p>"
          ],
          "explanation": "<p>Removing unauthenticated access from the bucket through a bucket policy ensures that the S3 bucket is protected from unauthorized access. Granting the CodeBuild project's service role explicit permissions to access the S3 bucket aligns with the principle of least privilege. Using the AWS CLI within the build spec ensures secure and seamless access to the bucket.<br><br></p><p><strong>CORRECT: </strong>\"Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script\" is incorrect.</p><p>Adding the bucket name to the AllowedBuckets section does not address the security issue. The CodeBuild project needs proper IAM permissions to access the S3 bucket securely. Relying on unauthenticated access remains insecure and against the security team's requirements.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script\" is incorrect.</p><p>Enabling HTTPS basic authentication for the S3 bucket is not supported by AWS. Amazon S3 uses IAM policies and bucket policies for access control, not basic authentication. Additionally, using cURL to pass tokens does not align with AWS's recommended best practices.</p><p><strong>INCORRECT:</strong> \"Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key\" is incorrect.</p><p>Using IAM access keys and secret access keys directly is less secure than granting a CodeBuild service role the necessary permissions. Hardcoding access keys in the build spec or environment can lead to security vulnerabilities, such as accidental exposure of credentials.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html</a></p>"
        }
      },
      {
        "id": 134588369,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A digital payment gateway system is running in AWS which serves thousands of businesses worldwide. It is hosted in an Auto Scaling Group of EC2 instances behind an Application Load Balancer with an Amazon RDS database in a Multi-AZ deployment configuration. The company is using several CloudFormation templates in deploying the new version of the system. The <code>AutoScalingRollingUpdate</code> policy is used to control how CloudFormation handles rolling updates for their Auto Scaling group which replaces the old instances based on the parameters they have set. Lately, there were a lot of failed deployments which has caused system unavailability issues and business disruptions. They want to find out what's preventing their Auto Scaling group from updating correctly during a stack update.&nbsp; </p><p>In this scenario, how should the DevOps engineer troubleshoot this issue? (Select THREE.)</p>",
          "answers": [
            "<p>Switch from <code>AutoScalingRollingUpdate</code> to <code>AutoScalingReplacingUpdate</code> policy by modifying the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template. Set the <code>WillReplace</code> property to true. </p>",
            "<p>In your <code>AutoScalingRollingUpdate</code> policy, set the <code>WaitOnResourceSignals</code> property to false.</p>",
            "<p>In your <code>AutoScalingRollingUpdate</code> policy, set the value of the <code>MinSuccessfulInstancesPercent</code> property to prevent AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch.</p>",
            "<p>During a rolling update, suspend the following Auto Scaling processes: <code>HealthCheck</code>, <code>ReplaceUnhealthy</code>, <code>AZRebalance</code>, <code>AlarmNotification</code>, and <code>ScheduledActions</code>.</p>",
            "<p>Suspend the following Auto Scaling processes that are related with your ELB: <code>Launch</code>, <code>Terminate</code>, and <code>AddToLoadBalancer</code>.</p>",
            "<p>Set the <code>WaitOnResourceSignals</code> property to true in your <code>AutoScalingRollingUpdate</code> policy.</p>"
          ],
          "explanation": "<p>The AWS::AutoScaling::AutoScalingGroup resource uses the UpdatePolicy attribute to define how an Auto Scaling group resource is updated when the AWS CloudFormation stack is updated. If you don't have the right settings configured for the UpdatePolicy attribute, your rolling update can produce unexpected results.</p><p>You can use the <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate\">AutoScalingRollingUpdate policy</a> to control how AWS CloudFormation handles rolling updates for an Auto Scaling group. This common approach keeps the same Auto Scaling group, and then replaces the old instances based on the parameters that you set.</p><p>The <strong>AutoScalingRollingUpdate</strong> policy supports the following configuration options:</p><p><br></p><pre class=\"prettyprint linenums\">    \"UpdatePolicy\": {\n      \"AutoScalingRollingUpdate\": {\n        \"MaxBatchSize\": Integer,\n        \"MinInstancesInService\": Integer,\n        \"MinSuccessfulInstancesPercent\": Integer,\n        \"PauseTime\": String,\n        \"SuspendProcesses\": [ List of processes ],\n        \"WaitOnResourceSignals\": Boolean\n      }\n    }\n</pre><p><br></p><p>Using a rolling update has a risk of system outages and performance degradation due to the decreased availability of your running EC2 instances. If you want to ensure high availability of your application, you can also use the <em>AutoScalingReplacingUpdate</em> policy to perform an immediate rollback of the stack without any possibility of failure.</p><p>To find out what's preventing your Auto Scaling group from updating correctly during a stack update, work through the following troubleshooting scenarios as needed:</p><p><strong>- Configure WaitOnResourceSignals and PauseTime to avoid problems with success signals</strong></p><p>In your <em>AutoScalingRollingUpdate</em> policy, set the <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate-waitonresourcesignals\"><em>WaitOnResourceSignals</em></a> property to false. Take note that if <em>WaitOnResourceSignals</em> is set to true, <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate-pausetime\"><em>PauseTime</em></a> changes to a timeout value. AWS CloudFormation waits to receive a success signal until the maximum time specified by the <em>PauseTime</em> value. If a signal is not received, AWS CloudFormation cancels the update. Then, AWS CloudFormation rolls back the stack with the same settings, including the same PauseTime value.</p><p><strong>- Configure MinSuccessfulInstancesPercent to avoid stack rollback</strong></p><p>If you're replacing a large number of instances during a rolling update and waiting for a success signal for each instance, complete the following: In your <em>AutoScalingRollingUpdate</em> policy, set the value of the <em>MinSuccessfulInstancesPercent</em> property. Take note that setting the <em>MinSuccessfulInstancesPercent</em> property prevents AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch.</p><p><strong>- Configure SuspendProcesses to avoid unexpected changes to the Auto Scaling group</strong></p><p>During a rolling update, suspend the following Auto Scaling processes: <em>HealthCheck</em>, <em>ReplaceUnhealthy</em>, <em>AZRebalance</em>, <em>AlarmNotification</em>, and <em>ScheduledActions</em>. It is quite important to know that if you're using your Auto Scaling group with Elastic Load Balancing (ELB), you should not suspend the following processes: <em>Launch</em>, <em>Terminate</em>, and <em>AddToLoadBalancer</em>. These processes are required to make rolling updates. Take note that if an unexpected scaling action changes the state of the Auto Scaling group during a rolling update, the update can fail. The failure can result from an inconsistent view of the group by AWS CloudFormation.</p><p>Based on the above information, the correct answers are:</p><p><strong>- In your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy, set the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> property to false.</strong></p><p><strong>- In your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy, set the value of the </strong><code><strong>MinSuccessfulInstancesPercent</strong></code><strong> property to prevent AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch</strong></p><p><strong>- During a rolling update, suspend the following Auto Scaling processes: </strong><code><strong>HealthCheck</strong></code><strong>, </strong><code><strong>ReplaceUnhealthy</strong></code><strong>, </strong><code><strong>AZRebalance</strong></code><strong>, </strong><code><strong>AlarmNotification</strong></code><strong>, and </strong><code><strong>ScheduledActions</strong></code></p><p><br></p><p>The option that says: <strong>Switch from </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> to </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy by modifying the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template. Set the </strong><code><strong>WillReplace</strong></code><strong> property to true</strong> is incorrect because although the <code><strong><em>AutoScalingReplacingUpdate</em></strong></code><strong><em> </em></strong>policy provides an immediate rollback of the stack without any possibility of failure, this solution is not warranted since the scenario asks for the options that will help troubleshoot the issue.</p><p>The option that says: <strong>Suspend the following Auto Scaling processes that are related with your ELB: </strong><code><strong>Launch</strong></code><strong>, </strong><code><strong>Terminate</strong></code><strong>, and </strong><code><strong>AddToLoadBalancer</strong></code> is incorrect because these processes are required by the ELB to make rolling updates.</p><p>The option that says: <strong>Set the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> property to true in your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy</strong> is incorrect. The <code>WaitOnResourceSignals</code> property should be set to false instead of true, to determine what prevents the Auto Scaling group from being updated correctly during a stack update.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 67357164,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>Consider a multi-account setup within AWS Organizations where a company is running a data ingestion application on Amazon EC2 instances through several Auto Scaling groups. These instances lack internet access due to sensitive data handling, and VPC endpoints have been deployed accordingly. The application operates on a custom AMI designed specifically for its needs.</p>\n\n<p>To effectively manage and troubleshoot the application, system administrators require automated and centralized login access to the EC2 instances. Additionally, the company's security team needs to be notified whenever such instances are accessed.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you suggest to satisfy these requirements?</p>\n",
          "answers": [
            "<p>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>",
            "<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>",
            "<p>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</p>",
            "<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>This option suggests using EC2 Image Builder to create an updated custom AMI with AWS Systems Manager Agent included. The Auto Scaling group is configured to attach the AmazonSSMManagedInstanceCore role to the instances, thereby enabling centralized management through Systems Manager, as it grants the EC2 instances the permissions needed for core Systems Manager functionality. Session Manager can be used for secure logins, and session details can be logged to Amazon S3. Additionally, an S3 event notification can be set up to alert the security team about new file uploads using Amazon SNS. This option aligns well with the requirement for centralized access and monitoring.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q35-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/image-builder/\">https://aws.amazon.com/image-builder/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</strong> - This option includes unnecessary internet access for sensitive data via the combination of NAT Gateway and bastion host, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - EC2 Instance Connect is a way to distribute short-lived SSH keys and control access by IAM policies. The actual connection is created with SSH client and all normal requirements (network connectivity, user on host etc) apply. The host must also have ec2-instance-connect \u201cagent\u201d installed. EC2 Instance Connect still requires public IP and network connectivity, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions. Instead, SCPs are JSON policies that specify the maximum permissions for the affected accounts. So, for the given use case, you cannot use SCP for granting EC2 instance access to centralized and automated login for the Systems Manager.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/image-builder/\">https://aws.amazon.com/image-builder/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n"
        }
      }
    ],
    "answers": {
      "67357164": [
        "b"
      ],
      "75949088": [
        "b"
      ],
      "75949138": [
        "b",
        "c",
        "d"
      ],
      "82921382": [
        "a"
      ],
      "99528205": [
        "c"
      ],
      "115961525": [
        "c",
        "d"
      ],
      "134588369": [
        "b",
        "c",
        "d"
      ],
      "134588423": [
        "a"
      ],
      "134588489": [
        "a"
      ],
      "138248227": [
        "b"
      ]
    }
  },
  {
    "id": "1770281167029",
    "date": "2026-02-05T08:46:07.030Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 20,
    "incorrect": 0,
    "unanswered": 0,
    "total": 20,
    "percent": 100,
    "duration": 7327923,
    "questions": [
      {
        "id": 82921340,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A financial services company has a solution in place to track all the API calls made by users, applications, and SDK within the AWS account. Recently, it has experienced a hack and could find a user amongst the logs that did some compromising API calls. The company wants to know with 100% certainty that the log files represent the correct sequence of events and have not been altered. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following would you suggest as the most effective solution?</p>\n",
          "answers": [
            "<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</p>",
            "<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</p>",
            "<p>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</p>",
            "<p>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p>How CloudTrail Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation. This feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p>For the given use-case, to track API calls made within your account, you need to use AWS CloudTrail. Then the right way to verify log integrity would be to use the CloudTrail <code>validate-logs</code> command.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</strong> -  S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as \u201cwrite once read many\u201d (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn't provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won't give us the information on who made the API calls.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won't give us the information of who made the API calls. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn't provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n"
        }
      },
      {
        "id": 115961493,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company runs an application across thousands of EBS-backed Amazon EC2 instances. The company needs to ensure availability of the application and requires that instances are restarted when an EC2 instance retirement event is scheduled.</p><p>How can this a DevOps engineer automate this task?</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch alarm for EC2 status checks. Configure the alarm to trigger an Amazon SNS notification to the operations team and have them stop and start affected instances.</p>",
            "<p>Create a rule in Amazon EventBridge with AWS Health as the source and look for instance retirement scheduled events. Run an AWS Systems Manager automation document that stops and starts affected instances.</p>",
            "<p>Enable EC2 Auto Recovery on all instances. Configure an Amazon CloudWatch alarm with the alarm action set to Recover. Specify a time for recovery that is outside of business hours.</p>",
            "<p>Create a rule in Amazon EventBridge with Amazon EC2 as the source and look for EC2 instance state-change notifications that indicate the instance is shutting down. Run an AWS Systems Manager automation document that starts the affected instances.</p>"
          ],
          "explanation": "<p>An EC2 instance is scheduled for retirement when AWS detects an irreparable failure in the infrastructure that's hosting your instance. You are required to stop and then start the instance at your preferred time before the instance retirement date. Stopping and starting the instance moves the instance to another healthy host.</p><p>The best way to automate this process is to create a rule in Amazon EventBridge that looks for AWS Health events. The specific event is:</p><p>\u201cAWS_EC2_INSTANCE_RETIREMENT_SCHEDULED\u201d</p><p>When this event occurs EventBridge can trigger an AWS Systems Manager automation document that stops and starts the EC2 instances.</p><p><strong>CORRECT: </strong>\"Create a rule in Amazon EventBridge with AWS Health as the source and look for instance retirement scheduled events. Run an AWS Systems Manager automation document that stops and starts affected instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for EC2 status checks. Configure the alarm to trigger an Amazon SNS notification to the operations team and have them stop and start affected instances\" is incorrect.</p><p>Status checks do not inform us that an instance retirement event is scheduled, they let us know if there are issues that are affecting the instances or hosts.</p><p><strong>INCORRECT:</strong> \"Enable EC2 Auto Recovery on all instances. Configure an Amazon CloudWatch alarm with the alarm action set to Recover. Specify a time for recovery that is outside of business hours\" is incorrect.</p><p>Auto Recovery will recover an instance automatically, but this is not related to retirement events. You also cannot configure a time schedule in the alarm action.</p><p><strong>INCORRECT:</strong> \"Create a rule in Amazon EventBridge with Amazon EC2 as the source and look for EC2 instance state-change notifications that indicate the instance is shutting down. Run an AWS Systems Manager automation document that starts the affected instances\" is incorrect.</p><p>This would restart all instances that are shutdown, so the scope is too broad. We specifically want to target only the instances that are affected by retirement events.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-retirement/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-retirement/</a></p><p><a href=\"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html\">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 75949044,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The information security policy of a company has been updated and now requires that all Amazon EBS volumes must be encrypted. Any volumes that are not encrypted should be marked as non-compliant. The company uses AWS Organizations to manage multiple accounts. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is applied.</p><p>Which solution will accomplish this MOST efficiently?</p>",
          "answers": [
            "<p>Enable the AWS Config ec2-ebs-encryption-by-default rule to check whether EBS encryption is enabled. Deploy the rule in the management account of the Organization.</p>",
            "<p>Run a scheduled AWS Lambda function in each account that checks the encryption status of EBS volumes in the account. Publish the report to a centralized Amazon S3 bucket. Use Amazon Athena to analyze the data.</p>",
            "<p>Create an AWS Config rule at the AWS organization level to check whether EBS encryption is enabled. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization.</p>",
            "<p>Apply an SCP in Organizations that uses conditional expressions to prevent the launch of Amazon EC2 instances that do not have encrypted EBS volumes. Apply the SCP to all AWS accounts.</p>"
          ],
          "explanation": "<p>AWS Config allows you to manage AWS Config rules across all AWS accounts within an organization. You can:</p><ul><li><p>Centrally create, update, and delete AWS Config rules across all accounts in your organization.</p></li><li><p>Deploy a common set of AWS Config rules across all accounts and specify accounts where AWS Config rules should not be created.</p></li><li><p>Use the APIs from the master account in AWS Organizations to enforce governance by ensuring that the underlying AWS Config rules are not modifiable by your organization\u2019s member accounts.</p></li></ul><p>The DevOps engineer should create an organization level rule and then setup an SCP that prevents any modifications from happening that would stop the rule from running.</p><p>The engineer can use the AWS Config \u201cec2-ebs-encryption-by-default\u201d rule. This rule checks that Amazon Elastic Block Store (EBS) encryption is enabled by default. The rule is NON_COMPLIANT if the encryption is not enabled.</p><p><strong>CORRECT: </strong>\"Create an AWS Config rule at the AWS organization level to check whether EBS encryption is enabled. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable the AWS Config ec2-ebs-encryption-by-default rule to check whether EBS encryption is enabled. Deploy the rule in the management account of the Organization\" is incorrect.</p><p>Deploying the rule in the management account will not suffice as the company has multiple accounts in an AWS Organizations configuration. The rule must be deployed across all accounts.</p><p><strong>INCORRECT:</strong> \"Run a scheduled AWS Lambda function in each account that checks the encryption status of EBS volumes in the account. Publish the report to a centralized Amazon S3 bucket. Use Amazon Athena to analyze the data\" is incorrect.</p><p>While this may provide the required data, this is not the most efficient solution. Using Config is preferable as it will have less overhead and is designed specifically for compliance purposes and is a superior solution.</p><p><strong>INCORRECT:</strong> \"Apply an SCP in Organizations that uses conditional expressions to prevent the launch of Amazon EC2 instances that do not have encrypted EBS volumes. Apply the SCP to all AWS accounts\" is incorrect.</p><p>SCPs don't affect users or roles in the management account and condition elements may not affect users logged in with root user credentials. AWS Config will be a better solution for this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/ec2-ebs-encryption-by-default.html\">https://docs.aws.amazon.com/config/latest/developerguide/ec2-ebs-encryption-by-default.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>"
        }
      },
      {
        "id": 75949046,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company manages a continuous integration and continuous delivery (CI/CD) pipeline that includes a Jenkins implementation that runs on Amazon EC2. The security team has requested that all build artifacts are encrypted as they contain company sensitive data.</p><p>Which changes should a DevOps engineer make to meet these requirements whilst reducing operational overhead for ongoing management?</p>",
          "answers": [
            "<p>Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes.</p>",
            "<p>Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption.</p>",
            "<p>Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group.</p>",
            "<p>Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM).</p>"
          ],
          "explanation": "<p>The existing Jenkins implementation runs on an Amazon EC2 instance, and this requires more operational management compared to using AWS CodeBuild which is a managed service. The DevOps engineer should replace Jenkins with AWS CodeBuild which is a fully managed build service.</p><p>CodeBuild compiles source code, runs unit tests, and produces artifacts that are ready to deploy. Encryption for build artifacts such as a cache, logs, exported raw test report data files, and build results, is enabled by default, and uses AWS managed keys (AWS KMS).</p><p><strong>CORRECT: </strong>\"Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes\" is incorrect.</p><p>This solution may be secure, but it does require more operational management compared to using AWS CodeBuild and is therefore not the best option.</p><p><strong>INCORRECT:</strong> \"Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group\" is incorrect.</p><p>Moving Jenkins to an Auto Scaling group does not reduce the operational management. This is also an option that requires more overhead to manage compared to using a managed service such as AWS CodeBuild.</p><p><strong>INCORRECT:</strong> \"Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM)\" is incorrect.</p><p>Artifacts cannot be encrypted using ACM as that is a service that issues SSL/TLS certificates which are used for encryption in-transit rather than encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949098,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 3",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is concerned about the security of their Amazon EC2 instances. They require an automated solution for identifying security vulnerabilities on the instances and notifying the security team. They also require an audit trail of all login activities on the EC2 instances.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Kinesis Client Library (KCL) to capture system logs and save them to an Amazon S3 bucket.</p>",
            "<p>Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture an audit trail using system logs and view login activity in the AWS CloudTrail console.</p>",
            "<p>Use AWS GuardDuty to detect vulnerabilities on the EC2 instances. Configure the AWS X-Ray daemon to gather trace data and add metrics to Amazon CloudWatch. View the audit trail of login activities in the CloudWatch console.</p>",
            "<p>Use Amazon Inspector to automatically detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and upload them to Amazon CloudWatch Logs.</p>"
          ],
          "explanation": "<p>Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. This is the best service to use for automatic detection of security vulnerabilities on the EC2 instances.</p><p>The unified CloudWatch agent enables you to collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances.</p><p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs. The system logs that are collected will include information on all login activities on the EC2 instances.</p><p><strong>CORRECT: </strong>\"Use Amazon Inspector to automatically detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and upload them to Amazon CloudWatch Logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Kinesis Client Library (KCL) to capture system logs and save them to an Amazon S3 bucket\" is incorrect.</p><p>Systems Manager Patch Manager can install patches to resolve vulnerabilities but does not provide automated detection of vulnerabilities. Instead, it scans to see if specific patches are installed or not. The KCL is used with Kinesis Data Streams for processing streaming data and does not collect system logs from EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture an audit trail using system logs and view login activity in the AWS CloudTrail console\" is incorrect.</p><p>As above, Systems Manager is not suitable for this task and does not capture auditing information by processing system logs.</p><p><strong>INCORRECT:</strong> \"Use AWS GuardDuty to detect vulnerabilities on the EC2 instances. Configure the AWS X-Ray daemon to gather trace data and add metrics to Amazon CloudWatch. View the audit trail of login activities in the CloudWatch console\" is incorrect.</p><p>GuardDuty is a threat detection service that monitors for malicious activity. It does not detect vulnerabilities on EC2 instances. The X-Ray service is used for gathering trace data for troubleshooting and understanding application performance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/inspector/features/\">https://aws.amazon.com/inspector/features/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-inspector/\">https://digitalcloud.training/amazon-inspector/</a></p>"
        }
      },
      {
        "id": 143860749,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 6",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is deploying an application in four AWS Regions across North America, Europe, and Asia. The application will be used by millions of users. The application must allow users to submit data through the application layer in each Region and have it saved in a low-latency database layer. The company also must ensure that the data can be read through the application layer in each Region.</p><p>Which solution will meet these requirements with the LOWEST latency of reads and writes?</p>",
          "answers": [
            "<p>Create a table in Amazon DynamoDB and enable global tables in each of the four Regions.</p>",
            "<p>Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions.</p>",
            "<p>Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions.</p>",
            "<p>Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions.</p>"
          ],
          "explanation": "<p>Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p><p>This is the only workable solution in the list that provides both reads and writes in each Region that are replicated across the other Regions. This is also the best solution as it provides low latency reads and writes.</p><p><strong>CORRECT: </strong>\"Create a table in Amazon DynamoDB and enable global tables in each of the four Regions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions\" is incorrect.</p><p>This solutions does not provide local writes in each Region as the Read Replicas cannot be written to. Therefore, this solution only offers low latency reads in each Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions\" is incorrect.</p><p>Replication groups in ElastiCache are used within a Region and not across Regions so this solution does not work.</p><p><strong>INCORRECT:</strong> \"Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions\" is incorrect.</p><p>This solution provides a database in each Region, there is no mechanism for replication. You can create replica instances in different Regions but that would only provide low latency reads (as with RDS), and not low latency writes.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>"
        }
      },
      {
        "id": 75949126,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 3",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company runs several business-critical applications on Amazon EC2 instances in an Amazon VPC. The company requires the applications to be available 24/7 and there should be no outages except for pre-arranged updates. The DevOps team requires an automated notification mechanism that lets them know if the state of any of the instance\u2019s changes.</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch alarm for the StatusCheckFailed_System metric and select the EC2 action to recover the instance.</p>",
            "<p>Create an Amazon EventBridge rule with an event pattern configured with the \u2018EC2 Instance State-change Notification\u2019 event type and send an Amazon SNS notification.</p>",
            "<p>Create an Amazon CloudWatch alarm for the StatusCheckFailed_Instance metric and select the EC2 action to reboot the instance.</p>",
            "<p>Create a scheduled AWS Lambda function that checks the running state of the Amazon EC2 instances and updates the AWS Personal Health Dashboard.</p>"
          ],
          "explanation": "<p>To receive email notifications when your instance changes state, create an Amazon SNS topic and then create an EventBridge rule for the EC2 Instance State-change Notification event. This will trigger a notification by SNS whenever the state of an EC2 instance changes.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule with an event pattern configured with the \u2018EC2 Instance State-change Notification\u2019 event type and send an Amazon SNS notification\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a scheduled AWS Lambda function that checks the running state of the Amazon EC2 instances and updates the AWS Personal Health Dashboard\" is incorrect.</p><p>The Personal Health Dashboard shows issues that may affect your resources. AWS updates the dashboard; it is not something you can do yourself.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for the StatusCheckFailed_System metric and select the EC2 action to recover the instance\" is incorrect.</p><p>This would recover an instance if there were an issue with the underlying hardware that requires AWS to fix. However, the requirement here is simply to notify the team if the instance changes state.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for the StatusCheckFailed_Instance metric and select the EC2 action to reboot the instance\" is incorrect.</p><p>As above, the team requires a notification if the state of the instance changes, they do not ask for automated recovery or restarting of instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instance-state-changes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instance-state-changes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>"
        }
      },
      {
        "id": 134588447,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company developed a web portal for gathering Census data within the city. The household information uploaded on the portal contains personally identifiable information (PII) and is stored in encrypted files on an Amazon S3 bucket. The object indexes are saved on an Amazon DynamoDB table.</p><p>Data security is a top priority, and S3 access logs along with AWS CloudTrail have been enabled to track access to S3 objects. To further enhance security, there is a need to verify that data access meets compliance standards and to receive alerts in case of any risk of unauthorized access or inadvertent data leaks.</p><p>Which of the following AWS services enables this?</p>",
          "answers": [
            "<p>Use Amazon Macie to monitor and detect usage patterns on your S3 data.</p>",
            "<p>Use AWS Security Hub to aggregate security findings from multiple AWS services.</p>",
            "<p>Use Amazon GuardDuty to monitor malicious activity on your S3 data.</p>",
            "<p>Use Amazon Inspector to alert you whenever a security violation is detected on your S3 data.</p>"
          ],
          "explanation": "<p><strong>Amazon Macie</strong> is an ML-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization.</p><p>Amazon Macie continuously monitors data access activity for anomalies and delivers alerts when it detects the risk of unauthorized access or inadvertent data leaks. Amazon Macie has the ability to detect global access permissions inadvertently being set on sensitive data, detect uploading of API keys inside source code, and verify sensitive customer data is being stored and accessed in a manner that meets their compliance standards.</p><p><img src=\"https://media.tutorialsdojo.com/public/MacieAlerts_2AUG2023.jpg\">Hence, the correct answer is:<strong><em> </em>Use Amazon Macie to monitor and detect usage patterns on your S3 data.</strong></p><p>The option that says: <strong>Use AWS Security Hub to aggregate security findings from multiple AWS services</strong> is incorrect because AWS Security Hub simply aggregates security findings from services like Macie and GuardDuty but does not directly monitor or classify sensitive data in S3. While useful for a centralized security overview, it does not proactively identify PII exposure risks.</p><p>The option that says: <strong>Use Amazon GuardDuty to monitor malicious activity on your S3 data </strong>is incorrect. Although GuardDuty can continuously monitor malicious activity and unauthorized behavior in your Amazon S3 bucket, it still is not capable of recognizing sensitive data such as personally identifiable information (PII), which is required in the scenario.</p><p>The option that says: <strong>Use Amazon Inspector to alert you whenever a security violation is detected on your S3 data<em> </em></strong>is incorrect because Inspector is typically an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html \">https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html</a></p><p><a href=\"https://aws.amazon.com/macie/faq/\">https://aws.amazon.com/macie/faq/</a></p><p><a href=\"https://docs.aws.amazon.com/macie/index.html \">https://docs.aws.amazon.com/macie/index.html</a></p><p><br></p><p><strong>Check out this Amazon Macie Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-macie/?src=udemy\">https://tutorialsdojo.com/amazon-macie/</a></p>"
        }
      },
      {
        "id": 82921344,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An industrial appliances company would like to take advantage of AWS Systems Manager to manage their on-premise instances and their EC2 instances. This will allow them to run some SSM RunCommand across their hybrid fleet. The company would also like to effectively manage the size of the fleet. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>How would you set up the on-premise server to achieve this objective?</p>\n",
          "answers": [
            "<p>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>",
            "<p>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>",
            "<p>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>",
            "<p>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p>AWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. With Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status.</p>\n\n<p>How Systems Manager Works:\n<img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\">\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants AssumeRole trust to the Systems Manager service. You only need to create a service role for a hybrid environment once for each AWS account.</p>\n\n<p>To set up servers and virtual machines (VMs) in your hybrid environment as managed instances, you need to create a managed-instance activation. After you complete the activation, you immediately receive an Activation Code and Activation ID. You specify this Code/ID combination when you install SSM agents on servers and VMs in your hybrid environment. The Code/ID provides secure access to the Systems Manager service from your managed instances.</p>\n\n<p>In the Instance limit field, specify the total number of on-premises servers or VMs that you want to register with AWS as part of the activation. This means you don't need to create a unique activation Code/ID for each managed instance.</p>\n\n<p>After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as managed instances. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix \"mi-\". Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong> - As mentioned in the explanation earlier, the on-premise instances use the prefix \"mi-\" whereas the Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><strong>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p><strong>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong></p>\n\n<p>Both these options suggest using the Access Key ID and the Secret Access Key ID to register your on-premise servers which is considered a bad practice from a security perspective. Instead, you should use an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html</a></p>\n\n<p><a href=\"#\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p>\n"
        }
      },
      {
        "id": 134588373,
        "correct_response": [
          "d",
          "e",
          "f"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer working for a company that manages multiple teams sharing a single AWS account is tasked with overseeing the production infrastructure. The teams primarily store media and images in Amazon S3 buckets; some buckets are configured for public internet access, while others are restricted to internal applications. To ensure security and compliance, the company wants to leverage AWS Trusted Advisor to identify public buckets and verify that only intended users have &lt;code&gt;List&lt;/code&gt; access. Additionally, the company needs to be notified whenever a public bucket has incorrect permissions and requires automatic remediation if needed.</p><p>Which of the following actions should the DevOps engineer implement to meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Set up a custom Amazon Inspector rule that checks public S3 buckets permissions. Send an action to AWS Systems Manager to correct the S3 bucket policy.</p>",
            "<p>Set up a custom AWS Config rule to execute a default remediation action to update the permissions on the public S3 bucket.</p>",
            "<p>Create an AWS Lambda function that executes every hour to refresh Trusted Advisor scan results via API. Subscribe to Trusted Advisor notification messages to receive the results.</p>",
            "<p>Set up a custom Config rule that checks public S3 bucket permissions. Then, send a non-compliance notification to your subscribed Amazon SNS topic.</p>",
            "<p>Set up a custom Config rule to check public S3 bucket permissions and send an event to Amazon EventBridge when policy violations are detected. Configure the EventBridge rule to trigger a Lambda function, automatically updating the S3 bucket permissions.</p>",
            "<p>Utilize EventBridge to monitor Trusted Advisor security recommendation results and then set a trigger to send an email using SNS to notify you about the results of the check.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. You can configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic. This way, you can be notified when AWS Config evaluates your custom or managed rules against your resources.</p><p>AWS Config can monitor your Amazon Simple Storage Service (S3) bucket ACLs and policies for violations that allow public read or public write access. If AWS Config finds a policy violation, it can trigger an Amazon EventBridge rule to trigger an AWS Lambda function which either corrects the S3 bucket ACL, or notifies you via Amazon Simple Notification Service (Amazon SNS) that the policy is in violation and allows public read or public write access.</p><p><img src=\"https://media.tutorialsdojo.com/public/how-AWSconfig-works.png\"></p><p>You can use Amazon EventBridge to detect and react to changes in the status of Trusted Advisor checks. Then, based on the rules that you create, Amazon EventBridge invokes one or more target actions when a check status changes to the value you specify in a rule. Depending on the type of status change, you might want to send notifications, capture status information, take corrective action, initiate events, or take other actions.</p><p>Hence, the correct answers are:</p><p><strong>- Set up a custom Config rule that checks public S3 bucket permissions. Then, send a non-compliance notification to your subscribed Amazon SNS topic.</strong></p><p><strong>- Set up a custom Config rule to check public S3 bucket permissions and send an event to Amazon EventBridge when policy violations are detected. Configure the EventBridge rule to trigger a Lambda function, automatically updating the S3 bucket permissions.</strong></p><p><strong>- Utilize EventBridge to monitor Trusted Advisor security recommendation results and then set a trigger to send an email using SNS to notify you about the results of the check.</strong></p><p>The option that says: <strong>Set up a custom Amazon Inspector rule that checks public S3 buckets permissions. Send an action to AWS Systems Manager to correct the S3 bucket policy </strong>is incorrect because Amazon Inspector is just an automated security assessment service that is primarily used for EC2 instances. You have to use AWS Config instead.</p><p>The option that says: <strong>Set up a custom AWS Config rule to execute a default remediation action to update the permissions on the public S3 bucket </strong>is incorrect because there is no default remediation action. This should be integrated with the AWS Systems Manager Automation service where you can configure the actions for your remediation.</p><p>The option that says: <strong>Create an AWS Lambda function that executes every hour to refresh Trusted Advisor scan results via API. Subscribe to Trusted Advisor notification messages to receive the results </strong>is incorrect because it is better to use AWS Config instead of AWS Trusted Advisor in this scenario. Moreover, Trusted Advisor only sends the summary notification every week so this won't notify you immediately about your non-compliant resources.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-config-to-monitor-for-and-respond-to-amazon-s3-buckets-allowing-public-access/ \">https://aws.amazon.com/blogs/security/how-to-use-aws-config-to-monitor-for-and-respond-to-amazon-s3-buckets-allowing-public-access/</a></p><p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html\">https://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html</a></p><p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html\">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p><p><br></p><p><strong>Check out these AWS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><a href=\"https://tutorialsdojo.com/aws-trusted-advisor/?src=udemy\">https://tutorialsdojo.com/aws-trusted-advisor/</a></p>"
        }
      },
      {
        "id": 99528209,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A critical production application running on AWS uses automatic scaling. The operations team must run updates on the application that affect only one instance at a time. The deployment process must ensure all remaining instances continue to serve traffic. The deployment must roll back if the update causes the CPU utilization of the updated instance to exceed 85%.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Configure AWS CodeDeploy with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Use the CodeDeployDefault.OneAtAtime configuration for deployment. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>",
            "<p>Configure AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group.</p>",
            "<p>Configure AWS Elastic Beanstalk with a load balancer and use AWS Auto Scaling. Create an alarm based on the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>",
            "<p>Configure AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>"
          ],
          "explanation": "<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS.</p><p>Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period. You can monitor metrics such as instance CPU utilization.</p><p>If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover).</p><p>For this scenario the deployment can use the CodeDeployDefault.OneAtAtime strategy which ensures that only one instance will be taken out of action at a time, and automatic rollbacks if the alarm threshold is exceeded. This meets all requirements.</p><p><strong>CORRECT: </strong>\"Configure AWS CodeDeploy with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Use the CodeDeployDefault.OneAtAtime configuration for deployment. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group\" is incorrect.</p><p>CodeDeploy is a much better solution as it is designed for this purpose. This answer pieces together several services in a more complex solution.</p><p><strong>INCORRECT:</strong> \"Configure AWS Elastic Beanstalk with a load balancer and use AWS Auto Scaling. Create an alarm based on the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is incorrect.</p><p>Elastic Beanstalk uses ELB health checks (basic health), or with enhanced health it monitors application logs and the state of your environment's other resources. It does not use CloudWatch alarms.</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is incorrect.</p><p>Systems Manager cannot perform blue/green updates though it can be used with other AWS Developer Tools as part of a solution for deploying updates in this manner.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528193,
        "correct_response": [
          "c",
          "e"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer launched an Amazon EC2 instance in an Amazon VPC. The instance must download an object from a restricted Amazon S3 bucket. When trying to download the object, a 403 Access Denied error was received.</p><p>What are two possible causes for this error? (Select TWO.)</p>",
          "answers": [
            "<p>Default encryption is enabled on the S3 bucket.</p>",
            "<p>The object has been moved to Amazon Glacier.</p>",
            "<p>The bucket policy does not grant permission.</p>",
            "<p>S3 versioning is enabled on the S3 bucket.</p>",
            "<p>There is an issue with the IAM role configuration.</p>"
          ],
          "explanation": "<p>The most likely cause of this error message is that either the S3 bucket policy or IAM role configuration has an error or simply does not grant the required permissions. The DevOps engineer should review the bucket policy or associated IAM user policies for any statements that might be denying access.</p><p>The engineer should also verify that the requests to the bucket meet any conditions in the bucket policy or IAM policies, checking for any incorrect deny statements, missing actions, or incorrect spacing in the policy.</p><p><strong>CORRECT: </strong>\"The bucket policy does not grant permission\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"There is an issue with the IAM role configuration\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Default encryption is enabled on the S3 bucket\" is incorrect.</p><p>This would not affect access to the object as the object would be automatically decrypted by S3.</p><p><strong>INCORRECT:</strong> \"The object has been moved to Amazon Glacier\" is incorrect.</p><p>With Glacier an AccessDeniedException error would be received if permission is not granted.</p><p><strong>INCORRECT:</strong> \"S3 versioning is enabled on the S3 bucket\" is incorrect.</p><p>Versioning does not affect access permissions to an object.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>"
        }
      },
      {
        "id": 143860745,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is using AWS Storage Gateway for a branch office location. The gateway is configured in file gateway mode in front of an Amazon S3 bucket that contains files that must be processed by workers in the branch office. Each night a batch process uploads many files to the S3 bucket. Users have reported that the new files are not visible in the morning though they do exist in the S3 bucket.</p><p>How can a DevOps engineer ensure that the files become visible?</p>",
          "answers": [
            "<p>Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration.</p>",
            "<p>Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded.</p>",
            "<p>Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway.</p>",
            "<p>Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command.</p>"
          ],
          "explanation": "<p>In file gateway mode the users and applications in the branch office can access the storage using either NFS or SMB protocols. The new objects in the bucket may sometimes not be visible. The RefreshCache operation refreshes the cached inventory of objects for the specified file share. This operation finds objects in the Amazon S3 bucket that were added, removed, or replaced since the gateway last listed the bucket's contents and cached the results.</p><p><strong>CORRECT: </strong>\"Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded\" is incorrect.</p><p>Volume Gateway mode uses block storage targets (iSCSI) rather than file storage targets (NFS/SMB) and would therefore be a major change to the architecture. Event notifications cannot be used to refresh the cache in a storage gateway.</p><p><strong>INCORRECT:</strong> \"Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway\" is incorrect. S3 replication cannot be used to replicate to storage gateway.</p><p><strong>INCORRECT:</strong> \"Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration\" is incorrect. Transfer acceleration is used to improve upload speeds and does not assist at all in this situation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html\">https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>"
        }
      },
      {
        "id": 115961513,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>",
          "answers": [
            "<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>",
            "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>",
            "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>",
            "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"
          ],
          "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 82921374,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.</p>\n\n<p>How can you improve the user experience with the least effort?</p>\n",
          "answers": [
            "<p>Enhance the health check so that the return status code corresponds to the connectivity to the database</p>",
            "<p>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</p>",
            "<p>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</p>",
            "<p>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enhance the health check so that the return status code corresponds to the connectivity to the database</strong></p>\n\n<p>Configuring health checks for the Application Load Balancer (ALB) is an important step to ensure that your AWS Cloud application runs smoothly. The ALB Health Check is configured with a protocol and port number to call on the target instances. A healthy EC2 instance is one that issues a response to a health check call with an HTTP 200 response code. Instances that return a status code that is other than the 2XX range or which time out are designated as being unhealthy and will not receive traffic from the ELB.</p>\n\n<p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q69-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>You could just add a simple health check endpoint to the ALB which accepts a request and immediately responds with an HTTP status of 200. This approach provides for a fast health check, but would not meet the requirement for the given use-case. You need to improve the quality of the health check and make sure it returns a proper status code. As the application depends on the database, you need to ensure that you include health checks for these components when determining the health of your service.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</strong> - Migrating to Beanstalk would require significant effort and even then it won't help gather detailed database-specific health checks.</p>\n\n<p><strong>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</strong> - Health Checks for the ALB are pretty basic and only work with the HTTP return status code, not the payload itself.</p>\n\n<p><strong>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</strong> - Route53 health checks can only be used to prevent DNS records from being returned from a DNS query, so it won't help for routing to specific instances behind an ALB (that's why we have health checks at the ALB level).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf\">https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf</a></p>\n"
        }
      },
      {
        "id": 99528211,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. It should not be possible to create IAM users unless the user account making the requires is specific on an exception list. A DevOps engineer must apply these restrictions.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>",
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>"
          ],
          "explanation": "<p>With AWS Organizations Service Control Policies (SCPs) you can restrict access to specific resources or define conditions for when SCPs are in effect. The condition element is supported when the Effect element has a value of Deny.</p><p>The StringNotLike condition will check the exception list for the aws:username that made the request. If the username is not present the request will be denied.</p><p><strong>CORRECT: </strong>\"Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list\" is incorrect.</p><p>The API action iam:CreateLoginProfile creates a password for the specified IAM user. This is not used to create the user account.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The API action iam:GetUser retrieves information about the specified IAM user, including the user's creation date, path, unique ID, and ARN.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The function should be used to validate that the iam:CreateUser request is being initiated by a user on the exception list. It should not delete the requesting user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>"
        }
      },
      {
        "id": 134588415,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A startup is developing an AI-powered traffic monitoring portal that will be hosted in AWS Cloud. The design of the cloud architecture should be highly available and fault-tolerant to avoid unnecessary outages that can affect the users. A DevOps Engineer was instructed to implement the architecture and also set up a system that automatically assesses applications for exposure, vulnerabilities, and deviations from the AWS best practices. </p><p>Among the options below, which is the MOST appropriate architecture that you should implement?</p>",
          "answers": [
            "<p>Use AWS Shield for automated security assessment to help improve the security and compliance of your applications. Launch an Auto Scaling group of Amazon EC2 instances on two Availability Zones with an Application Load Balancer in front. Set up a MySQL RDS database instance with Multi-AZ deployments configuration. Using Amazon Route 53, create a non-alias A record for the root domain to point to the load balancer.</p>",
            "<p>Use Amazon Inspector for automated security assessment to help improve the security and compliance of your applications. Launch an Auto Scaling group of Amazon EC2 instances on three Availability Zones. Set up an Application Load Balancer to distribute the incoming traffic. Set up an Amazon Aurora as the database tier. Using Amazon Route 53, create an alias record for the root domain to point to the load balancer.</p>",
            "<p>Use Amazon GuardDuty for automated security assessment to help improve the security and compliance of your applications. Set up an Amazon ElastiCache cluster for the database caching of the portal. Launch an Auto Scaling group of Amazon EC2 instances on four Availability Zones then associate it to an Application Load Balancer. Set up a MySQL RDS database instance with Multi-AZ deployments configuration and Read Replicas. Using Amazon Route 53, create a CNAME record for the root domain to point to the load balancer.</p>",
            "<p>Use Amazon Macie for automated security assessment to help improve the security and compliance of your applications. Set up Amazon DynamoDB as the database of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones with an Application Load Balancer in front to distribute the incoming traffic. Using Amazon Route 53, create a non-alias A record for the root domain to point to the load balancer.</p>"
          ],
          "explanation": "<p><strong>Amazon Inspector</strong> is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. These findings can be reviewed directly or as part of detailed assessment reports, which are available via the Amazon Inspector console or API.</p><p>Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Amazon Inspector assessments are offered to you as pre-defined rules packages mapped to common security best practices and vulnerability definitions. Examples of built-in rules include checking for access to your EC2 instances from the internet, remote root login being enabled, or vulnerable software versions installed. These rules are regularly updated by AWS security researchers.</p><p>If you host a website on multiple Amazon EC2 instances, you can distribute traffic to your website across the instances by using an Elastic Load Balancing (ELB) load balancer. The ELB service automatically scales the load balancer as traffic to your website changes over time. The load balancer can also monitor the health of its registered instances and route domain traffic only to healthy instances. To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. An alias record is a Route 53 extension to DNS. It's similar to a CNAME record, but you can create an alias record both for the root domain, such as tutorialsdojo.com, and for subdomains, such as <strong>www</strong>.tutorialsdojo.com. (You can create CNAME records only for subdomains.)</p><p><img src=\"https://media.tutorialsdojo.com/public/elb-tutorial-architecture-diagram_3AUG2023.png\"></p><p>You can use Amazon EC2 Auto Scaling to maintain a minimum number of running instances for your application at all times. Amazon EC2 Auto Scaling can detect when your instance or application is unhealthy and replace it automatically to maintain the availability of your application. You can also use Amazon EC2 Auto Scaling to scale your Amazon EC2 capacity up or down automatically based on demand, using criteria that you specify.</p><p>In this scenario, all of the options are highly available architectures. The main difference here is how they use Amazon Route 53. Keep in mind that you have to create an alias record in Amazon Route 53 in order to point to your load balancer.</p><p>Hence, the correct answer is: <strong>Use Amazon Inspector for automated security assessment to help improve the security and compliance of your applications. Launch an Auto Scaling group of Amazon EC2 instances on three Availability Zones. Set up an Application Load Balancer to distribute the incoming traffic. Set up an Amazon Aurora as the database tier. Using Amazon Route 53, create an alias record for the root domain to point to the load balancer.</strong></p><p>The option that says: <strong>Use Amazon Shield for automated security assessment to help improve the security and compliance of your applications. Launch an Auto Scaling group of Amazon EC2 instances on two Availability Zones with an Application Load Balancer in front. Set up a MySQL RDS database instance with Multi-AZ deployments configuration. Using Amazon Route 53, create a non-alias A record for the root domain to point to the load balancer</strong> is incorrect because AWS Shield is a managed Distributed Denial of Service (DDoS) protection service and not an automated security assessment. Moreover, you need to create an Alias record with the root DNS name and not an A record.</p><p>The option that says: <strong>Use Amazon GuardDuty for automated security assessment to help improve the security and compliance of your applications. Set up an Amazon ElastiCache cluster for the database caching of the portal. Launch an Auto Scaling group of Amazon EC2 instances on four Availability Zones then associate it to an Application Load Balancer. Set up a MySQL RDS database instance with Multi-AZ deployments configuration and Read Replicas. Using Amazon Route 53, create a CNAME record for the root domain to point to the load balancer</strong> is incorrect because Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts. The correct service that you should use is Amazon Inspector since this is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. In addition, you can create CNAME records only for subdomains and not for the zone apex or root domain.</p><p>The option that says: <strong>Use Amazon Macie for automated security assessment to help improve the security and compliance of your applications. Set up Amazon DynamoDB as the database of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones with an Application Load Balancer in front to distribute the incoming traffic. Using Amazon Route 53, create a non-alias A record for the root domain to point to the load balancer</strong> is incorrect because you should use Amazon Inspector instead of Amazon Macie since this is just a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. In addition, you should create an alias record with the root DNS name and not a non-alias A record.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><a href=\"http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>"
        }
      },
      {
        "id": 82921350,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>As part of the CICD pipeline, a DevOps Engineer is performing a functional test using a CloudFormation template that will later get deployed to production. That CloudFormation template creates an S3 bucket and a Lambda function which transforms images uploaded into S3 into thumbnails. To test the Lambda function, a few images are automatically uploaded and the thumbnail output is expected from the Lambda function on the S3 bucket. As part of the clean-up of these functional tests, the CloudFormation stack is deleted, but right now the delete fails.</p>\n\n<p>What's the reason and how could this issue be fixed?</p>\n",
          "answers": [
            "<p>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Create an additional Custom Resource backed by a Lambda function that performs a clean-up of the bucket</p>",
            "<p>The Lambda function is still using the S3 bucket and CloudFormation cannot, therefore, delete the S3 bucket. Place a <code>WaitCondition</code> on the Lambda function to fix the issue</p>",
            "<p>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Add the property <code>Delete: Force</code> to your CloudFormation template so that the S3 bucket is emptied before being deleted</p>",
            "<p>A StackPolicy prevents the CloudFormation template to be deleted. Clear the Stack Policy and try again</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Create an additional Custom Resource backed by a Lambda function that performs a clean-up of the bucket</strong></p>\n\n<p>In a CloudFormation template, you can use the AWS::CloudFormation::CustomResource or Custom::String resource type to specify custom resources. Custom resources provide a way for you to write custom provisioning logic in CloudFormation template and have CloudFormation run it during a stack operation, such as when you create, update or delete a stack.</p>\n\n<p>Some resources must be empty before they can be deleted. For example, you must delete all objects in an Amazon S3 bucket or remove all instances in an Amazon EC2 security group before you can delete the bucket or security group.</p>\n\n<p>For this use-case, the issue is that the S3 bucket is not empty before being deleted, therefore you must implement a Custom Resource backed by Lambda which will clean the bucket for you.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q20-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html\">https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Lambda function is still using the S3 bucket and CloudFormation cannot, therefore, delete the S3 bucket. Place a <code>WaitCondition</code> on the Lambda function to fix the issue</strong> - CloudFormation can delete resources while they're being used, and a <code>WaitCondition</code> can be attached to EC2 instances and Auto Scaling Groups and NOT to Lambda function. AWS further recommends that for Amazon EC2 and Auto Scaling resources, you use a CreationPolicy attribute instead of wait conditions. Add a CreationPolicy attribute to those resources, and use the cfn-signal helper script to signal when an instance creation process has completed successfully.</p>\n\n<p><strong>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Add the property <code>Delete: Force</code> to your CloudFormation template so that the S3 bucket is emptied before being deleted</strong> - This option has been added as a distractor. To clean it, you cannot use a <code>Delete: Force</code> as this is not a feature of CloudFormation.</p>\n\n<p><strong>A StackPolicy prevents the CloudFormation template to be deleted. Clear the Stack Policy and try again</strong> -  A stack policy is a JSON document that defines the update actions that can be performed on designated resources. Stack Policies are only used during CloudFormation stack updates.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html\">https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html</a></p>\n\n<p><a href=\"https://stackoverflow.com/questions/40383470/can-i-force-cloudformation-to-delete-non-empty-s3-bucket\">https://stackoverflow.com/questions/40383470/can-i-force-cloudformation-to-delete-non-empty-s3-bucket</a></p>\n"
        }
      },
      {
        "id": 82921422,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multi-national retail company is operating a multi-account strategy using AWS Organizations. Each account produces logs to CloudWatch Logs and the company would like to aggregate these logs under a single centralized account for archiving purposes. It needs the solution to be secure and centralized. The target destination for the logs should have little to no provisioning on the storage side.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution to meet these requirements?</p>\n",
          "answers": [
            "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3</p>",
            "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift</p>",
            "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon ES</p>",
            "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Lambda function on that log subscription, and implement a script to send the data to Amazon ES</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3</strong></p>\n\n<p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are Base64 encoded and compressed with the gzip format.</p>\n\n<p>For cross-account log data sharing with subscriptions, you can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Kinesis or Kinesis Data Firehose stream to perform custom processing and analysis. Therefore we have to subscribe the log destination to a Kinesis Firehose delivery stream, which in turn has a destination of S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift</strong> - The issue with this option is that the target for Kinesis Firehose is set as Amazon Redshift which is not a serverless service and requires provisioning.</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon ES</strong> - The issue with this option is that the target for Kinesis Firehose is set as Amazon ES which is not a serverless service and requires provisioning.</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Lambda function on that log subscription, and implement a script to send the data to Amazon ES</strong> - If the log destination is a Lambda function, this could work, but it will be a problem as this Lambda function sends the data to Amazon ES, which is not a serverless service and requires provisioning.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p>\n"
        }
      },
      {
        "id": 134588369,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A digital payment gateway system is running in AWS which serves thousands of businesses worldwide. It is hosted in an Auto Scaling Group of EC2 instances behind an Application Load Balancer with an Amazon RDS database in a Multi-AZ deployment configuration. The company is using several CloudFormation templates in deploying the new version of the system. The <code>AutoScalingRollingUpdate</code> policy is used to control how CloudFormation handles rolling updates for their Auto Scaling group which replaces the old instances based on the parameters they have set. Lately, there were a lot of failed deployments which has caused system unavailability issues and business disruptions. They want to find out what's preventing their Auto Scaling group from updating correctly during a stack update.&nbsp; </p><p>In this scenario, how should the DevOps engineer troubleshoot this issue? (Select THREE.)</p>",
          "answers": [
            "<p>Switch from <code>AutoScalingRollingUpdate</code> to <code>AutoScalingReplacingUpdate</code> policy by modifying the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template. Set the <code>WillReplace</code> property to true. </p>",
            "<p>In your <code>AutoScalingRollingUpdate</code> policy, set the <code>WaitOnResourceSignals</code> property to false.</p>",
            "<p>In your <code>AutoScalingRollingUpdate</code> policy, set the value of the <code>MinSuccessfulInstancesPercent</code> property to prevent AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch.</p>",
            "<p>During a rolling update, suspend the following Auto Scaling processes: <code>HealthCheck</code>, <code>ReplaceUnhealthy</code>, <code>AZRebalance</code>, <code>AlarmNotification</code>, and <code>ScheduledActions</code>.</p>",
            "<p>Suspend the following Auto Scaling processes that are related with your ELB: <code>Launch</code>, <code>Terminate</code>, and <code>AddToLoadBalancer</code>.</p>",
            "<p>Set the <code>WaitOnResourceSignals</code> property to true in your <code>AutoScalingRollingUpdate</code> policy.</p>"
          ],
          "explanation": "<p>The AWS::AutoScaling::AutoScalingGroup resource uses the UpdatePolicy attribute to define how an Auto Scaling group resource is updated when the AWS CloudFormation stack is updated. If you don't have the right settings configured for the UpdatePolicy attribute, your rolling update can produce unexpected results.</p><p>You can use the <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate\">AutoScalingRollingUpdate policy</a> to control how AWS CloudFormation handles rolling updates for an Auto Scaling group. This common approach keeps the same Auto Scaling group, and then replaces the old instances based on the parameters that you set.</p><p>The <strong>AutoScalingRollingUpdate</strong> policy supports the following configuration options:</p><p><br></p><pre class=\"prettyprint linenums\">    \"UpdatePolicy\": {\n      \"AutoScalingRollingUpdate\": {\n        \"MaxBatchSize\": Integer,\n        \"MinInstancesInService\": Integer,\n        \"MinSuccessfulInstancesPercent\": Integer,\n        \"PauseTime\": String,\n        \"SuspendProcesses\": [ List of processes ],\n        \"WaitOnResourceSignals\": Boolean\n      }\n    }\n</pre><p><br></p><p>Using a rolling update has a risk of system outages and performance degradation due to the decreased availability of your running EC2 instances. If you want to ensure high availability of your application, you can also use the <em>AutoScalingReplacingUpdate</em> policy to perform an immediate rollback of the stack without any possibility of failure.</p><p>To find out what's preventing your Auto Scaling group from updating correctly during a stack update, work through the following troubleshooting scenarios as needed:</p><p><strong>- Configure WaitOnResourceSignals and PauseTime to avoid problems with success signals</strong></p><p>In your <em>AutoScalingRollingUpdate</em> policy, set the <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate-waitonresourcesignals\"><em>WaitOnResourceSignals</em></a> property to false. Take note that if <em>WaitOnResourceSignals</em> is set to true, <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate-pausetime\"><em>PauseTime</em></a> changes to a timeout value. AWS CloudFormation waits to receive a success signal until the maximum time specified by the <em>PauseTime</em> value. If a signal is not received, AWS CloudFormation cancels the update. Then, AWS CloudFormation rolls back the stack with the same settings, including the same PauseTime value.</p><p><strong>- Configure MinSuccessfulInstancesPercent to avoid stack rollback</strong></p><p>If you're replacing a large number of instances during a rolling update and waiting for a success signal for each instance, complete the following: In your <em>AutoScalingRollingUpdate</em> policy, set the value of the <em>MinSuccessfulInstancesPercent</em> property. Take note that setting the <em>MinSuccessfulInstancesPercent</em> property prevents AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch.</p><p><strong>- Configure SuspendProcesses to avoid unexpected changes to the Auto Scaling group</strong></p><p>During a rolling update, suspend the following Auto Scaling processes: <em>HealthCheck</em>, <em>ReplaceUnhealthy</em>, <em>AZRebalance</em>, <em>AlarmNotification</em>, and <em>ScheduledActions</em>. It is quite important to know that if you're using your Auto Scaling group with Elastic Load Balancing (ELB), you should not suspend the following processes: <em>Launch</em>, <em>Terminate</em>, and <em>AddToLoadBalancer</em>. These processes are required to make rolling updates. Take note that if an unexpected scaling action changes the state of the Auto Scaling group during a rolling update, the update can fail. The failure can result from an inconsistent view of the group by AWS CloudFormation.</p><p>Based on the above information, the correct answers are:</p><p><strong>- In your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy, set the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> property to false.</strong></p><p><strong>- In your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy, set the value of the </strong><code><strong>MinSuccessfulInstancesPercent</strong></code><strong> property to prevent AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch</strong></p><p><strong>- During a rolling update, suspend the following Auto Scaling processes: </strong><code><strong>HealthCheck</strong></code><strong>, </strong><code><strong>ReplaceUnhealthy</strong></code><strong>, </strong><code><strong>AZRebalance</strong></code><strong>, </strong><code><strong>AlarmNotification</strong></code><strong>, and </strong><code><strong>ScheduledActions</strong></code></p><p><br></p><p>The option that says: <strong>Switch from </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> to </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy by modifying the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template. Set the </strong><code><strong>WillReplace</strong></code><strong> property to true</strong> is incorrect because although the <code><strong><em>AutoScalingReplacingUpdate</em></strong></code><strong><em> </em></strong>policy provides an immediate rollback of the stack without any possibility of failure, this solution is not warranted since the scenario asks for the options that will help troubleshoot the issue.</p><p>The option that says: <strong>Suspend the following Auto Scaling processes that are related with your ELB: </strong><code><strong>Launch</strong></code><strong>, </strong><code><strong>Terminate</strong></code><strong>, and </strong><code><strong>AddToLoadBalancer</strong></code> is incorrect because these processes are required by the ELB to make rolling updates.</p><p>The option that says: <strong>Set the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> property to true in your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy</strong> is incorrect. The <code>WaitOnResourceSignals</code> property should be set to false instead of true, to determine what prevents the Auto Scaling group from being updated correctly during a stack update.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      }
    ],
    "answers": {
      "75949044": [
        "c"
      ],
      "75949046": [
        "b"
      ],
      "75949098": [
        "d"
      ],
      "75949126": [
        "b"
      ],
      "82921340": [
        "a"
      ],
      "82921344": [
        "a"
      ],
      "82921350": [
        "a"
      ],
      "82921374": [
        "a"
      ],
      "82921422": [
        "a"
      ],
      "99528193": [
        "c",
        "e"
      ],
      "99528209": [
        "a"
      ],
      "99528211": [
        "c"
      ],
      "115961493": [
        "b"
      ],
      "115961513": [
        "b"
      ],
      "134588369": [
        "b",
        "c",
        "d"
      ],
      "134588373": [
        "d",
        "e",
        "f"
      ],
      "134588415": [
        "b"
      ],
      "134588447": [
        "a"
      ],
      "143860745": [
        "d"
      ],
      "143860749": [
        "a"
      ]
    }
  },
  {
    "id": "1770263232970",
    "date": "2026-02-05T03:47:12.970Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 19,
    "incorrect": 1,
    "unanswered": 0,
    "total": 20,
    "percent": 95,
    "duration": 6321134,
    "questions": [
      {
        "id": 99528195,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company requires an automated solution that terminates Amazon EC2 instances that have been logged into manually within 24 hours of the login event. The applications running in the account are launched using Auto Scaling groups and the CloudWatch Logs agent is configured on all instances.</p><p>How should a DevOps engineer build the automation?</p>",
          "answers": [
            "<p>Create a CloudWatch alarm that will trigger on AWS API call events in CloudTrail. Configure the alarm to send a message to an Amazon SQS queue. Create an AWS Lambda function that processes messages from the queue and terminates the instances that produced the login event.</p>",
            "<p>Create a CloudWatch alarm that will trigger on login events. Send the notification to a Kinesis Data Firehose stream. Configure the stream to send notifications to an SNS topic and instruct the operations team to subscribe to the topic and terminate EC2 instances that produced login events within 24 hours.</p>",
            "<p>Create a CloudWatch Logs subscription filter that delivers logs to an AWS Lambda function. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged.</p>",
            "<p>Create a CloudWatch Logs subscription filter that delivers logs to an AWS Step Functions state machine. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged.</p>"
          ],
          "explanation": "<p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to Kinesis, Lambda, or Kinesis Data Firehose. When log events are sent to the receiving service, they are base64 encoded and compressed with the gzip format.</p><p>The Lambda function that receives the log events can process the log files looking for entries that indicate that a manual login event occurred and add a tag. Another Lambda function that runs on a schedule can then look for instances that have been tagged and terminate them.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Logs subscription filter that delivers logs to an AWS Lambda function. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs subscription filter that delivers logs to an AWS Step Functions state machine. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged\" is incorrect.</p><p>You cannot configure Step Functions to receive the events.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that will trigger on AWS API call events in CloudTrail. Configure the alarm to send a message to an Amazon SQS queue. Create an AWS Lambda function that processes messages from the queue and terminates the instances that produced the login event\" is incorrect.</p><p>The API calls logged via CloudTrail will not show that a manual login event occurred as that does not require an AWS API call to be made. The manual login data would be included in the logs delivered to CloudWatch Logs by the CloudWatch agent.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that will trigger on login events. Send the notification to a Kinesis Data Firehose stream. Configure the stream to send notifications to an SNS topic and instruct the operations team to subscribe to the topic and terminate EC2 instances that produced login events within 24 hours\" is incorrect.</p><p>You can configure a KDS stream to receive the events but would then need a record processor (KCL worker) to process the events, you cannot send directly to an SNS topic. Also, requiring the operations team to process the terminations is not automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 67357150,
        "correct_response": [
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A support team wants to be notified via an Amazon Simple Notification Service (Amazon SNS) notification when an AWS Glue job fails a retry.</p>\n\n<p>As a DevOps Engineer, how will you implement this requirement?</p>\n",
          "answers": [
            "<p>Configure Amazon EventBridge events for AWS Glue. Configure an Amazon Simple Notification Service (Amazon SNS) notification when the Glue job fails a retry</p>",
            "<p>Amazon Simple Notification Service (Amazon SNS) cannot retry failures, leverage Amazon Simple Queue Service (Amazon SQS) dead-letter queues to retry the failed Glue jobs</p>",
            "<p>Check the AWS Personal Health Dashboard for failed AWS Glue jobs. Schedule an AWS Lambda function to pick the failed event from the service health dashboard and trigger an Amazon Simple Notification Service (Amazon SNS) notification when a retry fails</p>",
            "<p>Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found</strong></p>\n\n<p>Amazon EventBridge events for AWS Glue can be used to create Amazon SNS alerts, but the alerts might not be specific enough for certain situations. To receive SNS notifications for certain AWS Glue Events, such as an AWS Glue job failing on retry, you can use AWS Lambda. You can create a Lambda function to do the following:</p>\n\n<ol>\n<li>Check the incoming event for a specific string.</li>\n<li>Publish a message to Amazon SNS if the string in the event matches the string in the Lambda function.</li>\n</ol>\n\n<p>To use an AWS Lambda function to receive an email from SNS when any of your AWS Glue jobs fail a retry, do the following:\n1. Create an Amazon SNS topic.\n2. Create an AWS Lambda function.\n3. Create an Amazon EventBridge event that uses the Lambda function to initiate email notifications.</p>\n\n<p>AWS Lambda function logic:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q28-i1.jpg\">\nvia - <a href=\"https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts\">https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon EventBridge events for AWS Glue. Configure an Amazon Simple Notification Service (Amazon SNS) notification when the Glue job fails a retry</strong> - Amazon EventBridge cannot be directly used without a Lambda function since the use case needs notifications only for Glue job retry failure. So this logic has to be included in a Lambda function.</p>\n\n<p><strong>Amazon Simple Notification Service (Amazon SNS) cannot retry failures, leverage Amazon Simple Queue Service (Amazon SQS) dead-letter queues to retry the failed Glue jobs</strong> - The use case is not about retrying the Glue job, but about sending an SNS notification when the retry of the job fails.</p>\n\n<p><strong>Check the AWS Personal Health Dashboard for failed AWS Glue jobs. Schedule an AWS Lambda function to pick the failed event from the service health dashboard and trigger an Amazon Simple Notification Service (Amazon SNS) notification when a retry fails</strong> - AWS Personal Health Dashboard provides proactive notifications of scheduled activities, such as any changes to the infrastructure powering your resources, enabling you to better plan for events that may affect you. This option is not relevant to the given requirements.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts\">https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts</a></p>\n"
        }
      },
      {
        "id": 115961527,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>When deploying a newly developed application on AWS, a DevOps team notices an intermittent error when attempting to make a connection to the application.</p><p>The application has a two-tier architecture with an AWS Lambda function backed by an Amazon API gateway and a NoSQL database as the data store.</p><p>The DevOps team noticed that sometime after deployment the error stops occurring. This application is deployed by AWS CodeDeploy and the Lambda function is deployed as the last step of pipeline.</p><p>What is the most efficient way for a DevOps engineer to resolve the issue?</p>",
          "answers": [
            "<p>Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function.</p>",
            "<p>Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond.</p>",
            "<p>Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed.</p>",
            "<p>Use the ValidateService hook to validate that the deployment was completed successfully.</p>"
          ],
          "explanation": "<p>An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Here are descriptions of the hooks available for use in your AppSpec file.</p><p>\u00b7 <strong>BeforeAllowTraffic</strong> \u2013 Use to run tasks before traffic is shifted to the deployed Lambda function version.</p><p>\u00b7 <strong>AfterAllowTraffic</strong> \u2013 Use to run tasks after all traffic is shifted to the deployed Lambda function version.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-44-54-3b27da721fdb31cd0114ba6bbff8f1d5.jpg\"><p><strong>CORRECT: </strong>\"Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond\" is incorrect.</p><p>You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><strong>INCORRECT:</strong> \"Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed\" is incorrect.</p><p>Since the error resolves after some time, the issue will most likely be resolved by ensuring the application is not brought online until it is ready.</p><p><strong>INCORRECT:</strong> \"Use the ValidateService hook to validate that the deployment was completed successfully\" is incorrect.</p><p>This is used to verify the deployment was completed successfully, this will only detect deployment status and will not help in this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949074,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company has hired a DevOps engineer to provide consultancy services. The company runs Oracle and PostgreSQL services on Amazon RDS for storing large quantities of data generated by manufacturing equipment.</p><p>The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team.</p><p>To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p><p>Which solution should the DevOps engineer recommend to achieve these requirements?</p>",
          "answers": [
            "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift.</p>"
          ],
          "explanation": "<p>The AWS Database Migration Service (DMS) helps migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS DMS the data can be continuously replicated with high availability and the multiple databases can be consolidated into a petabyte-scale data warehouse.</p><p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance. During a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-02-48-34cbd839e896555130df72dfa20efc35.jpg\"><p><strong>CORRECT: </strong>\u201cUse AWS Database Migration Service to replicate the data from the databases into Amazon Redshift\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>AWS Glue is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.</p><p><strong>INCORRECT:</strong> \"Use Amazon EMR to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon EMR (previously called Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon Kinesis Data Streams is a fully managed, serverless data streaming service that stores and ingests various streaming data in real time at any scale. It is not suitable for migrating data between databases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>"
        }
      },
      {
        "id": 134588407,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A multinational company is using multiple AWS accounts for its global cloud architecture. The AWS resources in their production account are shared among various business units of the company. A single business unit may have one or more AWS accounts that have resources in the production account. Recently, there were a lot of incidents in which the developers from a specific business unit accidentally terminated the Amazon EC2 instances owned by another business unit. A DevOps Engineer was tasked to come up with a solution to only allow a specific business unit who owns the EC2 instances and other AWS resources to terminate their own resources. </p><p>How should the Engineer implement a multi-account strategy to satisfy this requirement?</p>",
          "answers": [
            "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access.</p>",
            "<p>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources.</p>",
            "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</p>",
            "<p>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts.</p>"
          ],
          "explanation": "<p><strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an <em>organization</em> that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.</p><p>You can use organizational units (OUs) to group accounts together to administer as a single unit. This greatly simplifies the management of your accounts. For example, you can attach a policy-based control to an OU, and all accounts within the OU automatically inherit the policy. You can create multiple OUs within a single organization, and you can create OUs within other OUs. Each OU can contain multiple accounts, and you can move accounts from one OU to another. However, OU names must be unique within a parent OU or root.</p><p><img src=\"https://media.tutorialsdojo.com/aws-organizations.jpg\"></p><p>Resource-level permissions refer to the ability to specify which resources users are allowed to perform actions on. Amazon EC2 has partial support for resource-level permissions. This means that for certain Amazon EC2 actions, you can control when users are allowed to use those actions based on conditions that have to be fulfilled or specific resources that users are allowed to use. For example, you can grant users permissions to launch instances, but only of a specific type and only using a specific AMI.</p><p>Hence, the correct answer is: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</strong></p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access</strong> is incorrect. The AWSServiceRoleForOrganizations service-linked role is primarily used to only allow AWS Organizations to create service-linked roles for other AWS services. This service-linked role is present in all organizations and not just in a specific OU.</p><p>The option that says: <strong>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources<em> </em></strong>is incorrect. Although the use of the AWS Control Tower is right, the aggregator feature is simply an AWS Config resource type that collects AWS Config configuration and compliance data from the following various AWS accounts. In addition, you have to launch the Customizations for AWS Control Tower (CfCT) on the same AWS region where your AWS Control Tower landing zone is deployed, and not on a different account, to put it in effect.</p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts</strong> is incorrect. AWS Service Catalog simply allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A more suitable service to use here is AWS Organizations.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>"
        }
      },
      {
        "id": 82921426,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a business travel solutions company wants to use CodeDeploy to ensure zero downtime during deployments through rolling updates. The team wants to deploy the company's flagship web application on a set of 5 EC2 instances running behind an Application Load Balancer. The team would like the deployment to be gradual and to automatically rollback in case of a failed deployment, which is determined by the application not being able to pass health checks.</p>\n\n<p>As a DevOps Engineer, which of the following options would you recommend for the given use-case?</p>\n",
          "answers": [
            "<p>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</p>",
            "<p>Integrate CodeDeploy with the Application Load Balancer. In case the Application Load Balancers fails the health checks on the instances where the new version has been deployed, it will notify CodeDeploy. Configure CodeDeploy to rollback on deployment failures</p>",
            "<p>In the <code>AfterInstall</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</p>",
            "<p>Create a CloudWatch Event rule on CodeDeploy to invoke a Lambda function upon deployment on every instance. The Lambda function tests the health check, and if it fails, stops the CodeDeploy deployment using the <code>StopDeployment</code> API, and then start a new deployment of the old version using the <code>CreateDeployment</code> API</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</strong></p>\n\n<p>CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>The AppSpec file is used to manage each deployment as a series of lifecycle event hooks, which are defined in the file. During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file.</p>\n\n<p>Sample appspec file:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p>\n\n<p>List of Lifecycle Event hooks for EC2 deployment:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q10-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>Lifecycle Event hooks availability for EC2 deployment and rollback scenarios:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q10-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>For the given use-case, you can use <code>ValidateService</code> hook to verify that the deployment was completed successfully. This is the last deployment lifecycle event. You can configure CodeDeploy to rollback if this hook fails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Integrate CodeDeploy with the Application Load Balancer. In case the Application Load Balancers fails the health checks on the instances where the new version has been deployed, it will notify CodeDeploy. Configure CodeDeploy to rollback on deployment failures</strong> - Integrating CodeDeploy with the Application Load Balancer will ensure traffic isn't forwarded to the instances that CodeDeploy is currently deploying to, but the health check feature is not integrated with CodeDeploy and therefore you cannot rollback when the Application Load Balancers fails the health check.</p>\n\n<p><strong>In the <code>AfterInstall</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</strong> - The <code>AfterInstall</code> hook in <code>appspec.yml</code> is before <code>StartApplication</code> and therefore won't be able to test the application's health checks. You can use the <code>AfterInstall</code> hook for tasks such as configuring your application or changing file permissions.</p>\n\n<p><strong>Create a CloudWatch Event rule on CodeDeploy to invoke a Lambda function upon deployment on every instance. The Lambda function tests the health check, and if it fails, stops the CodeDeploy deployment using the <code>StopDeployment</code> API, and then start a new deployment of the old version using the <code>CreateDeployment</code> API</strong> - The CloudWatch Event rule won't work as it is not granular at each instance's level, and CodeDeploy has a native feature for doing rollbacks, instead of doing API calls via <code>StopDeployment</code> and <code>CreateDeployment</code>.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n"
        }
      },
      {
        "id": 75949158,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": true,
        "prompt": {
          "question": "<p>A financial services company requires that DevOps engineers should not log directly into Amazon EC2 instances that process highly sensitive data except in exceptional circumstances. The security team requires a notification within 15 minutes if a DevOps engineer does log in to an instance.</p><p>Which solution will meet these requirements with the least operational overhead?</p>",
          "answers": [
            "<p>Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>"
          ],
          "explanation": "<p>The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. The agent includes the following components:</p><p>\u00b7 A plug-in to the AWS CLI that pushes log data to CloudWatch Logs.</p><p>\u00b7 A script (daemon) that initiates the process to push data to CloudWatch Logs.</p><p>\u00b7 A cron job that ensures that the daemon is always running.</p><p>You can create metric filters to match terms in your log events and convert log data into metrics. When a metric filter matches a term, it increments the metric's count. For example, you can create a metric filter that counts the number of times the word <strong><em>ERROR</em></strong> occurs in your log events.</p><p>In this case the metric filter can search for user login data and then if this information is found it can send an SNS notification to the security team.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>The Systems Manager agent will not gather this information from EC2 instances. The CloudWatch Logs agent must be installed.</p><p><strong>INCORRECT:</strong> \"Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>CloudTrail will only report on API activity, and this does not include login data from an Amazon EC2 instance.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>This is possible though it is not the best solution as it requires the script to be rerun on a regular basis and requires more operational overhead to create and maintain.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 67357124,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A developer configured an AWS CloudFormation template to create custom resource necessary for the project. The AWS Lambda function for the custom resource executed successfully as seen by the successful creation of the custom resource. But, the CloudFormation stack is not transitioning from in-progress status (CREATE_IN_PROGRESS) to completion status (CREATE_COMPLETE).</p>\n\n<p>Which step did the developer possibly miss for the successful completion of the CloudFormation stack?</p>\n",
          "answers": [
            "<p>Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL</p>",
            "<p>The AWS CloudFormation resource <code>AWS::CloudFormation::CustomResource</code> should be used to specify a custom resource in the template</p>",
            "<p>If the template developer and custom resource provider are configured to the same person or entity then CloudFormation stack completion fails</p>",
            "<p>After executing the <code>send</code> method in the <code>cfn-response</code> module, the Lambda function terminates, so anything written after this method is ignored</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL</strong></p>\n\n<p>Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. Use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic.</p>\n\n<p>The custom resource provider processes the AWS CloudFormation request and returns a response of SUCCESS or FAILED to the pre-signed URL. The custom resource provider responds with a JSON-formatted file and uploads it to the pre-signed S3 URL. If this URL is not provided, the calling template will not get an update of the status of the Lambda function and will remain in an in-progress state.</p>\n\n<p>How custom resources work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The AWS CloudFormation resource <code>AWS::CloudFormation::CustomResource</code> should be used to specify a custom resource in the template</strong> - You can use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. However, this has no bearing on the given use case.</p>\n\n<p><strong>If the template developer and custom resource provider are configured to the same person or entity then CloudFormation stack completion fails</strong> - This statement is incorrect. The template developer and custom resource provider can be the same person or entity.</p>\n\n<p><strong>After executing the <code>send</code> method in the <code>cfn-response</code> module, the Lambda function terminates, so anything written after this method is ignored</strong> - The <code>cfn-response</code> module is available only when you use the ZipFile property to write your source code. This is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html</a></p>\n"
        }
      },
      {
        "id": 134588403,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A business has its AWS accounts managed by AWS Organizations and has employees in different countries. The business is reviewing its AWS account security policies and is looking for a way to monitor its AWS accounts for unusual behavior that is associated with an IAM identity. The business wants to:</p><ul><li><p>send a notification to any employee for whom the unusual activity is detected.</p></li><li><p>send a notification to the user's team leader.</p></li><li><p>an external messaging platform will send the notifications. The platform requires a target user-id for each recipient.</p></li></ul><p>The business already has an API that can be used to retrieve the team leader's and the employee's user-id from IAM user names.<br><br>Which solution will satisfy the requirements?</p>",
          "answers": [
            "<p>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
            "<p>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
            "<p>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>",
            "<p>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function.</p>"
          ],
          "explanation": "<p><strong>Amazon GuardDuty </strong>is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. A <strong>GuardDuty finding</strong> represents a potential security issue detected within your network. GuardDuty generates a finding whenever it detects unexpected and potentially malicious activity in your AWS environment.</p><p><img src=\"https://media.tutorialsdojo.com/public/amazon-guardduty-dop-c02.png\"></p><p>In this scenario, findings from Amazon GuardDuty are published to Amazon EventBridge as events that can be used to trigger a Lambda function which will send notifications to the external messaging platform.</p><p>Hence, the correct answer is: <strong>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and</strong><br><strong>invoke the Lambda function.</strong></p><p>The option that says: <strong>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a rule in Amazon EventBridge from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function</strong> is incorrect because Amazon Detective will not by itself detect unusual activity. Detective provides analysis information related to a given finding.</p><p>The option that says: <strong>Choose an AWS account in the organization to serve as the Amazon GuardDuty administrator. Add the business' AWS accounts as GuardDuty member accounts that are associated with the GuardDuty administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the GuardDuty administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function</strong> is incorrect because Amazon SNS can filter messages by attributes and not by message contents. An EventBridge rule would be required to publish to the SNS topic.</p><p>The option that says: <strong>Choose an AWS account in the organization to serve as the Amazon Detective administrator. Add the business' AWS accounts as Detective member accounts that are associated with the Detective administrator account. Create a Lambda function to perform the user-id lookup and deliver notifications to the external messaging platform. Create a topic in SNS from the Detective administrator account to match the Impact:IAMUser/AnomalousBehavior notification type and invoke the Lambda function</strong> is incorrect because Amazon Detective will not by itself detect unusual activity. In addition, Amazon SNS can filter messages by attributes and not by message contents. An EventBridge rule would be required to publish to the SNS topic.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings.html</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-guardduty/?src=udemy\">https://tutorialsdojo.com/amazon-guardduty/</a></p>"
        }
      },
      {
        "id": 75949064,
        "correct_response": [
          "b",
          "d",
          "e"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer builds an artifact locally and then uploads it to an Amazon S3 bucket. The application has a local cache that must be cleared as part of the deployment. The engineer executes a command to do this, retrieves the artifact from Amazon S3, and unzips the artifact to complete the deployment.</p><p>The engineer wants to migrate to an automated CI/CD solution and incorporate checks to stop and roll back the deployment in the event of a failure. This requires tracking the progression of the deployment.</p><p>Which combination of actions will accomplish this? (Select THREE.)</p>",
          "answers": [
            "<p>Check the code into a code repository. On each pull into master use Amazon CloudWatch Events to trigger an AWS Lambda function that builds the artifact and uploads it to Amazon S3.</p>",
            "<p>Write a custom script that clears the cache and specify the script in the BeforeInstall lifecycle hook in the AppSpec file.</p>",
            "<p>Add user data to the Amazon EC2 instances that contains script to clear the cache. Once deployed, test the application. If it is not successful, deploy it again.</p>",
            "<p>Set up an AWS CodePipeline pipeline to deploy the application. Check the code into a code repository as a source for the pipeline.</p>",
            "<p>Use AWS CodeBuild to build the artifact and upload it to Amazon S3. Use AWS CodeDeploy to deploy the artifact to the Amazon EC2 instances.</p>",
            "<p>Use AWS Systems Manager to download the artifact from Amazon S3 and deploy it to all the EC2 instances.</p>"
          ],
          "explanation": "<p>The engineer wants to build an automated CI/CD pipeline. Therefore, the best solution is to use a code repository such as CodeCommit for committing the code. Once committed a CodePipeline will automatically pick up the changes and initiate CodeBuild which will build the artifacts and upload the S3.</p><p>After the build artifact has been uploaded CodeDeploy can then be used to deploy the application. The AppSpec file is used by CodeDeploy during deployments. The engineer should add the script to clear the cache to the BeforeInstall lifecycle hook, so it is executed before the install occurs.</p><p><strong>CORRECT: </strong>\"Write a custom script that clears the cache and specify the script in the BeforeInstall lifecycle hook in the AppSpec file\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Set up an AWS CodePipeline pipeline to deploy the application. Check the code into a code repository as a source for the pipeline\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS CodeBuild to build the artifact and upload it to Amazon S3. Use AWS CodeDeploy to deploy the artifact to the Amazon EC2 instances\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check the code into a code repository. On each pull into master use Amazon CloudWatch Events to trigger an AWS Lambda function that builds the artifact and uploads it to Amazon S3\" is incorrect.</p><p>A better solution is to use CodePipeline which is designed for automating CI/CD pipelines and CodeBuild for building the artifact.</p><p><strong>INCORRECT:</strong> \"Add user data to the Amazon EC2 instances that contains script to clear the cache. Once deployed, test the application. If it is not successful, deploy it again\" is incorrect.</p><p>User data only runs when the instance is first started so is not useful for running any commands after that time.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to download the artifact from Amazon S3 and deploy it to all of the EC2 instances\" is incorrect.</p><p>Systems Manager is not suitable for deploying application updates and CodeDeploy should be used instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 134588439,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An educational startup has developed an e-learning platform hosted in AWS, where they sell their online courses. The CTO is planning to release an online forum that will allow students to interact with each other and post their questions. This new feature requires a new DynamoDB table named <code>Thread</code> in which the partition key is <code>ForumName</code> and the sort key is <code>Subject</code>. Below is a diagram that shows how the items in the table must be organized:</p><p><img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/LSI_01.png \"></p><p>The forum must also find all of the threads that were posted in a particular forum within the last three months as part of the system's quarterly reporting.</p><p>Which of the following is the MOST suitable solution that you should implement?</p>",
          "answers": [
            "<p>Refactor the e-learning platform to do a <code>Scan</code> operation in the entire <code>Thread</code> table. Discard any posts that were not within the specified time frame.</p>",
            "<p>Create a new DynamoDB table with a local secondary index. Refactor the e-learning platform to use the <code>Query</code> operation for search and utilize the <code>LastPostDateTime</code> attribute as the sort key.</p>",
            "<p>Configure the e-learning platform to use a <code>Query</code> operation for the entire <code>Thread</code> table. Discard any posts that were not within the specified time frame.</p>",
            "<p>Create a new DynamoDB table with a global secondary index. Refactor the e-learning platform to use a <code>Query</code> operation for search and to utilize the <code>LastPostDateTime</code> attribute as the sort key.</p>"
          ],
          "explanation": "<p><strong>DynamoDB</strong> supports two types of secondary indexes:</p><p>- <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">Global secondary index</a> \u2014 an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions.</p><p>- <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">Local secondary index</a> \u2014 an index that has the same partition key as the base table, but a different sort key. A local secondary index is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.</p><p><br></p><p>A local secondary index maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.</p><p>Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to <code>Scan</code> the entire <em>Thread </em>table and discard any posts that were not within the specified time frame. With a local secondary index, a <code>Query</code> operation could use <em>LastPostDateTime</em> as a sort key and find the data quickly.</p><p>In the provided scenario, you can create a local secondary index named <em>LastPostIndex </em>to meet the requirements. Note that the partition key is the same as that of the <em>Thread</em> table, but the sort key is <em>LastPostDateTime </em>as shown in the diagram below:</p><p><img src=\"https://media.tutorialsdojo.com/public/LastPostIndex%C2%A0.png\"></p><p>Hence, the most effective solution in this scenario is to: <strong>Create a new DynamoDB table with a local secondary index. Refactor the e-learning platform to use the </strong><code><strong>Query</strong></code><strong> operation for search and utilize the </strong><code><strong>LastPostDateTime</strong></code><strong> attribute as the sort key.</strong></p><p>The option that says: <strong>Refactor the e-learning platform to do a </strong><code><strong>Scan</strong></code><strong> operation in the entire </strong><code><strong>Thread</strong></code><strong> table. Discard any posts that were not within the specified time frame</strong> is incorrect. Although this option is valid, this solution would consume a large amount of provisioned read-throughput and take a long time to complete. This is not a scalable solution, and the time it takes to fetch the data will continue to increase as the table grows.</p><p>The option that says: <strong>Configure the e-learning platform to use a </strong><code><strong>Query</strong></code><strong> operation for the entire </strong><code><strong>Thread table</strong></code><strong>. Discard any posts that were not within the specified time frame</strong><em> </em>is incorrect because using the Query operation is not sufficient to meet this requirement. You have to create a local secondary index when you create the table to narrow down the results and to improve the performance of your application.</p><p>The option that says: <strong>Create a new DynamoDB table with a global secondary index. Refactor the e-learning platform to use a </strong><code><strong>Query</strong></code><strong> operation for search and to utilize the </strong><code><strong>LastPostDateTime</strong></code><strong> attribute as the sort key</strong> is incorrect because using a local secondary index is a more appropriate solution to be used in this scenario. Take note that in this scenario, it is still using the same partition key (<code>ForumName</code>) but with an alternate sort key (<code>LastPostDateTime</code>) which warrants the use of a local secondary index.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 115961515,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 5",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The security team at a company requires a solution to identify activities that indicate that Amazon EC2 instances have been compromised. The solution should notify them by email if issues are discovered.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Configure AWS GuardDuty to identify activities that indicate the EC2 instances have been compromised. Create an Amazon EventBridge rule with an event source set to \u2018aws.guardduty\u2019. Send a notification using an Amazon SNS topic when the specified events are logged.</p>",
            "<p>Configure Amazon Inspector to identify activities that indicate the EC2 instances have been compromised. Configure Inspector to send notifications directly via an Amazon SNS topic when there are changes in the state of findings.</p>",
            "<p>Install the AWS Systems Manager agent on the EC2 instances and attach an instance profile with the necessary permissions. Configure Systems Manager to alert via an Amazon SNS topic if malicious activities are detected on the EC2 instances.</p>",
            "<p>Create an AWS CloudTrail trail and log API events that indicate account compromise. Create an alarm in Amazon CloudWatch based on a custom metric filter for the API events. Send a notification via an Amazon SNS topic.</p>"
          ],
          "explanation": "<p>Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, EC2 workloads, container applications, and data stored in Amazon Simple Storage Service (S3)</p><p>GuardDuty provides threat detection services that cover several types of activity. One of these is instance compromise as per the following description of the scope of this service:</p><p><em>Activity indicating an instance compromise, such as cryptocurrency mining, backdoor command, and control (C&amp;C) activity, malware using domain generation algorithms (DGA), outbound denial of service activity, unusually high network traffic volume, unusual network protocols, outbound instance communication with a known malicious IP, temporary Amazon EC2 credentials used by an external IP address, and data exfiltration using DNS.</em></p><p>The GuardDuty findings are automatically logged to Amazon CloudWatch Events. You can then use EventBridge to create a rule that triggers an action when certain events that match the filter pattern are logged. In this case the trigger will be Amazon SNS as the security team require an email notification.</p><p><strong>CORRECT: </strong>\"Configure AWS GuardDuty to identify activities that indicate the EC2 instances have been compromised. Create an Amazon EventBridge rule with an event source set to \u2018aws.guardduty\u2019. Send a notification using an Amazon SNS topic when the specified events are logged\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure Amazon Inspector to identify activities that indicate the EC2 instances have been compromised. Configure Inspector to send notifications directly via an Amazon SNS topic when there are changes in the state of findings\" is incorrect.</p><p>Inspector does not identify instance compromise activities, use GuardDuty for this use case.</p><p><strong>INCORRECT:</strong> \"Install the AWS Systems Manager agent on the EC2 instances and attach an instance profile with the necessary permissions. Configure Systems Manager to alert via an Amazon SNS topic if malicious activities are detected on the EC2 instances\" is incorrect.</p><p>Systems Manager does not identify instance compromise activities, use GuardDuty for this use case.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail trail and log API events that indicate account compromise. Create an alarm in Amazon CloudWatch based on a custom metric filter for the API events. Send a notification via an Amazon SNS topic\" is incorrect.</p><p>The question asks for identifying activities relating to instance compromise rather than account compromise. Either way, GuardDuty is the best tool for this job as it will discover possible compromise by looking at CloudTrail API events as well as other data sources.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/features/\">https://aws.amazon.com/guardduty/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-guardduty/\">https://digitalcloud.training/aws-guardduty/</a></p>"
        }
      },
      {
        "id": 67357130,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The security policy of a company mandates encrypting all AMIs that the company shares across its AWS accounts. An AWS account (Account A) has a custom AMI that is not encrypted. This AMI needs to be shared with another AWS Account B. Account B has Amazon EC2 instances configured with an Auto Scaling group that will use the AMI. Account A already has an AWS Key Management Service (AWS KMS) key.</p>\n\n<p>As a DevOps Engineer, which combination of steps will you take to share the AMI with Account B while adhering to the company's security policy? (Select two)</p>\n",
          "answers": [
            "<p>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</p>",
            "<p>In Account B, create a KMS grant that delegates permissions to the service-linked role attached to the Auto Scaling group</p>",
            "<p>In Account A, create an encrypted AMI from the unencrypted version with AWS managed key. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</p>",
            "<p>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify permissions on the AMI to be accessible from Account B</p>",
            "<p>In Account A, create an encrypted AMI from the unencrypted version and encrypt the associated EBS snapshots with it. Specify the KMS key in the copy action. Share the encrypted AMI with Account B</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong></p>\n\n<p><strong>In Account B, create a KMS grant that delegates permissions to the service-linked role attached to the Auto Scaling group</strong></p>\n\n<p>As per the security policy of the company, encrypt the unencrypted AMI using the KMS key. The encrypted snapshots must be encrypted with a KMS key. You can\u2019t share AMIs that are backed by snapshots that are encrypted with the default AWS-managed key.</p>\n\n<p>Amazon EC2 Auto Scaling uses service-linked roles to delegate permissions to other AWS services. Amazon EC2 Auto Scaling service-linked roles are predefined and include permissions that Amazon EC2 Auto Scaling requires to call other AWS services on your behalf. The predefined permissions also include access to your AWS-managed keys. However, they do not include access to your customer-managed keys, allowing you to maintain full control over these keys.</p>\n\n<p>If you create a customer-managed key in a different account than the Auto Scaling group, you must use a grant in combination with the key policy to allow cross-account access to the key. This is a two-step process (refer to the image attached)</p>\n\n<ol>\n<li><p>The first policy allows Account A to give an IAM user or role in the specified Account B permission to create a grant for the key. However, this does not by itself give any users access to the key.</p></li>\n<li><p>Then, from Account B which contains the Auto Scaling group, create a grant that delegates the relevant permissions to the appropriate service-linked role. The <code>Grantee Principal</code> element of the grant is the ARN of the appropriate service-linked role. The <code>key-id</code> is the ARN of the key.</p></li>\n</ol>\n\n<p>Key policy sections that allow cross-account access to the customer-managed key:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q18-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version with AWS managed key. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong></p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify permissions on the AMI to be accessible from Account B</strong></p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and encrypt the associated EBS snapshots with it. Specify the KMS key in the copy action. Share the encrypted AMI with Account B</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html</a></p>\n"
        }
      },
      {
        "id": 82921430,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at an auditing firm has deployed its flagship application on Elastic Beanstalk that processes invoices uploaded by customers in CSV form. The invoices can be quite big, with up to 10MB and 1,000,000 records total. Processing is CPU intensive which results in slowing down the application. Customers are sent an email when the processing is done, through the use of a cron job. The auditing firm has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>What do you recommend for the application to ensure a good performance and address scalability requirements?</p>\n",
          "answers": [
            "<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>",
            "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>",
            "<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>",
            "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>AWS Elastic Beanstalk enables you to manage all of the resources that run your application as environments. An environment is a collection of AWS resources running an application version. When you launch an Elastic Beanstalk environment, you need to choose an environment tier. An application that serves HTTP requests runs in a web server environment tier. A backend environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier.</p>\n\n<p>Elastic Beanstalk Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p>When you create a web server environment, Beanstalk provisions the resources required to run your application. AWS resources created for this type of environment include one elastic load balancer, an Auto Scaling group, and one or more Amazon Elastic Compute Cloud (Amazon EC2) instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p>AWS resources created for a worker environment tier include an ASG, one or more Amazon EC2 instances, and an IAM role. For the worker environment tier, Beanstalk also creates and provisions an SQS queue if you don\u2019t already have one. When you launch a worker environment, Beanstalk installs the necessary support files for your programming language of choice and a daemon on each EC2 instance in the ASG. The daemon reads messages from an SQS queue. The daemon sends data from each message that it reads to the web application running in the worker environment for processing.</p>\n\n<p>For the given use-case, the worker tier is used to asynchronously process the invoices from an SQS queue. SQS size limit is 256KB and therefore the files must be uploaded to S3 and a reference to them should be sent to SQS by the web tier. Finally, the <code>cron.yml</code> file must be defined on the worker tier. Using this strategy we have decoupled our processing tier from our web tier, and CPU usage will go down as a result. The worker tier will also be able to easily scale in case many invoices are uploaded.</p>\n\n<p>Elastic Beanstalk Worker environment:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/aeb-architecture_worker.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong> - As mentioned in the explanation above, the worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The <code>cron.yml</code> file must be defined on the worker tier, it is not supported by the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n"
        }
      },
      {
        "id": 143860765,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer is troubleshooting problems with an application which uses AWS Lambda to process messages in an Amazon SQS standard queue. The function sometimes fails to process the messages in the queue. The engineer needs to analyze the events to determine the cause of the issue and update the function code.</p><p>Which action should the engineer take to achieve this outcome?</p>",
          "answers": [
            "<p>Enable FIFO support for the queue to preserve ordering of the messages.</p>",
            "<p>Configure a redrive policy to move the messages to a dead-letter queue.</p>",
            "<p>Enable long-polling by increasing WaitTimeSeconds parameter.</p>",
            "<p>Configure a delay queue by increasing the DelaySeconds parameter.</p>"
          ],
          "explanation": "<p>You can configure a dead-letter queue (DLQ) by specifying a redrive policy on the SQS queue. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p><p><strong>CORRECT: </strong>\"Configure a redrive policy to move the messages to a dead-letter queue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable FIFO support for the queue to preserve ordering of the messages\" is incorrect.</p><p>You cannot enable FIFO on a standard queue, and it does not help with this issue anyway.</p><p><strong>INCORRECT:</strong> \"Enable long-polling by increasing WaitTimeSeconds parameter\" is incorrect.</p><p>Long polling just helps with efficiency of API calls as it waits for messages to appear in the queue rather than returning an immediate response. This does not assist with isolating messages for analysis.</p><p><strong>INCORRECT:</strong> \"Configure a delay queue by increasing the DelaySeconds parameter\" is incorrect.</p><p>A delay queue simply delays visibility of the messages for a specified time. This does not assist with this issue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>"
        }
      },
      {
        "id": 82921392,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A cyber forensics company would like to ensure that CloudTrail is always enabled in its AWS account. It also needs to have an audit trail of the status for CloudTrail. In the case of compliance breaches, the company would like to automatically resolve them.</p>\n\n<p>As a DevOps Engineer, how can you implement a solution for this requirement?</p>\n",
          "answers": [
            "<p>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</p>",
            "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the 'everyone' group automatically</p>",
            "<p>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>",
            "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/config/faq/\">https://aws.amazon.com/config/faq/</a></p>\n\n<p>You need to have an AWS Config rule to maintain auditability and track compliance over time. You can use the <code>cloudtrail-enabled</code> Config managed rule to check whether AWS CloudTrail is enabled in your AWS account. You can use <code>cloudtrail-security-trail-enabled</code> Config managed rules to check that there is at least one AWS CloudTrail trail defined with security best practices. To be alerted of compliance issues, use a CloudWatch Event rule and then hook it to a Lambda function that will re-enable CloudTrail automatically.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the 'everyone' group automatically</strong></p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong></p>\n\n<p>IAM users in a group with a deny policy sounds like a great idea at first, but then you have to remember you can create IAM roles, and they won't have that restriction, and as such you will be able to assume these roles and then issue API calls on CloudTrail to de-activate it. This solution won't work and therefore both these options are incorrect.</p>\n\n<p><strong>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong> - You need to have an AWS Config rule to maintain auditability and track compliance over time, as using the Lambda function to trigger an API call would tell you about the CloudTrail status only at that point in time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/config/faq/\">https://aws.amazon.com/config/faq/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n"
        }
      },
      {
        "id": 115961505,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 5",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application uses an Elastic Load Balancer (ELB) in front of an Auto Scaling group of Amazon EC2 instances. A recent update to the application has resulted in longer times to run and complete bootstrap scripts. The instances often become healthy before they are ready to accept traffic resulting in errors. A DevOps engineer must prevent the instances from being registered with Elastic Load Balancing until the instances are ready to accept traffic.</p><p>Which solution meets these requirements?</p>",
          "answers": [
            "<p>Create an AWS Lambda function that uses the Auto Scaling API to suspend the health check processes until the bootstrap scripts are complete.</p>",
            "<p>Increase the health check grace period from 300 seconds to 600 seconds to ensure the instances are not marked as healthy before they are ready to accept traffic.</p>",
            "<p>Use an Auto Scaling lifecycle hook to verify that the bootstrap scripts have completed before registering the instances with the ELB.</p>",
            "<p>Increase the health check timeout from the default value to 120 seconds and configure the healthy threshold count to 5.</p>"
          ],
          "explanation": "<p>Amazon EC2 Auto Scaling offers the ability to add lifecycle hooks to your Auto Scaling groups. These hooks let you create solutions that are aware of events in the Auto Scaling instance lifecycle, and then perform a custom action on instances when the corresponding lifecycle event occurs.</p><p>A popular use of lifecycle hooks is to control when instances are registered with Elastic Load Balancing. By adding a launch lifecycle hook to your Auto Scaling group, you can ensure that your bootstrap scripts have completed successfully and the applications on the instances are ready to accept traffic before they are registered to the load balancer at the end of the lifecycle hook.</p><p>The following illustration shows the transitions between Auto Scaling instance states:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-33-11-47dc5f5dc473e042857e287968db6fd8.jpg\"><p><strong>CORRECT: </strong>\"Use an Auto Scaling lifecycle hook to verify that the bootstrap scripts have completed before registering the instances with the ELB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increase the health check timeout from the default value to 120 seconds and configure the healthy threshold count to 5\" is incorrect.</p><p>The question specifically states that the solution should prevent the instances from being registered with Elastic Load Balancing until the instances are ready to accept traffic. This solution does not meet the requirement.</p><p><strong>INCORRECT:</strong> \"Increase the health check grace period from 300 seconds to 600 seconds to ensure the instances are not marked as healthy before they are ready to accept traffic\" is incorrect.</p><p>The HealthCheckGracePeriod parameter for the Auto Scaling group helps Amazon EC2 Auto Scaling distinguish unhealthy instances from newly launched instances that are not yet ready to serve traffic. This grace period can prevent Amazon EC2 Auto Scaling from marking InService instances as unhealthy and terminating them before they have time to finish initializing. This does not affect registration with the load balancer.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that uses the Auto Scaling API to suspend the health check processes until the bootstrap scripts are complete\" is incorrect.</p><p>A better solution would be to suspend registration with the load balancer but that was not offered. Suspending health check processes does not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 138248201,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps Engineer in a leading aerospace engineering company has a hybrid cloud architecture that connects its on-premises data center with AWS via Direct Connect Gateway. There is a new requirement to implement an automated OS patching solution for all of the Windows servers hosted on-premises as well as in AWS Cloud. The AWS Systems Manager service should be utilized to automate the patching of the servers.</p><p>Which combination of steps should be set up to satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS <code>AssumeRoleWithSAML</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
            "<p>Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS <code>AssumeRole</code> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>mi-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager Patch Manager.</p>",
            "<p>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an <code>i-</code> prefix. Apply the patches using the Systems Manager State Manager.</p>"
          ],
          "explanation": "<p>A hybrid environment includes on-premises servers and virtual machines (VMs) that have been configured for use with Systems Manager, including VMs in other cloud environments. After following the steps below, the users who have been granted permissions by the AWS account administrator can use AWS Systems Manager to configure and manage their organization's on-premises servers and virtual machines (VMs).</p><p>To configure your hybrid servers and VMs for AWS Systems Manager, just follow these provided steps:</p><p>1. Complete General Systems Manager Setup Steps<br>2. Create an IAM Service Role for a Hybrid Environment<br>3. Install a TLS certificate on On-Premises Servers and VMs<br>4. Create a Managed-Instance Activation for a Hybrid Environment<br>5. Install SSM Agent for a Hybrid Environment (Windows)<br>6. Install SSM Agent for a Hybrid Environment (Linux)<br>7. (Optional) Enable the Advanced-Instances Tier</p><p><br></p><p>Configuring your hybrid environment for Systems Manager enables you to do the following:</p><p>- Create a consistent and secure way to remotely manage your hybrid workloads from one location using the same tools or scripts.</p><p>- Centralize access control for actions that can be performed on your servers and VMs by using AWS Identity and Access Management (IAM).</p><p>- Centralize auditing and your view into the actions performed on your servers and VMs by recording all actions in AWS CloudTrail.</p><p>- Centralize monitoring by configuring CloudWatch Events and Amazon SNS to send notifications about service execution success.</p><p><img src=\"https://media.tutorialsdojo.com/public/how-it-works.png\">After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as <em>managed instances</em>. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix \"mi-\". Amazon EC2 instance IDs use the prefix \"i-\".</p><p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants <code>AssumeRole</code> trust to the Systems Manager service. You only need to create the service role for a hybrid environment once for each AWS account.</p><p>Hence, the correct answers are:</p><p><strong>- Set up a single IAM service role for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRole</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation.</strong></p><p><strong>- Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>mi-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager.</strong></p><p>The option that says: <strong>Set up several IAM service roles for AWS Systems Manager to enable the service to execute the STS </strong><code><strong>AssumeRoleWithSAML</strong></code><strong> operation. Allow the generation of service tokens by registering the IAM role. Use the service role to perform the managed-instance activation </strong>is incorrect because you have to execute the <code><em>AssumeRole </em></code>operation instead and not the <code><em>AssumeRoleWithSAML</em></code><em> </em>operation<em>. </em>Moreover, you only need to set up a single IAM service role.</p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager Patch Manager </strong>is incorrect because the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix in the SSM console and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix<strong><em>.</em></strong></p><p>The option that says: <strong>Download and install the SSM Agent on the hybrid servers using the activation codes and activation IDs obtained. Register the servers or virtual machines on-premises to the AWS Systems Manager service. In the SSM console, the hybrid instances will show with an </strong><code><strong>i-</strong></code><strong> prefix. Apply the patches using the Systems Manager State Manager</strong> is incorrect because the AWS Systems Manager State Manager is just a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. You have to apply the patches using the Systems Manager Patch Manager instead. In addition, the hybrid instances will show with an <code><strong><em>mi-</em></strong></code><strong><em> </em></strong>prefix and not with an <code><strong><em>i-</em></strong></code><strong><em> </em></strong>prefix.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248157,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is hosting their high-frequency trading application in AWS which serves millions of investors around the globe. The application is hosted in an Auto Scaling Group of EC2 instances behind an Application Load Balancer with an Amazon DynamoDB database. The architecture was deployed using a CloudFormation template with a Route 53 record. There recently was a production deployment that had caused system degradation and outage, costing the company a significant monetary loss due to their application's unavailability. As a result, the company instructed their DevOps engineer to implement an efficient strategy for deploying updates to their web application with the ability to perform an immediate rollback of the stack. All deployments should maintain the normal number of active EC2 instances to keep the performance of the application.</p><p>Which of the following should the DevOps engineer implement to satisfy these requirements?&nbsp; </p>",
          "answers": [
            "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template to use the <code>AutoScalingReplacingUpdate</code> policy. Update the required properties for the new Auto Scaling group policy. Set the <code>WillReplace</code> property to true.</p>",
            "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template to use the <code>AutoScalingReplacingUpdate</code> policy. Set the <code>WillReplace</code> property to false. Also, specify the <code>AutoScalingRollingUpdate</code> policy to update instances that are in an Auto Scaling group in batches.</p>",
            "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template to use the <code>AutoScalingRollingUpdate</code> policy. Update the required properties for the new Auto Scaling group policy. Set the <code>MinSuccessfulInstancesPercent</code> property, as well as its corresponding <code>WaitOnResourceSignals</code> and <code>PauseTime</code> properties.</p>",
            "<p>Configure the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::DeploymentUpdates</code> resource in the CloudFormation template to use the <code>AutoScalingReplacingUpdate</code> policy. Update the required properties for the new Auto Scaling group policy. Set the <code>WillReplace</code> property to false.</p>"
          ],
          "explanation": "<p>If you plan to launch an Auto Scaling group of EC2 instances, you can configure the <code>AWS::AutoScaling::AutoScalingGroup</code> resource type reference in your CloudFormation template to define an Amazon EC2 Auto Scaling group with the specified name and attributes. To configure Amazon EC2 instances launched as part of the group, you can specify a launch template. It is recommended that you use a launch template to make sure that you can use the latest features of Amazon EC2, such as T2 Unlimited instances.</p><p>You can add an UpdatePolicy attribute to your Auto Scaling group to perform rolling updates (or replace the group) when a change has been made to the group.</p><p>To specify how AWS CloudFormation handles replacement updates for an Auto Scaling group, use the <code>AutoScalingReplacingUpdate</code> policy. This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-11-22_03-48-11-a352802536c1d852293fc176ddad72a9.png\">During replacement, AWS CloudFormation retains the old group until it finishes creating the new one. If the update fails, AWS CloudFormation can roll back to the old Auto Scaling group and delete the new Auto Scaling group. While AWS CloudFormation creates the new group, it doesn't detach or attach any instances. After successfully creating the new Auto Scaling group, AWS CloudFormation deletes the old Auto Scaling group during the cleanup process.</p><p>When you set the <code>WillReplace</code> parameter, remember to specify a matching CreationPolicy. If the minimum number of instances (specified by the MinSuccessfulInstancesPercent property) doesn't signal success within the Timeout period (specified in the CreationPolicy policy), the replacement update fails, and AWS CloudFormation rolls back to the old Auto Scaling group.</p><p>Hence, the correct answer is: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy. Update the required properties for the new Auto Scaling group policy. Set the </strong><code><strong>WillReplace</strong></code><strong> property to true.</strong></p><p>The option that says: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy. Set the </strong><code><strong>WillReplace</strong></code><strong> property to false. Also, specify the </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy to update instances that are in an Auto Scaling group in batches</strong> is incorrect because if both the <code>AutoScalingReplacingUpdate</code> and <code>AutoScalingRollingUpdate</code> policies are specified, setting the <code>WillReplace</code> property to <code>true</code> gives <code>AutoScalingReplacingUpdate</code> precedence. But since this property is set to false, then the <code>AutoScalingRollingUpdate</code> policy will take precedence instead.</p><p>The option that says: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy. Update the required properties for the new Auto Scaling group policy. Set the </strong><code><strong>MinSuccessfulInstancesPercent</strong></code><strong> property, you must also enable the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> and </strong><code><strong>PauseTime</strong></code><strong> properties</strong> is incorrect because this type of deployment will affect the existing compute capacity of your application. The rolling update doesn't maintain the total number of active EC2 instances during deployment. A better solution is to use the <em>AutoScalingReplacingUpdate</em> policy instead, which will create a separate Auto Scaling group and is able to perform an immediate rollback of the stack in the event of an update failure.</p><p>The option that says: <strong>Configure the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::DeploymentUpdates</strong></code><strong> resource in the CloudFormation template to use the </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy. Update the required properties for the new Auto Scaling group policy. Set the </strong><code><strong>WillReplace</strong></code><strong> property to false</strong> is incorrect because there is no <code>AWS::AutoScaling::DeploymentUpdates</code> resource. You have to use the <code>AWS::AutoScaling::AutoscalingGroup</code> resource instead and set the <code>WillReplace</code> property to true.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 82921376,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": true,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A financial planning company runs a tax optimization application that allows people to enter their personal financial information and get recommendations. The company is committed to the maximum security for the Personally identifiable information (PII) data in S3 buckets, and as part of compliance requirements, it needs to implement a solution to be alerted in case of new PII and its access in S3.</p>\n\n<p>As an AWS Certified DevOps Engineer, which solution would you recommend such that it needs MINIMUM development effort?</p>\n",
          "answers": [
            "<p>Enable Amazon Macie on the selected S3 buckets. Setup alerting using CloudWatch Events</p>",
            "<p>Enable Amazon GuardDuty on the select S3 buckets. Setup alerting using CloudWatch Alarms</p>",
            "<p>Create an Amazon Lambda function that is integrated with Amazon Sagemaker to detect PII data. Integrate the Lambda function with S3 events for PUT requests</p>",
            "<p>Set up an S3 bucket policy that filters requests containing PII data using a conditional statement</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable Amazon Macie on the selected S3 buckets. Setup alerting using CloudWatch Events</strong></p>\n\n<p>Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3.</p>\n\n<p>How Macie Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/macie/Product-Page-Diagram_AWS-Macie@2x.369dcc5a001e7a44b121d65637ff82b60b809148.png\">\nvia - <a href=\"https://aws.amazon.com/macie/\">https://aws.amazon.com/macie/</a></p>\n\n<p>For the given use-case, you can enable Macie on specific S3 buckets and then configure SNS notifications via CloudWatch events for Macie alerts.\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q63-i1.jpg\"></p>\n\n<p>For a deep-dive on how to query PII data using Macie, please refer to this excellent blog:\n<a href=\"https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/\">https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Amazon GuardDuty on the select S3 buckets. Setup alerting using CloudWatch Alarms</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.</p>\n\n<p>How GuardDuty Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png\">\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p><strong>Create an Amazon Lambda function that is integrated with Amazon Sagemaker to detect PII data. Integrate the Lambda function with S3 events for PUT requests</strong> - Amazon Lambda + Sagemager might work but it requires significant development effort and probably won't yield excellent results.</p>\n\n<p><strong>Set up an S3 bucket policy that filters requests containing PII data using a conditional statement</strong> - S3 bucket policies cannot be used to analyze the data payload in a request. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/\">https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/</a></p>\n\n<p><a href=\"https://aws.amazon.com/macie/\">https://aws.amazon.com/macie/</a></p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n"
        }
      }
    ],
    "answers": {
      "67357124": [
        "a"
      ],
      "67357130": [
        "a",
        "b"
      ],
      "67357150": [
        "d"
      ],
      "75949064": [
        "b",
        "d",
        "e"
      ],
      "75949074": [
        "d"
      ],
      "75949158": [
        "d"
      ],
      "82921376": [
        "a"
      ],
      "82921392": [
        "a"
      ],
      "82921426": [
        "a"
      ],
      "82921430": [
        "a"
      ],
      "99528195": [
        "c"
      ],
      "115961505": [
        "c"
      ],
      "115961515": [
        "a"
      ],
      "115961527": [
        "a"
      ],
      "134588403": [
        "c"
      ],
      "134588407": [
        "c"
      ],
      "134588439": [
        "b"
      ],
      "138248157": [
        "a"
      ],
      "138248201": [
        "b",
        "c"
      ],
      "143860765": [
        "b"
      ]
    }
  },
  {
    "id": "1770242634433",
    "date": "2026-02-04T22:03:54.433Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 19,
    "incorrect": 1,
    "unanswered": 0,
    "total": 20,
    "percent": 95,
    "duration": 3455044,
    "questions": [
      {
        "id": 82921444,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a leading bitcoin wallet and exchange services company is trying to deploy a CloudFormation template that contains a Lambda Function, an S3 bucket, an IAM role, and a DynamoDB table from CodePipeline but the team is getting an <code>InsufficientCapabilitiesException</code>.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you suggest fixing this issue?</p>\n",
          "answers": [
            "<p>Enable the IAM Capability on the CodePipeline configuration for the Deploy CloudFormation stage action</p>",
            "<p>Update the CodePipeline IAM Role so it has permissions to create all the resources mentioned in the CloudFormation template</p>",
            "<p>Fix the CloudFormation template as there is circular dependency and CloudFormation does not have that capability</p>",
            "<p>Increase the service limits for your S3 bucket limits as you've reached it</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable the IAM Capability on the CodePipeline configuration for the Deploy CloudFormation stage action</strong></p>\n\n<p>With AWS CloudFormation and CodePipeline, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. For example, you can create a workflow that automatically builds a test stack when you submit an updated template to a code repository. After AWS CloudFormation builds the test stack, you can test it and then decide whether to push the changes to a production stack. Use CodePipeline to build a continuous delivery workflow by building a pipeline for AWS CloudFormation stacks. CodePipeline has built-in integration with AWS CloudFormation, so you can specify AWS CloudFormation-specific actions, such as creating, updating, or deleting a stack, within a pipeline.</p>\n\n<p>You can use IAM with AWS CloudFormation to control what users can do with AWS CloudFormation, such as whether they can view stack templates, create stacks, or delete stacks.</p>\n\n<p>For the given use-case, <code>InsufficientCapabilitiesException</code> means that the CloudFormation stack is trying to create an IAM role but it doesn't have those specified capabilities. As such it must be configured in CodePipeline configuration for the Deploy CloudFormation stage action.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q19-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the CodePipeline IAM Role so it has permissions to create all the resources mentioned in the CloudFormation template</strong> - The given exception is not related to the permissions of the user or the CodePipeline IAM Role running the CloudFormation template, so this option is incorrect.</p>\n\n<p><strong>Fix the CloudFormation template as there is circular dependency and CloudFormation does not have that capability</strong> - A circular dependency, as the name implies, means that two resources are dependent on each other or that a resource is dependent on itself.</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/b7eb6c689c037217079766fdb77c3bac3e51cb4c/2019/03/19/circular-dependency-sg-group.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n\n<p>This option is incorrect as a circular dependency would trigger another error such as this:</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/b7eb6c689c037217079766fdb77c3bac3e51cb4c/2019/03/19/CircularDependencyerror.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n\n<p><strong>Increase the service limits for your S3 bucket limits as you've reached it</strong> - This option has been added as a distractor as the exception has nothing to do with service limits for the S3 bucket.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html#using-iam-capabilities\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html#using-iam-capabilities</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n"
        }
      },
      {
        "id": 99528193,
        "correct_response": [
          "c",
          "e"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer launched an Amazon EC2 instance in an Amazon VPC. The instance must download an object from a restricted Amazon S3 bucket. When trying to download the object, a 403 Access Denied error was received.</p><p>What are two possible causes for this error? (Select TWO.)</p>",
          "answers": [
            "<p>Default encryption is enabled on the S3 bucket.</p>",
            "<p>The object has been moved to Amazon Glacier.</p>",
            "<p>The bucket policy does not grant permission.</p>",
            "<p>S3 versioning is enabled on the S3 bucket.</p>",
            "<p>There is an issue with the IAM role configuration.</p>"
          ],
          "explanation": "<p>The most likely cause of this error message is that either the S3 bucket policy or IAM role configuration has an error or simply does not grant the required permissions. The DevOps engineer should review the bucket policy or associated IAM user policies for any statements that might be denying access.</p><p>The engineer should also verify that the requests to the bucket meet any conditions in the bucket policy or IAM policies, checking for any incorrect deny statements, missing actions, or incorrect spacing in the policy.</p><p><strong>CORRECT: </strong>\"The bucket policy does not grant permission\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"There is an issue with the IAM role configuration\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Default encryption is enabled on the S3 bucket\" is incorrect.</p><p>This would not affect access to the object as the object would be automatically decrypted by S3.</p><p><strong>INCORRECT:</strong> \"The object has been moved to Amazon Glacier\" is incorrect.</p><p>With Glacier an AccessDeniedException error would be received if permission is not granted.</p><p><strong>INCORRECT:</strong> \"S3 versioning is enabled on the S3 bucket\" is incorrect.</p><p>Versioning does not affect access permissions to an object.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>"
        }
      },
      {
        "id": 143860765,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is troubleshooting problems with an application which uses AWS Lambda to process messages in an Amazon SQS standard queue. The function sometimes fails to process the messages in the queue. The engineer needs to analyze the events to determine the cause of the issue and update the function code.</p><p>Which action should the engineer take to achieve this outcome?</p>",
          "answers": [
            "<p>Enable FIFO support for the queue to preserve ordering of the messages.</p>",
            "<p>Configure a redrive policy to move the messages to a dead-letter queue.</p>",
            "<p>Enable long-polling by increasing WaitTimeSeconds parameter.</p>",
            "<p>Configure a delay queue by increasing the DelaySeconds parameter.</p>"
          ],
          "explanation": "<p>You can configure a dead-letter queue (DLQ) by specifying a redrive policy on the SQS queue. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p><p><strong>CORRECT: </strong>\"Configure a redrive policy to move the messages to a dead-letter queue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable FIFO support for the queue to preserve ordering of the messages\" is incorrect.</p><p>You cannot enable FIFO on a standard queue, and it does not help with this issue anyway.</p><p><strong>INCORRECT:</strong> \"Enable long-polling by increasing WaitTimeSeconds parameter\" is incorrect.</p><p>Long polling just helps with efficiency of API calls as it waits for messages to appear in the queue rather than returning an immediate response. This does not assist with isolating messages for analysis.</p><p><strong>INCORRECT:</strong> \"Configure a delay queue by increasing the DelaySeconds parameter\" is incorrect.</p><p>A delay queue simply delays visibility of the messages for a specified time. This does not assist with this issue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>"
        }
      },
      {
        "id": 75949062,
        "correct_response": [
          "a",
          "d",
          "f"
        ],
        "prompt": {
          "question": "<p>A company needs is deploying a new application in AWS and requires a CI/CD pipeline to automate process. The company requires that the entire CI/CD pipeline can be re-provisioned in different AWS accounts or Regions within minutes.</p><p>The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. A DevOps engineer has already created an AWS CodeCommit repository to store the source code.</p><p>Which combination of actions should the DevOps engineer take to meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Configure an AWS CodePipeline pipeline with a build stage using AWS CodeBuild.</p>",
            "<p>Copy the build artifact from CodeCommit to Amazon S3.</p>",
            "<p>Launch Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline.</p>",
            "<p>Launch Amazon EC2 instances in an AWS Elastic Beanstalk environment and configure the environment as the deployment target in AWS CodePipeline.</p>",
            "<p>Implement an Amazon SQS queue to decouple the pipeline components.</p>",
            "<p>Provision all resources using AWS CloudFormation.</p>"
          ],
          "explanation": "<p>The DevOps engineer should create a pipeline using AWS CodePipeline to automate the entire deployment. The CodeCommit repository can be used as the source. The combinations of CodeBuild with Elastic Beanstalk provides a way to build, test, and deploy with automatic rollback upon failure.</p><p>The question also requires that the entire CI/CD pipeline can be recreated in different accounts and Regions. For this reason the pipeline should be deployed using AWS CloudFormation. The templates can then be easily used to recreate the entire stack.</p><p><strong>CORRECT: </strong>\"Configure an AWS CodePipeline pipeline with a build stage using AWS CodeBuild\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Launch Amazon EC2 instances in an AWS Elastic Beanstalk environment and configure the environment as the deployment target in AWS CodePipeline\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Provision all resources using AWS CloudFormation\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Copy the build artifact from CodeCommit to Amazon S3\" is incorrect.</p><p>There is no need to do this, CodeCommit can be used directly as a source for source code and build artifacts.</p><p><strong>INCORRECT:</strong> Implement an Amazon SQS queue to decouple the pipeline components\" is incorrect.</p><p>CodePipeline has its own built-in capabilities for passing information durably between stages and does not require decoupling using Amazon SQS.</p><p><strong>INCORRECT:</strong> \"Launch Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline\" is incorrect.</p><p>This solution would not provide the automatic rollback upon failure requested. Automatic rollback can be implemented when using Elastic Beanstalk with CodeBuild. Otherwise, you would need CodeDeploy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949068,
        "correct_response": [
          "b",
          "c"
        ],
        "prompt": {
          "question": "<p>To increase security, a company plans to use AWS Systems Manager Session Manager to managed Amazon EC2 instances rather than SSH. The connectivity to Session Manager should also use a private network connection.</p><p>Which configuration actions should be taken to implement this? (Select TWO.)</p>",
          "answers": [
            "<p>Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range.</p>",
            "<p>Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager.</p>",
            "<p>Create a VPC endpoint for Systems Manager in the Region where the instances are running.</p>",
            "<p>Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables.</p>",
            "<p>Establish private Session Manager connectivity using the instance IDs of EC2 instances.</p>"
          ],
          "explanation": "<p>Systems Manager Session Manager enables secure remote access to EC2 instances without the need to open ports for SSH or create bastion hosts. You can connect to instances through Session Manager privately by establishing a VPC endpoint in your VPC. This ensures that all connectivity takes place using private addresses.</p><p>The EC2 instances must have the Systems Manager agent running and they must have permissions to be able to communicate with the Systems Manager service. An instance profile can be easily attached to provide these permissions to instances.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-59-28-dac1a7fafdbe3507d5ca4b1fe07ed213.jpg\"><p><strong>CORRECT: </strong>\"Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Systems Manager in the Region where the instances are running\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range\" is incorrect.</p><p>Port 22 is used by the SSH protocol and is not required by Systems Manager Session Manager.</p><p><strong>INCORRECT:</strong> \"Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables\" is incorrect.</p><p>You cannot connect to EC2 instances via a NAT gateway. These gateways are used for outbound (internet) connectivity from the EC2 instances in private subnets.</p><p><strong>INCORRECT:</strong> \"Establish private Session Manager connectivity using the instance IDs of EC2 instances\" is incorrect.</p><p>You cannot connect privately to instances simply by connecting via the instance ID. You must establish a VPC endpoint and attach the necessary permissions to instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 134588459,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>You are deploying a critical web application with Elastic Beanstalk using the \u201cRolling\u201d deployment policy. Your Elastic Beanstalk environment configuration has an RDS DB instance attached to it and used by your application servers. The deployment failed when you deployed a major version. And it took even more time to rollback changes because you have to manually redeploy the old version. </p><p>Which of the following options will you implement to prevent this from happening in future deployments?</p>",
          "answers": [
            "<p>Configure <code>Rolling with additional batch</code> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</p>",
            "<p>Configure <code>All at once</code> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</p>",
            "<p>Configure <code>Immutable</code> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</p>",
            "<p>Implement a Blue/green deployment strategy in your Elastic Beanstalk environment for future deployments of your web application. Ensure that the RDS DB instance is still tightly coupled with the environment.</p>"
          ],
          "explanation": "<p><strong>Immutable</strong> environment updates are an alternative to rolling updates. Immutable environment updates ensure that configuration changes that require replacing instances are applied efficiently and safely. If an immutable environment update fails, the rollback process requires only terminating an Auto Scaling group. A failed rolling update, on the other hand, requires performing an additional rolling update to roll back the changes.</p><p>To perform an immutable environment update, Elastic Beanstalk creates a second, temporary Auto Scaling group behind your environment's load balancer to contain the new instances. First, Elastic Beanstalk launches a single instance with the new configuration in the new group. This instance serves traffic alongside all of the instances in the original Auto Scaling group that are running the previous configuration.</p><p>When the first instance passes health checks, Elastic Beanstalk launches additional instances with the new configuration, matching the number of instances running in the original Auto Scaling group. When all of the new instances pass health checks, Elastic Beanstalk transfers them to the original Auto Scaling group, and terminates the temporary Auto Scaling group and old instances.</p><p>Refer to the table below for the characteristics of each deployment method as well as the amount of time it takes to do the deployment, as seen in the <strong>Deploy Time</strong> column:</p><p><img src=\"https://media.tutorialsdojo.com/public/DeploymentMethods_2AUG2023.png\"></p><p>Hence, the correct answer is: <strong>Configure </strong><code><strong>Immutable</strong></code><strong> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</strong></p><p>The option that says: <strong>Configure </strong><code><strong>Rolling with additional batch</strong></code><strong> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application </strong>is incorrect because this deployment type is just similar to rolling deployments and hence, this will not help alleviate the root cause of the issue in this scenario.</p><p>The option that says: <strong>Configure </strong><code><strong>All at once</strong></code><strong> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application<em> </em></strong>is incorrect because this will cause a brief downtime during deployment and hence, this is not ideal for deploying your critical production applications.</p><p>The option that says: <strong>Implement a Blue/green deployment strategy in your Elastic Beanstalk environment for future deployments of your web application. Ensure that the RDS DB instance is still tightly coupled with the environment</strong> is incorrect because a Blue/green deployment requires that your environment runs independently of your production database. This means that you have to decouple your RDS database from your environment. If your Elastic Beanstalk environment has an attached Amazon RDS DB instance, the data will be lost if you terminate the original (blue) environment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rollingupdates.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rollingupdates.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 138248237,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The Development team of a leading IT consultancy company would like to add a manual approval action before their new application versions are deployed to their production environment. The approval action must be strictly enforced even if the unit and integration tests are all successful. They have set up a pipeline using CodePipeline to orchestrate the workflow of their continuous integration and continuous delivery processes. The new versions of the application are built using CodeBuild and are deployed to a fleet of Amazon EC2 instances using CodeDeploy. </p><p>Which of the following provides the SIMPLEST and the MOST cost-effective solution?</p>",
          "answers": [
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</p>",
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>"
          ],
          "explanation": "<p>You can automate your release process by using AWS CodePipeline to test your code and run your builds with CodeBuild. You can create reports in CodeBuild that contain details about tests that are run during builds.</p><p>You can create tests such as unit tests, configuration tests, and functional tests. The test file format can be JUnit XML or Cucumber JSON. Create your test cases with any test framework that can create files in one of those formats (for example, Surefire JUnit plugin, TestNG, and Cucumber). To create a test report, you add a report group name to the buildspec file of a build project with information about your test cases. When you run the build project, the test cases are run and a test report is created. You do not need to create a report group before you run your tests. If you specify a report group name, CodeBuild creates a report group for you when you run your reports. If you want to use a report group that already exists, you specify its ARN in the buildspec file.</p><p>In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p><img src=\"https://media.tutorialsdojo.com/public/pipeline.png\"></p><p>If the action is approved, the pipeline execution resumes. If the action is rejected\u2014or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping\u2014the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p>Hence, the correct answer is: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</strong></p><p>The option that says: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. You can just simply set up a manual approval action instead of creating a custom action. That takes a lot of effort to configure including the development of a custom job worker.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. It is tedious to automatically perform the unit and integration tests using AWS Step Functions. You can just use CodeBuild to handle all of the tests.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. This solution entails an additional burden to install, configure and launch a third-party CI/CD tool in Amazon EC2. A more simple solution is to just use CodeBuild for tests.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p>"
        }
      },
      {
        "id": 82921412,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>An IT company is deploying a Python Flask based application and would like to ensure that it has a base AMI that contains the necessary Python runtime, as well as OS patches. That AMI must be used able to be referenced programmatically from across all regions in your account in a scalable way. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>Which of the following options would you recommend for this use-case? (Select two)</p>\n",
          "answers": [
            "<p>Create an SSM Automation document to create the AMI in a repeatable manner</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Use AWS Inspector to create a patched AMI using the latest working AMI</p>",
            "<p>Use AWS Lambda to create a patched AMI using the latest working AMI</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an SSM Automation document to create the AMI in a repeatable manner</strong></p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>You can use AWS Systems Manager to create a maintenance window, and then register an Automation task to automate the creation of the AMIs. This process is applicable for both Windows and Linux instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q46-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</strong></p>\n\n<p>The AMI ID is region-scoped, so the AMI must be copied across regions and therefore each SSM parameter store will have different AMI ID values. But you can still use the same SSM Parameter Store key across all regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</strong> - The AMI ID is region-scoped and the AMI must be copied across regions for the solution to work. This option only copies the value of the AMI ID across regions but the AMI itself stays in one region. So this option is incorrect.</p>\n\n<p><strong>Use AWS Inspector to create a patched AMI using the latest working AMI</strong> - AWS Inspector can be leveraged to analyze EC2 instance OS and network vulnerabilities. You cannot use Inspector to create a patched AMI.</p>\n\n<p><strong>Use AWS Lambda to create a patched AMI using the latest working AMI</strong> - AWS Lambda cannot be used to create AMIs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n"
        }
      },
      {
        "id": 82921340,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A financial services company has a solution in place to track all the API calls made by users, applications, and SDK within the AWS account. Recently, it has experienced a hack and could find a user amongst the logs that did some compromising API calls. The company wants to know with 100% certainty that the log files represent the correct sequence of events and have not been altered. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following would you suggest as the most effective solution?</p>\n",
          "answers": [
            "<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</p>",
            "<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</p>",
            "<p>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</p>",
            "<p>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p>How CloudTrail Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation. This feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p>For the given use-case, to track API calls made within your account, you need to use AWS CloudTrail. Then the right way to verify log integrity would be to use the CloudTrail <code>validate-logs</code> command.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</strong> -  S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as \u201cwrite once read many\u201d (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn't provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won't give us the information on who made the API calls.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won't give us the information of who made the API calls. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn't provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n"
        }
      },
      {
        "id": 75949166,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An application sits behind a Network Load Balancer (NLB) that is configured with a TLS listener. The DevOps team must analyze traffic patterns and require information about the connections made by clients. The data that is captured must be stored securely with encryption at rest and should only be accessible to the DevOps team members.</p><p>Which actions should a DevOps engineer take?</p>",
          "answers": [
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing provides access logs that capture detailed information about the TLS requests sent to your Network Load Balancer. You can use these access logs to analyze traffic patterns and troubleshoot issues. The logs are sent to an Amazon S3 bucket you configure as the logging destination. This bucket can be encrypted using one of the available server-side encryption options.</p><p>When you enable access logging, you must specify an S3 bucket for the access logs. The policy must grant permission to the AWS service account \u2018delivery.logs.amazonaws.com\u2019. In this case, the DevOps team also require permissions to access the bucket, and this can be granted through an IAM policy attached to the team members, most likely via an IAM group.</p><p><strong>CORRECT: </strong>\"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account\" is incorrect.</p><p>This will not allow read access for the DevOps team as the only permission is write access.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 75949066,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. When bringing new EC2 instances online, the application must be tested before traffic can be directed to the instances.</p><p>Which Auto Scaling process should a DevOps engineer suspend to allow testing to be performed without removing the instances from the Auto Scaling group?</p>",
          "answers": [
            "<p>Suspend the process Health Check.</p>",
            "<p>Suspend the process AddToLoadBalancer.</p>",
            "<p>Suspend the process Replace Unhealthy.</p>",
            "<p>Suspend the process AZ Rebalance.</p>"
          ],
          "explanation": "<p>Auto Scaling processes can be suspended and then resumed. You might want to do this, for example, so that you can investigate a configuration issue that is causing the process to fail, or to prevent Amazon EC2 Auto Scaling from marking instances unhealthy and replacing them while you are making changes to your Auto Scaling group.</p><p>In this scenario the engineer wants to test the instances before directing traffic to them whilst keeping them in the auto scaling group. The best process to suspend in this case is AddToLoadBalancer.</p><p>This will prevent the instances from being added to the load balancer so no traffic will be directed to them.</p><p><strong>CORRECT: </strong>\"Suspend the process AddToLoadBalancer\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the process Health Check\" is incorrect.</p><p>This will stop all health checks from running. This may not have the desired effect as instances will not be marked as healthy.</p><p><strong>INCORRECT:</strong> \"Suspend the process Replace Unhealthy\" is incorrect.</p><p>This is useful if you want to ensure that unhealthy instances are not terminated and replaced.</p><p><strong>INCORRECT:</strong> \"Suspend the process AZ Rebalance\" is incorrect.</p><p>This would be used to suspend rebalancing of instances across availability zones.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 99528213,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A web application runs on Amazon EC2 instances in an EC2 Auto Scaling group behind an Application Load Balancer (ALB). A DevOps engineer needs to implement a strategy for deploying updates that meets the following requirements:</p><p>\u00b7 Automatically launches the new version of the application on a second set of instances with the same capacity as the old version of the application.</p><p>\u00b7 Maintains the old version unchanged while the new version is launched.</p><p>\u00b7 Shifts traffic to the new version when the instances are fully deployed.</p><p>\u00b7 Terminates the old fleet of instances automatically 1 hour after shifting traffic.</p><p>Which solution will satisfy these requirements?</p>",
          "answers": [
            "<p>Create an AWS CloudFormation template and configure a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to redirect traffic to the new ALB.</p>",
            "<p>Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from old set of instances to the new one. Create an application version lifecycle policy that terminates the original environment after 1 hour.</p>",
            "<p>Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour and deploy the application.</p>",
            "<p>Use AWS CodeDeploy and create a deployment group that uses a blue/green deployment configuration. Use the BlueInstanceTerminationOption to terminate the instances in the blue environment after 1 hour.</p>"
          ],
          "explanation": "<p>When you use a blue/green deployment strategy with AWS CodeDeploy it can be configured to automatically copy the auto scaling group and launch instances running the new version of the application in a new environment. The capacity of the new auto scaling group can be the same and the original ASG can be maintained until the launch is complete.</p><p>The instances in the new environment can be a new target group behind the ALB. The traffic can be shifted across to the new environment when the launch is complete by modifying the target group setting in the ALB. This is performed automatically by CodeDeploy.</p><p>The BlueInstanceTerminationOption can be used to configure whether instances in the original environment are terminated when a blue/green deployment is successful. The options are:</p><ul><li><p>TERMINATE: Instances are terminated after a specified wait time.</p></li><li><p>KEEP_ALIVE: Instances are left running after they are deregistered from the load balancer and removed from the deployment group.</p></li></ul><p><strong>CORRECT: </strong>\"Use AWS CodeDeploy and create a deployment group that uses a blue/green deployment configuration. Use the BlueInstanceTerminationOption to terminate the instances in the blue environment after 1 hour\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudFormation template and configure a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to redirect traffic to the new ALB\" is incorrect.</p><p>A CloudFormation retention policy configures which resources should be retained when a stack is deleted. Retention policies are not suitable for implementing blue/green deployments.</p><p><strong>INCORRECT:</strong> \"Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from old set of instances to the new one. Create an application version lifecycle policy that terminates the original environment after 1 hour\" is incorrect.</p><p>A lifecycle policy tells Elastic Beanstalk to delete application versions that are old, or to delete application versions when the total number of versions for an application exceeds a specified number. This is not used for blue/green deployments.</p><p><strong>INCORRECT:</strong> \"Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour and deploy the application\" is incorrect.</p><p>You can use the Resources key in a configuration file to create and customize AWS resources in your environment. However this is not suitable for implementing a blue/green deployment strategy and the ALB does not need to be deleted. Instead, traffic simply needs to be shifted from one target group to another (when using the correct deployment configuration).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html\">https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 115961507,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps manager has been asked to optimize the costs associated with Amazon EBS volumes. There are many unattached EBS volumes which should be deleted if not used for 14 days. The manager asked a DevOps engineer to create an automation solution that deletes volumes that have been unattached for 14 days or more.</p><p>Which solution will accomplish this?</p>",
          "answers": [
            "<p>Use the AWS Config ec2-volume-inuse-check managed rule to mark unattached volumes are non-compliant. Create a new Amazon CloudWatch Events rule scheduled to invoke an AWS Lambda function in 14 days to delete the non-compliant volumes.</p>",
            "<p>Use Amazon EC2 and Amazon Data Lifecycle Manager to configure a volume lifecycle policy. Set the interval period for unattached EBS volumes to 14 days and set the retention rule to delete. Set the policy target volumes as *.</p>",
            "<p>Create an Amazon CloudWatch Events rule to invoke an AWS Lambda function daily. Configure the function to find unattached EBS volumes and tag them with the current date and delete unattached volumes that have tags with dates that are more than 14 days old.</p>",
            "<p>Use AWS Trusted Advisor to detect EBS volumes that have been detached for more than 14 days. Invoke an AWS Lambda function that creates a snapshot and then deletes the EBS volume.</p>"
          ],
          "explanation": "<p>CloudWatch Events can be configured with a rule to run an AWS Lambda function on a schedule. The function can be written to find unattached volumes and tag them with the current date. It should avoid tagging volumes that are already tagged and it should delete volumes which have dates older than 14 days. This meets the requirements of the question.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch Events rule to invoke an AWS Lambda function daily. Configure the function to find unattached EBS volumes and tag them with the current date and delete unattached volumes that have tags with dates that are more than 14 days old\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Trusted Advisor to detect EBS volumes that have been detached for more than 14 days. Invoke an AWS Lambda function that creates a snapshot and then deletes the EBS volume\" is incorrect.</p><p>Trusted Advisor checks for underutilized volumes but the criteria is that a volume is unattached or had less than 1 IOPS per day for the past 7 days.</p><p><strong>INCORRECT:</strong> \"Use the AWS Config ec2-volume-inuse-check managed rule to mark unattached volumes are non-compliant. Create a new Amazon CloudWatch Events rule scheduled to invoke an AWS Lambda function in 14 days to delete the non-compliant volumes\" is incorrect.</p><p>The function should run daily to delete those that have reached 14 days since being attached. This answer indicates that a specific rule is created for each instance of a non-compliant volume which is inefficient.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 and Amazon Data Lifecycle Manager to configure a volume lifecycle policy. Set the interval period for unattached EBS volumes to 14 days and set the retention rule to delete. Set the policy target volumes as *\" is incorrect.</p><p>DLM assists with automating the lifecycle of snapshots and AMIs. In this case the DevOps team need to automate deletion of unattached EBS volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 134588421,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An international IT consulting firm has multiple on-premises data centers across the globe. Their technical team regularly uploads financial and regulatory files from each of their respective data centers to a centralized web portal hosted in AWS. It uses an Amazon S3 bucket named <code>financial-tdojo-reports</code> to store the data. Another team downloads various reports from a CloudFront web distribution that uses the same Amazon S3 bucket as the origin. A DevOps Engineer noticed that the staff are using both the CloudFront link and the direct Amazon S3 URLs to download the reports. The IT Security team of the company considered this as a security risk, and they recommended to re-design the architecture. A new system must be implemented that prevents anyone from bypassing the CloudFront distribution and disable direct access from Amazon S3 URLs. </p><p>What should the Engineer do to meet the above requirement?</p>",
          "answers": [
            "<p>Create an Origin Access Control (OAC) and associate it with the S3 bucket origin in the CloudFront distribution. Update the bucket policy for the <code>financial-tdojo-reports</code> bucket to allow access only from the specified CloudFront distribution.</p>",
            "<p>Set up a custom SSL in your CloudFront web distribution instead of the default SSL. For each user, revoke the existing permission to access Amazon S3 URLs to download the objects.</p>",
            "<p>In the CloudFront web distribution, set up a field-level encryption configuration and for each user, revoke the existing permission to access Amazon S3 URLs to download the objects.</p>",
            "<p>Configure the distribution to use Signed URLs and create a Signed URL signing key pair (a public/private key pair). Grant permission to the public key to access the objects in the S3 bucket.</p>"
          ],
          "explanation": "<p><strong>Amazon CloudFront</strong> is a content delivery network (CDN) service provided by AWS. It securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. Amazon CloudFront provides various options to secure content delivery from S3 buckets, including preventing direct access to S3 and forcing downloads through the CloudFront distribution.</p><p><strong>Origin Access Control</strong> is a feature of Amazon CloudFront that enhances security for accessing S3 buckets and other supported origins. It allows you to restrict access to your origin resources, ensuring that they can only be accessed through designated CloudFront distributions. Some of the key points about OAC are:</p><ul><li><p>It uses AWS Identity and Access Management (IAM) service principals for authentication.</p></li><li><p>OAC employs short-term credentials with frequent rotations for improved security.</p></li><li><p>It supports comprehensive HTTP methods and server-side encryption with AWS KMS (SSE-KMS).</p></li><li><p>OAC can be used with S3 buckets in all AWS regions, as well as with AWS Lambda function URL origins.</p></li></ul><p><img src=\"https://media.tutorialsdojo.com/public/CloudFront_6AUG2023.png\"></p><p>By creating an Origin Access Control (OAC) setting in the CloudFront distribution and updating the S3 bucket policy to grant access only to the CloudFront OAC, you effectively prevent direct access to the S3 bucket from any other source, including direct S3 URLs. All access to the objects in the S3 bucket must go through the CloudFront distribution, meeting the security requirement.</p><p>Hence, the correct answer is: <strong>Create an Origin Access Control (OAC) and associate it with the S3 bucket origin in the CloudFront distribution. Update the bucket policy for the </strong><code><strong>financial-tdojo-reports</strong></code><strong> bucket to allow access only from the specified CloudFront distribution.</strong></p><p>The option that says: <strong>Set up a custom SSL in your CloudFront web distribution instead of the default SSL. For each user, revoke the existing permission to access Amazon S3 URLs to download the objects</strong> is incorrect because SSL is not needed in this particular scenario. What you need to implement is an OAC.</p><p>The option that says: <strong>In the CloudFront web distribution, set up a field-level encryption configuration and for each user, revoke the existing permission to access Amazon S3 URLs to download the objects</strong> is incorrect because the field-level encryption configuration is primarily used for safeguarding sensitive fields in your CloudFront. Therefore, it is not suitable for this scenario.</p><p>The option that says: <strong>Configure the distribution to use Signed URLs and create a Signed URL signing key pair (a public/private key pair). Grant permission to the public key to access the objects in the S3 bucket</strong> is incorrect because Signed URLs are used to provide temporary access to specific objects in an S3 bucket, typically for a limited time period. While this approach can restrict direct access to the S3 bucket, it requires generating and managing Signed URLs for each object, which can be cumbersome and not scalable, especially if there are many objects or frequent updates.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 115961501,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application is being deployed using an AWS CodePipeline pipeline. The pipeline includes an AWS CodeBuild stage which downloads source code from AWS CodeCommit, pulls data from an S3 bucket, and builds and tests the application before deployment.</p><p>A DevOps engineer has discovered that the S3 data is not being successfully downloaded due to a permissions issue.</p><p>How can the permissions be assigned to CodeBuild in the MOST secure manner?</p>",
          "answers": [
            "<p>Use an aws:Referer condition key in the CodeBuild project settings. Update the buildspec to use the AWS CLI to download the data.</p>",
            "<p>Modify the service role for the CodeBuild project to include permissions for S3. Use the AWS CLI to download the data.</p>",
            "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the buildspec to use cURL to pass the token and download the data.</p>",
            "<p>Configure an IAM access key and a secret access key in the application code and use the AWS CLI to download the data.</p>"
          ],
          "explanation": "<p>The most likely issue is that the service role used by AWS CodeBuild does not have the correct permissions to download the data securely from the Amazon S3 bucket. CodeBuild uses the service role for all operations that are performed on your behalf. Therefore, the role must have the permissions needed during the build stage.</p><p>In this case, simply adding the correct permissions statements to the policy attached to the service role should resolve the permission issue. The data can then be downloaded from S3 using the AWS CLI by specifying commands in the buildspec document.</p><p><strong>CORRECT: </strong>\"Modify the service role for the CodeBuild project to include permissions for S3. Use the AWS CLI to download the data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure an IAM access key and a secret access key in the application code and use the AWS CLI to download the data\" is incorrect.</p><p>This is an insecure method of using credentials and should be avoided. It would also not provide the permissions needed by CodeBuild as the service gets those permissions from the service role.</p><p><strong>INCORRECT:</strong> \"Use an aws:Referer condition key in the CodeBuild project settings. Update the buildspec to use the AWS CLI to download the data\" is incorrect.</p><p>The condition key referenced is used in policies to restrict access to specific HTTP referers. This is not useful here as it does not provide any permissions to CodeBuild.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the buildspec to use cURL to pass the token and download the data\" is incorrect.</p><p>You cannot configure different authentication options on S3 as it is a managed service. You can only limit who can access the bucket and objects and under what conditions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 67357164,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>Consider a multi-account setup within AWS Organizations where a company is running a data ingestion application on Amazon EC2 instances through several Auto Scaling groups. These instances lack internet access due to sensitive data handling, and VPC endpoints have been deployed accordingly. The application operates on a custom AMI designed specifically for its needs.</p>\n\n<p>To effectively manage and troubleshoot the application, system administrators require automated and centralized login access to the EC2 instances. Additionally, the company's security team needs to be notified whenever such instances are accessed.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you suggest to satisfy these requirements?</p>\n",
          "answers": [
            "<p>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>",
            "<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>",
            "<p>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</p>",
            "<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>This option suggests using EC2 Image Builder to create an updated custom AMI with AWS Systems Manager Agent included. The Auto Scaling group is configured to attach the AmazonSSMManagedInstanceCore role to the instances, thereby enabling centralized management through Systems Manager, as it grants the EC2 instances the permissions needed for core Systems Manager functionality. Session Manager can be used for secure logins, and session details can be logged to Amazon S3. Additionally, an S3 event notification can be set up to alert the security team about new file uploads using Amazon SNS. This option aligns well with the requirement for centralized access and monitoring.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q35-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/image-builder/\">https://aws.amazon.com/image-builder/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</strong> - This option includes unnecessary internet access for sensitive data via the combination of NAT Gateway and bastion host, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - EC2 Instance Connect is a way to distribute short-lived SSH keys and control access by IAM policies. The actual connection is created with SSH client and all normal requirements (network connectivity, user on host etc) apply. The host must also have ec2-instance-connect \u201cagent\u201d installed. EC2 Instance Connect still requires public IP and network connectivity, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions. Instead, SCPs are JSON policies that specify the maximum permissions for the affected accounts. So, for the given use case, you cannot use SCP for granting EC2 instance access to centralized and automated login for the Systems Manager.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/image-builder/\">https://aws.amazon.com/image-builder/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n"
        }
      },
      {
        "id": 82921456,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A media streaming solutions company has deployed an application that allows its customers to view movies in real-time. The application connects to an Amazon Aurora database, and the entire stack is currently deployed in the United States. The company has plans to expand to Europe and Asia for its operations. It needs the <code>movies</code> table to be accessible globally but needs the <code>users</code> and <code>movies_watched</code> table to be regional only.</p>\n\n<p>As a DevOps Engineer, how would you implement this with minimal application refactoring?</p>\n",
          "answers": [
            "<p>Use an Amazon Aurora Global Database for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</p>",
            "<p>Use an Amazon Aurora Global Database for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</p>",
            "<p>Use a DynamoDB Global Table for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</p>",
            "<p>Use a DynamoDB Global Table for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database.</p>\n\n<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.</p>\n\n<p>For the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (movies table) and the other one for the local tables (users and movies_watched tables).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB Global Table for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB Global Table for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p>Here, we want minimal application refactoring. DynamoDB and Aurora have a completely different API, due to Aurora being SQL and DynamoDB being NoSQL. So all three options are incorrect, as they have DynamoDB as one of the components.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/faqs/\">https://aws.amazon.com/rds/aurora/faqs/</a></p>\n"
        }
      },
      {
        "id": 82921404,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An analytics company is capturing metrics for its AWS services and applications using CloudWatch metrics. It needs to be able to go back up to 7 years in time for visualizing these metrics due to regulatory requirements. As a DevOps Engineer at the company, you have been tasked with designing a solution that will help the company comply with the regulations.</p>\n\n<p>Which of the following options requires the minimum development effort to address the given requirements?</p>\n",
          "answers": [
            "<p>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in Amazon ES. Create a Kibana dashboard on top to visualize the metrics</p>",
            "<p>Create a CloudWatch dashboard on top of CloudWatch metrics. Enable 'Extended Retention' on CloudWatch metrics, and implement an AWS Config rule that checks for this setting. If the AWS Config rule is non-compliant, use an Auto Remediation to turn it back on</p>",
            "<p>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in S3. Create a CloudWatch dashboard on top of the metrics in S3</p>",
            "<p>Create a CloudWatch metrics stream and direct it to Kinesis Firehose Firehose delivery stream. Send all the metrics data into S3 using Firehose, and create a QuickSight dashboard to visualize the metrics. Use Athena to query for specific time ranges</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch metrics stream and direct it to Kinesis Firehose Firehose delivery stream. Send all the metrics data into S3 using Firehose, and create a QuickSight dashboard to visualize the metrics. Use Athena to query for specific time ranges</strong></p>\n\n<p>You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations.</p>\n\n<p>You can create a metric stream and direct it to an Amazon Kinesis Data Firehose delivery stream that delivers your CloudWatch metrics to a data lake such as Amazon S3.  You can then use tools such as Amazon Athena to get insight into cost optimization, resource performance, and resource utilization for given data ranges. In additon, you can use a QuickSight dashboard to visualize the metrics.</p>\n\n<p>The Kinesis Data Firehose delivery stream must trust CloudWatch through an IAM role that has write permissions to Kinesis Data Firehose. These permissions can be limited to the single Kinesis Data Firehose delivery stream that the CloudWatch metric stream uses. The IAM role must trust the streams.metrics.cloudwatch.amazonaws.com service principal.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in Amazon ES. Create a Kibana dashboard on top to visualize the metrics</strong></p>\n\n<p>A CloudWatch metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time. For example, the CPU usage of a particular EC2 instance is one metric provided by Amazon EC2. The data points themselves can come from any application or business activity from which you collect data.</p>\n\n<p>Metrics cannot be deleted, but they automatically expire after 15 months if no new data is published to them. Data points older than 15 months expire on a rolling basis; as new data points come in, data older than 15 months is dropped.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q36-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>\n\n<p>As the CloudWatch metrics can only be retained for 15 months, we need to use a CloudWatch Event rule and trigger a Lambda function to extract metrics and send them for long term retention to facilitate visual analysis. Here, the only solution that works end-to-end is to send the data to Amazon ES, and use Kibana to create graphs.</p>\n\n<p>Amazon Elasticsearch (ES) Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud.</p>\n\n<p>How Amazon ElasticSearch Works:\n<img src=\"https://d1.awsstatic.com/re19/ESS_HIW.db99588614cbf44e2d62ef9c9c173ebfe41e2834.png\">\nvia - <a href=\"https://aws.amazon.com/elasticsearch-service/\">https://aws.amazon.com/elasticsearch-service/</a></p>\n\n<p>ES is commonly deployed as part of the ELK stack which is an acronym used to describe a stack that comprises three popular open-source projects: Elasticsearch, Logstash, and Kibana. The ELK stack gives you the ability to aggregate logs from all your systems and applications, analyze these logs, and create visualizations for application and infrastructure monitoring, faster troubleshooting, security analytics, and more.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/Elasticsearch/Amazon%20ES%20ELK%20diagram.9d830908067fb7bedb52c6738126f2dfe18b611a.png\">\nvia - <a href=\"https://aws.amazon.com/elasticsearch-service/the-elk-stack/\">https://aws.amazon.com/elasticsearch-service/the-elk-stack/</a></p>\n\n<p>As this option requires you to create a Lambda function that will execute a custom API to export the metrics into an ES cluster, so it's not the best fit for the given requirement as it involves significant development effort.</p>\n\n<p><strong>Create a CloudWatch dashboard on top of CloudWatch metrics. Enable 'Extended Retention' on CloudWatch metrics, and implement an AWS Config rule that checks for this setting. If the AWS Config rule is non-compliant, use an Auto Remediation to turn it back on</strong> - This option has been added as a distractor as CloudWatch metrics do not have an 'Extended Retention' feature.</p>\n\n<p><strong>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in S3. Create a CloudWatch dashboard on top of the metrics in S3</strong> - S3 based data can be integrated easily with QuickSight, however, CloudWatch dashboards can only consume CloudWatch metrics and NOT data/metrics from S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-metric-streams-trustpolicy.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-metric-streams-trustpolicy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticsearch-service/\">https://aws.amazon.com/elasticsearch-service/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticsearch-service/the-elk-stack/\">https://aws.amazon.com/elasticsearch-service/the-elk-stack/</a></p>\n"
        }
      },
      {
        "id": 82921420,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An e-commerce company has deployed a Spring application on Elastic Beanstalk running the Java platform. As a DevOps Engineer at the company, you are referencing an RDS PostgreSQL database through an environment variable so that your application can use it for storing its data. You are using a library to perform a database migration in case the schema changes. Upon deploying updates to Beanstalk, you have seen the migration fail because all the EC2 instances running the new version try to run the migration on the RDS database.</p>\n\n<p>How can you fix this issue?</p>\n",
          "answers": [
            "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</p>",
            "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</p>",
            "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</p>",
            "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</strong></p>\n\n<p>You can use Elastic Beanstalk configuration files (.ebextensions) with your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the option_settings key to modify the environment configuration. You can choose from general options for all environments and platform-specific options.</p>\n\n<p>You may want to customize and configure the software that your application depends on. You can use the commands key to execute commands on the EC2 instance. The commands run before the application and web server are set up and the application version file is extracted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n\n<p>You can use the container_commands key to execute commands that affect your application source code. Container commands run after the application and web server have been set up and the application version archive has been extracted, but before the application version is deployed. You can use leader_only to only run the command on a single instance, or configure a test to only run the command when a test command evaluates to true.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q25-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n\n<p>If you specify a <code>commands</code> block, every EC2 instance will run it and it does not support the <code>leader_only</code> attribute. Therefore you must use <code>container_commands</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</strong> - As mentioned earlier, if you specify a <code>commands</code> block, every EC2 instance will run it and it does not support the <code>leader_only</code> attribute. So this option is incorrect.</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</strong></p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</strong></p>\n\n<p>The <code>lock_mode: true</code> attribute has been added as a distractor and it does not exist. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/35788499/what-is-difference-between-commands-and-container-commands-in-elasticbean-talk/40096352#40096352\">https://stackoverflow.com/questions/35788499/what-is-difference-between-commands-and-container-commands-in-elasticbean-talk/40096352#40096352</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n"
        }
      },
      {
        "id": 75949084,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A DevOps manager requires a disaster recovery (DR) solution for a workload deployed on Amazon EC2 instances in an Amazon VPC within the us-east-1 Region. The DR solution should enable data replication to a staging area subnet in another AWS Region. The DR solution minimize operational overhead and enable fast failover and failback.</p><p>Which solution meets these requirements?</p>",
          "answers": [
            "<p>Use AWS Elastic Disaster Recovery to replicate data to a staging area subnet in another Region using the replication agent installed on the EC2 instance.</p>",
            "<p>Use AWS Resource Access Manager (RAM) to share the VPC across Regions and configure AWS DataSync to synchronize data between source and target systems.</p>",
            "<p>Create a resource group for the EC2 instances and configure DR protection using AWS Resilience Hub. Configure RPO/RTO settings and enable automated recovery.</p>",
            "<p>Use AWS Backup to enable cross-Region snapshots for the EC2 instances. Use AWS Fault Injection Simulator to enable automated recovery.</p>"
          ],
          "explanation": "<p>AWS Elastic Disaster Recovery (AWS DRS) minimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applications using affordable storage, minimal compute, and point-in-time recovery.</p><p>You can increase IT resilience when you use AWS Elastic Disaster Recovery to replicate on-premises or cloud-based applications running on supported operating systems. Use the AWS Management Console to configure replication and launch settings, monitor data replication, and launch instances for drills or recovery.</p><p>Set up AWS Elastic Disaster Recovery on your source servers to initiate secure data replication. Your data is replicated to a staging area subnet in your AWS account, in the AWS Region you select. The staging area design reduces costs by using affordable storage and minimal compute resources to maintain ongoing replication.</p><p><strong>CORRECT: </strong>\"Use AWS Elastic Disaster Recovery to replicate data to a staging area subnet in another Region using the replication agent installed on the EC2 instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Resource Access Manager (RAM) to share the VPC across Regions and configure AWS DataSync to synchronize data between source and target systems\" is incorrect.</p><p>AWS RAM cannot be used to share VPCs across Regions. Also, DataSync is not suitable for synchronizing the application data on Amazon EC2 instances.</p><p><strong>INCORRECT:</strong> \"Create a resource group for the EC2 instances and configure DR protection using AWS Resilience Hub. Configure RPO/RTO settings and enable automated recovery\" is incorrect.</p><p>AWS Resilience Hub provides a central place to define, validate, and track the resilience of applications on AWS. It cannot be used to configure DR protection or enable automated recovery.</p><p><strong>INCORRECT:</strong> \"Use AWS Backup to enable cross-Region snapshots for the EC2 instances. Use AWS Fault Injection Simulator to enable automated recovery\" is incorrect.</p><p>You can use AWS Backup to enable cross-Region backups of EC2 instances. However, AWS Fault Injection Simulator is not used for automated recovery, it is used for running fault injection experiments to improve an application\u2019s performance, observability, and resiliency.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html\">https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html</a></p>"
        }
      }
    ],
    "answers": {
      "67357164": [
        "b"
      ],
      "75949062": [
        "a",
        "d",
        "f"
      ],
      "75949066": [
        "b"
      ],
      "75949068": [
        "b",
        "c"
      ],
      "75949084": [
        "a"
      ],
      "75949166": [
        "a"
      ],
      "82921340": [
        "a"
      ],
      "82921404": [
        "d"
      ],
      "82921412": [
        "a",
        "b"
      ],
      "82921420": [
        "a"
      ],
      "82921444": [
        "a"
      ],
      "82921456": [
        "b"
      ],
      "99528193": [
        "c",
        "e"
      ],
      "99528213": [
        "d"
      ],
      "115961501": [
        "b"
      ],
      "115961507": [
        "c"
      ],
      "134588421": [
        "a"
      ],
      "134588459": [
        "c"
      ],
      "138248237": [
        "a"
      ],
      "143860765": [
        "b"
      ]
    }
  },
  {
    "id": "1770201964204",
    "date": "2026-02-04T10:46:04.204Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 16,
    "incorrect": 4,
    "unanswered": 0,
    "total": 20,
    "percent": 80,
    "duration": 14561653,
    "questions": [
      {
        "id": 134588501,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>The company operates a fleet of 400 Amazon EC2 instances to support a High-Performance Computing (HPC) application. The fleet is configured with target tracking scaling and is managed behind an Application Load Balancer (ALB). There is a requirement to create a simple web page hosted on an Amazon S3 bucket that displays the status of the EC2 instances in the fleet. This web page must be updated whenever a new instance is launched or terminated. Additionally, a searchable log of these events is required for review at a later time.</p><p>Which of the following options should be implemented to meet these requirements?</p>",
          "answers": [
            "<p>Create an Amazon EventBridge rule for the scale-in/scale-out event and deliver these logs to Amazon CloudWatch Logs. Manually export the CloudWatch Log group to the S3 bucket to view them.</p>",
            "<p>Write your AWS Lambda function to update the simple webpage on S3 and send event logs to CloudWatch Logs. Create an EventBridge rule to invoke the Lambda for scale-in/scale-out events.</p>",
            "<p>Create an EventBridge rule for the scale-in/scale-out event and create two targets for the event - one target to the webpage S3 bucket and another to deliver event logs to CloudWatch Logs.</p>",
            "<p>No need to do anything. AWS CloudTrail records these events and you can view and search the logs on the console.</p>"
          ],
          "explanation": "<p>You can create an <strong>AWS Lambda function</strong> that logs the changes in state for an Amazon EC2 instance. You can create a rule that runs the function whenever there is a state transition or a transition to one or more states that are of interest. Be sure to assign proper permissions to your Lambda function to write to S3 and to send logs to CloudWatch Logs. After creating the Lambda function, create a rule on Amazon EventBridge that will watch for the scale-in/scale-out events. Set a trigger to run your Lambda function which will then update the S3 webpage and send logs to CloudWatch Logs.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-event-pattern-27Jan2025.png\"></p><p>Hence, the correct answer is: <strong>Write your AWS Lambda function to update the simple webpage on S3 and send event logs to CloudWatch Logs. Create an EventBridge rule to invoke the Lambda for scale-in/scale-out events.</strong></p><p>The option that says: <strong>Create an Amazon EventBridge rule for the scale-in/scale-out event and deliver these logs to Amazon CloudWatch Logs. Manually export the CloudWatch Log group to the S3 bucket to view them </strong>is incorrect because you have to export the logs manually on an S3 bucket. This action is not recommended since you can simply use a Lambda function to log the changes of the EC2 instances to a file in the S3 bucket.</p><p>The option that says: <strong>Create an EventBridge rule for the scale-in/scale-out event and create two targets for the event - one target to the webpage S3 bucket and another to deliver event logs to CloudWatch Logs </strong>is incorrect because you cannot set an S3 bucket or object as a target for an EventBridge rule.</p><p>The option that says: <strong>No need to do anything. AWS CloudTrail records these events and you can view and search the logs on the console </strong>is incorrect. Although this is true, it would be hard to search all the relevant logs from the trail. Moreover, it typically doesn\u2019t provide a solution for the required S3 web page.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Tutorials.html \">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Tutorials.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 82921342,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a social media company, you have deployed your application in an Auto Scaling group (ASG) using CloudFormation. You would like to update the Auto Scaling Group to have all the instances reference the newly created launch configuration, which upgrades the instance type. Your ASG currently contains 6 instances and you need at least 4 instances to be up at all times.</p>\n\n<p>Which configuration should you use in the CloudFormation template?</p>\n",
          "answers": [
            "<p>AutoScalingRollingUpdate</p>",
            "<p>AutoScalingReplacingUpdate</p>",
            "<p>AutoScalingLaunchTemplateUpdate</p>",
            "<p>AutoScalingLaunchConfigurationUpdate</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>AutoScalingRollingUpdate</strong></p>\n\n<p>To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the AutoScalingRollingUpdate policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. AutoScalingRollingUpdate is perfect for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q72-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AutoScalingReplacingUpdate</strong> - To specify how AWS CloudFormation handles replacement updates for an Auto Scaling group, you should use the AutoScalingReplacingUpdate policy. This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group. This option will create a new ASG entirely, so this is ruled out.</p>\n\n<p><strong>AutoScalingLaunchTemplateUpdate</strong></p>\n\n<p><strong>AutoScalingLaunchConfigurationUpdate</strong></p>\n\n<p>AutoScalingLaunchTemplateUpdate and AutoScalingLaunchConfigurationUpdate do not exist, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n"
        }
      },
      {
        "id": 82921406,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A multi-national retail company has defined tagging guidelines and standard for all its resources in AWS and would like to create a dashboard to visualize the compliance of all the resources with the ability to find out the non-compliant resources. The company has hired you as an AWS Certified DevOps Engineer Professional to develop a solution for this requirement.</p>\n\n<p>Which of the following options would you suggest to address the use-case?</p>\n",
          "answers": [
            "<p>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</p>",
            "<p>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</p>",
            "<p>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</p>",
            "<p>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</strong></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p>Here, we can use AWS Config to track resource configuration, and we could create a rule to track the tagging of these resources. All the changes to resource configuration as well as tagging of resources are streamed to an SNS topic.</p>\n\n<p>A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags make it easier to manage, search for, and filter resources. You can use tags to categorize your AWS resources in different ways, for example, by purpose, owner, or environment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p>You can set up the Required-Tag managed rule for Config which requires up to 6 tags with optional values in a single rule. Previously, each rule accepted only a single tag/value combo. Additionally, the Required-Tag managed rule now accepts a comma-separated list of values for each checked tag. This allows for a rule to be compliant if any one of a supplied list of tags is present on the resource.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</strong> - AWS Service Catalog enabling AWS customers to create and deliver standardized services that provide the necessary control, while still empowering developers to choose the services that best fit their needs. You cannot use Service Catalog to get an inventory of all the resources in your account.</p>\n\n<p><strong>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</strong> - SSM inventory will only help with understanding what is installed on your managed instances. To view Systems Manager Inventory history and change tracking for all of your managed instances, you need to use AWS Config itself.</p>\n\n<p><strong>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</strong> - CloudTrail is used to track API calls, not resources. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n"
        }
      },
      {
        "id": 82921316,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>You are working as a DevOps Engineer at an e-commerce company and have a deployed a Node.js application on Elastic Beanstalk. You would like to track error rates and specifically, you need to ensure by looking at the application log, that you do not have more than 100 errors in a 5 minutes interval. In case you are getting too many errors, you would like to be alerted via email.</p>\n\n<p>Which of the following options represents the most efficient solution in your opinion?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</strong></p>\n\n<p>You can search and filter the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created.</p>\n\n<p>You can use metric filters to search for and match terms, phrases, or values in your log events. When a metric filter finds one of the terms, phrases, or values in your log events, you can increment the value of a CloudWatch metric. For example, you can create a metric filter to search for and count the occurrence of the word ERROR in your log events.</p>\n\n<p>CloudWatch Logs Metric Filter Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p>For the given use-case, you can have Beanstalk send the logs to CloudWatch Logs, and then create a metric filter. This will create a metric for us (and not an alarm), and on top of the metric, you can create a CloudWatch Alarm. This alarm will send a notification to SNS, which will, in turn, send us emails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - You cannot directly set a CloudWatch Alarm as a target for a CloudWatch Logs Metric Filter. You will first need to create a metric filter which can then be used to create a CloudWatch metric to be eventually used in a CloudWatch Alarm.</p>\n\n<p><strong>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</strong> - The Elastic Beanstalk Health Metrics will not track the errors sent out to a log file, so it does not meet the requirements of the use-case. Besides, CloudWatch alarm cannot be used to work on top of the Elastic Beanstalk Health Metrics.</p>\n\n<p><strong>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - Implementing custom logic in your Node.js application may seem like a good idea, but then you have to remember that your application can be distributed amongst many servers with Beanstalk, and as such it will not be possible to track the \"100 errors\" across all instances using this methodology.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p>\n"
        }
      },
      {
        "id": 75949164,
        "correct_response": [
          "b",
          "d"
        ],
        "prompt": {
          "question": "<p>A company is deploying a new application that uses Amazon EC2 instances for the web tier and a MySQL database for the database tier. An Application Load Balancer (ALB) will be used in front of the web tier. The company requires an RPO of 2 hours and an RTO of 10 minutes for the solution.</p><p>Which combination of deployment strategies will meet these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Create two Amazon Aurora clusters spread across two Regions. Use AWS Database Migration Service (AWS DMS) to synchronize changes.</p>",
            "<p>Create an Amazon Aurora global database in two Regions for the database tier. In the event of a failure, promote the secondary Region to take on read/write responsibilities.</p>",
            "<p>Create an Amazon Aurora multi-master cluster across multiple Regions for the database tier. Configure the database in an active-active mode across the Regions.</p>",
            "<p>Deploy the application in two Regions and create Amazon Route 53 failover-based routing records pointing to the ALB in each Regions. Enable health checks for the records.</p>",
            "<p>Deploy the application in two Regions and create Amazon Route 53 latency-based routing records pointing to the ALB in each Regions. Enable health checks for the records.</p>"
          ],
          "explanation": "<p>To meet the RTO and RPO requirements the best solution for the database tier is to use Amazon Aurora global database. This solution provides replication from the Aurora storage layer across Regions. The reader endpoint in the secondary endpoint can be promoted in the event of a DR scenario to be the main database which takes on read/write responsibilities.</p><p>For the web tier this can be placed behind ALBs in each Region. Amazon Route 53 failover-based routing policies with health checks should be created. These records will point to the ALBs in each Region and if the health checks fail in the primary Region, automatic failover to the secondary Region will occur.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-10-42-d7b7624f7e1003a9bd3098d979fb2910.jpg\"></p><p><strong>CORRECT: </strong>\"Create an Amazon Aurora global database in two Regions for the database tier. In the event of a failure, promote the secondary Region to take on read/write responsibilities\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Deploy the application in two Regions and create Amazon Route 53 failover-based routing records pointing to the ALB in each Regions. Enable health checks for the records\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create two Amazon Aurora clusters spread across two Regions. Use AWS Database Migration Service (AWS DMS) to synchronize changes\" is incorrect.</p><p>The better solution is to use Aurora Global Database as this will simplify the failover process which can be instantiated through a few API calls. Also, this replication uses Aurora replication with low latency and the Aurora reader endpoint can also be utilized in the second Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Aurora multi-master cluster across multiple Regions for the database tier. Configure the database in an active-active mode across the Regions\" is incorrect.</p><p>It is not possible to create multi-master clusters across Regions, they work within a Region only.</p><p><strong>INCORRECT:</strong> \"Deploy the application in two Regions and create Amazon Route 53 latency-based routing records pointing to the ALB in each Regions. Enable health checks for the records\" is incorrect.</p><p>Failover routing policy records should be created, not latency routing records. With latency records users closer to the secondary Region will be directed there but the DB layer is not in read/write mode except in a DR scenario.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>"
        }
      },
      {
        "id": 138248111,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company is planning to deploy a new version of their legacy application in AWS which is deployed to an Auto Scaling group of EC2 instances with an Application Load Balancer in front. To avoid any disruption of their services, they need to implement canary testing first before all of the traffic is shifted to the new application version.</p><p>Which of the following solutions can meet this requirement?</p>",
          "answers": [
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</p>",
            "<p>Do a Canary deployment using CodeDeploy with a <code>CodeDeployDefault.LambdaCanary10Percent30Minutes</code> deployment configuration.</p>",
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers.</p>",
            "<p>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment.</p>"
          ],
          "explanation": "<p>The purpose of a canary deployment is to reduce the risk of deploying a new version that impacts the <a href=\"https://wa.aws.amazon.com/wat.concept.workload.en.html\" title=\"The set of components that together deliver business value.\">workload</a>. The method will incrementally deploy the new version, making it visible to new users in a slow fashion. As you gain confidence in the deployment, you will deploy it to replace the current version in its entirety.</p><p><img src=\"https://media.tutorialsdojo.com/public/Upgrades_Image1.jpeg\"></p><p>To properly implement the canary deployment, you should do the following steps:</p><p>- Use a router or load balancer that allows you to send a small percentage of users to the new version.</p><p>- Use a dimension on your KPIs to indicate which version is reporting the metrics.</p><p>- Use the metric to measure the success of the deployment; this indicates whether the deployment should continue or rollback.</p><p>- Increase the load on the new version until either all users are on the new version or you have fully rolled back.</p><p><br></p><p>Hence, the correct answer is: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</strong></p><p>The option that says: <strong>Do a Canary deployment using CodeDeploy with a </strong><code><strong>CodeDeployDefault.LambdaCanary10Percent30Minutes</strong></code><strong> deployment configuration</strong> is incorrect because this specific configuration type is only applicable for Lambda functions and for the applications hosted in an Auto Scaling group.</p><p>The option that says: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers</strong> is incorrect because you can't use CloudFront to adjust the weight of the incoming traffic to your application. You should use Route 53 instead.</p><p>The option that says: <strong>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment</strong> is incorrect because you can only integrate a Network Load Balancer to your Amazon API Gateway. Moreover, this service is only applicable for APIs, not full-fledged web applications.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html\">https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/\">https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/</a></p>"
        }
      },
      {
        "id": 138248173,
        "correct_response": [
          "c",
          "e",
          "f"
        ],
        "prompt": {
          "question": "<p>A developer has been tasked with developing a mobile news homepage that curates several news sources into one page. The app mainly comprises several AWS Lambda functions configured as a deployment group on AWS CodeDeploy. The developer must test all APIs for each new app version before fully deploying it to production. The APIs use a set of Lambda validation scripts. The goal is to check the APIs during deployments, be notified of any API errors, and implement automatic rollback if the validation fails.</p><p>Which combination of the options below can help implement a solution for this scenario? (Select THREE.)</p>",
          "answers": [
            "<p>Add a step on CodeDeploy to trigger the Lambda validation scripts after deployment and invoke them after deployment to validate the new app version.</p>",
            "<p>Have CodeDeploy run the Lambda validations after the deployment to test with production traffic. When errors are found, have another trigger to rollback the deployment.</p>",
            "Define your Lambda validation scripts on the AppSpec lifecycle hook during deployment to run the validation using test traffic and trigger a rollback if checks fail.",
            "<p>Have Lambda send results to Amazon CloudWatch Alarms directly and trigger a rollback when 5xx reply errors are received during deployment.</p>",
            "<p>Associate an Amazon CloudWatch Alarm with the deployment group that can send a notification to an Amazon SNS topic when the threshold for 5xx errors is reached on CloudWatch.</p>",
            "<p>Configure the Lambda validation scripts to run during deployment and configure an Amazon CloudWatch Alarm that will trigger a rollback when the function validation fails.</p>"
          ],
          "explanation": "<p>You can use <strong>CloudWatch Alarms</strong> to track metrics on your new deployment and you can set thresholds for those metrics in your Auto Scaling groups being managed by CodeDeploy. This can invoke an action if the metric you are tracking crosses the threshold for a defined period of time. You can also monitor metrics such as instance CPU utilization, Memory utilization or custom metrics you have configured. If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance. You will also have the option to automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated. CodeDeploy will redeploy the last known working version of the application when it rolls back.</p><p>With <strong>Amazon</strong> <strong>SNS</strong>, you can create triggers that send notifications to subscribers of an Amazon SNS topic when specified events, such as success or failure events, occur in deployments and instances. CloudWatch Alarms can trigger sending out notifications to your configured SNS topic.</p><p><img alt=\"lifecycle-event-order-lam\" height=\"1347\" src=\"https://media.tutorialsdojo.com/public/lifecycle-event-order-lambda.png\" width=\"1000\"></p><p>The <code>BeforeAllowTraffic</code> and <code>AfterAllowTraffic</code> lifecycle hooks of the AppSpec.yaml file allows you to use Lambda functions to validate the new version task set using the test traffic during the deployment. For example, a Lambda function can serve traffic to the test listener and track metrics from the replacement task set. If rollbacks are configured, you can configure a CloudWatch alarm that triggers a rollback when the validation test in your Lambda function fails.</p><p>Hence, the correct answers are:</p><p><strong>- Define your Lambda validation scripts on the AppSpec lifecycle hook during deployment to run the validation using test traffic and trigger a rollback if checks fail.</strong></p><p><strong>- Associate an Amazon CloudWatch Alarm with the deployment group that can send a notification to an Amazon SNS topic when the threshold for 5xx errors is reached on CloudWatch.</strong></p><p><strong>- Configure the Lambda validation scripts to run during deployment and configure an Amazon CloudWatch Alarm that will trigger a rollback when the function validation fails.</strong></p><p>The option that says: <strong>Add a step on CodeDeploy to trigger the Lambda validation scripts after deployment and invoke them after deployment to validate the new app version<em> </em></strong>is incorrect because you only want the validation script to run before production traffic is flowing on the new app version. You can use AppSpec hooks to do this, which also includes an option to rollback when validation fails.</p><p>The option that says: <strong>Have CodeDeploy run the Lambda validations after the deployment to test with production traffic. When errors are found, have another trigger to rollback the deployment</strong> is incorrect because when the new app version is deployed to production, there's a possibility that clients will notice these errors. Rollback will take some time as the old version will need to be re-deployed. It is better to run the validation scripts during the deployment using the test traffic.</p><p>The option that says: <strong>Have Lambda send results to Amazon CloudWatch Alarms directly and trigger a rollback when 5xx reply errors are received during deployment </strong>is incorrect because CloudWatch Alarms simply can't receive direct test results from AWS Lambda. Lambda should log its test results to CloudWatch Logs, and Amazon EventBridge should monitor those logs to trigger actions, such as rolling back the deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html#deployment-steps-what-happens \">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html#deployment-steps-what-happens</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#deployments-rollback-and-redeploy-automatic-rollbacks \">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#deployments-rollback-and-redeploy-automatic-rollbacks</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p><p><br></p><p><strong>Check out these Amazon CloudWatch and Amazon SNS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-sns/?src=udemy\">https://tutorialsdojo.com/amazon-sns/</a></p>"
        }
      },
      {
        "id": 82921440,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.</p>\n\n<p>Which of the following options would you suggest? (Select two)</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</p>",
            "<p>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</p>",
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>Create a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p><strong>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>Understand how a pipeline execution state change rule works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Here we are only interested in pipeline failures, so we need to choose <code>CodePipeline Pipeline Execution State Change</code>.\nFinally, CloudWatch Event rules do not support Slack as a target, therefore we must create a Lambda function for it.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i2.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</strong> - CloudWatch Event rules do not support Slack as a target, so this option is incorrect.</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>**\nCreate a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>Here we are only interested in pipeline failures, so we just need to choose <code>CodePipeline Pipeline Execution State Change</code>. Therefore both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n"
        }
      },
      {
        "id": 134588369,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A digital payment gateway system is running in AWS which serves thousands of businesses worldwide. It is hosted in an Auto Scaling Group of EC2 instances behind an Application Load Balancer with an Amazon RDS database in a Multi-AZ deployment configuration. The company is using several CloudFormation templates in deploying the new version of the system. The <code>AutoScalingRollingUpdate</code> policy is used to control how CloudFormation handles rolling updates for their Auto Scaling group which replaces the old instances based on the parameters they have set. Lately, there were a lot of failed deployments which has caused system unavailability issues and business disruptions. They want to find out what's preventing their Auto Scaling group from updating correctly during a stack update.&nbsp; </p><p>In this scenario, how should the DevOps engineer troubleshoot this issue? (Select THREE.)</p>",
          "answers": [
            "<p>Switch from <code>AutoScalingRollingUpdate</code> to <code>AutoScalingReplacingUpdate</code> policy by modifying the <code>UpdatePolicy</code> of the <code>AWS::AutoScaling::AutoscalingGroup</code> resource in the CloudFormation template. Set the <code>WillReplace</code> property to true. </p>",
            "<p>In your <code>AutoScalingRollingUpdate</code> policy, set the <code>WaitOnResourceSignals</code> property to false.</p>",
            "<p>In your <code>AutoScalingRollingUpdate</code> policy, set the value of the <code>MinSuccessfulInstancesPercent</code> property to prevent AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch.</p>",
            "<p>During a rolling update, suspend the following Auto Scaling processes: <code>HealthCheck</code>, <code>ReplaceUnhealthy</code>, <code>AZRebalance</code>, <code>AlarmNotification</code>, and <code>ScheduledActions</code>.</p>",
            "<p>Suspend the following Auto Scaling processes that are related with your ELB: <code>Launch</code>, <code>Terminate</code>, and <code>AddToLoadBalancer</code>.</p>",
            "<p>Set the <code>WaitOnResourceSignals</code> property to true in your <code>AutoScalingRollingUpdate</code> policy.</p>"
          ],
          "explanation": "<p>The AWS::AutoScaling::AutoScalingGroup resource uses the UpdatePolicy attribute to define how an Auto Scaling group resource is updated when the AWS CloudFormation stack is updated. If you don't have the right settings configured for the UpdatePolicy attribute, your rolling update can produce unexpected results.</p><p>You can use the <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate\">AutoScalingRollingUpdate policy</a> to control how AWS CloudFormation handles rolling updates for an Auto Scaling group. This common approach keeps the same Auto Scaling group, and then replaces the old instances based on the parameters that you set.</p><p>The <strong>AutoScalingRollingUpdate</strong> policy supports the following configuration options:</p><p><br></p><pre class=\"prettyprint linenums\">    \"UpdatePolicy\": {\n      \"AutoScalingRollingUpdate\": {\n        \"MaxBatchSize\": Integer,\n        \"MinInstancesInService\": Integer,\n        \"MinSuccessfulInstancesPercent\": Integer,\n        \"PauseTime\": String,\n        \"SuspendProcesses\": [ List of processes ],\n        \"WaitOnResourceSignals\": Boolean\n      }\n    }\n</pre><p><br></p><p>Using a rolling update has a risk of system outages and performance degradation due to the decreased availability of your running EC2 instances. If you want to ensure high availability of your application, you can also use the <em>AutoScalingReplacingUpdate</em> policy to perform an immediate rollback of the stack without any possibility of failure.</p><p>To find out what's preventing your Auto Scaling group from updating correctly during a stack update, work through the following troubleshooting scenarios as needed:</p><p><strong>- Configure WaitOnResourceSignals and PauseTime to avoid problems with success signals</strong></p><p>In your <em>AutoScalingRollingUpdate</em> policy, set the <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate-waitonresourcesignals\"><em>WaitOnResourceSignals</em></a> property to false. Take note that if <em>WaitOnResourceSignals</em> is set to true, <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate-pausetime\"><em>PauseTime</em></a> changes to a timeout value. AWS CloudFormation waits to receive a success signal until the maximum time specified by the <em>PauseTime</em> value. If a signal is not received, AWS CloudFormation cancels the update. Then, AWS CloudFormation rolls back the stack with the same settings, including the same PauseTime value.</p><p><strong>- Configure MinSuccessfulInstancesPercent to avoid stack rollback</strong></p><p>If you're replacing a large number of instances during a rolling update and waiting for a success signal for each instance, complete the following: In your <em>AutoScalingRollingUpdate</em> policy, set the value of the <em>MinSuccessfulInstancesPercent</em> property. Take note that setting the <em>MinSuccessfulInstancesPercent</em> property prevents AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch.</p><p><strong>- Configure SuspendProcesses to avoid unexpected changes to the Auto Scaling group</strong></p><p>During a rolling update, suspend the following Auto Scaling processes: <em>HealthCheck</em>, <em>ReplaceUnhealthy</em>, <em>AZRebalance</em>, <em>AlarmNotification</em>, and <em>ScheduledActions</em>. It is quite important to know that if you're using your Auto Scaling group with Elastic Load Balancing (ELB), you should not suspend the following processes: <em>Launch</em>, <em>Terminate</em>, and <em>AddToLoadBalancer</em>. These processes are required to make rolling updates. Take note that if an unexpected scaling action changes the state of the Auto Scaling group during a rolling update, the update can fail. The failure can result from an inconsistent view of the group by AWS CloudFormation.</p><p>Based on the above information, the correct answers are:</p><p><strong>- In your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy, set the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> property to false.</strong></p><p><strong>- In your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy, set the value of the </strong><code><strong>MinSuccessfulInstancesPercent</strong></code><strong> property to prevent AWS CloudFormation from rolling back the entire stack if only a single instance fails to launch</strong></p><p><strong>- During a rolling update, suspend the following Auto Scaling processes: </strong><code><strong>HealthCheck</strong></code><strong>, </strong><code><strong>ReplaceUnhealthy</strong></code><strong>, </strong><code><strong>AZRebalance</strong></code><strong>, </strong><code><strong>AlarmNotification</strong></code><strong>, and </strong><code><strong>ScheduledActions</strong></code></p><p><br></p><p>The option that says: <strong>Switch from </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> to </strong><code><strong>AutoScalingReplacingUpdate</strong></code><strong> policy by modifying the </strong><code><strong>UpdatePolicy</strong></code><strong> of the </strong><code><strong>AWS::AutoScaling::AutoscalingGroup</strong></code><strong> resource in the CloudFormation template. Set the </strong><code><strong>WillReplace</strong></code><strong> property to true</strong> is incorrect because although the <code><strong><em>AutoScalingReplacingUpdate</em></strong></code><strong><em> </em></strong>policy provides an immediate rollback of the stack without any possibility of failure, this solution is not warranted since the scenario asks for the options that will help troubleshoot the issue.</p><p>The option that says: <strong>Suspend the following Auto Scaling processes that are related with your ELB: </strong><code><strong>Launch</strong></code><strong>, </strong><code><strong>Terminate</strong></code><strong>, and </strong><code><strong>AddToLoadBalancer</strong></code> is incorrect because these processes are required by the ELB to make rolling updates.</p><p>The option that says: <strong>Set the </strong><code><strong>WaitOnResourceSignals</strong></code><strong> property to true in your </strong><code><strong>AutoScalingRollingUpdate</strong></code><strong> policy</strong> is incorrect. The <code>WaitOnResourceSignals</code> property should be set to false instead of true, to determine what prevents the Auto Scaling group from being updated correctly during a stack update.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-replacingupdate</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 82921444,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a leading bitcoin wallet and exchange services company is trying to deploy a CloudFormation template that contains a Lambda Function, an S3 bucket, an IAM role, and a DynamoDB table from CodePipeline but the team is getting an <code>InsufficientCapabilitiesException</code>.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you suggest fixing this issue?</p>\n",
          "answers": [
            "<p>Enable the IAM Capability on the CodePipeline configuration for the Deploy CloudFormation stage action</p>",
            "<p>Update the CodePipeline IAM Role so it has permissions to create all the resources mentioned in the CloudFormation template</p>",
            "<p>Fix the CloudFormation template as there is circular dependency and CloudFormation does not have that capability</p>",
            "<p>Increase the service limits for your S3 bucket limits as you've reached it</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable the IAM Capability on the CodePipeline configuration for the Deploy CloudFormation stage action</strong></p>\n\n<p>With AWS CloudFormation and CodePipeline, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. For example, you can create a workflow that automatically builds a test stack when you submit an updated template to a code repository. After AWS CloudFormation builds the test stack, you can test it and then decide whether to push the changes to a production stack. Use CodePipeline to build a continuous delivery workflow by building a pipeline for AWS CloudFormation stacks. CodePipeline has built-in integration with AWS CloudFormation, so you can specify AWS CloudFormation-specific actions, such as creating, updating, or deleting a stack, within a pipeline.</p>\n\n<p>You can use IAM with AWS CloudFormation to control what users can do with AWS CloudFormation, such as whether they can view stack templates, create stacks, or delete stacks.</p>\n\n<p>For the given use-case, <code>InsufficientCapabilitiesException</code> means that the CloudFormation stack is trying to create an IAM role but it doesn't have those specified capabilities. As such it must be configured in CodePipeline configuration for the Deploy CloudFormation stage action.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q19-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the CodePipeline IAM Role so it has permissions to create all the resources mentioned in the CloudFormation template</strong> - The given exception is not related to the permissions of the user or the CodePipeline IAM Role running the CloudFormation template, so this option is incorrect.</p>\n\n<p><strong>Fix the CloudFormation template as there is circular dependency and CloudFormation does not have that capability</strong> - A circular dependency, as the name implies, means that two resources are dependent on each other or that a resource is dependent on itself.</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/b7eb6c689c037217079766fdb77c3bac3e51cb4c/2019/03/19/circular-dependency-sg-group.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n\n<p>This option is incorrect as a circular dependency would trigger another error such as this:</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/b7eb6c689c037217079766fdb77c3bac3e51cb4c/2019/03/19/CircularDependencyerror.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n\n<p><strong>Increase the service limits for your S3 bucket limits as you've reached it</strong> - This option has been added as a distractor as the exception has nothing to do with service limits for the S3 bucket.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html#using-iam-capabilities\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html#using-iam-capabilities</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n"
        }
      },
      {
        "id": 134588473,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A multinational corporation has multiple AWS accounts that are consolidated using AWS Organizations. A new system should be configured for security purposes to detect suspicious activities in any of its accounts, such as SSH brute force attacks or compromised Amazon EC2 instances that serve malware. All gathered information must be centrally stored in its dedicated security account for audit purposes, and the events should be stored in an Amazon S3 bucket.</p><p>Which solution should a DevOps Engineer implement to meet this requirement?</p>",
          "answers": [
            "<p>Automatically detect SSH brute force or malware attacks by enabling Amazon Macie in every account. Set up the security account as the Macie Administrator for every member account of the organization. Create an Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, which should push the findings to an S3 bucket.</p>",
            "<p>Automatically detect SSH brute force or malware attacks by only enabling Amazon Macie in the security account. Configure the security account as the Macie Administrator for every member account. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket.</p>",
            "<p>Automatically detect SSH brute force or malware attacks by enabling Amazon GuardDuty in every account. Configure the security account as the GuardDuty Administrator for all member accounts in the organization. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p>",
            "<p>Automatically detect SSH brute force or malware attacks by only enabling Amazon GuardDuty in the security account. Set up the security account as the GuardDuty Administrator for every member account. Create a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket.</p>"
          ],
          "explanation": "<p><strong>Amazon GuardDuty</strong> is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. The cloud simplifies the collection and aggregation of account and network activities. Still, it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in the AWS Cloud. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats. GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs. With a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with AWS EventBridge, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.</p><p><img alt=\"Amazon GuardDuty\" height=\"378\" src=\"https://media.tutorialsdojo.com/public/product-page-diagram-Amazon-GuardDuty_how-it-works_2AUG2023.png\" width=\"1000\"></p><p>GuardDuty enables and manages across multiple accounts efficiently. All member account findings can be aggregated through the multi-account feature with a GuardDuty administrator account. This allows the security team to manage all GuardDuty findings across the organization in one account. The aggregated findings are also available through EventBridge, making integration with an existing enterprise event management system easy.</p><p>Hence, the correct answer is: <strong>Automatically detect SSH brute force or malware attacks by enabling Amazon GuardDuty in every account. Configure the security account as the GuardDuty Administrator for all member accounts in the organization. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</strong></p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by enabling Amazon Macie in every account. Set up the security account as the Macie Administrator for every member account of the organization. Create an Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, which should push the findings to an S3 bucket</strong> is incorrect because you have to use Amazon GuardDuty instead of Amazon Macie. Note that Amazon Macie cannot detect SSH brute force or malware attacks.</p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by only enabling Amazon Macie in the security account. Configure the security account as the Macie Administrator for every member account. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket</strong> is incorrect because you don't need to create a custom shell script in Lambda or use Kinesis Data Streams. You can just configure the EventBridge rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by only enabling Amazon GuardDuty in the security account. Set up the security account as the GuardDuty Administrator for every member account. Create a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket </strong>is incorrect. Although using Amazon GuardDuty in this scenario is valid, the implementation for storing the findings is incorrect. You can typically configure the EventBridge rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-manage-amazon-guardduty-security-findings-across-multiple-accounts/\">https://aws.amazon.com/blogs/security/how-to-manage-amazon-guardduty-security-findings-across-multiple-accounts/</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-guardduty/?src=udemy\">https://tutorialsdojo.com/amazon-guardduty/</a></p>"
        }
      },
      {
        "id": 99528219,
        "correct_response": [
          "a",
          "e"
        ],
        "prompt": {
          "question": "<p>A company runs many different workloads across hundreds of Amazon EC2 instances. The DevOps team requires that all instances have standard configurations. These configurations include standard logging, metrics, security assessments, and weekly patching.</p><p>Which combination of actions meets these requirements with the most operational efficiency? (Select TWO.)</p>",
          "answers": [
            "<p>Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances.</p>",
            "<p>Use Amazon Inspector to install and manage AWS Systems Manager, AWS OpsWorks, and the Amazon CloudWatch agent on all instances.</p>",
            "<p>Use AWS OpsWorks and execute custom recipes to patch the EC2 instances. Use AWS Systems Manager Run Command to initiate regular Amazon Inspector assessment runs.</p>",
            "<p>Use AWS CloudFormation with change sets to deploy AMIs patched by Systems Manager Patch Manager to existing EC2 instances. Use AWS Config to require regular Amazon Inspector assessment runs.</p>",
            "<p>Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon EventBridge to schedule Amazon Inspector assessment runs.</p>"
          ],
          "explanation": "<p>The most operationally efficient solution is to use AWS Systems Manager patch manager for patching and Amazon Inspector for assessing the security status of the instances. The Amazon CloudWatch agent can also be installed on the instances to get enhanced metrics and logging.</p><p>Systems Manager can be used to install and manage the other components. You must have the Systems Manager agent installed on the instances and the instance profile attached must have permissions to Systems Manager.</p><p>The patching process can be implemented by using Systems Manager Run Command to schedule tasks that deploy the updates using Systems Manager Patch Manager. EventBridge is ideal for scheduling the Amazon Inspector assessment runs.</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon EventBridge to schedule Amazon Inspector assessment runs\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector to install and manage AWS Systems Manager, AWS OpsWorks, and the Amazon CloudWatch agent on all instances\" is incorrect.</p><p>Inspector is not a service that can be used to install anything, it simply runs assessments against your instances and lets you know if there are any security concerns.</p><p><strong>INCORRECT:</strong> \"Use AWS OpsWorks and execute custom recipes to patch the EC2 instances. Use AWS Systems Manager Run Command to initiate regular Amazon Inspector assessment runs\" is incorrect.</p><p>OpsWorks would be less operationally efficient for this purpose. Systems Manager Patch Manager is a better way to deploy patches as it is designed for this purpose and provides many features to simplify and optimize the patching process.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation with change sets to deploy AMIs patched by Systems Manager Patch Manager to existing EC2 instances. Use AWS Config to require regular Amazon Inspector assessment runs\" is incorrect.</p><p>You cannot deploy patched AMIs to existing instances. AMIs are used to deploy the instance initially but you cannot redeploy the AMI without wiping the instance state. Config also cannot be used to enforce an Amazon Inspector assessment run.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p><p><a href=\"https://digitalcloud.training/amazon-inspector/\">https://digitalcloud.training/amazon-inspector/</a></p>"
        }
      },
      {
        "id": 75949174,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
          "answers": [
            "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
            "<p>Add test stages before and after the deploy action.</p>",
            "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
            "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"
          ],
          "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 115961491,
        "correct_response": [
          "c",
          "e"
        ],
        "prompt": {
          "question": "<p>To improve security, a company plans to use AWS Systems Manager Session Manager to manage EC2 instances instead of using key pairs. The company also requires that access to Session Manager goes across private networks only.</p><p>Which combinations of actions will accomplish this? (Select TWO.)</p>",
          "answers": [
            "<p>Update all EC2 instance security groups to allow SSH port TCP 22 inbound from the VPC CIDR.</p>",
            "<p>Deploy an AWS Site to Site VPN in the relevant AWS Region for private access to Systems Manager.</p>",
            "<p>Attach an IAM policy providing the required Systems Manager permissions to an existing IAM instance profile.</p>",
            "<p>Run the \u2018aws configure\u2019 command on all EC2 instances to add access keys that provide the required Systems Manager permissions.</p>",
            "<p>Create VPC endpoints for Systems Manager in the relevant AWS Region to provide private access.</p>"
          ],
          "explanation": "<p>Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage Amazon EC2 instances, edge devices, and on-premises servers and virtual machines (VMs).</p><p>Session Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. You do not need to open SSH ports in security groups and can enable private access using VPC endpoints.</p><p>To manage your instances using Session Manager you must install the Systems Manager agent on them and provide the necessary permissions for management. Permissions should be assigned through IAM instance profiles and policies.</p><p><strong>CORRECT: </strong>\"Attach an IAM policy providing the required Systems Manager permissions to an existing IAM instance profile\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create VPC endpoints for Systems Manager in the relevant AWS Region to provide private access\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update all EC2 instance security groups to allow SSH port TCP 22 inbound from the VPC CIDR\" is incorrect.</p><p>With Systems Manager Session Manager you do not need to open SSH ports.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Site to Site VPN in the relevant AWS Region for private access to Systems Manager\" is incorrect.</p><p>The private access between Session Manager and EC2 instances happens within AWS via VPC endpoints. A VPN cannot be used.</p><p><strong>INCORRECT:</strong> \"Run the \u2018aws configure\u2019 command on all EC2 instances to add access keys that provide the required Systems Manager permissions\" is incorrect.</p><p>This is a less secure method of providing the permissions needed by the EC2 instances. The better option is to use IAM instance profiles and policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 82921386,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A Silicon Valley based startup runs a news discovery web application and it uses CodeDeploy to deploy the web application on a set of 20 EC2 instances behind an Application Load Balancer. The ALB is integrated with CodeDeploy. The DevOps teams at the startup would like the deployment to be gradual and to automatically rollback in case of unusually high maximum CPU utilization for the EC2 instances while traffic is being served.</p>\n\n<p>How can you implement this?</p>\n",
          "answers": [
            "<p>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a CloudWatch Alarm on top of that metric. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch alarm</p>",
            "<p>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, measure the CPU utilization for 5 minutes. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</p>",
            "<p>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</p>",
            "<p>Create a CloudWatch metric for the maximum CPU utilization of your Application Load Balancer. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a CloudWatch Alarm on top of that metric. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch alarm</strong></p>\n\n<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time. You can monitor metrics such as instance CPU utilization. If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover). You can also automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated. CodeDeploy will redeploy the last known working version of the application when it rolls back. Previously, you needed to manually initiate a deployment if you wanted to roll back a deployment. For the given use-case, you should use the underlying metric as the maximum CPU for your EC2 instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q11-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p>Configure advanced options for a deployment group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q11-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, measure the CPU utilization for 5 minutes. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</strong> - If you are using the <code>ValidateService</code> hook because your CodeDeploy is integrated with the ALB, traffic will not be served and you won't observe high CPU utilization.</p>\n\n<p><strong>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</strong> - CodeDeploy rollbacks only work with CloudWatch alarms, not CloudWatch metrics. So this option is incorrect.</p>\n\n<p><strong>Create a CloudWatch metric for the maximum CPU utilization of your Application Load Balancer. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</strong> - This option has been added as a distractor as you would want to watch out for the maximum CPU utilization of the EC2 instances and not the Application Load Balancer. In addition, CodeDeploy rollbacks only work with CloudWatch alarms, not CloudWatch metrics.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n"
        }
      },
      {
        "id": 138248179,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A software company hosts backend services within a private subnet of an Amazon Virtual Private Cloud (VPC). The company is integrating with a new external partner that requires IPv6 connectivity. The company's security team manages access through a security group configured to allow outbound traffic to all approved partner integrations.</p><p>As part of the integration process, the DevOps team allocated IPv6 addresses to the backend services. They\u2019ve also updated the security group's outbound rules to include the partner's IPv6 CIDR range. Despite these changes, the services failed to establish communication with the partner's systems, even though they continued to communicate internally without issue.</p><p>What additional steps should be implemented to enable outbound connectivity?</p>",
          "answers": [
            "<p>Update the NAT Gateway settings to enable IPv6. Configure the subnet\u2019s route table to direct all IPv6 traffic ( <code>::/0</code> ) to the NAT gateway.</p>",
            "<p>Deploy a NAT instance in the VPC and disable the source/destination check on the instance. Add a new route with <code>0.0.0.0/0</code> as the destination and the NAT instance as the target.</p>",
            "<p>Attach an egress-only internet gateway to the VPC. Configure the subnet\u2019s route table to direct all IPv6 traffic ( <code>::/0</code> ) to the egress-only internet gateway.</p>",
            "<p>Attach a new internet gateway to the VPC. Add a new route with <code>0.0.0.0/0</code> as the destination and select the internet gateway as the target.</p>"
          ],
          "explanation": "<p>An egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances within the VPC to the internet and prevents the internet from initiating an IPv6 connection with instances within the VPC. Unlike an internet gateway (IGW), which allows both inbound and outbound IPv4 traffic, an egress-only internet gateway allows only outbound IPv6 traffic.</p><p>To ensure backend systems can communicate with an external system, create an egress-only internet gateway in the VPC and then add a route to the route table pointing all IPv6 traffic (<code>::/0</code>) or a specific range of IPv6 addresses to the egress-only internet gateway.</p><p><img src=\"https://media.tutorialsdojo.com/public/EgressOnlyInternetGateway.png\"></p><p>An egress-only internet gateway is stateful: it forwards traffic from the instances in the subnet to the internet or other AWS services and then sends the response back to the instances. Security groups can be used for instances in the private subnet to control traffic to and from the instances. Similarly, network ACL can be used to control traffic to and from the subnet for which the egress-only internet gateway routes traffic.</p><p>Hence, the correct answer is: <strong>Attach an egress-only internet gateway to the VPC. Configure the subnet\u2019s route table to direct all IPv6 traffic (</strong><code><strong>::/0</strong></code><strong>) to the egress-only internet gateway.</strong></p><p>The option that says: <strong>Update the NAT Gateway settings to enable IPv6. Configure the subnet\u2019s route table to direct all IPv6 traffic (</strong><code><strong>::/0</strong></code><strong>) to the NAT gateway. (</strong><code><strong>::/0</strong></code><strong>) to the NAT gateway</strong> is incorrect. NAT Gateways only support IPv4 traffic and not IPv6. Enabling IPv6 in the settings of the NAT gateway is not possible. Although NAT Gateways support IPv6 to IPv4 translation via NAT64, they only allow IPv6 AWS resources to communicate with IPv4 resources, not IPv6-to-IPv6 communication.</p><p>The option that says: <strong>Deploy a NAT instance in the VPC and disable the source/destination check on the instance. Add a new route with </strong><code><strong>0.0.0.0/0</strong></code><strong> as the destination and the NAT instance as the target</strong> is incorrect. This approach describes a solution for IPv4 traffic, not IPv6. Note that 0.0.0.0/0 represents an IPV4 CIDR range.</p><p>The option that says: <strong>Attach a new internet gateway to the VPC. Add a new route with </strong><code><strong>0.0.0.0/0</strong></code><strong> as the destination and select the internet gateway as the target</strong> is incorrect. Although an internet gateway supports both IPv4 and IPv6 traffic, the specified CIDR range of 0.0.0.0/0 pertains only to IPv4. Furthermore, a VPC can have only one internet gateway attached at any given time.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a></p><p><strong>Check out this VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc</a></p>"
        }
      },
      {
        "id": 138248129,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company has an application hosted in an Auto Scaling group of EC2 instances which calls an external API with a URL of <code>http://api.tutorialsdojo.com</code> as part of its processing. There was a recent deployment that changed the protocol of the URL from HTTP to HTTPS but after that, the application has stopped working properly. The DevOps engineer has verified using his POSTMAN tool that the external API works without any issues and the VPC being utilized is still using the default network ACL. </p><p>Which of the following is the MOST appropriate course of action that the engineer should take to determine the root cause of this problem?</p>",
          "answers": [
            "<p>Log in to the AWS Management Console and then in the VPC flow logs, look for <code>ACCEPT</code> records which were originated from the Auto Scaling group. Verify that the ingress security group rules of the Auto Scaling Group allow the incoming traffic from the external API.</p>",
            "Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the existing egress security group rules of the Auto Scaling Group, as well as the network ACL, allow the outgoing traffic to the external API.",
            "<p>Log in to the AWS Management Console and look for <code>REJECT</code> records in the VPC flow logs which originated from the Auto Scaling group. Verify that the egress security group rules of the Auto Scaling Group allow the outgoing traffic to the external API.</p>",
            "Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the ingress security group rules of the Auto Scaling Group, as well as the network ACL, allow the incoming traffic from the external API."
          ],
          "explanation": "<p><strong>Amazon Virtual Private Cloud</strong> provides features that you can use to increase and monitor the security of your virtual private cloud (VPC):</p><p><strong>Security groups</strong>: Security groups act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level. When you launch an instance, you can associate it with one or more security groups that you've created. Each instance in your VPC could belong to a different set of security groups. If you don't specify a security group when you launch an instance, the instance is automatically associated with the default security group for the VPC.</p><p><strong>Network access control lists (ACLs)</strong>: Network ACLs act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level.</p><p><strong>Flow logs</strong>: Flow logs capture information about the IP traffic going to and from network interfaces in your VPC. You can create a flow log for a VPC, subnet, or individual network interface. Flow log data is published to CloudWatch Logs or Amazon S3, and can help you diagnose overly restrictive or overly permissive security group and network ACL rules.</p><p><strong>Traffic mirroring</strong>: You can copy network traffic from an elastic network interface of an Amazon EC2 instance. You can then send the traffic to out-of-band security and monitoring appliances.</p><p>You can use AWS Identity and Access Management to control who in your organization has permission to create and manage security groups, network ACLs, and flow logs. For example, you can give only your network administrators that permission, but not personnel who only need to launch instances.</p><p><img src=\"https://media.tutorialsdojo.com/security-diagram.png\"></p><p>For HTTP traffic, you must add an inbound rule on port 80 from the source address 0.0.0.0/0. For HTTPS traffic, add an inbound rule on port 443 from the source address 0.0.0.0/0. These inbound rules allow traffic from IPv4 addresses. To allow IPv6 traffic, add inbound rules on the same ports from the source address ::/0. Because security groups are stateful, the return traffic from the instance to users is allowed automatically, so you don't need to modify the security group's outbound rules.</p><p>The default network ACL is configured to allow all traffic to flow in and out of the subnets with which it is associated. Each network ACL also includes a rule whose rule number is an asterisk. This rule ensures that if a packet doesn't match any of the other numbered rules, it's denied.</p><p>In this scenario, the change of the URL from HTTP to HTTPS means that the application is using port 443 and not port 80 anymore. Since the application is the one that initiates the call to the external API, it makes sense to check if the egress security group rules allow outgoing HTTPS (443) traffic.</p><p>Hence, the correct answer is:<strong> Log in to the AWS Management Console and look for </strong><code><strong>REJECT</strong></code><strong> records in the VPC flow logs which originated from the Auto Scaling group. Verify that the egress security group rules of the Auto Scaling Group allow the outgoing traffic to the external API.</strong></p><p>The option that says: <strong>Log in to the AWS Management Console and then in the VPC flow logs, look for </strong><code><strong>ACCEPT</strong></code><strong> records which were originated from the Auto Scaling group. Verify that the ingress security group rules of the Auto Scaling Group allow the incoming traffic from the external API </strong>is incorrect because you should first check the egress rules (instead of ingress) of your security group first. Remember that it is the Auto Scaling group of EC2 instances that initiates the call to the external API and not the other way around. In addition, it is more effective if you look for <code><em>REJECT</em></code> records instead of <code>ACCEPT</code> records in VPC Flow Logs to view the details of the failed connection to your external API.</p><p>The option that says: <strong>Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the existing egress security group rules of the Auto Scaling Group, as well as the network ACL, allow the outgoing traffic to the external API</strong> is incorrect because, in the first place, the scenario didn't mention that the CloudWatch Logs agent is installed in the EC2 instances. Although it is right to check the existing egress security group rules, you don't need to check the network ACL since the architecture is already using a default one which is configured to allow all traffic.</p><p>The option that says: <strong>Log in to the AWS Management Console and view the application logs in Amazon CloudWatch Logs to troubleshoot the issue. Verify that the ingress security group rules of the Auto Scaling Group, as well as the network ACL, allow the incoming traffic from the external API</strong> is incorrect because you have to check the egress security group rules first instead. The scenario also didn't mention that the CloudWatch Logs agent is installed in the EC2 instances, which means that you might not be able to view the application logs in CloudWatch.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/\">https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><br></p><p><strong>Amazon VPC Overview:</strong></p><p><a href=\"https://youtu.be/oIDHKeNxvQQ\">https://youtu.be/oIDHKeNxvQQ</a><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p>"
        }
      },
      {
        "id": 67357176,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An e-commerce company is deploying its flagship application on Amazon EC2 instances. The DevOps team at the company needs a solution to query both the application logs as well as the AWS account API activity.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you recommend to meet these requirements?</p>\n",
          "answers": [
            "<p>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</p>",
            "<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</p>",
            "<p>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</p>",
            "<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</strong></p>\n\n<p>CloudTrail is enabled by default for your AWS account. You can use Event history in the CloudTrail console to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. This includes activity made through the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. For an ongoing record of events in your AWS account, you can create a trail. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all AWS Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs.</p>\n\n<p>You can also configure CloudTrail with CloudWatch Logs to monitor your trail API logs and be notified when specific activity occurs. When you configure your trail to send events to CloudWatch Logs, CloudTrail sends only the events that match your trail settings. For example, if you configure your trail to log data events only, your trail sends data events only to your CloudWatch Logs log group. CloudTrail supports sending data, Insights, and management events to CloudWatch Logs.</p>\n\n<p>You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is CWAgent, although you can specify a different namespace when you configure the agent. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs</p>\n\n<p>For the given use case, you can have both AWS CloudTrail as well as the Amazon CloudWatch Agent deliver the respective logs to CloudWatch Logs. You can then use the CloudWatch Logs Insights to query both sets of logs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. So, this option is incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</strong> - You cannot deliver the API logs from AWS CloudTrail to Kinesis Data Streams. Similarly, you cannot have the Amazon CloudWatch Agent deliver logs from the EC2 instances to Kinesis Data Streams. So, both these options are incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use Amazon Athena to query logs published to Amazon CloudWatch Logs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html</a></p>\n"
        }
      },
      {
        "id": 75949092,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is developing an application that calls AWS Lambda functions. The functions must connect to a database and credentials must not be stored in source code. The credentials for connection to the database must be regularly rotated to meet security policy requirements.</p><p>What should a DevOps engineer do to meet these requirements?</p>",
          "answers": [
            "<p>Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID.</p>",
            "<p>Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID.</p>",
            "<p>Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution.</p>",
            "<p>Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID.</p>"
          ],
          "explanation": "<p>AWS Secrets Manager is ideal for this scenario as it can be used to securely store the secrets. Automatic rotation can be enabled for several AWS databases and can be configured through custom Lambda functions for other databases.</p><p><strong>CORRECT: </strong>\"Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID\" is incorrect.</p><p>CloudHSM is used for storing encryption keys, not connection credentials.</p><p><strong>INCORRECT:</strong> \"Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution\" is incorrect.</p><p>This is not a secure method of storing the credentials and Secrets Manager is more secure.</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID\" is incorrect.</p><p>KMS is used for storing encryption keys, not connection credentials.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>"
        }
      },
      {
        "id": 82921420,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An e-commerce company has deployed a Spring application on Elastic Beanstalk running the Java platform. As a DevOps Engineer at the company, you are referencing an RDS PostgreSQL database through an environment variable so that your application can use it for storing its data. You are using a library to perform a database migration in case the schema changes. Upon deploying updates to Beanstalk, you have seen the migration fail because all the EC2 instances running the new version try to run the migration on the RDS database.</p>\n\n<p>How can you fix this issue?</p>\n",
          "answers": [
            "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</p>",
            "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</p>",
            "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</p>",
            "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</strong></p>\n\n<p>You can use Elastic Beanstalk configuration files (.ebextensions) with your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the option_settings key to modify the environment configuration. You can choose from general options for all environments and platform-specific options.</p>\n\n<p>You may want to customize and configure the software that your application depends on. You can use the commands key to execute commands on the EC2 instance. The commands run before the application and web server are set up and the application version file is extracted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n\n<p>You can use the container_commands key to execute commands that affect your application source code. Container commands run after the application and web server have been set up and the application version archive has been extracted, but before the application version is deployed. You can use leader_only to only run the command on a single instance, or configure a test to only run the command when a test command evaluates to true.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q25-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n\n<p>If you specify a <code>commands</code> block, every EC2 instance will run it and it does not support the <code>leader_only</code> attribute. Therefore you must use <code>container_commands</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</strong> - As mentioned earlier, if you specify a <code>commands</code> block, every EC2 instance will run it and it does not support the <code>leader_only</code> attribute. So this option is incorrect.</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</strong></p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</strong></p>\n\n<p>The <code>lock_mode: true</code> attribute has been added as a distractor and it does not exist. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/35788499/what-is-difference-between-commands-and-container-commands-in-elasticbean-talk/40096352#40096352\">https://stackoverflow.com/questions/35788499/what-is-difference-between-commands-and-container-commands-in-elasticbean-talk/40096352#40096352</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n"
        }
      }
    ],
    "answers": {
      "67357176": [
        "b"
      ],
      "75949092": [
        "b"
      ],
      "75949164": [
        "b",
        "d"
      ],
      "75949174": [
        "c"
      ],
      "82921316": [
        "a"
      ],
      "82921342": [
        "a"
      ],
      "82921386": [
        "a"
      ],
      "82921406": [
        "a"
      ],
      "82921420": [
        "a"
      ],
      "82921440": [
        "a",
        "b"
      ],
      "82921444": [
        "a"
      ],
      "99528219": [
        "a",
        "e"
      ],
      "115961491": [
        "c",
        "e"
      ],
      "134588369": [
        "c",
        "d",
        "f"
      ],
      "134588473": [
        "c"
      ],
      "134588501": [
        "c"
      ],
      "138248111": [
        "a"
      ],
      "138248129": [
        "c"
      ],
      "138248173": [
        "b",
        "c",
        "e"
      ],
      "138248179": [
        "c"
      ]
    }
  },
  {
    "id": "1770068031933",
    "date": "2026-02-02T21:33:51.933Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 13,
    "incorrect": 7,
    "unanswered": 0,
    "total": 20,
    "percent": 65,
    "duration": 2434898,
    "questions": [
      {
        "id": 115961511,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company uses a tagging strategy to allocate usage costs for AWS resources. An application runs on Amazon EC2 instances in an Auto scaling group. The Amazon EBS volumes that are attached to instances are being created without the correct cost center tags. A DevOps engineer must correct the configuration to ensure the EBS volumes are tagged appropriately.</p><p>What is the MOST efficient solution that meets this requirement?</p>",
          "answers": [
            "<p>Update the Auto Scaling group launch template to include the cost center tags for EBS volumes.</p>",
            "<p>Update the Auto Scaling group to include the cost center tags. Set the PropagateAtLaunch property to true.</p>",
            "<p>Use AWS Config to enforce tagging at EBS volume creation time and deny creation of any volumes that do not have the appropriate cost center tags.</p>",
            "<p>Use Tag Editor to scan the account for EBS volumes that are missing the tags and then add the cost center tags to the volumes.</p>"
          ],
          "explanation": "<p>You can tag new or existing Auto Scaling groups. You can also propagate tags from an Auto Scaling group to the EC2 instances that it launches.</p><p>Tags are not propagated to Amazon EBS volumes. To add tags to Amazon EBS volumes, specify the tags in a launch template.</p><p><strong>CORRECT: </strong>\"Update the Auto Scaling group launch template to include the cost center tags for EBS volumes\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update the Auto Scaling group to include the cost center tags. Set the PropagateAtLaunch property to true\" is incorrect.</p><p>As noted above, you cannot propagate tags from an Auto Scaling group to EBS volumes.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to enforce tagging at EBS volume creation time and deny creation of any volumes that do not have the appropriate cost center tags\" is incorrect.</p><p>AWS Config can be used to report on compliance but cannot stop volume creation.</p><p><strong>INCORRECT:</strong> \"Use Tag Editor to scan the account for EBS volumes that are missing the tags and then add the cost center tags to the volumes\" is incorrect.</p><p>Tag Editor is not an efficient solution as it would involve manual work. The correct solution is fully automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-tagging.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-tagging.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 75949136,
        "correct_response": [
          "d",
          "e"
        ],
        "prompt": {
          "question": "<p>A media company is developing a new application which will be supported by mobile devices, tablets, and desktops. Each platform is customized for a different user experience and various viewing modes based on the resources being requested by users. This is enabled by path-based routing behind the scenes, using instances deployed on Amazon EC2. An auto scaling group has been configured to ensure EC2 instances are highly scalable.</p><p>Which of the following combinations will ensure high performance and minimal cost? (Select TWO.)</p>",
          "answers": [
            "<p>Utilize a Network Load Balancer behind auto scaling group.</p>",
            "<p>Utilize Amazon Route 53 with traffic flow policies.</p>",
            "<p>Utilize a static website hosted behind an Amazon S3 bucket.</p>",
            "<p>Amazon CloudFront with Lambda@Edge.</p>",
            "<p>Utilize an Application Load Balancer behind auto scaling group.</p>"
          ],
          "explanation": "<p><strong>Amazon CloudFront with Lambda@Edge</strong> - Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. Lambda@Edge runs your code in response to events generated by the Amazon CloudFront content delivery network (CDN). By combining Lambda@Edge with other AWS services, developers can build dynamic, powerful web applications at the edge that automatically scale up and down\u2014with zero origin infrastructure and administrative effort required for automatic scaling, backups, or data center redundancy.</p><p>By using Lambda@Edge to dynamically route requests to different origins based on different viewer characteristics, you can balance the load on your origins, while improving the performance for your users. For example, you can route requests to origins within a home region, based on a viewer's location.</p><p>With Lambda@Edge, you don't have to provision or manage infrastructure in multiple locations around the world. You pay only for the compute time you consume - there is no charge when your code is not running.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-53-55-d101810f6e21e0583638ef6be55f9c9c.jpg\"><p><strong>Application Load Balancer</strong> - An Application Load Balancer serves as the single point of contact for clients. The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. This increases the availability of your application. You add one or more listeners to your load balancer. ALB offers support for Path conditions. You can configure rules for your listener that forward requests based on the URL in the request. This enables you to structure your application as smaller services, and route requests to the correct service based on the content of the URL.</p><p><strong>CORRECT: </strong>\"Amazon CloudFront with Lambda@Edge and utilize an application load balancer behind auto scaling group\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> Utilize a Network Load Balancer behind auto scaling group\" is incorrect.</p><p>The Application Load Balancer works at the Application Layer (Layer 7 of the OSI model, Request level). The Network Load Balancer works at the Transport layer (Layer 4 of the OSI model). The NLB just forwards requests whereas ALB examines the contents of the HTTP request header to determine where to route the request. So, the ALB can perform content-based routing while NLB cannot.</p><p><strong>INCORRECT</strong>: \"Utilize a static website hosted behind a S3 bucket\" is incorrect.</p><p>It is mentioned in the question that the application is hosted on an Amazon EC2 instances and that a solution is needed for performance and cost-effectiveness. Hence, using Amazon S3 as a static website is not suitable answer.</p><p><strong>INCORRECT</strong>: \"Utilize Amazon Route 53 with traffic flow policies\" is incorrect.</p><p>Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect. Amazon Route 53 is fully compliant with IPv6 as well.</p><p>Traffic Flow policy allows an Amazon Web Services customer to define how end-user traffic is routed to application endpoints through a visual interface. Route 53 can route between domains and subdomains but is not useful for path-based traffic flows.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/edge/\">https://aws.amazon.com/lambda/edge/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/create-subdomain-route-53/\">https://aws.amazon.com/premiumsupport/knowledge-center/create-subdomain-route-53/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>"
        }
      },
      {
        "id": 143860755,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A DevOps team is developing a PHP web application which will be deployed on Amazon EC2 instances. The application has been designed to use a MySQL database. The team requires a deployment model in which they can reliably build, test, and deploy new updates daily, without downtime or degraded performance. The application must also be able to scale to meet an unpredictable number of concurrent users.</p><p>Which action will allow the team to quickly meet these objectives?</p>",
          "answers": [
            "<p>Create two Amazon ECS services: one for test and one for production. Upload the web application code to the test ECS tasks using the AWS CLI. Test the application and if the tests pass, upload the code to the production ECS tasks.</p>",
            "<p>Create an AWS CloudFormation template which deploys an Auto Scaling group of EC2 instances behind an Application Load Balancer. Use AWS CodeBuild to build and test the PHP application. Install the application and the MySQL database on each EC2 instance. Update the stack to deploy new application versions.</p>",
            "<p>Use AWS OpsWorks to create a stack for the application with a DynamoDB database layer, an Application Load Balancing layer, and an Amazon EC2 instance layer. Use Chef recipes to build and deploy the application. Use custom health checks to run unit tests on each instance with rollback on failure.</p>",
            "<p>Create two auto scaling AWS Elastic Beanstalk environments: one for test and one for production. Build the application using AWS CodeBuild and use Amazon RDS to store data. When new versions of the applications have passed all tests, use the Elastic Beanstalk 'swap cname' to promote the test environment to production.</p>"
          ],
          "explanation": "<p>Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application might become unavailable to users for a short period of time. To avoid this, perform a blue/green deployment. To do this, deploy the new version to a separate environment, and then swap the CNAMEs of the two environments to redirect traffic to the new version instantly.</p><p>This solution that meets the requirements of this question by ensuring that the team can build, test, and deploy new updates daily, without downtime or degraded performance. The auto scaling support for the environments will automatically scale the infrastructure to ensure that the application can meet changing demands.</p><p><strong>CORRECT: </strong>\"Create two auto scaling AWS Elastic Beanstalk environments: one for test and one for production. Build the application using AWS CodeBuild and use Amazon RDS to store data. When new versions of the applications have passed all tests, use the Elastic Beanstalk 'swap cname' to promote the test environment to production\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create two Amazon ECS services: one for test and one for production. Upload the web application code to the test ECS tasks using the AWS CLI. Test the application and if the tests pass, upload the code to the production ECS tasks\" is incorrect.</p><p>Using the CLI to upload code is not a best practice. It is better to build and test with a service such as AWS CodeBuild. The main problem with this solution is there is no method of deploying updates in a blue/green strategy and to be able to rollback changes when necessary.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudFormation template which deploys an Auto Scaling group of EC2 instances behind an Application Load Balancer. Use AWS CodeBuild to build and test the PHP application. Install the application and the MySQL database on each EC2 instance. Update the stack to deploy new application versions\" is incorrect.</p><p>CloudFormation stacks are best used for infrastructure deployments and updates and not for application/code level changes.</p><p><strong>INCORRECT:</strong> \"Use AWS OpsWorks to create a stack for the application with a DynamoDB database layer, an Application Load Balancing layer, and an Amazon EC2 instance layer. Use Chef recipes to build and deploy the application. Use custom health checks to run unit tests on each instance with rollback on failure\" is incorrect.</p><p>DynamoDB is not a suitable solution when the question states that the application has been designed to work with MySQL which is an SQL type of database. A code rewrite would be required to support a NoSQL database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 67357142,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A CloudFormation stack consists of the following AWS resources - Amazon Simple Storage Service (Amazon S3) bucket, Amazon Amazon Elastic Compute Cloud (Amazon EC2) instance, and an Amazon EBS Volume. Due to a high-impact security issue, the DevOps team has been asked to rename the AWS CloudFormation stack. However, the resources created by the stack cannot be deleted for business purposes.</p>\n\n<p>What steps will you take to rename the CloudFormation stack without deleting the resources created?</p>\n",
          "answers": [
            "<p>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of the S3 bucket and EC2 instance. Add a 'Snapshot' attribute to the deletion policy of Amazon EBS Volume. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> and 'Snapshot' attribute from the stack to revert to the original template</p>",
            "<p>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> attribute from the stack to revert to the original template</p>",
            "<p>Launch a new CloudFormation stack that deploys all the resources of the stack. Launch another CloudFormation stack that deploys all the resources of the stack with a new name. In the second CloudFormation stack use, the <code>DependsOn</code> attribute to create the resource dependency on the first stack. Delete both the stacks while retaining the resources</p>",
            "<p>Use CloudFormation registry to create custom hooks for all the resources of the CloudFormation stack. Then, delete the original CloudFormation stack and recreate a new one with the updated name. Using the hooks created earlier, import the resources back into the newly created stack</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> attribute from the stack to revert to the original template</strong></p>\n\n<p>With the DeletionPolicy attribute, you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. To keep a resource when its stack is deleted, specify <code>Retain</code> for that resource. You can use <code>Retain</code> for any resource. For resources that support snapshots, such as <code>AWS::EC2::Volume</code>, specify Snapshot to have CloudFormation create a snapshot before deleting the resource.</p>\n\n<p>These steps need to be followed:\n1. Launch a CloudFormation stack that deploys the necessary resources.\n2. Add a <code>Retain</code> attribute to the deletion policy of all the resources deployed by the stack.\n3. Delete the stack and verify that the resources are retained.\n4. Create a new stack and import the resources that were retained from the original stack. This stack is created with a new name.\n5. Remove the <code>Retain</code> attribute from the stack to revert to the original template.</p>\n\n<p>Refer to the example that walks through the process of retaining a single resource\u2014a VPC\u2014when changing the name of a CloudFormation stack:</p>\n\n<p>Process overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q24-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/\">https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of S3 bucket and EC2 instance. Add a 'Snapshot' attribute to the deletion policy of Amazon EBS Volume. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> and 'Snapshot' attribute from the stack to revert to the original template</strong> - 'Snapshot' attribute will still delete the resource after taking its snapshot. So this option is incorrect.</p>\n\n<p><strong>Use the CloudFormation registry to create custom hooks for all the resources of the CloudFormation stack. Then, delete the original CloudFormation stack and recreate a new one with the updated name. Using the hooks created earlier, import the resources back into the newly created stack</strong> - A hook is an executable custom logic that automatically inspects resources before they're provisioned. Hooks can inspect the resources that CloudFormation is about to provision. If a hook finds any resource that doesn't comply with your organizational guidelines, it can prevent CloudFormation from continuing the provisioning process. Any hook based logic is not useful for the given use case.</p>\n\n<p><strong>Launch a new CloudFormation stack that deploys all the resources of the stack. Launch another CloudFormation stack that deploys all the resources of the stack with a new name. In the second CloudFormation stack use, <code>DependsOn</code> attribute to create the resource dependency on the first stack. Delete both stacks while retaining the resources</strong> - This option just acts as a distractor, as it does not solve the given issue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/\">https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html\">https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html</a></p>\n"
        }
      },
      {
        "id": 143860773,
        "correct_response": [
          "a",
          "d"
        ],
        "prompt": {
          "question": "<p>A company is migrating an important production application to the AWS Cloud. The application includes a containerized web tier and a MySQL database. The application must be deployed with high availability and fault tolerance and must minimize cost.</p><p>How should a DevOps engineer refactor the application? (Select TWO.)</p>",
          "answers": [
            "<p>Migrate the web tier to an AWS Fargate cluster and configure a service and attach an Application Load Balancer.</p>",
            "<p>Migrate the web tier to an Auto Scaling group of EC2 instances across multiple AZs and use an Application Load Balancer.</p>",
            "<p>Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ.</p>",
            "<p>Migrate the MySQL database to an Amazon RDS Multi-AZ deployment.</p>",
            "<p>Migrate the MySQL database to an Amazon DynamoDB with Global Tables.</p>"
          ],
          "explanation": "<p>AWS Fargate is a managed and serverless service for running Docker containers. It is part of the Amazon ECS family of AWS services and supports multiple availability zones for all deployments. To attach a load balancer you must create a service and you can define the desired count of ECS tasks to run.</p><p>The database tier of the application can be deployed on a highly available Amazon RDS database that has a multi-AZ replica. This provides fault tolerance across multiple AZs.</p><p><strong>CORRECT: </strong>\"Migrate the web tier to an AWS Fargate cluster and configure a service and attach an Application Load Balancer\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Migrate the MySQL database to an Amazon RDS Multi-AZ deployment\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the web tier to an Auto Scaling group of EC2 instances across multiple AZs and use an Application Load Balancer\" is incorrect.</p><p>EC2 is not the most cost-effective platform for this solution as the application is already containerized and therefore should run on Amazon ECS for best performance and cost efficiency.</p><p><strong>INCORRECT:</strong> \"Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ\" is incorrect.</p><p>A Read replica in another AZ can be used as an asynchronous backup target but the promotion time can be quite long in a failure scenario. A multi-AZ instance allows fast failover.</p><p><strong>INCORRECT:</strong> \"Migrate the MySQL database to an Amazon DynamoDB with Global Tables\" is incorrect.</p><p>DynamoDB is a NoSQL database, so it is not suitable for migrating MySQL which is a relational database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html</a></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>"
        }
      },
      {
        "id": 75949146,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
          "answers": [
            "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
            "<p>The new AZ has not been enabled for the ALB.</p>",
            "<p>The AMI has not been updated to include the new AZ.</p>",
            "<p>There are no subnets associated with the new AZ.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 138248235,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company is developing a serverless application that uses AWS Lambda, AWS SAM, and Amazon API Gateway. There is a requirement to fully automate the backend Lambda deployment in such a way that the deployment will automatically run whenever a new commit is pushed to a GitHub repository. There should also be a separate environment pipeline for TEST and PROD environments. In addition, the TEST environment should be the only one that allows automatic deployment.</p><p>How can a DevOps Engineer satisfy these requirements?</p>",
          "answers": [
            "<p>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>"
          ],
          "explanation": "<p>In <strong>AWS CodePipeline</strong>, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p>If the action is approved, the pipeline execution resumes. If the action is rejected\u2014or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping\u2014the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p><img src=\"https://media.tutorialsdojo.com/public/PipelineFlow.png\"></p><p>Hence, the correct answer is: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</strong></p><p>The option that says: <strong>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline</strong> is incorrect. You should add a manual approval step on the PROD pipeline as mentioned in the requirements of the scenario.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline</strong> is incorrect. You don't need to create separate GitHub repositories for the two environments. You just need to create two different branches from a single repository.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline </strong>is incorrect. You should add the manual approval step on the PROD pipeline only, excluding the TEST pipeline. Moreover, you don't need to create separate GitHub repositories for the two environments. You only need to create two different branches from a single repository.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html \">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 138248207,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A software development company is using GitHub, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline for its CI/CD process. To further improve their systems, they need to implement a solution that automatically detects and reacts to changes in the state of their deployments in AWS CodeDeploy. Any changes must be rolled back automatically if the deployment process fails, and a notification must be sent to the DevOps Team's Slack channel for easy monitoring.</p><p>Which of the following is the MOST suitable configuration that you should implement to satisfy this requirement?</p>",
          "answers": [
            "<p>Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team's Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when a deployment fails</code> setting.</p>",
            "<p>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when alarm thresholds are met</code> setting.</p>",
            "<p>Configure a CodeDeploy agent to send notification to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful.</p>",
            "<p>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team's Slack Channel when the <code>PutLifecycleEventHookExecutionStatus</code> API call has been detected. Rollback the changes by using the AWS CLI.</p>"
          ],
          "explanation": "<p>You can monitor <strong>CodeDeploy</strong> deployments using the following CloudWatch tools: Amazon EventBridge, CloudWatch alarms, and Amazon CloudWatch Logs.</p><p>Reviewing the logs created by the CodeDeploy agent and deployments can help you troubleshoot the causes of deployment failures. As an alternative to reviewing CodeDeploy logs on one instance at a time, you can use CloudWatch Logs to monitor all logs in a central location.</p><p>You can use <strong>Amazon EventBridge </strong>(formerly known as Amazon CloudWatch Events) to detect and react to changes in the state of an instance or a deployment (an \"event\") in your CodeDeploy operations. Then, based on the rules you create, EventBridge will invoke one or more target actions when a deployment or instance enters the state you specify in a rule. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-rule-12-09-2024.png\"></p><p>You can select the following types of targets when using EventBridge as part of your CodeDeploy operations:</p><p>- AWS Lambda functions</p><p>- Kinesis streams</p><p>- Amazon SQS queues</p><p>- Built-in targets (CloudWatch alarm actions)</p><p>- Amazon SNS topics</p><p>The following are some use cases:</p><p>- Use a Lambda function to pass a notification to a Slack channel whenever deployments fail.</p><p>- Push data about deployments or instances to a Kinesis stream to support comprehensive, real-time status monitoring.</p><p>- Use CloudWatch alarm actions to automatically stop, terminate, reboot, or recover Amazon EC2 instances when a deployment or instance event you specify occurs.</p><p>Hence, the correct answer is:<strong> Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team's Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when a deployment fails</strong></code><strong> setting.</strong></p><p>The option that says: <strong>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when alarm thresholds are met</strong></code><strong> setting</strong> is incorrect because CloudWatch Alarm can't directly send a message to a Slack Channel. You have to use an EventBridge with an associated Lambda function to notify the DevOps Team via Slack.</p><p>The option that says:<strong> Configure a CodeDeploy agent to send a notification to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful</strong> is incorrect because a CodeDeploy agent is primarily used for deployment and not for sending custom messages to non-AWS resources such as a Slack Channel.</p><p>The option that says: <strong>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team's Slack Channel when the </strong><code><strong>PutLifecycleEventHookExecutionStatus</strong></code><strong> API call has been detected. Rollback the changes by using the AWS CLI</strong> is incorrect because this API simply sets the result of a Lambda validation function. This is not a suitable solution since invoking various API calls is not necessary at all. You simply have to integrate an EventBridge rule with an associated Lambda function to your CodeDeploy project in order to meet the specified requirement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 115961513,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>",
          "answers": [
            "<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>",
            "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>",
            "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>",
            "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"
          ],
          "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 138248145,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A popular e-commerce website that has customers across the globe is hosted in the us-east-1 AWS region with a backup site in the us-west-1 region. Due to an unexpected regional outage in the us-east-1 region, the company initiated its disaster recovery plan and turned on the backup site. However, the company discovered that the actual failover still entails several hours of manual effort to prepare and switch over the database. The company also noticed the database missing up to three hours of data transactions during the regional outage.</p><p>Which of the following solutions should the DevOps engineer implement to improve the RTO and RPO of the website for the cross-region failover?</p>",
          "answers": [
            "<p>Configure an Amazon RDS Multi-AZ Deployment configuration and place the standby instance in the us-west-1 region. Set up the RDS option group to enable multi-region availability for native automation of cross-region recovery as well as for continuous data replication. Set up a notification system using Amazon SNS which is integrated with AWS Health API to monitor RDS-related systems events and notify the Operations team. In the actual failover where the primary database instance is down, RDS will automatically make the standby instance in the backup region as the primary instance.</p>",
            "<p>Use Step Functions with 2 Lambda functions that call the RDS API to create a snapshot of the database, create a cross-region snapshot copy, and restore the database instance from a snapshot in the backup region. Use Amazon EventBridge to trigger the function to take a database snapshot every hour. Set up an SNS topic that will receive published messages from AWS Health API, RDS availability, and other events that will trigger the Lambda function to create a cross-region snapshot copy. During failover, configure the Lambda function to restore the database from a snapshot in the backup region.</p>",
            "<p>Create a snapshot every hour using Amazon RDS scheduled instance lifecycle events which will also allow you to monitor specific RDS events. Perform a cross-region snapshot copy into the us-west-1 backup region once the <code>SnapshotCreateComplete</code> event occurred. Create an Amazon CloudWatch Alert which will trigger an action to restore the Amazon RDS database snapshot in the backup region when the CPU Utilization metric of the RDS instance in CloudWatch falls to 0% for more than 15 minutes.</p>",
            "<p>Use an ECS cluster to host a custom python script that calls the RDS API to create a snapshot of the database, create a cross-region snapshot copy, and restore the database instance from a snapshot in the backup region. Create a rule in Amazon EventBridge that triggers the Lambda function to take an hourly snapshot of a database instance. Set up an SNS topic that will receive published messages about AWS-initiated RDS events from Trusted Advisor that will trigger the function to create a cross-region snapshot copy. During failover, restore the database from a snapshot in the backup region.</p>"
          ],
          "explanation": "<p>When you copy a snapshot to an AWS Region that is different from the source snapshot's AWS Region, the first copy is a full snapshot copy, even if you copy an incremental snapshot. A full snapshot copy contains all of the data and metadata required to restore the DB instance. After the first snapshot copy, you can copy incremental snapshots of the same DB instance to the same destination region within the same AWS account.</p><p><img src=\"https://media.tutorialsdojo.com/public/DBSnapshotCopy1.png\"></p><p>An incremental snapshot contains only the data that has changed after the most recent snapshot of the same DB instance. Incremental snapshot copying is faster and results in lower storage costs than full snapshot copying. Incremental snapshot copying across AWS Regions is supported for both unencrypted and encrypted snapshots. For shared snapshots, copying incremental snapshots is not supported. For shared snapshots, all of the copies are full snapshots, even within the same region.</p><p>Depending on the AWS Regions involved and the amount of data to be copied, a cross-region snapshot copy can take hours to complete. In some cases, there might be a large number of cross-region snapshot copy requests from a given source AWS Region. In these cases, Amazon RDS might put new cross-region copy requests from that source AWS Region into a queue until some in-progress copies complete. No progress information is displayed about copy requests while they are in the queue. Progress information is displayed when the copy starts.</p><p>Take note that when you copy a source snapshot that is a snapshot copy, the copy isn't incremental because the snapshot copy doesn't include the required metadata for incremental copies.</p><p>Hence, the correct solution is:<strong> Use Step Functions with 2 Lambda functions that call the RDS API to create a snapshot of the database, create a cross-region snapshot copy, and restore the database instance from a snapshot in the backup region. Use Amazon EventBridge to trigger the function to take a database snapshot every hour. Set up an SNS topic that will receive published messages from AWS Health API, RDS availability, and other events that will trigger the Lambda function to create a cross-region snapshot copy. During failover, configure the Lambda function to restore the database from a snapshot in the backup region.</strong></p><p>The option that says: <strong>Configure an Amazon RDS Multi-AZ Deployment configuration and place the standby instance in the us-west-1 region. Set up the RDS option group to enable multi-region availability for native automation of cross-region recovery as well as for continuous data replication. Set up a notification system using Amazon SNS which is integrated with AWS Health API to monitor RDS-related systems events and notify the Operations team. In the actual failover where the primary database instance is down, RDS will automatically make the standby instance in the backup region as the primary instance </strong>is incorrect because the standby instance of an Amazon RDS Multi-AZ database can only be placed in the same AWS region where the primary instance is hosted. Thus, you cannot failover to the standby instance as your replacement for your primary instance in another region. A better solution would be to set up a cross-region snapshot copy from the primary to the backup region. Another solution would be to use Read Replicas since these can be placed in another AWS region.</p><p>The option that says: <strong>Create a snapshot every hour using Amazon RDS scheduled instance lifecycle events which will also allow you to monitor specific RDS events. Perform a cross-region snapshot copy into the us-west-1 backup region once the </strong><code><strong>SnapshotCreateComplete</strong></code><strong> event occurred. Create an Amazon CloudWatch Alert which will trigger an action to restore the Amazon RDS database snapshot in the backup region when the CPU Utilization metric of the RDS instance in CloudWatch falls to 0% for more than 15 minutes </strong>is incorrect because you cannot create a snapshot using the Amazon RDS scheduled instance lifecycle events.</p><p>The option that says: <strong>Use an ECS cluster to host a custom python script that calls the RDS API to create a snapshot of the database, create a cross-region snapshot copy, and restore the database instance from a snapshot in the backup region. Create a scheduled rule using Amazon EventBridge that triggers the Lambda function to snapshot a database instance every hour. Set up an SNS topic that will receive published messages about AWS-initiated RDS events from Trusted Advisor that will trigger the function to create a cross-region snapshot copy. During failover, restore the database from a snapshot in the backup region </strong>is incorrect because the AWS Trusted Advisor doesn't provide any information regarding AWS-initiated RDS events. You should use the AWS Health API instead. Moreover, providing an ECS cluster is unnecessary just to host a custom Python program when you can simply use Lambda functions instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopySnapshot.AcrossRegions</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p>"
        }
      },
      {
        "id": 99528221,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company manages several legacy applications that all generate different log formats. The logs need to be standardized so they can be queried and analyzed. A DevOps engineer needs a solution for standardizing the log formats before writing them to<br>an Amazon S3 bucket.</p><p>How can this requirement be met at the LOWEST cost?</p>",
          "answers": [
            "<p>Configure the application to send its logs to an Amazon OpenSearch cluster and use Lambda to normalize the logs and export to S3.</p>",
            "<p>Configure the application to send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight.</p>",
            "<p>Configure the application to send the log files to an Amazon S3 bucket and use Amazon Redshift Spectrum to normalize the logs in place.</p>",
            "<p>Configure the Amazon Kinesis Agent to upload the logs to Amazon Kinesis Data Firehose and use an AWS Lambda function to normalize the log files before they are loaded to Amazon S3.</p>"
          ],
          "explanation": "<p>Kinesis Data Firehose can invoke an AWS Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Kinesis Data Firehose data transformation when you create your delivery stream.</p><p>Each server can run the Kinesis agent and upload logs to the Kinesis Data Firehose delivery stream. Then the Lambda function will normalize the log files before they are loaded to the S3 bucket.</p><p><strong>CORRECT: </strong>\"Configure the Amazon Kinesis Agent to upload the logs to Amazon Kinesis Data Firehose and use an AWS Lambda function to normalize the log files before they are loaded to Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the application to send its logs to an Amazon OpenSearch cluster and use Lambda to normalize the logs and export to S3\" is incorrect.</p><p>The Amazon OpenSearch service is not a suitable solution for this scenario as it is a service that is used for searching and analyzing data sets. In this case the data simply needs to be transformed (normalized) before loading it to S3 so there is no need to involve OpenSearch.</p><p><strong>INCORRECT:</strong> \"Configure the application to send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight\" is incorrect.</p><p>QuickSight is used for analysis but in this scenario we simply need a solution for normalizing the log files before loading to S3.</p><p><strong>INCORRECT:</strong> \"Configure the application to send the log files to an Amazon S3 bucket and use Amazon Redshift Spectrum to normalize the logs in place\" is incorrect.</p><p>To use RedShift Spectrum you must have a RedShift cluster in place and these run-on Amazon EC2 instances. Therefore, this solution is unlikely to be the lowest cost option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html\">https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>"
        }
      },
      {
        "id": 134588379,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A developer is developing a mobile quiz app hosted on AWS and is using AWS CodeDeploy to deploy the application on a cluster of EC2 instances. A hotfix needs to be applied after the scheduled deployment. These are files not included in the current application revision, so the developer uploaded the files manually to the target instances. On the next application release, the hotfix files were included in the new revision, but the deployment fails, and CodeDeploy auto-rolls back to the previous version. However, it was noticed that the hotfix files that were manually added were missing, and the application was not working properly.</p><p>Which of the following is the possible cause, and how will it be prevented in the future?</p>",
          "answers": [
            "<p>By default, CodeDeploy removes all files on the deployment location and the auto rollback will deploy the old revision files cleanly. Choose \u201cOverwrite the content\u201d option for future deployments so that only the files included in the old app revision will be overwritten and the existing contents will be retained.</p>",
            "<p>By default, CodeDeploy retains all files on the deployment location but the auto rollback will deploy the old revision files cleanly. Choose \u201cOverwrite the content\u201d option for future deployments so that only the files included in the old app revision will be overwritten and the existing contents will be retained.</p>",
            "<p>By default, CodeDeploy removes all files on the deployment location and the auto rollback will deploy the old revision files cleanly. Choose \u201cRetain the content\u201d option for future deployments so that only the files included in the old app revision will be deployed and the existing contents will be retained.</p>",
            "<p>By default, CodeDeploy retains all files on the deployment location but the auto rollback will deploy the old revision files cleanly. Choose \u201cFail the deployment\u201d option for future deployments so that only the files included in the old app revision will be deployed and the existing contents will be retained.</p>"
          ],
          "explanation": "<p>As part of the deployment process, the CodeDeploy agent removes from each instance all the files installed by the most recent deployment. If files that weren\u2019t part of a previous deployment appear in target deployment locations, you can choose what CodeDeploy does with them during the next deployment:</p><p><strong>Fail the deployment</strong> \u2014 An error is reported and the deployment status is changed to Failed.</p><p><strong>Overwrite the content</strong> \u2014 The version of the file from the application revision replaces the version already on the instance.</p><p><strong>Retain the content</strong> \u2014 The file in the target location is kept and the version in the application revision is not copied to the instance.</p><p><img src=\"https://media.tutorialsdojo.com/public/Deployment_2AUG2023.png\"></p><p>Hence, the correct answer is: <strong>By default, CodeDeploy removes all files on the deployment location and the auto rollback will deploy the old revision files cleanly. Choose \u201cRetain the content\u201d option for future deployments so that only the files included in the old app revision will be deployed and the existing contents will be retained.</strong></p><p>The option that says: <strong>By default, CodeDeploy removes all files on the deployment location and the auto rollback will deploy the old revision files cleanly. Choose \u201cOverwrite the content\u201d option for future deployments so that only the files included in the old app revision will be overwritten and the existing contents will be retained</strong> is incorrect. Since you want to retain the already existing hotfix files, you should use the \u201cRetain the content\u201d option instead.</p><p>The option that says: <strong>By default, CodeDeploy retains all files on the deployment location but the auto rollback will deploy the old revision files cleanly. Choose \u201cOverwrite the content\u201d option for future deployments so that only the files included in the old app revision will be overwritten and the existing contents will be retained</strong> is incorrect. The \u201cOverwrite content\u201d option will simply remove the files and not retain them. As part of the deployment process, the CodeDeploy agent removes from each instance all the files installed by the most recent deployment and does not retain them. Moreover, the \u201cRetain the content\u201d option is a more suitable option to choose in this scenario for future deployments.</p><p>The option that says: <strong>By default, CodeDeploy retains all files on the deployment location but the auto rollback will deploy the old revision files cleanly. Choose \"Fail the deployment\" option for future deployments so that only the files included in the old app revision will be deployed and the existing contents will be retained</strong> is incorrect because, as part of the deployment process, the CodeDeploy agent will just remove the file from each instance installed by the most recent deployment and not retain them. You should also choose the \u201cRetain the content\u201d option for future deployments.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#deployments-rollback-and-redeploy-content-options\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#deployments-rollback-and-redeploy-content-options</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 82921374,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.</p>\n\n<p>How can you improve the user experience with the least effort?</p>\n",
          "answers": [
            "<p>Enhance the health check so that the return status code corresponds to the connectivity to the database</p>",
            "<p>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</p>",
            "<p>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</p>",
            "<p>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enhance the health check so that the return status code corresponds to the connectivity to the database</strong></p>\n\n<p>Configuring health checks for the Application Load Balancer (ALB) is an important step to ensure that your AWS Cloud application runs smoothly. The ALB Health Check is configured with a protocol and port number to call on the target instances. A healthy EC2 instance is one that issues a response to a health check call with an HTTP 200 response code. Instances that return a status code that is other than the 2XX range or which time out are designated as being unhealthy and will not receive traffic from the ELB.</p>\n\n<p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q69-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>You could just add a simple health check endpoint to the ALB which accepts a request and immediately responds with an HTTP status of 200. This approach provides for a fast health check, but would not meet the requirement for the given use-case. You need to improve the quality of the health check and make sure it returns a proper status code. As the application depends on the database, you need to ensure that you include health checks for these components when determining the health of your service.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</strong> - Migrating to Beanstalk would require significant effort and even then it won't help gather detailed database-specific health checks.</p>\n\n<p><strong>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</strong> - Health Checks for the ALB are pretty basic and only work with the HTTP return status code, not the payload itself.</p>\n\n<p><strong>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</strong> - Route53 health checks can only be used to prevent DNS records from being returned from a DNS query, so it won't help for routing to specific instances behind an ALB (that's why we have health checks at the ALB level).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf\">https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf</a></p>\n"
        }
      },
      {
        "id": 134588503,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company has confidential files containing patent information stored in an Amazon S3 bucket in the US East (Ohio) region. Among the team members working on the project, actions need to be monitored on the objects, such as PUT, GET, and DELETE operations. The goal is to search and review these actions for auditing purposes easily.</p><p>Which solution will meet this requirement MOST cost-effectively?</p>",
          "answers": [
            "<p>Enable logging on your Amazon S3 bucket to save the access logs on a separate S3 bucket. Import logs to Amazon OpenSearch service to easily search and query the logs.</p>",
            "<p>Enable logging on your S3 bucket. Create an Amazon EventBridge rule that watches the object-level events of your S3 bucket. Create a target on the rule to send the event logs to an Amazon CloudWatch Log group. View and search the logs on CloudWatch Logs.</p>",
            "<p>Create an AWS CloudTrail trail to track and store your S3 API call logs in an S3 bucket. Create an AWS Lambda function that logs data events of your S3 bucket. Trigger this Lambda function using the Amazon EventBridge rule for every action taken on your S3 objects. View the logs on the Amazon CloudWatch Logs group.</p>",
            "<p>Create an Amazon CloudWatch Log group and configure S3 logging to send object-level events to this log group. View and search the event logs on the created CloudWatch log group.</p>"
          ],
          "explanation": "<p>When an event occurs in your account, CloudTrail evaluates whether the event matches the settings for your trails. Only events that match your trail settings are delivered to your Amazon S3 bucket and Amazon CloudWatch Logs log group.</p><p>You can configure your trails to log the following:</p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#logging-data-events\"><strong>Data events</strong></a>: These events provide insight into the resource operations performed on or within a resource. These are also known as data plane operations.</p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#logging-management-events\"><strong>Management events</strong></a>: Management events provide insight into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Management events can also include non-API events that occur in your account. For example, when a user logs in to your account, CloudTrail logs the <code>ConsoleLogin</code> event.</p><p>You can configure multiple trails differently so that the trails process and log only the events that you specify. For example, one trail can log read-only data and management events so that all read-only events are delivered to one S3 bucket. Another trail can log only write-only data and management events, so that all write-only events are delivered to a separate S3 bucket.</p><p>You can also configure your trails to have one trail log and deliver all management events to one S3 bucket, and configure another trail to log and deliver all data events to another S3 bucket.</p><p><img alt=\"AWS CloudTrail\" src=\"https://media.tutorialsdojo.com/public/td-aws-cloudtrail-13Feb2025.jpg\" width=\"1000\"></p><p>Data events provide insight into the resource operations performed on or within a resource. These are also known as data plane operations. Data events are often high-volume activities.</p><p>Example data events include:</p><p>- Amazon S3 object-level API activity (for example, <code>GetObject</code>, <code>DeleteObject</code>, and <code>PutObject</code> API operations)</p><p>- AWS Lambda function execution activity (the <code>Invoke</code> API)</p><p>Data events are disabled by default when you create a trail. To record CloudTrail data events, you must explicitly add the supported resources or resource types for which you want to collect activity to a trail.</p><p>You can log the object-level API operations on your S3 buckets. Before Amazon EventBridge can match these events, you must use AWS CloudTrail to set up a trail configured to receive these events. To log data events for an S3 bucket to AWS CloudTrail and EventBridge, create a trail. A trail captures API calls and related events in your account and delivers the log files to an S3 bucket that you specify. After you create a trail and configure it to capture the log files you want, you need to be able to find the log files and interpret the information they contain.</p><p>Typically, log files appear in your bucket within 15 minutes of the recorded AWS API call or other AWS event. Then you need to create a Lambda function to log data events for your S3 bucket. Finally, you need to create a trigger to run your Lambda function in response to an Amazon S3 data event. You can create this rule on Amazon EventBridge and setting Lambda function as the target. Your logs will show up on the CloudWatch log group, which you can view and search as needed.</p><p>Hence, the correct answer is: <strong>Create an AWS CloudTrail trail to track and store your S3 API call logs in an S3 bucket. Create an AWS Lambda function that logs data events of your S3 bucket. Trigger this Lambda function using the Amazon EventBridge rule for every action taken on your S3 objects. View the logs on the Amazon CloudWatch Logs group.</strong></p><p>The option that says: <strong>Enable logging on your Amazon S3 bucket to save the access logs on a separate S3 bucket. Import logs to Amazon OpenSearch service to easily search and query the logs</strong> is incorrect because this would simply incur more cost if you provision an OpenSearch cluster. Take note that the scenario asks for the most cost-effective solution.</p><p>The option that says: <strong>Enable logging on your S3 bucket. Create an Amazon EventBridge rule that watches the object-level events of your S3 bucket. Create a target on the rule to send the event logs to an Amazon CloudWatch Log group. View and search the logs on CloudWatch Logs</strong> is incorrect because before, Amazon EventBridge could match API call events. You must use AWS CloudTrail first to set up a trail configured to receive these events.</p><p>The option that says: <strong>Create an Amazon CloudWatch Log group and configure S3 logging to send object-level events to this log group. View and search the event logs on the created CloudWatch log group</strong> is incorrect because you can't typically configure an S3 bucket to directly send access logs to a CloudWatch log group.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/cloudtrail-update-capture-and-process-amazon-s3-object-level-api-activity/\">https://aws.amazon.com/blogs/aws/cloudtrail-update-capture-and-process-amazon-s3-object-level-api-activity/</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#example-logging-all-S3-objects\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-and-data-events-with-cloudtrail.html#example-logging-all-S3-objects</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p>"
        }
      },
      {
        "id": 138248189,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company is planning to migrate its online customer portal to AWS. It should be hosted in AWS Elastic Beanstalk and use Amazon RDS MySQL in Multi-AZ configuration for its database. A DevOps Engineer was instructed to ensure that the application resources must be at full capacity during deployment by using a new group of instances. The solution should also include a way to roll back the change easily and prevent issues caused by partially completed rolling deployments. The application performance should not be affected while a new version of the app is being deployed. </p><p>Which is the MOST cost-effective deployment set up that the DevOps Engineer should implement to meet these requirements?</p>",
          "answers": [
            "<p>Host the online customer portal using AWS Elastic Beanstalk and integrate it to an external Amazon RDS MySQL database in Multi-AZ deployments configuration. Configure the Elastic Beanstalk to use blue/green deployment for releasing the new application version to a new environment. Swap the CNAME in the two environments to redirect traffic to the new version using the <code>Swap Environment URLs</code> feature. Once the deployment has been successfully implemented, keep the old environment running as a backup.</p>",
            "<p>Host the online customer portal using AWS Elastic Beanstalk coupled with an Amazon RDS MySQL database. In the Elastic Beanstalk database configuration, set the <code>Availability</code> option to <code>High (Multi-AZ)</code> to run a warm backup in a second Availability Zone. Use the <code>All at once</code> deployment policy to release the new application version.</p>",
            "<p>Host the online customer portal using AWS Elastic Beanstalk coupled with Amazon RDS MySQL database as part of the environment. For high availability, set the <code>Availability</code> option to <code>High (Multi-AZ)</code> in the Elastic Beanstalk database configuration to run a warm backup in a second Availability Zone. Use the <code>Rolling with additional batch</code> policy for application deployments.</p>",
            "<p>Host the online customer portal using AWS Elastic Beanstalk and integrate it to an external Amazon RDS MySQL database in Multi-AZ deployments configuration. Use immutable updates for application deployments.</p>"
          ],
          "explanation": "<p><strong>AWS Elastic Beanstalk</strong> provides several options for how <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">deployments</a> are processed, including deployment policies (<strong>All at once</strong>, <strong>Rolling</strong>, <strong>Rolling with additional batch</strong>, and <strong>Immutable</strong>) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's an automatically scaling environment (you didn't specify the <code>--single</code> option), it uses rolling deployments.</p><p>With rolling deployments, Elastic Beanstalk splits the environment's EC2 instances into batches and deploys the new version of the application to one batch at a time, leaving the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p><p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a <em>rolling deployment with an additional batch</em>. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.</p><p><em>Immutable deployments </em>perform an <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">immutable update</a> to launch a full set of new instances running the new version of the application in a separate Auto Scaling group alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-05-28_11-31-43-d1e6e5be5012c77c39ec15676d410483.png\">If your application doesn't pass all health checks, but still operates correctly at a lower health status, you can allow instances to pass health checks with a lower status, such as <code>Warning</code>, by modifying the <em>Healthy threshold </em>option. If your deployments fail because they don't pass health checks and you need to force an update regardless of health status, specify the <em>Ignore health check</em> option.</p><p>When you specify a batch size for rolling updates, Elastic Beanstalk also uses that value for rolling application restarts. Use rolling restarts when you need to restart the proxy and application servers running on your environment's instances without downtime.</p><p>AWS Elastic Beanstalk provides support for running Amazon Relational Database Service (Amazon RDS) instances in your Elastic Beanstalk environment. This works great for development and testing environments. However, it isn't ideal for a production environment because it ties the lifecycle of the database instance to the lifecycle of your application's environment.</p><p>Hence, the correct answer is: <strong>Host the online customer portal using AWS Elastic Beanstalk and integrate it to an external Amazon RDS MySQL database in Multi-AZ deployments configuration. Use immutable updates for application deployments.</strong></p><p>The option that says: <strong>Host the online customer portal using AWS Elastic Beanstalk and integrate it to an external Amazon RDS MySQL database in Multi-AZ deployments configuration. Configure the Elastic Beanstalk to use blue/green deployment for releasing the new application version to a new environment. Swap the CNAME in the two environments to redirect traffic to the new version using the </strong><code><strong>Swap Environment URLs</strong></code><strong> feature. Once the deployment has been successfully implemented, keep the old environment running as a backup</strong> is incorrect. Although using the blue/green deployment configuration is an ideal option, keeping the old environment running is not recommended since it entails a significant cost. Take note that the scenario asks for the most cost-effective solution, which is why the old environment should be deleted.</p><p>The option that says:<strong> Host the online customer portal using AWS Elastic Beanstalk coupled with an Amazon RDS MySQL database. In the Elastic Beanstalk database configuration, set the </strong><code><strong>Availability</strong></code><strong> option to </strong><code><strong>High (Multi-AZ)</strong></code><strong> to run a warm backup in a second Availability Zone. Use the </strong><code><strong>All at once</strong></code><strong> deployment policy to release the new application version</strong> is incorrect because this will deploy the new version to all existing instances and will not create new EC2 instances. Moreover, you should decouple your RDS database from Elastic Beanstalk, as this is tied to the lifecycle of the database instance and to the lifecycle of your application's environment.</p><p>The option that says: <strong>Host the online customer portal using AWS Elastic Beanstalk coupled with Amazon RDS MySQL database as part of the environment. For high availability, set the </strong><code><strong>Availability</strong></code><strong> option to </strong><code><strong>High (Multi-AZ)</strong></code><strong> in the Elastic Beanstalk database configuration to run a warm backup in a second Availability Zone. Use the </strong><code><strong>Rolling with additional batch</strong></code><strong> policy for application deployments </strong>is incorrect because this type of configuration could potentially cause partially completed rolling deployments. The new batch of instances is within the same Auto Scaling group and not in a new one. The rollback process is also cumbersome to implement, unlike the Immutable or Blue/Green deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 67357140,
        "correct_response": [
          "a",
          "d"
        ],
        "prompt": {
          "question": "<p>A media company extensively uses Amazon S3 buckets for storing images files, documents, and other business-specific data. The company has mandated enabling logging for all Amazon S3 buckets. The audit team publishes the reports of all AWS resources failing company security standards. Until recently, the security team would pick the list of noncompliant Amazon S3 buckets from the audit list and execute remediation actions manually for each resource. This process is not only time-consuming but also leaves noncompliant resources vulnerable for a long duration.</p>\n\n<p>Which combination of steps should a DevOps Engineer take to meet these requirements using an automated solution? (Select two)</p>\n",
          "answers": [
            "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></p>",
            "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</p>",
            "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</p>",
            "<p>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</p>",
            "<p>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></strong></p>\n\n<p><strong>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</strong></p>\n\n<p>The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. You can:</p>\n\n<ol>\n<li>Choose the remediation action you want to associate from a pre-populated list.</li>\n<li>Create your own custom remediation actions using AWS Systems Manager Automation documents.</li>\n</ol>\n\n<p>If a resource is still non-compliant after auto-remediation, you can set the rule to try auto-remediation again.</p>\n\n<p>For the above use case: You must have AWS Config enabled in your AWS account. The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config, and that role must have whatever permissions the SSM document requires.</p>\n\n<p>Steps to set up Auto Remediation for s3-bucket-logging-enabled:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q23-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</strong> - While you can create a custom remediation action using SSM, for this particular use case, it is not required since auto-remediation for S3 server logging is already present in the remediation action list of AWS Config rules.</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</strong> - As discussed above, the AWS config remediation action pre-populated list already has an action defined for enabling S3 logging. Hence, custom code is unnecessary for this use case.</p>\n\n<p><strong>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</strong>- While setting up remediation action if you want to pass the resource ID of non-compliant resources to the remediation action, choose the Resource ID parameter. If selected at runtime, the parameter is substituted with the ID of the resource to be remediated. This is not mandatory though.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n"
        }
      },
      {
        "id": 67357184,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A social media company has its web application hosted on Amazon EC2 instances that are deployed in a single AWS Region. The company has now expanded its operations into new geographies and the company wants to offer low-latency access for the application to its customers. To comply with different financial regulations of each geography, the application needs to operate in silos and the underlying instances in one region should not interact with instances running in other regions.</p>\n\n<p>Which of the following represents the most optimal solution to automate the application deployment to different AWS regions?</p>\n",
          "answers": [
            "<p>Create a shell script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions</p>",
            "<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions</p>",
            "<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application</p>",
            "<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation change set from an administrator account to launch stack instances that deploy the application to various other regions</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p>\n\n<p>Here are the relevant CloudFormation concepts:</p>\n\n<p>Template: A CloudFormation template is a JSON or YAML formatted text file. You can save these files with any extension, such as .json, .yaml, .template, or .txt. CloudFormation uses these templates as blueprints for building your AWS resources.</p>\n\n<p>Stack: When you use CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's CloudFormation template.</p>\n\n<p>Change set: If you need to make changes to the running resources in a stack, you update the stack. Before making changes to your resources, you can generate a change set, which is a summary of your proposed changes. Change sets allow you to see how your changes might impact your running resources, especially critical resources, before implementing them.</p>\n\n<p>Stack set: A stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template. All the resources included in each stack are defined by the stack set's CloudFormation template. As you create the stack set, you specify the template to use, in addition to any parameters and capabilities that the template requires.</p>\n\n<p>After you've defined a stack set, you can create, update, or delete stacks in the target accounts and AWS Regions you specify. When you create, update, or delete stacks, you can also specify operation preferences, such as the order of Regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. A stack set is a regional resource. If you create a stack set in one AWS Region, you can't see it or change it in other Regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p>For the given use case, you need to create a stack set that includes the CloudFormation template that you want to use to create stacks and then identify the AWS Regions in which you want to deploy stacks in your target accounts. A stack set ensures consistent deployment of the same stack resources, with the same settings, to all specified target accounts within the Regions you choose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application</strong> - You can create a stack using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. The <code>aws cloudformation create-stack</code> command supports only a <code>region</code> parameter for a single region and not <code>regions</code> parameter for multiple regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/</a></p>\n\n<p><strong>Create a shell script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions</strong> - Creating a script is not an elegant solution to provision the same infrastructure in multiple AWS regions since this functionality is directly offered by AWS in the form of stack sets.</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation change set from an administrator account to launch stack instances that deploy the application to various other regions</strong> - When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. You cannot use change sets to deploy an application to multiple AWS regions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n"
        }
      },
      {
        "id": 82921446,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A data intelligence and analytics company enables publishers to measure, analyze, and improve the impact of the advertising across their range of online deliverables. The DevOps team at the company wants to use CodePipeline to deploy code from CodeCommit with CodeDeploy. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How would you configure the EC2 instances to facilitate the deployment?</p>\n",
          "answers": [
            "<p>Create an EC2 instance with an IAM role giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</p>",
            "<p>Create an EC2 instance with an IAM user access credentials giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</p>",
            "<p>Create an EC2 instance with an IAM role giving access to the CodeCommit repository where CodeDeploy is deploying from. CodeDeploy will install the agent on the EC2 instance</p>",
            "<p>Create an EC2 instance with an IAM user access credentials giving access to the CodeCommit repository where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an EC2 instance with an IAM role giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</strong></p>\n\n<p>AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>CodeDeploy Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q8-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q8-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/codedeploy/faqs/\">https://aws.amazon.com/codedeploy/faqs/</a></p>\n\n<p>The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. A configuration file is placed on the instance when the agent is installed. This file is used to specify how the agent works. This configuration file specifies directory paths and other settings for AWS CodeDeploy to use as it interacts with the instance.</p>\n\n<p>For the given use-case, you can have the CodePipeline chain CodeCommit and CodeDeploy and have the source code available as a zip file in an S3 bucket to be used as a CodePipeline artifact. The EC2 instance must have an IAM role, and not an IAM user, to pull that file from S3. Finally, the EC2 instance must be properly tagged to be part of the correct deployment group and have the CodeDeploy agent installed on it.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an EC2 instance with an IAM user access credentials giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</strong> - It's a best practice to avoid using the IAM user access credentials to give the EC2 instance access to the S3 bucket where CodeDeploy is deploying from. You must leverage an IAM role to facilitate this access for the EC2 instance.</p>\n\n<p><strong>Create an EC2 instance with an IAM role giving access to the CodeCommit repository where CodeDeploy is deploying from. CodeDeploy will install the agent on the EC2 instance</strong> - CodeDeploy cannot automatically install the agent on the EC2 instance. You must ensure that the EC2 instance has the CodeDeploy agent installed. You must also tag the instance to have it part of a deployment group.</p>\n\n<p><strong>Create an EC2 instance with an IAM user access credentials giving access to the CodeCommit repository where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</strong> - It's a best practice to avoid using the IAM user access credentials to give the EC2 instance access to the S3 bucket where CodeDeploy is deploying from. You must leverage an IAM role to facilitate this access for the EC2 instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/primary-components.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/primary-components.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-configure.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-configure.html</a></p>\n"
        }
      },
      {
        "id": 143860775,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A financial organization is already utilizing a CI/CD pipeline to deploy applications in production. A DevOps team has automated the entire upgrade flow for a database upgrade process.</p><p>During deployment, they triggered a CI/CD pipeline with no human intervention, and the upgrade progressed smoothly. Then, 20 minutes in, the pipeline became stuck, and the update halted. It was discovered that a planned outage in an AWS region caused the pipeline to fail.</p><p>What solution can a DevOps engineer implement to prevent this from happening again without causing unnecessary delay or cost?</p>",
          "answers": [
            "<p>Schedule all database upgrades to avoid any planned outages from AWS in the relevant AWS Regions.</p>",
            "<p>Embed AWS Health API insights into the CI/CD pipelines using AWS Lambda functions to automatically stop deployments when an AWS Health event is reported in the Region.</p>",
            "<p>Utilize an additional instance of database and apply upgrades on passive database. When the upgrade is complete, route traffic to the passive database and perform a switch between database instances.</p>",
            "<p>Create an AWS CodePipeline pipeline stage to retrigger the pipeline and rerun the CI/CD process.</p>"
          ],
          "explanation": "<p>In AWS CodePipeline, you can create a new stage with a single action to asynchronously invoke a Lambda function. The function will call AWS Health <a href=\"https://docs.aws.amazon.com/health/latest/APIReference/API_DescribeEvents.html\">DescribeEvents API</a> to retrieve the list of active health incidents. Then, the function will complete the event analysis and decide whether it may impact the running deployment.</p><p>Finally, the function will call back CodePipeline with the evaluation results through either <a href=\"https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutJobSuccessResult.html\">PutJobSuccessResult</a> or <a href=\"https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutJobFailureResult.html\">PutJobFailureResult</a> API operations.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-20-46-825daf1581271e9aa301dbc5540619c9.jpg\"><p><strong>CORRECT: </strong>\"Embed AWS Health API insights into the CI/CD pipelines using AWS Lambda functions to automatically stop deployments when an AWS Health event is reported in the Region\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CodePipeline pipeline stage to retrigger the pipeline and rerun the CI/CD process\" is incorrect.</p><p>The issue with this option is that in case AWS health event is still there, the deployment might fail again.</p><p><strong>INCORRECT:</strong> \"Schedule all database upgrades to avoid any planned outages from AWS in the relevant AWS Regions\" is incorrect.</p><p>This is a manual step which will work but can unnecessarily delay the upgrade in case of business critical/ compliance upgrades.</p><p><strong>INCORRECT:</strong> \"Utilize an additional instance of database and apply upgrades on passive database. When the upgrade is complete, route traffic to the passive database and perform a switch between database instances\" is incorrect.</p><p>This is a high-cost option and above explained scenario is a better suited one.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/build-health-aware-ci-cd-pipelines/\">https://aws.amazon.com/blogs/devops/build-health-aware-ci-cd-pipelines/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949070,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>An application is deployed on Amazon EC2 instances in an Auto Scaling group. The application includes a process that sometimes fails causing the application to return an error. Restarting the process resolves the issue. The DevOps team are investigating the issue. While the investigation is ongoing an engineer needs a method of identifying the issue and quickly remediating it without affecting the capacity of the Auto Scaling group.</p><p>Which approach meets this requirement in the fastest possible time?</p>",
          "answers": [
            "<p>Run a regular cron job to execute commands and check if the process is running. If the process returns an error, run the AWS CLI set-instance-health command to set the health state of the specified instance to Unhealthy.</p>",
            "<p>Use Amazon EventBridge to run an AWS Lambda function that checks if the process is running. If the process returns an error, run the AWS CLI set-instance-health command to set the health state of the specified instance to Unhealthy.</p>",
            "<p>Install the Amazon CloudWatch agent on the EC2 instances and use the procstat plugin to monitor the application process and send metrics to CloudWatch. Use Systems Manager Run Command to restart any processes that are failed.</p>",
            "<p>Use Amazon EventBridge to run an event every 10 minutes. Configure the event to run an AWS Lambda function that takes instances out of service one-by-one, restarts them, and then places them back in service.</p>"
          ],
          "explanation": "<p>The Amazon CloudWatch agent procstat plugin continuously watches specified processes and reports their metrics to Amazon CloudWatch. After the data is in Amazon CloudWatch, you can associate alarms to trigger actions like notifying teams or remediations like restarting the processes, resizing the instances, and so on. In this case AWS Systems Manager Run Command can then be used to restart any processes that are failed.</p><p>This solution will resolve the issue in the fastest time as metrics can be reported on a 1-minute timeframe and restarting the process is much quicker than restarting the instance or terminating it and launching a replacement. It also does not affect the capacity of the Auto Scaling group.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the EC2 instances and use the procstat plugin to monitor the application process and send metrics to CloudWatch. Use Systems Manager Run Command to restart any processes that are failed\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Run a regular cron job to execute commands and check if the process is running. If the process returns an error, run the AWS CLI set-instance-health command to set the health state of the specified instance to Unhealthy\" is incorrect.</p><p>This will cause the instance to be terminated and a new instance will be launched. This could be slower than creating an automated process to restart the affected process. This also affects the capacity of the Auto Scaling group while the new instances are launched and brought into service.</p><p><strong>INCORRECT:</strong> \"Use Amazon EventBridge to run an AWS Lambda function that checks if the process is running. If the process returns an error, run the AWS CLI set-instance-health command to set the health state of the specified instance to Unhealthy\" is incorrect.</p><p>This is like the previous answer and has the same issues though it is potentially better as it uses AWS services to automate the process.</p><p><strong>INCORRECT:</strong> \"Use Amazon EventBridge to run an event every 10 minutes. Configure the event to run an AWS Lambda function that takes instances out of service one-by-one, restarts them, and then places them back in service\" is incorrect.</p><p>This may be the slowest of the options presented in terms of identification of the issue and it would be better to restart the process rather than restarting the instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/detecting-remediating-process-issues-on-ec2-instances-using-amazon-cloudwatch-aws-systems-manager/\">https://aws.amazon.com/blogs/mt/detecting-remediating-process-issues-on-ec2-instances-using-amazon-cloudwatch-aws-systems-manager/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      }
    ],
    "answers": {
      "67357140": [
        "a",
        "d"
      ],
      "67357142": [
        "b"
      ],
      "67357184": [
        "b"
      ],
      "75949070": [
        "c"
      ],
      "75949136": [
        "c",
        "e"
      ],
      "75949146": [
        "b"
      ],
      "82921374": [
        "a"
      ],
      "82921446": [
        "a"
      ],
      "99528221": [
        "d"
      ],
      "115961511": [
        "d"
      ],
      "115961513": [
        "b"
      ],
      "134588379": [
        "a"
      ],
      "134588503": [
        "b"
      ],
      "138248145": [
        "a"
      ],
      "138248189": [
        "a"
      ],
      "138248207": [
        "a"
      ],
      "138248235": [
        "c"
      ],
      "143860755": [
        "d"
      ],
      "143860773": [
        "b",
        "d"
      ],
      "143860775": [
        "b"
      ]
    }
  },
  {
    "id": "1770112760759",
    "date": "2026-02-03T09:59:20.759Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 19,
    "incorrect": 1,
    "unanswered": 0,
    "total": 20,
    "percent": 95,
    "duration": 12022877,
    "questions": [
      {
        "id": 82921344,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An industrial appliances company would like to take advantage of AWS Systems Manager to manage their on-premise instances and their EC2 instances. This will allow them to run some SSM RunCommand across their hybrid fleet. The company would also like to effectively manage the size of the fleet. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>How would you set up the on-premise server to achieve this objective?</p>\n",
          "answers": [
            "<p>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>",
            "<p>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>",
            "<p>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>",
            "<p>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p>AWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. With Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status.</p>\n\n<p>How Systems Manager Works:\n<img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\">\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants AssumeRole trust to the Systems Manager service. You only need to create a service role for a hybrid environment once for each AWS account.</p>\n\n<p>To set up servers and virtual machines (VMs) in your hybrid environment as managed instances, you need to create a managed-instance activation. After you complete the activation, you immediately receive an Activation Code and Activation ID. You specify this Code/ID combination when you install SSM agents on servers and VMs in your hybrid environment. The Code/ID provides secure access to the Systems Manager service from your managed instances.</p>\n\n<p>In the Instance limit field, specify the total number of on-premises servers or VMs that you want to register with AWS as part of the activation. This means you don't need to create a unique activation Code/ID for each managed instance.</p>\n\n<p>After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as managed instances. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix \"mi-\". Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong> - As mentioned in the explanation earlier, the on-premise instances use the prefix \"mi-\" whereas the Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><strong>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p><strong>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong></p>\n\n<p>Both these options suggest using the Access Key ID and the Secret Access Key ID to register your on-premise servers which is considered a bad practice from a security perspective. Instead, you should use an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html</a></p>\n\n<p><a href=\"#\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p>\n"
        }
      },
      {
        "id": 75949132,
        "correct_response": [
          "b",
          "c"
        ],
        "prompt": {
          "question": "<p>Several applications will be deployed using AWS CloudFormation. The applications must be deployed into multiple accounts the company manages. Multiple administrator accounts must be granted permissions to create and manage the CloudFormation stacks and operational overhead should be minimized.</p><p>Which actions meet these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Create an AWS Organization with consolidated billing enabled and all the accounts to the organization.</p>",
            "<p>Create an AWS Organization with all features enabled and add all the accounts to the organization.</p>",
            "<p>Enable trusted access with AWS Organizations and create CloudFormation stack sets using the management account.</p>",
            "<p>Create CloudFormation stacks in each account using cross-account IAM roles with permissions in each account.</p>",
            "<p>Enable trusted access with AWS Organizations and create CloudFormation stack sets using self-managed permissions.</p>"
          ],
          "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf.</p><p>In this case it is better to use AWS Organizations and service-managed permissions as this will reduce operational overhead and meet the requirements. Administrators can be granted permissions to CloudFormation to create and update stacks and the stacks will be deployed using service accounts.</p><p><strong>CORRECT: </strong>\"Create an AWS Organization with all features enabled and add all the accounts to the organization\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Enable trusted access with AWS Organizations and create CloudFormation stack sets using the management account\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with consolidated billing enabled and all the accounts to the organization\" is incorrect.</p><p>All features must be enabled to that trusted access can be used to integrate CloudFormation.</p><p><strong>INCORRECT:</strong> \"Create CloudFormation stacks in each account using cross-account IAM roles with permissions in each account\" is incorrect.</p><p>This requires more operational effort. Better to use AWS Organizations with trusted access and service-managed permissions rather than self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Enable trusted access with AWS Organizations and create CloudFormation stack sets using self-managed permissions\" is incorrect.</p><p>When using AWS Organizations and trusted access, service-managed permissions are used rather than self-managed permissions. This is the main advantage of enabling trusted access.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-orgs-enable-trusted-access.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-orgs-enable-trusted-access.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 115961507,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps manager has been asked to optimize the costs associated with Amazon EBS volumes. There are many unattached EBS volumes which should be deleted if not used for 14 days. The manager asked a DevOps engineer to create an automation solution that deletes volumes that have been unattached for 14 days or more.</p><p>Which solution will accomplish this?</p>",
          "answers": [
            "<p>Use the AWS Config ec2-volume-inuse-check managed rule to mark unattached volumes are non-compliant. Create a new Amazon CloudWatch Events rule scheduled to invoke an AWS Lambda function in 14 days to delete the non-compliant volumes.</p>",
            "<p>Use Amazon EC2 and Amazon Data Lifecycle Manager to configure a volume lifecycle policy. Set the interval period for unattached EBS volumes to 14 days and set the retention rule to delete. Set the policy target volumes as *.</p>",
            "<p>Create an Amazon CloudWatch Events rule to invoke an AWS Lambda function daily. Configure the function to find unattached EBS volumes and tag them with the current date and delete unattached volumes that have tags with dates that are more than 14 days old.</p>",
            "<p>Use AWS Trusted Advisor to detect EBS volumes that have been detached for more than 14 days. Invoke an AWS Lambda function that creates a snapshot and then deletes the EBS volume.</p>"
          ],
          "explanation": "<p>CloudWatch Events can be configured with a rule to run an AWS Lambda function on a schedule. The function can be written to find unattached volumes and tag them with the current date. It should avoid tagging volumes that are already tagged and it should delete volumes which have dates older than 14 days. This meets the requirements of the question.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch Events rule to invoke an AWS Lambda function daily. Configure the function to find unattached EBS volumes and tag them with the current date and delete unattached volumes that have tags with dates that are more than 14 days old\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Trusted Advisor to detect EBS volumes that have been detached for more than 14 days. Invoke an AWS Lambda function that creates a snapshot and then deletes the EBS volume\" is incorrect.</p><p>Trusted Advisor checks for underutilized volumes but the criteria is that a volume is unattached or had less than 1 IOPS per day for the past 7 days.</p><p><strong>INCORRECT:</strong> \"Use the AWS Config ec2-volume-inuse-check managed rule to mark unattached volumes are non-compliant. Create a new Amazon CloudWatch Events rule scheduled to invoke an AWS Lambda function in 14 days to delete the non-compliant volumes\" is incorrect.</p><p>The function should run daily to delete those that have reached 14 days since being attached. This answer indicates that a specific rule is created for each instance of a non-compliant volume which is inefficient.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 and Amazon Data Lifecycle Manager to configure a volume lifecycle policy. Set the interval period for unattached EBS volumes to 14 days and set the retention rule to delete. Set the policy target volumes as *\" is incorrect.</p><p>DLM assists with automating the lifecycle of snapshots and AMIs. In this case the DevOps team need to automate deletion of unattached EBS volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 67357148,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company having hundreds of AWS accounts manages its operations and security through a single organization created in AWS Organizations. As per the company's policy, AWS Config and AWS CloudTrail are enabled for all accounts. The security policy mandates configuring AWS Web Application Firewall (AWS WAF) web ACLs for all internet-facing Application Load Balancers (ALBs) and Amazon API Gateway APIs. However, monthly audit reports consistently report unsecured ALBs and API Gateway APIs.</p>\n\n<p>As a DevOps engineer, the security team has requested you to automate these configurations for all accounts to avoid oversight. What steps will you recommend?</p>\n",
          "answers": [
            "<p>Designate one of the AWS accounts in your organization as the administrator for Firewall Manager in AWS Organizations. Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>",
            "<p>Create an Amazon Systems Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>",
            "<p>Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>",
            "<p>Configure a managed rule in AWS Config to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p>AWS Firewall Manager offers the freedom to use multiple AWS accounts and to host applications in any desired region while maintaining centralized control over their organization\u2019s security settings and profile. Developers can develop and innovators can innovate, while the security team gains the ability to respond quickly, uniformly, and globally to potential threats and actual attacks.</p>\n\n<p>Firewall Manager is built around named policies that contain WAF rule sets and optional AWS Shield advanced protection. Each policy applies to a specific set of AWS resources, specified by account, resource type, resource identifier, or tag. Policies can be applied automatically to all matching resources, or to a subset that you select. Policies can include WAF rules drawn from within the organization, and also those created by AWS Partners such as Imperva, F5, Trend Micro, and other AWS Marketplace vendors. This gives your security team the power to duplicate their existing on-premises security posture in the cloud.</p>\n\n<p>Firewall Manager has three prerequisites:</p>\n\n<ol>\n<li><p>AWS Organizations \u2013 Your organization must be using AWS Organizations to manage your accounts and all features must be enabled.</p></li>\n<li><p>Firewall Administrator \u2013 You must designate one of the AWS accounts in your organization as the administrator for Firewall Manager. This gives the account permission to deploy AWS WAF rules across the organization.</p></li>\n<li><p>AWS Config \u2013 You must enable AWS Config for all of the accounts in the Organization so that Firewall Manager can detect newly created resources.</p></li>\n</ol>\n\n<p>Using AWS Firewall Manager to centrally manage your Web Application Portfolio:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/\">https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Amazon GuardDuty can automatically update the AWS Web Application Firewall Web Access Control Lists (WebACLs) and VPC Network Access Control Lists (NACLs) in response to GuardDuty findings. But, Amazon GuardDuty is a continuous security monitoring and threat detection service and not a security management service like AWS Firewall Manager. Even though GuardDuty can update web ACLs, it's a reaction to a threat. It cannot be used to proactively define rules for web ACLs across accounts to centrally manage the security infrastructure of an organization.</p>\n\n<p><strong>Configure a managed rule in AWS Config to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Enabling AWS Config is a prerequisite for using AWS Firewall Manager. AWS Config can track the status of resources, but AWS Firewall Manager is needed for centrally managing the security infrastructure.</p>\n\n<p>AWS Firewall Manager and AWS Config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/firewall-manager/\">https://aws.amazon.com/firewall-manager/</a></p>\n\n<p><strong>Create an Amazon Systems Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - This option has been added as a distractor and is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html\">https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html\">https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html</a></p>\n"
        }
      },
      {
        "id": 115961503,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The instances often come online before they are ready which leads to errors being experienced. The health check configuration provides a 60-second grace period and considers instances healthy after two 200 response codes from /index.php. This page can respond intermittently during the deployment process.</p><p>A DevOps engineer has been tasked with troubleshooting the errors. The instances should come online as soon as possible but not before they are ready.</p><p>Which strategy can be used to address this issue?</p><p>Which strategy would address this issue?</p>",
          "answers": [
            "<p>Increase the instance grace period from 60 seconds to 300 seconds, and the consecutive health check requirement from 2 to 3.</p>",
            "<p>Modify the deployment script to create a /health-check.php file at the beginning of the deployment and modify the health check path to point to that file.</p>",
            "<p>Increase the instance grace period from 60 seconds to 180 seconds and change the response code requirement from 200 to 202.</p>",
            "<p>Modify the deployment script to create a /health-check.php file at the end of the deployment and modify the health check path to point to that file.</p>"
          ],
          "explanation": "<p>The correct solution includes the creation of a health-check.php file at the end of the deployment when the application should be running consistently. The health check path is reconfigured to point to this file. This means that as soon as the application deployment is complete the health checks should start to return success (200) status codes and the instances will be marked as healthy.</p><p><strong>CORRECT: </strong>\"Modify the deployment script to create a /health-check.php file at the end of the deployment and modify the health check path to point to that file\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Modify the deployment script to create a /health-check.php file at the beginning of the deployment and modify the health check path to point to that file\" is incorrect.</p><p>This solution creates the health-check.php file too early in the process. The application may not be ready at this point so if the file is present the 200 response codes will be returned, and the instance will be marked as healthy despite the application not being ready to process requests. The question indicates that errors have been experienced due to the application being marked as healthy too early.</p><p><strong>INCORRECT:</strong> \"Increase the instance grace period from 60 seconds to 300 seconds, and the consecutive health check requirement from 2 to 3\" is incorrect.</p><p>The grace period helps Amazon EC2 Auto Scaling distinguish unhealthy instances from newly launched instances that are not yet ready to serve traffic. This grace period can prevent Amazon EC2 Auto Scaling from marking InService instances as unhealthy and terminating them before they have time to finish initializing. The issue experienced is not related to instances being terminated too early, it is related to instances coming online (marked as healthy) too early.</p><p>Changing the health check requirement from 2 to 3 may help delay bringing the application online but depends on the interval specified.</p><p><strong>INCORRECT:</strong> \"Increase the instance grace period from 60 seconds to 180 seconds and change the response code requirement from 200 to 202\" is incorrect.</p><p>As above, the changing the health check requirement from 2 to 3 may help delay bringing the application online but depends on the interval specified. Changing the response code requirement will not make any positive difference here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 82921342,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a social media company, you have deployed your application in an Auto Scaling group (ASG) using CloudFormation. You would like to update the Auto Scaling Group to have all the instances reference the newly created launch configuration, which upgrades the instance type. Your ASG currently contains 6 instances and you need at least 4 instances to be up at all times.</p>\n\n<p>Which configuration should you use in the CloudFormation template?</p>\n",
          "answers": [
            "<p>AutoScalingRollingUpdate</p>",
            "<p>AutoScalingReplacingUpdate</p>",
            "<p>AutoScalingLaunchTemplateUpdate</p>",
            "<p>AutoScalingLaunchConfigurationUpdate</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>AutoScalingRollingUpdate</strong></p>\n\n<p>To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the AutoScalingRollingUpdate policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. AutoScalingRollingUpdate is perfect for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q72-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AutoScalingReplacingUpdate</strong> - To specify how AWS CloudFormation handles replacement updates for an Auto Scaling group, you should use the AutoScalingReplacingUpdate policy. This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group. This option will create a new ASG entirely, so this is ruled out.</p>\n\n<p><strong>AutoScalingLaunchTemplateUpdate</strong></p>\n\n<p><strong>AutoScalingLaunchConfigurationUpdate</strong></p>\n\n<p>AutoScalingLaunchTemplateUpdate and AutoScalingLaunchConfigurationUpdate do not exist, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n"
        }
      },
      {
        "id": 134588473,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A multinational corporation has multiple AWS accounts that are consolidated using AWS Organizations. A new system should be configured for security purposes to detect suspicious activities in any of its accounts, such as SSH brute force attacks or compromised Amazon EC2 instances that serve malware. All gathered information must be centrally stored in its dedicated security account for audit purposes, and the events should be stored in an Amazon S3 bucket.</p><p>Which solution should a DevOps Engineer implement to meet this requirement?</p>",
          "answers": [
            "<p>Automatically detect SSH brute force or malware attacks by enabling Amazon Macie in every account. Set up the security account as the Macie Administrator for every member account of the organization. Create an Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, which should push the findings to an S3 bucket.</p>",
            "<p>Automatically detect SSH brute force or malware attacks by only enabling Amazon Macie in the security account. Configure the security account as the Macie Administrator for every member account. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket.</p>",
            "<p>Automatically detect SSH brute force or malware attacks by enabling Amazon GuardDuty in every account. Configure the security account as the GuardDuty Administrator for all member accounts in the organization. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p>",
            "<p>Automatically detect SSH brute force or malware attacks by only enabling Amazon GuardDuty in the security account. Set up the security account as the GuardDuty Administrator for every member account. Create a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket.</p>"
          ],
          "explanation": "<p><strong>Amazon GuardDuty</strong> is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. The cloud simplifies the collection and aggregation of account and network activities. Still, it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in the AWS Cloud. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats. GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs. With a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with AWS EventBridge, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.</p><p><img alt=\"Amazon GuardDuty\" height=\"378\" src=\"https://media.tutorialsdojo.com/public/product-page-diagram-Amazon-GuardDuty_how-it-works_2AUG2023.png\" width=\"1000\"></p><p>GuardDuty enables and manages across multiple accounts efficiently. All member account findings can be aggregated through the multi-account feature with a GuardDuty administrator account. This allows the security team to manage all GuardDuty findings across the organization in one account. The aggregated findings are also available through EventBridge, making integration with an existing enterprise event management system easy.</p><p>Hence, the correct answer is: <strong>Automatically detect SSH brute force or malware attacks by enabling Amazon GuardDuty in every account. Configure the security account as the GuardDuty Administrator for all member accounts in the organization. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</strong></p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by enabling Amazon Macie in every account. Set up the security account as the Macie Administrator for every member account of the organization. Create an Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Data Firehose, which should push the findings to an S3 bucket</strong> is incorrect because you have to use Amazon GuardDuty instead of Amazon Macie. Note that Amazon Macie cannot detect SSH brute force or malware attacks.</p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by only enabling Amazon Macie in the security account. Configure the security account as the Macie Administrator for every member account. Set up a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket</strong> is incorrect because you don't need to create a custom shell script in Lambda or use Kinesis Data Streams. You can just configure the EventBridge rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p><p>The option that says: <strong>Automatically detect SSH brute force or malware attacks by only enabling Amazon GuardDuty in the security account. Set up the security account as the GuardDuty Administrator for every member account. Create a new Amazon EventBridge rule in the security account. Configure the rule to send all findings to Amazon Kinesis Data Streams. Launch a custom shell script in the AWS Lambda function to read data from the Kinesis Data Streams and push it to the S3 bucket </strong>is incorrect. Although using Amazon GuardDuty in this scenario is valid, the implementation for storing the findings is incorrect. You can typically configure the EventBridge rule to send all findings to Amazon Data Firehose, pushing the findings to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-manage-amazon-guardduty-security-findings-across-multiple-accounts/\">https://aws.amazon.com/blogs/security/how-to-manage-amazon-guardduty-security-findings-across-multiple-accounts/</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-guardduty/?src=udemy\">https://tutorialsdojo.com/amazon-guardduty/</a></p>"
        }
      },
      {
        "id": 82921398,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A global health-care company has an EFS filesystem being used in eu-west-1. The company would like to plan for a disaster recovery strategy and backup that EFS file system in ap-southeast-2. It needs to have a hot copy of the data so that the applications can be re-deployed in ap-southeast-2 with a minimum RPO and RTO. The VPCs in each region are not peered with each other.</p>\n\n<p>How should a DevOps engineer implement a solution for this use-case?</p>\n",
          "answers": [
            "<p>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</p>",
            "<p>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create a Lambda Function in ap-southeast-2 for PUT on Amazon S3 and triggers an SSM Run Command to copy the files from S3 into EFS</p>",
            "<p>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Create a standby EFS cluster in ap-southeast-2 and mount it on the same EC2 cluster. Let the replication software perform EFS to EFS replication</p>",
            "<p>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create an EC2 replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</strong></p>\n\n<p>Metrics are the fundamental concept in CloudWatch. A metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time. You can use these metrics to verify that your system is performing as expected.</p>\n\n<p>Using custom metrics for your Auto Scaling groups and instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q68-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>RPO and RTO explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q68-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html</a></p>\n\n<p>For the given use-case, we need to create a custom metric via the application that captures the lag in file reads and then uses it for scaling the ASG managing the EC2 instances to replicate the source EFS cluster into S3. Use another ASG to copy data from S3 into EFS in the target AWS Region. Here we want minimum RPO so we want continuous replication, and minimum RTO so we want a hot EFS system ready to go. Please note that because the RPO and RTO are low, the cost of the solution will be very high.</p>\n\n<p>Side note (for your knowledge) the AWS DataSync service (not covered in the exam) can achieve EFS to EFS replication in a much more native way.</p>\n\n<p>Note: With this solution, as the files are copied to S3, the file Linux permissions would not be replicated.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Create a standby EFS cluster in ap-southeast-2 and mount it on the same EC2 cluster. Let the replication software perform EFS to EFS replication</strong> - As the VPCs are not peered, it's not possible to mount the EFS of two different regions onto the same EC2 cluster. We need to go through S3 for the replication.</p>\n\n<p><strong>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create an EC2 replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</strong></p>\n\n<p><strong>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create a Lambda Function in ap-southeast-2 for PUT on Amazon S3 and triggers an SSM Run Command to copy the files from S3 into EFS</strong></p>\n\n<p>As the target, EFS needs to have a hot copy of the data, so both these options are ruled out since there is a delay of an hour.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n"
        }
      },
      {
        "id": 67357100,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.</p>\n\n<p>As a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?</p>\n",
          "answers": [
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>AWS::EC2::Volume</code> that returns a compliance failure if the custom tag is not applied on the EBS volume. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong></p>\n\n<p>Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type.</p>\n\n<p>AWS Config managed rule that checks required-tags:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p>This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes.</p>\n\n<p>AWS Config rules remediation with automation runbooks architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it's possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n"
        }
      },
      {
        "id": 82921442,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a financial services company is deploying the flagship application in highly available mode using Elastic Beanstalk which has created an ASG and an ALB. The team has also specified a <code>.ebextensions</code> file to create an associated DynamoDB table. As a DevOps Engineer in the team, you would like to perform an update to the application but you need to make sure the DNS name won't change and that no new resources will be created. The application needs to remain available during the update.</p>\n\n<p>Which of the following options would you suggest to address the given requirements?</p>\n",
          "answers": [
            "<p>Use a rolling update with 20% at a time</p>",
            "<p>Use a blue/green deployment and swap CNAMEs</p>",
            "<p>Use immutable</p>",
            "<p>Use in-place</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a rolling update with 20% at a time</strong></p>\n\n<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, Immutable, and Traffic splitting) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's a scalable environment (you didn't specify the --single option), it uses rolling deployments.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Comparison of deployment method properties:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches (for this requirement, we shall use a batch with 20% of the instances) and deploys the new version of the application to one batch at a time. It leaves the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version. Therefore, for the given use-case, we should use a rolling update, which will keep our ASG, our instances, and ensure our application can still serve traffic.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a blue/green deployment and swap CNAMEs</strong> - In a blue/green deployment, you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. A blue/green deployment would create a new load balancer and ASG, but the CNAME swap would allow us to keep the same DNS name. So it does not meet the requirements for the given use-case.</p>\n\n<p><strong>Use immutable</strong> - Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. So this option does not meet the requirements for the given use-case.</p>\n\n<p><strong>Use in-place</strong> - In-place would not work even though it doesn't create any new resources because your application will be unavailable as all your instances will be updated at the same time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n"
        }
      },
      {
        "id": 75949112,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A startup company is launching a web application on AWS that uses Amazon EC2 instances behind an Application Load Balancer. The EC2 instances will store data in an Amazon RDS database and an Amazon DynamoDB table. The DevOps team require separate environments for development, testing, and production.</p><p>What is the MOST secure method of obtaining password credentials?</p>",
          "answers": [
            "<p>Configure access keys to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.</p>",
            "<p>Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Retrieve the database credentials from AWS Secrets Manager.</p>",
            "<p>Configure access keys to access AWS services. Retrieve the database credentials from an AWS Secrets Manager SecretString parameter.</p>",
            "<p>Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Store the database passwords in an encrypted config file in system metadata.</p>"
          ],
          "explanation": "<p>The most secure solution is to use a combination of an AWS IAM Instance Profile with a policy attached providing permissions to Amazon DynamoDB and AWS Secrets Manager for retrieving SecretString parameters for the Amazon RDS DB.</p><p>Instance profiles are far more secure compared to access keys are they leverage the AWS STS service to obtain temporary security credentials. No security credentials are stored on the EC2 instances when using this method.</p><p>For Amazon RDS you may need to use database connection credentials for authentication depending on your configuration. If this is the case you can securely store these credentials as SecretString (encrypted) parameters in AWS Secrets Manager. Your application can then issue API calls to Secrets Manager to securely retrieve the encrypted parameters when authentication is needed.</p><p><strong>CORRECT: </strong>\"Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Retrieve the database credentials from AWS Secrets Manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Store the database passwords in an encrypted config file in system metadata\" is incorrect.</p><p>Database passwords should not be stored in system metadata as this would be very insecure.</p><p><strong>INCORRECT:</strong> \"Configure access keys to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter\" is incorrect.</p><p>Access keys are less secure compared to using instance profiles as with access keys the actual access key ID and secret access key are stored in plaintext on the EC2 instance file system.</p><p><strong>INCORRECT:</strong> \"Configure access keys to access AWS services. Retrieve the database credentials from an AWS Secrets Manager SecretString parameter\" is incorrect.</p><p>As above, access keys should not be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html</a></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_CreateSecret.html\">https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_CreateSecret.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>"
        }
      },
      {
        "id": 82921318,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A graphics design company is experimenting with a new feature for an API and the objective is to pass the field <code>\"color\"</code> in the JSON payload to enable this feature. The new Lambda function should treat <code>\"color\": \"none\"</code> as a request from an older client. The company would like to only have to manage one Lambda function in the back-end while being able to support both old and new clients. The API Gateway API is currently deployed on the <code>v1</code> stage. Old clients include Android applications which may take time to be updated. The technical requirements mandate that the solution should support the old clients for years to come.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you recommend as the best fit for the given use-case?</p>\n",
          "answers": [
            "<p>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</p>",
            "<p>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</p>",
            "<p>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</p>",
            "<p>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway handles tasks such as traffic management, authorization and access control, monitoring, and API version management. API Gateway acts as a \"front door\" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications. Simply creating and developing an API Gateway API doesn't automatically make it callable by your users. To make it callable, you must deploy your API to a stage. A stage is a named reference to a deployment, which is a snapshot of the API.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>For the given use-case, API Gateway mappings must be used. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. As such, you must deploy a v2 API alongside the v1 API backed by the same Lambda function. Old clients will hit the v1 API, which will use a mapping template to add the static missing field <code>\"color\": \"none\"</code>. Newer clients will hit the v2 API and will have that field value included.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</strong> - If we release two separate Lambda functions (named lambda v1 and lambda v2), then we have to maintain them both and that would be going against the requirements of the given use-case.</p>\n\n<p><strong>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</strong> -  API Gateway mapping templates do not support adding default values for fields as these only support static fields.</p>\n\n<p><strong>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. For the given use-case, API Gateway caching is a distractor and should be disregarded.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n"
        }
      },
      {
        "id": 67357112,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company uses serverless application architecture to process thousands of requests using AWS Lambda with Amazon DynamoDB as the database. The Amazon API Gateway REST API is used to invoke an AWS Lambda function that loads a large amount of data from the Amazon DynamoDB database. This results in cold start latencies of 7-10 seconds. The DynamoDB tables have already been configured with DynamoDB Accelerator (DAX) to reduce latency. Yet, customers report of application latency, especially during peak access hours. The application receives maximum traffic between 2 PM -5 PM every day and gradually reduces thereafter, reporting a minimum traffic post 8 PM.</p>\n\n<p>How should a DevOps engineer configure the AWS Lambda function to reduce its latency at all times?</p>\n",
          "answers": [
            "<p>Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively</p>",
            "<p>Use the Lambda function layers for parallel processing of Lambda runtime requests. Deploy the Lambda function as a .zip file archive to utilize Lambda layer functionality</p>",
            "<p>Configure the Lambda function execution environment to use up to 10,240 MB of ephemeral storage. This transient cache for data between invocations can be accessed from the Lambda function code via /tmp space</p>",
            "<p>Configure the AWS Lambda function to use Reserved concurrency. Configure the application Auto Scaling on the Lambda function with reserved concurrency as half of the Lambda function instances invoked during peak traffic hours</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively</strong></p>\n\n<p>For a Lambda function, concurrency is the number of in-flight requests your function is handling at the same time. Provisioned concurrency is the number of pre-initialized execution environments you want to allocate to your function. These execution environments are prepared to respond immediately to incoming function requests. Configuring provisioned concurrency incurs charges to your AWS account.</p>\n\n<p>Estimating required provisioned concurrency- If your function is currently serving traffic, you can easily view its concurrency metrics using CloudWatch metrics. Specifically, the <code>ConcurrentExecutions</code> metric shows you the number of concurrent invocations for each function in your account.</p>\n\n<p>When working with provisioned concurrency, Lambda suggests including a 10% buffer on top of the amount of concurrency your function typically needs. For example, if your function usually peaks at 200 concurrent requests, set your provisioned concurrency at 220 instead (200 concurrent requests + 10% = 220 provisioned concurrency).</p>\n\n<p>You can alleviate cold start issues by configuring the optimum provisioned concurrency for the Lambda function.</p>\n\n<p>Optimizing latency with provisioned concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p>Accurately estimating required provisioned concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the AWS Lambda function to use Reserved concurrency. Configure the application Auto Scaling on the Lambda function with reserved concurrency as half of the Lambda function instances invoked during peak traffic hours</strong> - Reserved concurrency is the maximum number of concurrent instances you want to allocate to your function. When a function has reserved concurrency, no other function can use that concurrency. Reserved concurrency is well suited for use cases where traffic is constant and predictable over the day.</p>\n\n<p><strong>Configure the Lambda function execution environment to use up to 10,240 MB of ephemeral storage. This transient cache for data between invocations can be accessed from the Lambda function code via /tmp space</strong> - For data-intensive applications such as machine learning inference, and financial computations, customers often need to read and write large amounts of data to ephemeral storage. The Lambda execution environment provides an ephemeral file system for customers\u2019 code to access via /tmp space, which is preserved for the lifetime of the execution environment, and can provide a transient cache for data between invocations. This option acts as a distractor.</p>\n\n<p><strong>Use the Lambda function layers for parallel processing of Lambda runtime requests. Deploy the Lambda function as a .zip file archive to utilize Lambda layer functionality</strong> - A Lambda function layer is a .zip file archive that can contain additional code or data. A layer can contain libraries, a custom runtime, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. This option is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#calculating-concurrency\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#calculating-concurrency</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-configure-ephemeral-storage/\">https://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-configure-ephemeral-storage/</a></p>\n"
        }
      },
      {
        "id": 134588449,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>You\u2019re running a cluster of EC2 instances on AWS that serves authentication processes and session handling for your new application. After three weeks of operation, your monitoring team flagged your instances with constantly high CPU usage, and the login module response is very slow. Upon checking, it was discovered that your Amazon EC2 instances were hacked and exploited for mining Bitcoins. You have immediately taken down the cluster and created a new one with your custom AMI. You want to have a solution to detect compromised EC2 instances as well as detect malicious activity within your AWS environment. </p><p>Which of the following will help you with this?</p>",
          "answers": [
            "<p>Use Amazon Macie</p>",
            "<p>Use AWS Inspector</p>",
            "<p>Enable Amazon GuardDuty</p>",
            "<p>Enable VPC Flow Logs</p>"
          ],
          "explanation": "<p><strong>Amazon GuardDuty</strong> offers threat detection that enables you to continuously monitor and protect your AWS accounts and workloads. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately. This can include issues like escalations of privileges, uses of exposed credentials, or communication with malicious IPs, URLs, or domains.</p><p><img src=\"https://media.tutorialsdojo.com/public/product-page-diagram-Amazon-GuardDuty_how-it-works_2AUG2023.png\"></p><p>For example, GuardDuty can detect compromised EC2 instances serving malware or mining bitcoin. It also monitors AWS account access behavior for signs of compromise, such as unauthorized infrastructure deployments, like instances deployed in a region that has never been used, or unusual API calls, like a password policy change to reduce password strength.</p><p>Hence, the correct answer is: <strong>Enable Amazon GuardDuty<em>.</em></strong></p><p>The option that says: <strong>Use Amazon Macie </strong>is incorrect because this service is available to protect data stored in Amazon S3 only. It uses machine learning to automatically discover, classify, and protect sensitive data in AWS.</p><p>The option that says: <strong>Use AWS Inspector</strong> is incorrect because this service is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It will not detect possible malicious activity on your instances.</p><p>The option that says: <strong>Enable VPC Flow Logs </strong>is incorrect because this feature only collects traffic logs flowing to your VPC. It does not provide an analysis of the traffic logs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/faqs/ \">https://aws.amazon.com/guardduty/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html \">https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html</a></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p><br></p><p><strong>Check out this Amazon GuardDuty Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-guardduty/?src=udemy\">https://tutorialsdojo.com/amazon-guardduty/</a></p>"
        }
      },
      {
        "id": 134588427,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An IT consulting firm has a Docker application hosted on an Amazon ECS cluster in their AWS VPC. It has been encountering intermittent unavailability issues and time outs lately, which affects their production environment. A DevOps engineer was instructed to instrument the application to detect where high latencies are occurring and to determine the specific services and paths impacting application performance. </p><p>Which of the following steps should the Engineer do to accomplish this task properly?</p>",
          "answers": [
            "<p>Produce a Docker image that runs the X-Ray daemon. Upload the image to a Docker image repository, and then deploy it to your Amazon ECS cluster. Configure the network mode settings and port mappings in your task definition file to allow traffic on UDP port 2000.</p>",
            "<p>Produce a Docker image that runs the X-Ray daemon. Upload the image to a Docker image repository and then deploy it to your Amazon ECS cluster. Configure the network mode settings and port mappings in the container agent to allow traffic on TCP port 2000.</p>",
            "<p>In the Amazon ECS container agent configuration, pass a user data script in the <code>/etc/ecs/ecs.config</code> file that will install the X-Ray daemon. The script will automatically run when the Amazon ECS container instance is launched. Configure the network mode settings and port mappings in the container agent to allow traffic on TCP port 3000.</p>",
            "<p>Add the <code>xray-daemon.config</code> configuration file in your Docker image. Set up the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.</p>"
          ],
          "explanation": "<p>The <strong>AWS X-Ray SDK</strong> does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the SDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the daemon alongside your application.</p><p>To properly instrument your applications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container.</p><p><img src=\"https://media.tutorialsdojo.com/public/AWS-X-Ray-daemon_6AUG2023.png\"></p><p>The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service.</p><p>Hence, the correct steps to properly instrument the application is to: <strong>Produce a Docker image that runs the X-Ray daemon. Upload the image to a Docker image repository, and then deploy it to your Amazon ECS cluster. Configure the network mode settings and port mappings in your task definition file to allow traffic on UDP port 2000.</strong></p><p>The option that says: <strong>Produce a Docker image that runs the X-Ray daemon. Upload the image to a Docker image repository, and then deploy it to your Amazon ECS cluster. Configure the network mode settings and port mappings in the container agent to allow traffic on TCP port 2000 </strong>is incorrect because this should be done in the task definition and not in the container agent. Moreover, X-Ray is primarily using the UDP port 2000 so this should also be added alongside the TCP port mapping.</p><p>The option that says: <strong>In the Amazon ECS container agent configuration, pass a user data script in the </strong><code><strong>/etc/ecs/ecs.config</strong></code><strong> file that will install the X-Ray daemon. The script will automatically run when the Amazon ECS container instance is launched. Configure the network mode settings and port mappings in the container agent to allow traffic on TCP port 3000 </strong>is incorrect because X-Ray is using the UDP port 2000 and not TCP port 3000. In addition, it is better to configure your ECS task definitions instead of the Amazon ECS container agent to enable X-Ray.</p><p>The option that says: <strong>Add the </strong><code><strong>xray-daemon.config</strong></code><strong> configuration file in your Docker image. Set up the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000</strong> is incorrect because this step is not suitable for ECS. The <code>xray-daemon.config</code> configuration file is primarily used in Elastic Beanstalk.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html</a></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html</a></p><p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/scorekeep-ecs.html\">https://docs.aws.amazon.com/xray/latest/devguide/scorekeep-ecs.html</a></p><p><br></p><p><strong>Check out this AWS X-Ray Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-x-ray/?src=udemy\">https://tutorialsdojo.com/aws-x-ray/</a></p><p><br></p><p><strong>Instrumenting your Application with AWS X-Ray:</strong></p><p><a href=\"https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/?src=udemy\">https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/</a></p>"
        }
      },
      {
        "id": 134588511,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A Data Analytics team uses a large Hadoop cluster with 100 EC2 instance nodes for gathering trends about user behavior on the company\u2019s mobile app. Currently, the DevOps team forwards any email notification they receive from the AWS Health service to the Data Analytics team whenever there is a particular EC2 instance that needs maintenance or any old EC2 instance types that will be retired by Amazon soon. The Data Analytics team needs to determine how the maintenance will affect their cluster and ensure that their Hadoop Distributed File System (HDFS) component can recover from any failure.</p><p>Which of the following is the FASTEST way to automate this notification process?</p>",
          "answers": [
            "<p>Create an AWS EventBridge (Amazon CloudWatch Events) rule for AWS Health. Select EC2 Service and select the Events you want to get notified. Set the target to an Amazon SNS topic that the Data Analytics team is subscribed to.</p>",
            "<p>Create a CloudWatch Metric Filter for AWS Health Events of your related EC2 instances. Create a CloudWatch Alarm for this metric to notify you whenever the threshold is reached.</p>",
            "<p>Use AWS Systems Manager (SSM) Resource Data Sync to track the AWS Health Events of your related EC2 instances. Configure the SSM Resource Data Sync to automatically poll the AWS Health API for Amazon EC2 updates. Send a notification to the Data Analytics team whenever an EC2 event is matched</p>",
            "<p>Add your Data Analytics team email as an alternate contact for your AWS account. They will receive these EC2 maintenance emails directly so you won\u2019t have to forward it to them.</p>"
          ],
          "explanation": "<p>You can use AWS EventBridge (Amazon CloudWatch Events) to detect and react to changes in the status of AWS Personal Health Dashboard (AWS Health) events. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when an event matches the values that you specify in a rule.</p><p>Depending on the type of event, you can send notifications, capture event information, take corrective action, initiate events, or take other actions. In this scenario, you first need to create an Events rule for AWS Health and choose the \"EC2 service\u201d as well as specific categories of events or scheduled changes that are planned to your account.</p><p><img src=\"https://media.tutorialsdojo.com/aws-health-event-patterns.png\"></p><p>Then, you can then set the Target of this rule to an SNS topic on which the Data Analytics team will subscribe. For every event that matches this event rule, a notification will be sent to the subscribers of your SNS topic.</p><p>Hence, the correct answer is: <strong>Create an AWS EventBridge (Amazon CloudWatch Events) rule for AWS Health. Select EC2 Service and select the Events you want to get notified. Set the target to an Amazon SNS topic that the Data Analytics team is subscribed to.</strong></p><p>The option that says: <strong>Create a CloudWatch Metric Filter for AWS Health Events of your related EC2 instances. Create a CloudWatch Alarm for this metric to notify you whenever the threshold is reached</strong> is incorrect because you can only create a Metric filter for CloudWatch log groups. Take note that the AWS Health events are not automatically sent to CloudWatch Log groups; thus, you have to manually set up a data stream for AWS Health.</p><p>The option that says: <strong>Use AWS Systems Manager (SSM) Resource Data Sync to track the AWS Health Events of your related EC2 instances. Configure the SSM Resource Data Sync to automatically poll the AWS Health API for Amazon EC2 updates. Send a notification to the Data Analytics team whenever an EC2 event is matched </strong>is incorrect. Keep in mind that the AWS Systems Manager resource data sync feature is primarily used in AWS Systems Manager Inventory to send inventory data collected from all of your managed nodes to a single Amazon Simple Storage Service (Amazon S3) bucket. The SSM resource data sync will automatically update the centralized data in Amazon S3 when new inventory data is collected. This resource data sync feature is not applicable in this scenario.</p><p>The option that says: <strong>Add your Data Analytics team email as an alternate contact for your AWS account. They will receive these EC2 maintenance emails directly, so you won\u2019t have to forward it to them </strong>is incorrect because the Data Analytics team will also receive other financial or account/billing-related emails using this setup. This is a security risk since the Data Analytics team email could potentially be compromised and be used to access your AWS account.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html\">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-cloudwatch-events-track-and-respond-to-changes-to-your-aws-resources/ \">https://aws.amazon.com/blogs/aws/new-cloudwatch-events-track-and-respond-to-changes-to-your-aws-resources/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide//automating_with_cloudwatch_events.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide//automating_with_cloudwatch_events.html</a></p>"
        }
      },
      {
        "id": 138248149,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An enterprise resource planning application is hosted in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer which uses an Amazon DynamoDB table as its data store. For the application\u2019s reporting module, there are five AWS Lambda functions which are reading from the DynamoDB Streams of the table to count the number of products, monitor the inventory, generate reports, move new items to an Amazon Data Firehose for analytics, and many more. The operations team discovered that in peak times, the Lambda functions are getting a stream throttling error and some of the requests fail which affects the performance of the reporting module.</p><p>Which of the following is the MOST scalable and cost-effective solution with the LEAST amount of operational overhead?</p>",
          "answers": [
            "<p>Delete all of the Lambda functions and move all of the processing on an Amazon ECS Cluster with Auto Scaling enabled using AWS App Runner. Set up AWS Glue to consume the DynamoDB stream which will be processed by ECS. Re-factor the reporting module to use Amazon Managed Service for Apache Flink Studio.</p>",
            "<p>Refactor your architecture to use Amazon Kinesis Adapter for real-time processing of streaming data at a massive scale instead of directly consuming the stream using Lambda. Re-architect the reporting module to use Managed Service for Apache Flink Studio.</p>",
            "<p>Create a new local secondary index (LSI) in the DynamoDB table to improve the performance of the queries. Double the allocated RCU of the table. Refactor the Lambda functions to directly query from the table and disable the DynamoDB streams. Increase the concurrency limits of each Lambda function to avoid throttling errors and set the <code>ParallelizationFactor</code> to 10. Re-architect the reporting module to use Managed Service for Apache Flink Studio.</p>",
            "<p>Use the Amazon CodeGuru service to optimize the codebase of the reporting module. Create a new global secondary index (GSI) in the DynamoDB table to improve the performance of the queries. Increase the allocated RCU of the table. Disable the DynamoDB streams and refactor the Lambda functions to directly query from the table. Refactor the reporting module to use Managed Service for Apache Flink Studio.</p>"
          ],
          "explanation": "<p>Using the Amazon Kinesis Adapter is the recommended way to consume streams from Amazon DynamoDB. The DynamoDB Streams API is intentionally similar to that of Kinesis Data Streams, a service for real-time processing of streaming data at a massive scale. In both services, data streams are composed of shards, which are containers for stream records. Both services' APIs contain <code>ListStreams</code>, <code>DescribeStream</code>, <code>GetShards</code>, and <code>GetShardIterator</code> operations. (Although these DynamoDB Streams actions are similar to their counterparts in Kinesis Data Streams, they are not 100 percent identical.)</p><p>You can write applications for Kinesis Data Streams using the Kinesis Client Library (KCL). The KCL simplifies coding by providing useful abstractions above the low-level Kinesis Data Streams API.</p><p>As a DynamoDB Streams user, you can use the design patterns found within the KCL to process DynamoDB Streams shards and stream records. To do this, you use the DynamoDB Streams Kinesis Adapter. The Kinesis Adapter implements the Kinesis Data Streams interface so that the KCL can be used for consuming and processing records from DynamoDB Streams.</p><p><img src=\"https://media.tutorialsdojo.com/amazon-dynamodb-streams-console-saa-c03.png\"></p><p>Amazon Managed Service for Apache Flink Studio is the easiest way to transform and analyze streaming data in real time using Apache Flink, an open-source framework and engine for processing data streams. Amazon Managed Service for Apache Flink Studio simplifies building and managing Apache Flink workloads and allows you to easily integrate applications with other AWS services.</p><p>Hence, the correct answer is: <strong>Refactor your architecture to use Amazon Kinesis Adapter for real-time processing of streaming data at a massive scale instead of directly consuming the stream using Lambda. Re-architect the reporting module to use Managed Service for Apache Flink Studio.</strong></p><p>The option that says: <strong>Delete all of the Lambda functions and move all of the processing on an Amazon ECS Cluster with Auto Scaling enabled using AWS App Runner. Set up AWS Glue to consume the DynamoDB stream which will be processed by ECS. Re-factor the reporting module to use Amazon Managed Service for Apache Flink Studio</strong> is incorrect because using an ECS Cluster is more expensive than using Lambda functions. The use of AWS Glue is not warranted and is irrelevant here since this is just a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics, and not for consuming DynamoDB streams.</p><p>The option that says: <strong>Create a new local secondary index (LSI) in the DynamoDB table to improve the performance of the queries. Double the allocated RCU of the table. Refactor the Lambda functions to directly query from the table and disable the DynamoDB streams. Increase the concurrency limits of each Lambda function to avoid throttling errors and set the </strong><code><strong>ParallelizationFactor</strong></code><strong> to 10. Re-architect the reporting module to use Managed Service for Apache Flink Studio </strong>is incorrect because doubling the allocated RCU of the table will significantly increase the cost of your architecture, and technically, this will not solve the throttling issue of the Lambda functions. The same goes for the creation of an LSI since the issue here is the consumption of the streams and not the actual DynamoDB query performance. Although increasing the concurrency limits of the Lambda function may help, this is not the most scalable solution to consume the high amount of DynamoDB Streams. The recommended way is to use an Amazon Kinesis Adapter instead in order to consume the streams at a massive scale.</p><p>The option that says: <strong>Use the Amazon CodeGuru service to optimize the codebase of the reporting module. Create a new global secondary index (GSI) in the DynamoDB table to improve the performance of the queries. Increase the allocated RCU of the table. Disable the DynamoDB streams and refactor the Lambda functions to directly query from the table. Refactor the reporting module to use Managed Service for Apache Flink Studio</strong> is incorrect because, as mentioned above, adding a GSI and increasing the allocated RCU are simply not related to this issue. In fact, querying directly from the table entails a lot of operation overhead since you have to develop the queries and maintain the indices as well as the capacity units (e.g. RCU, WCU) of your table.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.KCLAdapter.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.KCLAdapter.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>"
        }
      },
      {
        "id": 82921414,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at your company is using CodeDeploy to deploy new versions of a Lambda function after it has passed a CodeBuild check via your CodePipeline. Before deploying, the CodePipeline has a step in which it optionally kickstarts a restructuring of files on an S3 bucket that is forward compatible. That restructuring is done using a Step Function execution which invokes a Fargate task. The new Lambda function cannot work until the restructuring task has fully completed.</p>\n\n<p>As a DevOps Engineer, how can you ensure traffic isn't served to your new Lambda function until the task is completed?</p>\n",
          "answers": [
            "<p>In your <code>appspec.yml</code> file, include a <code>BeforeAllowTraffic</code> hook that checks on the completion of the Step Function execution</p>",
            "<p>In your <code>appspec.yml</code> file, include an <code>AfterAllowTraffic</code> hook that checks on the completion of the Step Function execution</p>",
            "<p>Enable Canary Deployment in CodeDeploy so that only a fraction of the service is served by the new Lambda function while the restructuring is happening</p>",
            "<p>Include an extra step in the Step Function to signal to CodeDeploy the completion of the restructuring and serve new traffic to the new Lambda function</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>In your <code>appspec.yml</code> file, include a <code>BeforeAllowTraffic</code> hook that checks on the completion of the Step Function execution</strong></p>\n\n<p>The AppSpec file is used to manage each deployment as a series of lifecycle event hooks, which are defined in the file. During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file.</p>\n\n<p>For AWS Lambda compute platform applications, the AppSpec file is used by CodeDeploy to determine:</p>\n\n<p>Which Lambda function version to deploy.</p>\n\n<p>Which Lambda functions to use as validation tests.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q12-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q12-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p>\n\n<p>The <code>BeforeAllowTraffic</code> hook is used to run tasks before traffic is shifted to the deployed Lambda function version. So for the given use-case, you can use this hook to check that the restructuring task has fully completed and then shift traffic to the newly deployed Lambda function version.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In your <code>appspec.yml</code> file, include an <code>AfterAllowTraffic</code> hook that checks on the completion of the Step Function execution</strong> - If you use an <code>AfterAllowTraffic</code> hook the new Lambda function will already serve traffic, so this option is incorrect.</p>\n\n<p><strong>Enable Canary Deployment in CodeDeploy so that only a fraction of the service is served by the new Lambda function while the restructuring is happening</strong> - Canary Deployments will send some traffic to the new Lambda function while the restructuring in S3 is still happening so that won't work.</p>\n\n<p><strong>Include an extra step in the Step Function to signal to CodeDeploy the completion of the restructuring and serve new traffic to the new Lambda function</strong> - There's no API to tell CodeDeploy to switch traffic to the new version of the Lambda function, therefore adding a step in your Step Function won't help.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p>\n"
        }
      },
      {
        "id": 82921316,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>You are working as a DevOps Engineer at an e-commerce company and have a deployed a Node.js application on Elastic Beanstalk. You would like to track error rates and specifically, you need to ensure by looking at the application log, that you do not have more than 100 errors in a 5 minutes interval. In case you are getting too many errors, you would like to be alerted via email.</p>\n\n<p>Which of the following options represents the most efficient solution in your opinion?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</p>",
            "<p>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</strong></p>\n\n<p>You can search and filter the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created.</p>\n\n<p>You can use metric filters to search for and match terms, phrases, or values in your log events. When a metric filter finds one of the terms, phrases, or values in your log events, you can increment the value of a CloudWatch metric. For example, you can create a metric filter to search for and count the occurrence of the word ERROR in your log events.</p>\n\n<p>CloudWatch Logs Metric Filter Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p>For the given use-case, you can have Beanstalk send the logs to CloudWatch Logs, and then create a metric filter. This will create a metric for us (and not an alarm), and on top of the metric, you can create a CloudWatch Alarm. This alarm will send a notification to SNS, which will, in turn, send us emails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - You cannot directly set a CloudWatch Alarm as a target for a CloudWatch Logs Metric Filter. You will first need to create a metric filter which can then be used to create a CloudWatch metric to be eventually used in a CloudWatch Alarm.</p>\n\n<p><strong>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</strong> - The Elastic Beanstalk Health Metrics will not track the errors sent out to a log file, so it does not meet the requirements of the use-case. Besides, CloudWatch alarm cannot be used to work on top of the Elastic Beanstalk Health Metrics.</p>\n\n<p><strong>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - Implementing custom logic in your Node.js application may seem like a good idea, but then you have to remember that your application can be distributed amongst many servers with Beanstalk, and as such it will not be possible to track the \"100 errors\" across all instances using this methodology.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p>\n"
        }
      },
      {
        "id": 82921366,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A mobility company connects people with taxi drivers and the DevOps team at the company uses CodeCommit as a backup and disaster recovery service for several of its DevOps processes. The team is creating a CICD pipeline so that your code in the CodeCommit master branch automatically gets packaged as a Docker container and published to ECR. The team would then like that image to be automatically deployed to an ECS cluster using a Blue/Green strategy.</p>\n\n<p>As an AWS Certified DevOps Engineer, which of the following options would you recommend as the most efficient solution to meet the given requirements?</p>\n",
          "answers": [
            "<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, start a CodeDeploy stage with a target being your ECS service</p>",
            "<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, create a new task definition automatically using CodeCommit and apply that task definition to the ECS service using a CloudFormation action</p>",
            "<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Create a CloudWatch Event Rule that will react to pushes to ECR and invoke CodeDeploy, the target of which should be the ECS cluster</p>",
            "<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables passed in through CodeBuild configuration, the values being those from your user. Upon the success of that CodeBuild stage, create a new task definition automatically using CodePipeline and apply that task definition to the ECS service using a CloudFormation action</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, start a CodeDeploy stage with a target being your ECS service</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>CodeBuild is a fully managed continuous integration service in the cloud. CodeBuild compiles source code, runs tests, and produces packages that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build. You can include a buildspec as part of the source code or you can define a buildspec when you create a build project.</p>\n\n<p>You can use CodeBuild to acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. You should note that acquiring ECR credentials must be done using IAM roles and CLI helpers on CodeBuild, not environment variables, especially not via your user access and secret key.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, create a new task definition automatically using CodeCommit and apply that task definition to the ECS service using a CloudFormation action</strong> - You should note that both CodeDeploy and CloudFormation can support blue/green deployment for ECS. However, you cannot create a new task definition using CodeCommit, so this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q6-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html</a></p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Create a CloudWatch Event Rule that will react to pushes to ECR and invoke CodeDeploy, the target of which should be the ECS cluster</strong> - CloudWatch Event Rule does not support CodeDeploy as a target, therefore CodeDeploy must be invoked from your CodePipeline.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q6-i3.jpg\"></p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables passed in through CodeBuild configuration, the values being those from your user. Upon the success of that CodeBuild stage, create a new task definition automatically using CodePipeline and apply that task definition to the ECS service using a CloudFormation action</strong> - As mentioned in the explanation above, ECR credentials must be acquired using IAM roles and CLI helpers on CodeBuild, not environment variables, especially not via your AWS access key ID and secret access key.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2020/05/aws-cloudformation-now-supports-blue-green-deployments-for-amazon-ecs/\">https://aws.amazon.com/about-aws/whats-new/2020/05/aws-cloudformation-now-supports-blue-green-deployments-for-amazon-ecs/</a></p>\n"
        }
      }
    ],
    "answers": {
      "67357100": [
        "d"
      ],
      "67357112": [
        "a"
      ],
      "67357148": [
        "a"
      ],
      "75949112": [
        "b"
      ],
      "75949132": [
        "b",
        "c"
      ],
      "82921316": [
        "a"
      ],
      "82921318": [
        "a"
      ],
      "82921342": [
        "a"
      ],
      "82921344": [
        "a"
      ],
      "82921366": [
        "a"
      ],
      "82921398": [
        "a"
      ],
      "82921414": [
        "a"
      ],
      "82921442": [
        "a"
      ],
      "115961503": [
        "d"
      ],
      "115961507": [
        "a"
      ],
      "134588427": [
        "a"
      ],
      "134588449": [
        "c"
      ],
      "134588473": [
        "c"
      ],
      "134588511": [
        "a"
      ],
      "138248149": [
        "b"
      ]
    }
  },
  {
    "id": "1770033194197",
    "date": "2026-02-02T11:53:14.197Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 9,
    "incorrect": 1,
    "unanswered": 0,
    "total": 10,
    "percent": 90,
    "duration": 899192,
    "questions": [
      {
        "id": 134588393,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. </p><p>As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?</p>",
          "answers": [
            "<p>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/ssm-patch-baselines.jpg\"></p><p><strong>Patch Manager</strong> uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p><strong>AWS Config</strong> provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements. Hence, the correct option is: <strong>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</strong></p><p>The option that says: <strong>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, it is stated in the scenario that the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC </strong>is incorrect because Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise, however, it does not check if your EC2 instances are using an approved AMI or not.</p><p>The option that says: <strong>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect because the AWS Shield Advanced service is more suitable in preventing DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 115961493,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company runs an application across thousands of EBS-backed Amazon EC2 instances. The company needs to ensure availability of the application and requires that instances are restarted when an EC2 instance retirement event is scheduled.</p><p>How can this a DevOps engineer automate this task?</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch alarm for EC2 status checks. Configure the alarm to trigger an Amazon SNS notification to the operations team and have them stop and start affected instances.</p>",
            "<p>Create a rule in Amazon EventBridge with AWS Health as the source and look for instance retirement scheduled events. Run an AWS Systems Manager automation document that stops and starts affected instances.</p>",
            "<p>Enable EC2 Auto Recovery on all instances. Configure an Amazon CloudWatch alarm with the alarm action set to Recover. Specify a time for recovery that is outside of business hours.</p>",
            "<p>Create a rule in Amazon EventBridge with Amazon EC2 as the source and look for EC2 instance state-change notifications that indicate the instance is shutting down. Run an AWS Systems Manager automation document that starts the affected instances.</p>"
          ],
          "explanation": "<p>An EC2 instance is scheduled for retirement when AWS detects an irreparable failure in the infrastructure that's hosting your instance. You are required to stop and then start the instance at your preferred time before the instance retirement date. Stopping and starting the instance moves the instance to another healthy host.</p><p>The best way to automate this process is to create a rule in Amazon EventBridge that looks for AWS Health events. The specific event is:</p><p>\u201cAWS_EC2_INSTANCE_RETIREMENT_SCHEDULED\u201d</p><p>When this event occurs EventBridge can trigger an AWS Systems Manager automation document that stops and starts the EC2 instances.</p><p><strong>CORRECT: </strong>\"Create a rule in Amazon EventBridge with AWS Health as the source and look for instance retirement scheduled events. Run an AWS Systems Manager automation document that stops and starts affected instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for EC2 status checks. Configure the alarm to trigger an Amazon SNS notification to the operations team and have them stop and start affected instances\" is incorrect.</p><p>Status checks do not inform us that an instance retirement event is scheduled, they let us know if there are issues that are affecting the instances or hosts.</p><p><strong>INCORRECT:</strong> \"Enable EC2 Auto Recovery on all instances. Configure an Amazon CloudWatch alarm with the alarm action set to Recover. Specify a time for recovery that is outside of business hours\" is incorrect.</p><p>Auto Recovery will recover an instance automatically, but this is not related to retirement events. You also cannot configure a time schedule in the alarm action.</p><p><strong>INCORRECT:</strong> \"Create a rule in Amazon EventBridge with Amazon EC2 as the source and look for EC2 instance state-change notifications that indicate the instance is shutting down. Run an AWS Systems Manager automation document that starts the affected instances\" is incorrect.</p><p>This would restart all instances that are shutdown, so the scope is too broad. We specifically want to target only the instances that are affected by retirement events.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-retirement/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-retirement/</a></p><p><a href=\"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html\">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 82921452,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A multi-national retail company is in the process of capturing all of its infrastructure as code using CloudFormation. The infrastructure inventory is huge and will contain a networking stack, an application stack, a data stack, and so on. Some teams are ready to move ahead with the process while others are lagging, and there is a desire to keep all the infrastructure version controlled.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?</p>\n",
          "answers": [
            "<p>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</p>",
            "<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</p>",
            "<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</p>",
            "<p>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</strong></p>\n\n<p>While using CloudFormation, you work with templates and stacks. You create templates to describe your AWS resources and their properties. When you use AWS CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's AWS CloudFormation template.</p>\n\n<p>In CloudFormation the best practice is to separate stacks into individual, separate logical components that have dependencies on each other. To link through these dependencies, the best is to use Exports and Imports. Each individual CloudFormation template must be a separate file.</p>\n\n<p>CloudFormation best practices:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</strong></p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</strong></p>\n\n<p>The issue with both these options is that different teams are working on different pieces of the infrastructure with their own timelines, so it's difficult to combine all elements of the infrastructure into a single master template. It's much better to have one template per logical element of the infrastructure that is owned by the respective team and then use outputs and exports to reference values in the stacks. Nested Stacks can be helpful if a component configuration (such as a Load Balancer) can be reused across many stacks.</p>\n\n<p><strong>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</strong> - Using outputs and exports for individual templates is much better than collaborating via pull requests at code repository level. Using individual templates gives ownership to the contributing team to make sure that the CloudFormation templates are always functional and ready to be referenced in other stacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n"
        }
      },
      {
        "id": 82921356,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A social media company is running its flagship application via an Auto-Scaling group (ASG) which has 15 EC2 instances spanning across 3 Availability Zones (AZs). The current average CPU utilization of the group sits at 15% off-peak time. During peak time, it goes all the way to 45%, and these peak times happen predictably during business hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How can you improve the instance utilization while reducing cost and maintaining application availability?</p>\n",
          "answers": [
            "<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</p>",
            "<p>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</p>",
            "<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</p>",
            "<p>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</strong></p>\n\n<p>With target tracking scaling policies, you choose a scaling metric and set a target value. Application Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value.</p>\n\n<p>Target tracking scaling policies for Amazon EC2 Auto Scaling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size that are specified by the scaling action. For the given use-case, you can create two separate scheduled actions that take care of the required minimum capacity during both peak and off-peak times.</p>\n\n<p>Here, we need a scaling policy that tracks a good CPU usage of 75% and adjusts the minimum desired capacity through scheduled actions so it doesn't disrupt the number of EC2 instances negatively at any time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</strong></p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</strong></p>\n\n<p>If a Lambda function terminates 9 instances because they're in an ASG, the desired capacity won't have changed and the ASG will re-create instances automatically. Therefore both these options are incorrect.</p>\n\n<p><strong>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</strong> - UpdatePolicy for CloudFormation cannot help define Scheduled Actions. There's a special ScheduledActions property for that.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n"
        }
      },
      {
        "id": 82921384,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.</p>\n\n<p>What can be done to improve the performance of the setup?</p>\n",
          "answers": [
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</p>",
            "<p>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong></p>\n\n<p>A golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can also add the web application as part of the golden AMI. You can think of it as an input base AMI for creating a standardized application-specific golden AMI.</p>\n\n<p>Once you create a golden AMI for a product (a product can be a standardized OS-AMI that you want to distribute to accounts in your organization or an application-specific AMI you want to let your business unit(s) deploy in their environment), you can validate whether the AMI meets your expectations, and choose to approve or reject the AMI.</p>\n\n<p>About the golden AMI pipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q66-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</strong> - The local cache warmup can unfortunately not be improved, as caching is dynamic and data may change over time. So creating an AMI with a copy of the local data cache just serves as a distractor.</p>\n\n<p><strong>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong> - You cannot migrate from ElastiCache to DynamoDB for the given use-case, as it's primarily a NoSQL database and not a caching solution (You could use DAX as a caching solution with DynamoDB). Besides, the existing database is RDS MySQL which is a relational database, so DynamoDB does not really fit into this mix.</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</strong> - You cannot use Lambda to configure the instance local cache at boot time as caching is dynamic and data may change over time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n"
        }
      },
      {
        "id": 82921440,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.</p>\n\n<p>Which of the following options would you suggest? (Select two)</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</p>",
            "<p>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</p>",
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>Create a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p><strong>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>Understand how a pipeline execution state change rule works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Here we are only interested in pipeline failures, so we need to choose <code>CodePipeline Pipeline Execution State Change</code>.\nFinally, CloudWatch Event rules do not support Slack as a target, therefore we must create a Lambda function for it.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i2.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</strong> - CloudWatch Event rules do not support Slack as a target, so this option is incorrect.</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>**\nCreate a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>Here we are only interested in pipeline failures, so we just need to choose <code>CodePipeline Pipeline Execution State Change</code>. Therefore both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n"
        }
      },
      {
        "id": 82921396,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at an e-commerce company is working with the in-house security team to improve the security workflow of the code release process. The DevOps team would like to initiate a 3rd party code vulnerability analysis tool for every push done to code in your CodeCommit repository. The code has to be sent via an external API.</p>\n\n<p>As an AWS Certified DevOps Engineer, how would you implement this most efficiently?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an AWS Lambda function that will request the code from CodeCommit, zip it and send it to the 3rd party API</p>",
            "<p>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an S3 bucket so that the code will be automatically zipped into S3. Create an S3 Event rule to trigger a Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API</p>",
            "<p>Create a CloudWatch Event rule on a schedule of 5 minutes that triggers a Lambda function that will check for new commits done on your CodeCommit repository. If new commits are detected, download and zip the code and then send it to the 3rd party API</p>",
            "<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an AWS Lambda function that will request the code from CodeCommit, zip it and send it to the 3rd party API</strong></p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p>\n\n<p>CloudWatch Events Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>For the given use-case, you can set up a CloudWatch Event rule for every push to the CodeCommit repository that would trigger the Lambda function configured as a target. The Lambda function would in turn request the code from CodeCommit, zip it and send it to the 3rd party API.</p>\n\n<p>CloudWatch Events Configuration:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i2.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an S3 bucket so that the code will be automatically zipped into S3. Create an S3 Event rule to trigger a Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API</strong> - CloudWatch Event Rules cannot have S3 buckets as a target. Although you can set an S3 trigger as a target, eventually you would still need to invoke the Lambda function via an S3 trigger to process the code via the API. Therefore it's efficient to directly invoke the Lambda function from the CloudWatch Event rule.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i3.jpg\"></p>\n\n<p><strong>Create a CloudWatch Event rule on a schedule of 5 minutes that triggers a Lambda function that will check for new commits done on your CodeCommit repository. If new commits are detected, download and zip the code and then send it to the 3rd party API</strong> - CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So this option is ruled out.</p>\n\n<p><strong>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected</strong> - The EC2 CodeCommit hook is a distractor and does not exist.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n"
        }
      },
      {
        "id": 99528201,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>An eCommerce company has operations in several countries around the world. The company runs an application in co-location facilities that uses Linux servers and a relational database running on MySQL. The application will be migrated to AWS and will include Amazon EC2 instances behind an Application Load Balancer in multiple AWS Regions. The database configuration has not yet been finalized.</p><p>A DevOps engineer has been asked to assist with determining the best solution for the database. The data includes product catalog information which must be served with low latency and customer purchase information which should be kept within each Region for compliance purposes.</p><p>Which database solution should the DevOps engineer recommend to meet these requirements with the LEAST changes to the application?</p>",
          "answers": [
            "<p>Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer purchase data.</p>",
            "<p>Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer purchase data.</p>",
            "<p>Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer purchase data.</p>",
            "<p>Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer purchase data.</p>"
          ],
          "explanation": "<p>The current database runs on MySQL which is a relational database. Therefore, to meet the requirement to minimize changes to the application the cloud solution for the database should also be a relational database. Otherwise significant changes to the application may be required. This rules out all answers that include DynamoDB as that service is a non-relational database.</p><p>Amazon Aurora provides read replicas for scaling read performance horizontally. These replicas can be within a Region or across Regions. The product catalog data can be provided at low latency within each AWS Region using cross-Region read replicas.</p><p>For the customer purchase data, this can be kept within each Region by implementing local Aurora database instances which can additionally have read replicas within the Region if additional read performance is required.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_22-38-09-545c686c23e14af309338aad4a3b523c.jpg\"><p><strong>CORRECT: </strong>\"Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer purchase data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer purchase data\" is incorrect.</p><p>RedShift is used for online analytics processing (OLAP) use cases as it is a data warehouse solution. In this case the solution calls for an online transaction processing (OLTP) type of database as it is processing transactions.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer purchase data\" is incorrect.</p><p>DynamoDB is a non-relational database, and the application is designed to work with a relational database. Using DynamoDB would require significant changes to the application.</p><p><strong>INCORRECT:</strong> \"Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer purchase data\" is incorrect.</p><p>As above, DynamoDB should not be used for this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>"
        }
      },
      {
        "id": 134588437,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A financial company has a total of over a hundred Amazon EC2 instances running across their development, testing, and production environments in AWS. Based on a recent IT review, the company initiated a new compliance rule that mandates a monthly audit of every Linux and Windows EC2 instances check for system performance issues. Each instance must have a logging function that collects various system details and retrieve custom metrics from installed applications or services. The DevOps team will periodically review these logs and analyze their contents using AWS Analytics tools, and the result will be stored in an S3 bucket. </p><p>Which is the MOST recommended way to collect and analyze logs from the instances with MINIMAL effort?</p>",
          "answers": [
            "<p>Configure and install the unified CloudWatch Logs agent in each Amazon EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
            "<p>Configure and install AWS SDK in each Amazon EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and analyze the log data of all instances using CloudWatch Logs Insights.</p>",
            "<p>Configure and install the AWS Systems Manager Agent (SSM Agent) in each EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
            "<p>Configure and install AWS Inspector Agent in each Amazon EC2 instance that will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all EC2 instances.</p>"
          ],
          "explanation": "<p>To collect logs from your Amazon EC2 instances and on-premises servers into CloudWatch Logs, AWS offers both a new unified CloudWatch agent, and an older CloudWatch Logs agent. It is recommended to use the unified CloudWatch agent, which has the following advantages:</p><p>- You can collect both logs and advanced metrics with the installation and configuration of just one agent.</p><p>- The unified agent enables the collection of logs from servers running Windows Server.</p><p>- If you are using the agent to collect CloudWatch metrics, the unified agent also enables the collection of additional system metrics, for in-guest visibility.</p><p>- The unified agent provides better performance.</p><p><img src=\"https://media.tutorialsdojo.com/public/LogsInsights-workflow_6AUG2023.png\"></p><p>CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you quickly and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p><p>CloudWatch Logs Insights includes a purpose-built query language with a few simple but powerful commands. CloudWatch Logs Insights provides sample queries, command descriptions, query autocompletion, and log field discovery to help you get started quickly. Sample queries are included for several types of AWS service logs.</p><p>Hence, the correct answer is: <strong>Configure and install the unified CloudWatch Logs agent in each Amazon EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</strong></p><p>The option that says: <strong>Configure and install AWS SDK in each Amazon EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and analyze the log data of all instances using CloudWatch Logs Insights</strong> is incorrect. Although this is a valid solution, this entails a lot of effort to implement as you have to allocate time to install the AWS SDK to each instance and develop a custom monitoring solution. Remember that the question is specifically looking for a solution that can be implemented with minimal effort. In addition, it is unnecessary and not cost-efficient to enable detailed monitoring in CloudWatch in order to meet the requirements of this scenario since this can be done using CloudWatch Logs.</p><p>The option that says: <strong>Configure and install the AWS Systems Manager Agent (SSM Agent) in each EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights</strong> is incorrect. Although this is also a valid solution, it is more efficient to use CloudWatch agent than an SSM agent. Manually connecting to an instance to view log files and troubleshoot an issue with SSM Agent is time-consuming hence, for more efficient instance monitoring, you can use the CloudWatch Agent instead to send the log data to Amazon CloudWatch Logs.</p><p>The option that says: <strong>Configure and install AWS Inspector Agent in each Amazon EC2 instance that will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all EC2 instances </strong>is incorrect because AWS Inspector is simply a security assessment service that only helps you in checking for unintended network accessibility of your EC2 instances and for vulnerabilities on those EC2 instances. Furthermore, setting up an Amazon CloudWatch dashboard is not suitable since it's primarily used for scenarios where you have to monitor your resources in a single view, even those resources that are spread across different AWS Regions. It is better to use CloudWatch Logs Insights instead since it enables you to interactively search and analyze your log data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html \">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>CloudWatch Agent vs SSM Agent vs Custom Daemon Scripts:</strong></p><p><a href=\"https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/?src=udemy\">https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/</a></p>"
        }
      },
      {
        "id": 75949098,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company is concerned about the security of their Amazon EC2 instances. They require an automated solution for identifying security vulnerabilities on the instances and notifying the security team. They also require an audit trail of all login activities on the EC2 instances.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Kinesis Client Library (KCL) to capture system logs and save them to an Amazon S3 bucket.</p>",
            "<p>Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture an audit trail using system logs and view login activity in the AWS CloudTrail console.</p>",
            "<p>Use AWS GuardDuty to detect vulnerabilities on the EC2 instances. Configure the AWS X-Ray daemon to gather trace data and add metrics to Amazon CloudWatch. View the audit trail of login activities in the CloudWatch console.</p>",
            "<p>Use Amazon Inspector to automatically detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and upload them to Amazon CloudWatch Logs.</p>"
          ],
          "explanation": "<p>Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. This is the best service to use for automatic detection of security vulnerabilities on the EC2 instances.</p><p>The unified CloudWatch agent enables you to collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances.</p><p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs. The system logs that are collected will include information on all login activities on the EC2 instances.</p><p><strong>CORRECT: </strong>\"Use Amazon Inspector to automatically detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and upload them to Amazon CloudWatch Logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Kinesis Client Library (KCL) to capture system logs and save them to an Amazon S3 bucket\" is incorrect.</p><p>Systems Manager Patch Manager can install patches to resolve vulnerabilities but does not provide automated detection of vulnerabilities. Instead, it scans to see if specific patches are installed or not. The KCL is used with Kinesis Data Streams for processing streaming data and does not collect system logs from EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture an audit trail using system logs and view login activity in the AWS CloudTrail console\" is incorrect.</p><p>As above, Systems Manager is not suitable for this task and does not capture auditing information by processing system logs.</p><p><strong>INCORRECT:</strong> \"Use AWS GuardDuty to detect vulnerabilities on the EC2 instances. Configure the AWS X-Ray daemon to gather trace data and add metrics to Amazon CloudWatch. View the audit trail of login activities in the CloudWatch console\" is incorrect.</p><p>GuardDuty is a threat detection service that monitors for malicious activity. It does not detect vulnerabilities on EC2 instances. The X-Ray service is used for gathering trace data for troubleshooting and understanding application performance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/inspector/features/\">https://aws.amazon.com/inspector/features/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-inspector/\">https://digitalcloud.training/amazon-inspector/</a></p>"
        }
      }
    ],
    "answers": {
      "75949098": [
        "d"
      ],
      "82921356": [
        "a"
      ],
      "82921384": [
        "b"
      ],
      "82921396": [
        "a"
      ],
      "82921440": [
        "a",
        "b"
      ],
      "82921452": [
        "a"
      ],
      "99528201": [
        "c"
      ],
      "115961493": [
        "b"
      ],
      "134588393": [
        "d"
      ],
      "134588437": [
        "a"
      ]
    }
  },
  {
    "id": "1770030376250",
    "date": "2026-02-02T11:06:16.250Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 17,
    "incorrect": 3,
    "unanswered": 0,
    "total": 20,
    "percent": 85,
    "duration": 5395989,
    "questions": [
      {
        "id": 138248229,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A multinational investment bank is implementing regulatory compliance checks over its AWS accounts. All API calls made on each AWS resource across the company's accounts must be monitored and tracked for auditing purposes. AWS CloudTrail will be used to monitor all API activities and detect sensitive security issues in the company's AWS accounts. The DevOps Team was assigned to come up with a solution to prevent CloudTrail from being disabled on some AWS accounts automatically.</p><p>What solution should be applied to ensure CloudTrail log deliveries experience the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of <code>Configuration changes</code>. By default, this managed rule will automatically remediate the accounts that disabled its CloudTrail.</p>",
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a <code>StopLogging</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>StartLogging</code> API on the resource ARN.</p>",
            "<p>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications.</p>",
            "<p>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a <code>DeleteTrail</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>CreateTrail</code> API on the resource ARN.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides <em>AWS managed rules</em>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly assess whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p><strong><img src=\"https://media.tutorialsdojo.com/public/td-aws-config-diagram-13Jan2025.png\"></strong>You can customize the behavior of a managed rule to suit your needs. For example, you can define the rule's scope to constrain which resources trigger an evaluation for the rule, such as EC2 instances or volumes. You can customize the rule's parameters to define attributes that your resources must have to comply with the rule. For example, you can customize a parameter to specify that your security group should block incoming traffic to a specific port number.</p><p>After you activate a rule, AWS Config compares your resources to the rule's conditions. After this initial evaluation, AWS Config continues to run evaluations each time one is triggered. The evaluation triggers are defined as part of the rule, and they can include the following types:</p><p><strong>Configuration changes</strong> \u2013 AWS Config triggers the evaluation when any resource that matches the rule's scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p><p><strong>Periodic</strong> \u2013 AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p><p>The cloudtrail-enabled checks whether AWS CloudTrail is enabled in your AWS account. Optionally, you can specify which S3 bucket, SNS topic, and Amazon CloudWatch Logs ARN to use.</p><p><img src=\"https://media.tutorialsdojo.com/aws-config-cloudtrail-enabled.JPG\"></p><p>Hence, the correct answer is: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a </strong><code><strong>StopLogging</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>StartLogging</strong></code><strong> API on the resource ARN.</strong></p><p>The option that says: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of </strong><code><strong>Configuration changes</strong></code><strong>. This managed rule will automatically remediate the accounts that disabled its CloudTrail </strong>is incorrect because, by default, AWS Config will not automatically remediate the accounts that disabled its CloudTrail. You must manually set this up using an Amazon EventBridge rule and a custom Lambda function that calls the StartLogging API to enable CloudTrail back again. Furthermore, the <code><strong>cloudtrail-enabled</strong></code> AWS Config managed rule is only available for the <code>periodic trigger</code> type and not <code>Configuration changes</code>.</p><p>The option that says: <strong>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications</strong> is incorrect. AWS Cloud Development Kit (AWS CDK) is only an open-source software development framework for building cloud applications and infrastructure using programming languages. It isn't used to check whether the CloudTrail is enabled in an AWS account.</p><p>The option that says: <strong>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a </strong><code><strong>DeleteTrail</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>CreateTrail</strong></code><strong> API on the resource ARN</strong> is incorrect. Instead, you should detect the <code>StopLogging</code> event and call the StartLogging API to enable CloudTrail again. The <code>DeleteTrail</code> and <code>CreateTrail</code> events, as their name implies, are simply for deleting and creating the trails respectively.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/\">https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/v2/guide/home.html\">https://docs.aws.amazon.com/cdk/v2/guide/home.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 75949112,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A startup company is launching a web application on AWS that uses Amazon EC2 instances behind an Application Load Balancer. The EC2 instances will store data in an Amazon RDS database and an Amazon DynamoDB table. The DevOps team require separate environments for development, testing, and production.</p><p>What is the MOST secure method of obtaining password credentials?</p>",
          "answers": [
            "<p>Configure access keys to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.</p>",
            "<p>Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Retrieve the database credentials from AWS Secrets Manager.</p>",
            "<p>Configure access keys to access AWS services. Retrieve the database credentials from an AWS Secrets Manager SecretString parameter.</p>",
            "<p>Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Store the database passwords in an encrypted config file in system metadata.</p>"
          ],
          "explanation": "<p>The most secure solution is to use a combination of an AWS IAM Instance Profile with a policy attached providing permissions to Amazon DynamoDB and AWS Secrets Manager for retrieving SecretString parameters for the Amazon RDS DB.</p><p>Instance profiles are far more secure compared to access keys are they leverage the AWS STS service to obtain temporary security credentials. No security credentials are stored on the EC2 instances when using this method.</p><p>For Amazon RDS you may need to use database connection credentials for authentication depending on your configuration. If this is the case you can securely store these credentials as SecretString (encrypted) parameters in AWS Secrets Manager. Your application can then issue API calls to Secrets Manager to securely retrieve the encrypted parameters when authentication is needed.</p><p><strong>CORRECT: </strong>\"Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Retrieve the database credentials from AWS Secrets Manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Store the database passwords in an encrypted config file in system metadata\" is incorrect.</p><p>Database passwords should not be stored in system metadata as this would be very insecure.</p><p><strong>INCORRECT:</strong> \"Configure access keys to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter\" is incorrect.</p><p>Access keys are less secure compared to using instance profiles as with access keys the actual access key ID and secret access key are stored in plaintext on the EC2 instance file system.</p><p><strong>INCORRECT:</strong> \"Configure access keys to access AWS services. Retrieve the database credentials from an AWS Secrets Manager SecretString parameter\" is incorrect.</p><p>As above, access keys should not be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html</a></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_CreateSecret.html\">https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_CreateSecret.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>"
        }
      },
      {
        "id": 143860765,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is troubleshooting problems with an application which uses AWS Lambda to process messages in an Amazon SQS standard queue. The function sometimes fails to process the messages in the queue. The engineer needs to analyze the events to determine the cause of the issue and update the function code.</p><p>Which action should the engineer take to achieve this outcome?</p>",
          "answers": [
            "<p>Enable FIFO support for the queue to preserve ordering of the messages.</p>",
            "<p>Configure a redrive policy to move the messages to a dead-letter queue.</p>",
            "<p>Enable long-polling by increasing WaitTimeSeconds parameter.</p>",
            "<p>Configure a delay queue by increasing the DelaySeconds parameter.</p>"
          ],
          "explanation": "<p>You can configure a dead-letter queue (DLQ) by specifying a redrive policy on the SQS queue. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p><p><strong>CORRECT: </strong>\"Configure a redrive policy to move the messages to a dead-letter queue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable FIFO support for the queue to preserve ordering of the messages\" is incorrect.</p><p>You cannot enable FIFO on a standard queue, and it does not help with this issue anyway.</p><p><strong>INCORRECT:</strong> \"Enable long-polling by increasing WaitTimeSeconds parameter\" is incorrect.</p><p>Long polling just helps with efficiency of API calls as it waits for messages to appear in the queue rather than returning an immediate response. This does not assist with isolating messages for analysis.</p><p><strong>INCORRECT:</strong> \"Configure a delay queue by increasing the DelaySeconds parameter\" is incorrect.</p><p>A delay queue simply delays visibility of the messages for a specified time. This does not assist with this issue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>"
        }
      },
      {
        "id": 138248237,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The Development team of a leading IT consultancy company would like to add a manual approval action before their new application versions are deployed to their production environment. The approval action must be strictly enforced even if the unit and integration tests are all successful. They have set up a pipeline using CodePipeline to orchestrate the workflow of their continuous integration and continuous delivery processes. The new versions of the application are built using CodeBuild and are deployed to a fleet of Amazon EC2 instances using CodeDeploy. </p><p>Which of the following provides the SIMPLEST and the MOST cost-effective solution?</p>",
          "answers": [
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</p>",
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>"
          ],
          "explanation": "<p>You can automate your release process by using AWS CodePipeline to test your code and run your builds with CodeBuild. You can create reports in CodeBuild that contain details about tests that are run during builds.</p><p>You can create tests such as unit tests, configuration tests, and functional tests. The test file format can be JUnit XML or Cucumber JSON. Create your test cases with any test framework that can create files in one of those formats (for example, Surefire JUnit plugin, TestNG, and Cucumber). To create a test report, you add a report group name to the buildspec file of a build project with information about your test cases. When you run the build project, the test cases are run and a test report is created. You do not need to create a report group before you run your tests. If you specify a report group name, CodeBuild creates a report group for you when you run your reports. If you want to use a report group that already exists, you specify its ARN in the buildspec file.</p><p>In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p><img src=\"https://media.tutorialsdojo.com/public/pipeline.png\"></p><p>If the action is approved, the pipeline execution resumes. If the action is rejected\u2014or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping\u2014the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p>Hence, the correct answer is: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</strong></p><p>The option that says: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. You can just simply set up a manual approval action instead of creating a custom action. That takes a lot of effort to configure including the development of a custom job worker.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. It is tedious to automatically perform the unit and integration tests using AWS Step Functions. You can just use CodeBuild to handle all of the tests.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. This solution entails an additional burden to install, configure and launch a third-party CI/CD tool in Amazon EC2. A more simple solution is to just use CodeBuild for tests.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p>"
        }
      },
      {
        "id": 75949064,
        "correct_response": [
          "b",
          "d",
          "e"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer builds an artifact locally and then uploads it to an Amazon S3 bucket. The application has a local cache that must be cleared as part of the deployment. The engineer executes a command to do this, retrieves the artifact from Amazon S3, and unzips the artifact to complete the deployment.</p><p>The engineer wants to migrate to an automated CI/CD solution and incorporate checks to stop and roll back the deployment in the event of a failure. This requires tracking the progression of the deployment.</p><p>Which combination of actions will accomplish this? (Select THREE.)</p>",
          "answers": [
            "<p>Check the code into a code repository. On each pull into master use Amazon CloudWatch Events to trigger an AWS Lambda function that builds the artifact and uploads it to Amazon S3.</p>",
            "<p>Write a custom script that clears the cache and specify the script in the BeforeInstall lifecycle hook in the AppSpec file.</p>",
            "<p>Add user data to the Amazon EC2 instances that contains script to clear the cache. Once deployed, test the application. If it is not successful, deploy it again.</p>",
            "<p>Set up an AWS CodePipeline pipeline to deploy the application. Check the code into a code repository as a source for the pipeline.</p>",
            "<p>Use AWS CodeBuild to build the artifact and upload it to Amazon S3. Use AWS CodeDeploy to deploy the artifact to the Amazon EC2 instances.</p>",
            "<p>Use AWS Systems Manager to download the artifact from Amazon S3 and deploy it to all the EC2 instances.</p>"
          ],
          "explanation": "<p>The engineer wants to build an automated CI/CD pipeline. Therefore, the best solution is to use a code repository such as CodeCommit for committing the code. Once committed a CodePipeline will automatically pick up the changes and initiate CodeBuild which will build the artifacts and upload the S3.</p><p>After the build artifact has been uploaded CodeDeploy can then be used to deploy the application. The AppSpec file is used by CodeDeploy during deployments. The engineer should add the script to clear the cache to the BeforeInstall lifecycle hook, so it is executed before the install occurs.</p><p><strong>CORRECT: </strong>\"Write a custom script that clears the cache and specify the script in the BeforeInstall lifecycle hook in the AppSpec file\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Set up an AWS CodePipeline pipeline to deploy the application. Check the code into a code repository as a source for the pipeline\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS CodeBuild to build the artifact and upload it to Amazon S3. Use AWS CodeDeploy to deploy the artifact to the Amazon EC2 instances\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check the code into a code repository. On each pull into master use Amazon CloudWatch Events to trigger an AWS Lambda function that builds the artifact and uploads it to Amazon S3\" is incorrect.</p><p>A better solution is to use CodePipeline which is designed for automating CI/CD pipelines and CodeBuild for building the artifact.</p><p><strong>INCORRECT:</strong> \"Add user data to the Amazon EC2 instances that contains script to clear the cache. Once deployed, test the application. If it is not successful, deploy it again\" is incorrect.</p><p>User data only runs when the instance is first started so is not useful for running any commands after that time.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to download the artifact from Amazon S3 and deploy it to all of the EC2 instances\" is incorrect.</p><p>Systems Manager is not suitable for deploying application updates and CodeDeploy should be used instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 138248181,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company would like to set up an audit process to ensure that the enterprise application is running exclusively on Amazon EC2 Dedicated Hosts. The company is also concerned about the increasing costs of its application software licensing from its third-party vendor. To meet the compliance requirement, a DevOps Engineer must create a workflow to audit the enterprise applications hosted in its Amazon VPC.</p><p>Which of the following options should the Engineer implement to satisfy the requirement with the LEAST administrative overhead?</p>",
          "answers": [
            "<p>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the <code>PutComplianceItems</code> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the <code>ListComplianceSummaries</code> API action.</p>",
            "<p>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the <code>inspector-scheduled-run</code> blueprint.</p>",
            "<p>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the <code>config-rule-change-triggered</code> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</p>",
            "<p>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data.</p>"
          ],
          "explanation": "<p>You can use <strong>AWS Config</strong> to record configuration changes for Dedicated Hosts, and instances that are launched, stopped, or terminated on them. You can then use the information captured by AWS Config as a data source for license reporting.</p><p>AWS Config records configuration information for Dedicated Hosts and instances individually and pairs this information through relationships. There are three reporting conditions:</p><p>- AWS Config recording status \u2014 When On, AWS Config is recording one or more AWS resource types, which can include Dedicated Hosts and Dedicated Instances. To capture the information required for license reporting, verify that hosts and instances are being recorded with the following fields.</p><p>- Host recording status \u2014 When Enabled, the configuration information for Dedicated Hosts is recorded.</p><p>- Instance recording status \u2014 When Enabled, the configuration information for Dedicated Instances is recorded.</p><p>If any of these three conditions are disabled, the icon in the Edit Config Recording button is red. To derive the full benefit of this tool, ensure that all three recording methods are enabled. When all three are enabled, the icon is green. To edit the settings, choose Edit Config Recording. You are directed to the <em>Set up AWS Config </em>page in the AWS Config console, where you can set up AWS Config and start recording for your hosts, instances, and other supported resource types. AWS Config records your resources after it discovers them, which might take several minutes.</p><p>After AWS Config starts recording configuration changes to your hosts and instances, you can get the configuration history of any host that you have allocated or released and any instance that you have launched, stopped, or terminated. For example, at any point in the configuration history of a Dedicated Host, you can look up how many instances are launched on that host, along with the number of sockets and cores on the host. For any of those instances, you can also look up the ID of its Amazon Machine Image (AMI). You can use this information to report on licensing for your own server-bound software that is licensed per-socket or per-core.</p><p>You can view configuration histories in any of the following ways.</p><p>- By using the AWS Config console. For each recorded resource, you can view a timeline page, which provides a history of configuration details. To view this page, choose the gray icon in the Config Timeline column of the Dedicated Hosts page.</p><p>- By running AWS CLI commands. First, you can use the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/configservice/list-discovered-resources.html\">list-discovered-resources</a> command to get a list of all hosts and instances. Then, you can use the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/configservice/get-resource-config-history.html#get-resource-config-history\">get-resource-config-history</a> command to get the configuration details of a host or instance for a specific time interval.</p><p>- By using the AWS Config API in your applications. First, you can use the <a href=\"https://docs.aws.amazon.com/config/latest/APIReference/API_ListDiscoveredResources.html\">ListDiscoveredResources</a> action to get a list of all hosts and instances. Then, you can use the <a href=\"https://docs.aws.amazon.com/config/latest/APIReference/API_GetResourceConfigHistory.html\">GetResourceConfigHistory</a> action to get the configuration details of a host or instance for a specific time interval.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-AWS-Config-Status-02-05-2025.png\"></p><p>Hence, the correct answer is: <strong>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the </strong><code><strong>config-rule-change-triggered</strong></code><strong> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</strong></p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the </strong><code><strong>PutComplianceItems</strong></code><strong> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the </strong><code><strong>ListComplianceSummaries</strong></code><strong> API action </strong>is incorrect because the AWS Systems Manager Configuration Compliance service is primarily used to scan your fleet of managed instances for patch compliance and configuration inconsistencies. A better solution is to use AWS Config to record the status of your Dedicated Hosts.</p><p>The option that says: <strong>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the </strong><code><strong>inspector-scheduled-run</strong></code><strong> blueprint</strong> is incorrect because Amazon Inspector is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not capable of recording the status of your EC2 instances nor detect if they are configured as a Dedicated Host.</p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data </strong>is incorrect. Although this may be a possible solution, it entails a lot of administrative effort in comparison to just using AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html \">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom \">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/\">https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and AWS Config Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-config/\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 75949148,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer needs to stream data from Amazon CloudWatch Logs to a VPC-based Amazon OpenSearch Service cluster. The DevOps engineer creates an OpenSearch subscription filter for the OpenSearch cluster. However, the DevOps engineer notices that the logs are not visible in Amazon OpenSearch.</p><p>What should the DevOps engineer do to solve this problem?</p>",
          "answers": [
            "<p>Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role.</p>",
            "<p>Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch.</p>",
            "<p>Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role.</p>",
            "<p>Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch.</p>"
          ],
          "explanation": "<p>The configuration requires that the Lambda IAM execution role that is specified in the CloudWatch Logs configuration must have the trust relationship configured to allow lambda.amazonaws.com to assume the role. The AWSLambdaVPCAccessExecutionRole policy must also be added to the function to allow access to the VPC-based OpenSearch cluster.</p><p><strong>CORRECT: </strong>\"Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role\" is incorrect.</p><p>The trust relationship is defined in the role attached to the CloudWatch Logs configuration.</p><p><strong>INCORRECT:</strong> \"Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch\" is incorrect.</p><p>This policy should be attached to the Lambda IAM execution role, not the OpenSearch cluster.</p><p><strong>INCORRECT:</strong> \"Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch\" is incorrect.</p><p>The subscription filter is used instead of using an export which is a better and more automated solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/\">https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-opensearch/\">https://digitalcloud.training/amazon-opensearch/</a></p>"
        }
      },
      {
        "id": 75949146,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
          "answers": [
            "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
            "<p>The new AZ has not been enabled for the ALB.</p>",
            "<p>The AMI has not been updated to include the new AZ.</p>",
            "<p>There are no subnets associated with the new AZ.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 134588451,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A custom web dashboard has been developed for the company that displays all instances running on AWS, including the details of each instance. The web application relies on an Amazon DynamoDB table that is updated whenever a new instance is created or terminated. Several auto-scaling groups of Amazon EC2 instances are in use, and there is a need for an effective method to update the DynamoDB table whenever an instance is created or terminated.</p><p>Which of the following steps should be implemented?</p>",
          "answers": [
            "<p>Create a custom script that runs on the instances that will run on scale-in and scale-out events to update the DynamoDB table with the necessary details.</p>",
            "<p>Configure an Amazon EventBridge target for your auto scaling Group that will trigger an AWS Lambda function when a lifecycle action occurs. Configure the function to update the DynamoDB table with the necessary instance details.</p>",
            "<p>Configure an Amazon CloudWatch alarm that will monitor the number of instances on your auto scaling group and trigger an AWS Lambda function to update the DynamoDB table with the necessary details.</p>",
            "<p>Define an AWS Lambda function as a notification target for the lifecycle hook for the auto scaling group. In the event of a scale-out or scale-in, the lifecycle hook will trigger the Lambda function to update the DynamoDB table with the necessary details.</p>"
          ],
          "explanation": "<p>Adding lifecycle hooks to your Auto Scaling group gives you greater control over how instances launch and terminate. Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them.</p><p>For example, your newly launched instance completes its startup sequence, and a lifecycle hook pauses the instance. While the instance is in a wait state, you can install or configure software on it, making sure that your instance is fully ready before it starts receiving traffic. For another example of the use of lifecycle hooks, when a scale-in event occurs, the terminating instance is first deregistered from the load balancer (if the Auto Scaling group is being used with Elastic Load Balancing). Then, a lifecycle hook pauses the instance before it is terminated. While the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p><p><img alt=\"Auto Scaling group - Lifecycle hooks\" height=\"636\" src=\"https://media.tutorialsdojo.com/public/lifecycle_hooks_2AUG2023.png\" width=\"1000\"></p><p>After you add lifecycle hooks to your Auto Scaling group, the workflow is shown as follows:</p><ol><li><p>The Auto Scaling group responds to scale-out events by launching instances and scale-in events by terminating instances.</p></li><li><p>The lifecycle hook puts the instance into a wait state (<code>Pending:Wait</code> or <code>Terminating:Wait</code>). The instance is paused until you continue or the timeout period ends.</p></li><li><p>You can perform a custom action using one or more of the following options:</p></li></ol><p>- Define an Amazon EventBridge (CloudWatch Events) target to invoke a Lambda function when a lifecycle action occurs. The Lambda function is invoked when Amazon EC2 Auto Scaling submits an event for a lifecycle action to EventBridge. The event contains information about the instance that is launching or terminating, and a token that you can use to control the lifecycle action.</p><p>- Define a notification target for the lifecycle hook. Amazon EC2 Auto Scaling sends a message to the notification target. The message contains information about the instance that is launching or terminating and a token that you can use to control the lifecycle action.</p><p>- Create a script that runs on the instance as the instance starts. The script can control the lifecycle action using the ID of the instance on which it runs.</p><p>4. By default, the instance remains in a wait state for one hour, and then the Auto Scaling group continues the launch or terminate process (<code>Pending:Proceed</code> or <code>Terminating:Proceed</code>). If you need more time, you can restart the timeout period by recording a heartbeat. If you finish before the timeout period ends, you can complete the lifecycle action, which continues the launch or termination process.</p><p>Hence, the correct answer is: <strong>Configure an Amazon EventBridge target for your auto scaling Group that will trigger an AWS Lambda function when a lifecycle action occurs. Configure the function to update the DynamoDB table with the necessary instance details.</strong></p><p>The option that says: <strong>Create a custom script that runs on the instances that will run on scale-in and scale-out events to update the DynamoDB table with the necessary details </strong>is incorrect. Although this is possible, it will be harder to execute since a custom script needs to be developed first and will be run only at instance creation and termination.</p><p>The option that says: <strong>Define an AWS Lambda function as a notification target for the lifecycle hook for the auto scaling group. In the event of a scale-out or scale-in, the lifecycle hook will trigger the Lambda function to update the DynamoDB table with the necessary details</strong> is incorrect because you cannot use AWS Lambda as a notification target for the lifecycle hook of an Auto Scaling group. You can only configure Amazon EventBridge, Amazon SNS, or Amazon SQS as notification targets.</p><p>The option that says: <strong>Configure an Amazon CloudWatch alarm that will monitor the number of instances on your auto scaling group and trigger an AWS Lambda function to update the DynamoDB table with the necessary details</strong> is incorrect because this option doesn't track the lifecycle action of the Auto Scaling group. In this scenario, it's better to create a lifecycle hook integrated with Amazon EventBridge and Lambda instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#adding-lifecycle-hooks \">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#adding-lifecycle-hooks</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-overview\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-overview</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 82921334,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.</p>\n\n<p>How can you do this most securely?</p>\n",
          "answers": [
            "<p>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</p>",
            "<p>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>",
            "<p>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</p>",
            "<p>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</strong></p>\n\n<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.</p>\n\n<p>You can use Secrets Manager to natively rotate credentials for Amazon Relational Database Service (RDS), Amazon DocumentDB, and Amazon Redshift. You can extend Secrets Manager to rotate other secrets, such as credentials for Oracle databases hosted on EC2 or OAuth refresh tokens, by modifying sample AWS Lambda functions available in the Secrets Manager documentation.</p>\n\n<p>To access PostgreSQL, you can use database credentials and they're best stored in Secrets Manager from a security best-practices perspective. Access to Secrets Manager itself is regulated using an IAM role with the requisite policy. You must write this IAM policy permitting your application on EC2 instances to access specific secrets. Then, in the application source code, you can replace secrets in plain text with code to retrieve these secrets programmatically using the Secrets Manager APIs. To access the DynamoDB table, you should also add the appropriate policy to this IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - As mentioned in the explanation above, Secrets Manager does NOT support DynamoDB, so this option is incorrect.</p>\n\n<p><strong>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</strong> - It's not recommended to store RDS credentials in a DynamoDB, as it can be accessed by everyone who has access to the underlying table. This constitutes a serious security threat.</p>\n\n<p><strong>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - Storing IAM user credentials in Secrets Manager is a distractor as IAM user credentials are not required to build a solution for this use-case. You can just use an IAM instance role for EC2 to access Secrets Manager.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/faqs/\">https://aws.amazon.com/secrets-manager/faqs/</a></p>\n"
        }
      },
      {
        "id": 138248159,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company has a suite of applications that are hosted in AWS and each app has its own AMI. Currently, a new AMI must be manually created and deployed to the server if there is a new application version. A DevOps engineer was instructed to automate the process of generating the AMIs to streamline the company's CI/CD workflow. The ID of the newly created AMI must be stored in a centralized location where other build pipelines can programmatically access it.</p><p>Which of the following is the MOST cost-effective way to accomplish this requirement with the LEAST amount of overhead?</p>",
          "answers": [
            "<p>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store.</p>",
            "<p>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</p>",
            "<p>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket.</p>",
            "<p>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table.</p>"
          ],
          "explanation": "<p><strong>Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p><br></p><p>Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machine Images (AMIs) and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for various reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img alt=\"Custom AMI\" height=\"771\" src=\"https://media.tutorialsdojo.com/public/custom_ami_1.gif\" width=\"1000\"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that the Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</strong></p><p>The option that says: <strong>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store </strong>is incorrect because manually customizing the image using an interactive shell and downloading each application image in an OVF file will simply entails a lot of effort. It is also better to use the AWS Systems Manager Automation instead of creating a new pipeline in AWS CodePipeline.</p><p>The option that says: <strong>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket </strong>is incorrect. Although you can technically generate an AMI using an EBS volume snapshot, this process is still tedious and entails a lot of configuration. Using the AWS Systems Manager Automation to generate the AMIs is a more suitable solution.</p><p>The option that says: <strong>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table </strong>is incorrect. Although this may work, this solution will only costs more to maintain than other options since it uses an EC2 instance and an Amazon DynamoDB table. There is also an associated overhead in configuring and using Packer for generating the AMIs and preparing the Jenkins pipeline.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/\">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248177,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A technology company is planning to develop its custom online forum that covers various AWS-related technologies. They are planning to use AWS Fargate to host the containerized application and Amazon DynamoDB as its data store. The DevOps team is instructed to define the schema of the DynamoDB table with the required indexes, partition key, sort key, projected attributes, and others. To minimize cost, the schema must support certain search operations using the least provisioned read capacity units. A <code>Thread</code> attribute contains the user comments in JSON format. The sample data set is shown in the diagram below:</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/LSI_01.png \"></p><p><br></p><p>The online forum should support searches within <code>ForumName</code> attribute for items where the <code>Subject</code> begins with a particular letter, such as 'a' or 'b'. It should allow fetches of items within the given <code>LastPostDateTime</code> time frame as well as the capability to return the threads that have been posted within the last quarter. </p><p>Which of the following schema configuration meets the above requirements?</p>",
          "answers": [
            "<p>Set the <code>Subject</code> attribute as the primary key and <code>ForumName</code> as the sort key. Create a Local Secondary Index with <code>LastPostDateTime</code> as the sort key and the <code>Thread</code> as a projected attribute.</p>",
            "<p>Set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key. Create a Local Secondary Index with <code>LastPostDateTime</code> as the sort key and the <code>Thread</code> as a projected attribute.</p>",
            "<p>Set the <code>Subject</code> attribute as the primary key and <code>ForumName</code> as the sort key. Create a Global Secondary Index with <code>Thread</code> as the sort key and <code>LastPostDateTime</code> as a projected attribute.</p>",
            "<p>Set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key. Create a Global Secondary Index with <code>Thread</code> as the sort key and fetch operations for <code>LastPostDateTime</code>.</p>"
          ],
          "explanation": "<p><strong>Amazon DynamoDB</strong> provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table and issue Query or Scan requests against these indexes.</p><p>A secondary index is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations. You can retrieve data from the index using a Query, in much the same way as you use Query with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns.</p><p>DynamoDB supports two types of secondary indexes:</p><p>- <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">Global secondary index</a> \u2014 an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions.</p><p>- <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">Local secondary index</a> \u2014 an index that has the same partition key as the base table, but a different sort key. A local secondary index is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.</p><p>A <strong>local secondary index</strong> maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.</p><p>Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to <code>Scan</code> the entire <em>Thread </em>table and discard any posts that were not within the specified time frame. With a local secondary index, a <code>Query</code> operation could use <em>LastPostDateTime</em> as a sort key and find the data quickly.</p><p>In the provided scenario, you can create a local secondary index named <em>LastPostIndex </em>to meet the requirements. Note that the partition key is the same as that of the <em>Thread</em> table, but the sort key is <em>LastPostDateTime </em>as shown in the diagram below:</p><p><img src=\"https://media.tutorialsdojo.com/public/LSI_02.png\">With <code>LastPostIndex</code>, an application could use <code>ForumName</code> and <code>LastPostDateTime</code> as query criteria. However, to retrieve any additional attributes, DynamoDB must perform additional read operations against the <code>Thread</code> table. These extra reads are known as <em>fetches</em>, and they can increase the total amount of provisioned throughput required for a query.</p><p>Suppose that you wanted to populate a webpage with a list of all the threads in \"S3\" and the number of replies for each thread, sorted by the last reply date/time beginning with the most recent reply. To populate this list, you would need the following attributes:</p><p><code>-Subject</code></p><p><code>-Replies</code></p><p><code>-LastPostDateTime</code></p><p>The most efficient way to query this data and to avoid fetch operations would be to project the <code>Replies</code> attribute from the table into the local secondary index, as shown in this diagram.</p><p><img src=\"https://media.tutorialsdojo.com/public/LSI_03.png\">DynamoDB stores all of the items with the same partition key value contiguously. In this example, given a particular ForumName, a Query operation could immediately locate all of the threads for that forum. Within a group of items with the same partition key value, the items are sorted by sort key value. If the sort key (Subject) is also provided in the query, DynamoDB can narrow down the results that are returned \u2014 for example, returning all of the threads in the \"S3\" forum that have a Subject beginning with the letter \"a\".</p><p>A <em>projection</em> is the set of attributes that is copied from a table into a secondary index. The partition key and sort key of the table are always projected into the index; you can project other attributes to support your application's query requirements. When you query an index, Amazon DynamoDB can access any attribute in the projection as if those attributes were in a table of their own.</p><p>Hence, the correct answer is: <strong>Set the </strong><code><strong>ForumName</strong></code><strong> attribute as the primary key and </strong><code><strong>Subject</strong></code><strong> as the sort key. Create a Local Secondary Index with </strong><code><strong>LastPostDateTime</strong></code><strong> as the sort key and the </strong><code><strong>Thread</strong></code><strong> as a projected attribute.</strong></p><p>The option that says: <strong>Set the </strong><code><strong>Subject</strong></code><strong> attribute as the primary key and </strong><code><strong>ForumName</strong></code><strong> as the sort key. Create a Local Secondary Index with </strong><code><strong>LastPostDateTime</strong></code><strong> as the sort key and the </strong><code><strong>Thread</strong></code><strong> as a projected attribute </strong>is incorrect because the scenario says that the online forum should support searches within <code>ForumName</code> attribute for items where the <code>Subject</code> begins with a particular letter. DynamoDB stores all of the items with the same partition key value contiguously. In this example, given a particular ForumName, a Query operation could immediately locate all of the threads for that forum. Within a group of items with the same partition key value, the items are sorted by sort key value. If the sort key (Subject) is also provided in the query, DynamoDB can narrow down the results that are returned\u2014for example, returning all of the threads in the \"S3\" forum that have a Subject beginning with the letter \"a\". Hence, you should set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key instead.</p><p>The option that says: <strong>Set the </strong><code><strong>ForumName</strong></code><strong> attribute as the primary key and </strong><code><strong>Subject</strong></code><strong> as the sort key. Create a Global Secondary Index with </strong><code><strong>Thread</strong></code><strong> as the sort key and fetch operations for </strong><code><strong>LastPostDateTime<em> </em></strong></code>is incorrect because using a fetches operation can increase the total amount of provisioned throughput required for a query. Remember that the scenario mentioned that the schema must support certain search operations using the least provisioned read capacity units to minimize cost. In addition, you should create an LSI instead of GSI.</p><p>The option that says: <strong>Set the </strong><code><strong>Subject</strong></code><strong> attribute as the primary key and </strong><code><strong>ForumName</strong></code><strong> as the sort key. Create a Global Secondary Index with </strong><code><strong>Thread</strong></code><strong> as the sort key and </strong><code><strong>LastPostDateTime</strong></code><strong> as a projected attribute </strong>is incorrect because you should use a Local Secondary Index instead. You should also set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>"
        }
      },
      {
        "id": 138248245,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An organization has several accounts managed within AWS Organizations. The DevOps Engineer in charge of the network AWS account has established an AWS Transit Gateway to direct all of the company's outgoing traffic, which is then directed through a firewall appliance for inspection. The firewall appliance logs all types of event severities such as <code>CRITICAL</code>, <code>ERROR</code>, <code>HIGH</code>, <code>MID</code>, <code>LOW</code>, and <code>ERROR</code> which are all sent to Amazon CloudWatch Logs.</p><p>During a recent security audit, no alerts were discovered for events that could cause disruptions to business continuity. The company has required that the security team receive an alert for immediate remediation when any <code>CRITICAL</code> events occur.</p><p>Which of the following is the MOST suitable solution that the DevOps Engineer should implement?</p>",
          "answers": [
            "<p>Monitor flow logs with Amazon Inspector to the network AWS account. Set up an EventBridge event rule in CloudWatch that will be triggered by <code>CRITICAL</code> Inspector events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic.</p>",
            "<p>Set up a CloudWatch metric filter that searches for <code>CRITICAL</code> events. Create a custom metric for the findings, then associate a CloudWatch alarm that will send a notification to an SNS topic. Subscribe the security team's email address to the SNS topic.</p>",
            "<p>Utilize AWS Firewall Manager to enforce uniform policies across all accounts. Set up an EventBridge event rule in CloudWatch that will be triggered by <code>CRITICAL</code> Firewall Manager events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic.</p>",
            "<p>Set up a CloudWatch Synthetics canary to monitor the firewall status. If the firewall enters a <code>CRITICAL</code> state or logs a <code>CRITICAL</code> event, configure a CloudWatch alarm to publish to an SNS topic. Subscribe the security team's email address to the SNS topic.</p>"
          ],
          "explanation": "<p>The ability to search and filter the log data arriving at CloudWatch Logs is achieved by creating one or more <strong>metric filters</strong>.</p><p><strong>Metric filters</strong> define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-cloudwatch-metric-filter.png\"></p><p>When creating a metric from a log filter, it has the capability to assign dimensions and a unit to the metric. If specifying a unit, be sure to specify the correct one when creating the filter. Changing the unit for the filter later will have no effect.</p><p>In this scenario, the firewall appliance already pushes logs to <strong>CloudWatch Logs</strong>. We can leverage <strong>CloudWatch metric filter </strong>to search and filter for the CRITICAL events and set a <strong>CloudWatch alarm </strong>to send notifications to the security team.</p><p>Hence, the correct answer is the option that says: <strong>Set up a CloudWatch metric filter that searches for </strong><code><strong>CRITICAL</strong></code><strong> events. Create a custom metric for the findings, then associate a CloudWatch alarm that will send a notification to an SNS topic. Subscribe the security team's email address to the SNS topic.</strong></p><p>The option that says: <strong>Set up a CloudWatch Synthetics canary to monitor the firewall status. If the firewall enters a </strong><code><strong>CRITICAL</strong></code><strong> state or logs a </strong><code><strong>CRITICAL</strong></code><strong> event, configure a CloudWatch alarm to publish to an SNS topic. Subscribe the security team's email address to the SNS topic </strong>is incorrect because CloudWatch Synthetics canary is used to create canaries, configurable scripts that run on a schedule, to monitor endpoints and APIs. In the scenario, the firewall appliance is already pushing to CloudWatch Logs. Metric filter is a much more suitable solution as it can filter the CRITICAL events.</p><p>The option that says: <strong>Monitor flow logs with Amazon Inspector to the network AWS account. Set up an EventBridge event rule in CloudWatch that will be triggered by </strong><code><strong>CRITICAL</strong></code><strong> Inspector events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic </strong>is incorrect because Amazon Inspector is a vulnerability management service that is commonly used to scan AWS workloads for software vulnerabilities and unintended network exposure. This service does not filter logs.</p><p>The option that says: <strong>Utilize AWS Firewall Manager to enforce uniform policies across all accounts. Set up an EventBridge event rule in CloudWatch that will be triggered by </strong><code><strong>CRITICAL</strong></code><strong> Firewall Manager events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic</strong> is incorrect because AWS Firewall Manager is primarily used to centrally configure and manage firewall rules across accounts and applications in AWS Organizations. It does not have a filter capability like CloudWatch metric filter.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html#search-filter-concepts\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html#search-filter-concepts</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_alarm_log_group_metric_filter.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_alarm_log_group_metric_filter.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 138248227,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A startup software company has several application teams that develop API services for its business. Each application team is responsible for services separated on different AWS accounts. The VPC of each AWS account was initially provisioned with a <code>192.168.0.0/24</code> CIDR block. The services are deployed on Amazon EC2 instances accessed on a secure HTTPS public endpoint of an Application Load Balancer. Integration between the services routes externally to the public internet.</p><p>As part of a security audit, there is a recommendation from the security team to re-architect the integration between services to communicate on HTTPS on the private network only. A solutions architect is asked to suggest a long-term solution considering the possibility of adding more VPCs in the future.</p><p>What should the solutions architect recommend?</p>",
          "answers": [
            "<p>Create peering connections between each VPC of the different AWS accounts. Add a new route to the route table for each peering connection, with the destination set to the CIDR block of the VPC and the target set to the ID of the peering connection. Create Network Load Balancers (NLB) in each VPC and use the NLB DNS names for services integration.</p>",
            "<p>Fix the overlapping IP address ranges by renumbering the IP networks. Create a transit gateway in a new AWS account in AWS Organizations, and configure the VPC attachments. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway across the organization. Create transit gateway attachments to each VPC of the AWS accounts. Add new routes with the destination set to each VPC CIDR block, and the target set to the transit gateway attachment. Create a Network Load Balancer (NLB) in each VPC and use this for communication between services.</p>",
            "<p>Create a new AWS account and set up a new VPC. Create resource shares in AWS Resource Access Manager (AWS RAM) to share the private subnets to specified accounts. Advise the application teams to host the services in EC2 instances deployed in the shared private subnets. Create a Network Load Balancer (NLB) for each account and share the DNS names for the services integration.</p>",
            "<p>Create a Network Load Balancer (NLB) in each of the account VPCs. Create an interface type VPC endpoint for <code>com.amazonaws.us-east-1.elasticloadbalancing</code> service and choose the VPC and subnets. In the AWS PrivateLink console, create a subscription for the VPC endpoint and choose the AWS account that will be allowed access to the NLB. Use the VPC endpoint DNS names for communication between services.</p>"
          ],
          "explanation": "<p>A transit gateway acts as a Regional virtual router for traffic flowing between your virtual private clouds (VPCs) and on-premises networks. A transit gateway scales elastically based on the volume of network traffic. It is a best practice to use a separate subnet for each transit gateway VPC attachment.</p><p>A transit gateway enables you to attach VPCs and VPN connections and route traffic between them. A transit gateway works across AWS accounts, and you can use AWS RAM to share your transit gateway with other accounts. After you share a transit gateway with another AWS account, the account owner can attach their VPCs to your transit gateway. A user from either account can delete the attachment at any time.</p><p><img alt=\"Transit Gateway with AWS RAM\" src=\"https://media.tutorialsdojo.com/public/TransitGatewayWithRAM.png\" width=\"1000\"></p><p>It is a high recommendation and the best option to renumber IP networks when possible, based on two reasons: cost, and simplicity. Changing network configurations is not easy, but it is beneficial in the long term because it removes the ongoing cost of running required components when connecting overlapping networks. Having non-overlapping IPs also makes troubleshooting easier when things go wrong, as resources can easily be identified to the network they are deployed to. This also removes the complexity of managing firewall rules across the organization.</p><p>Thus, the correct answer is: <strong>Fix the overlapping IP address ranges by renumbering the IP networks. Create a transit gateway in a new AWS account in AWS Organizations, and configure the VPC attachments. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway across the organization. Create transit gateway attachments to each VPC of the AWS accounts. Add new routes with the destination set to each VPC CIDR block, and the target set to the transit gateway attachment. Create a Network Load Balancer (NLB) in each VPC and use this for communication between services.</strong></p><p>The option that says: <strong>Create peering connections between each VPC of the different AWS accounts. Add a new route to the route table for each peering connection, with the destination set to the CIDR block of the VPC and the target set to the ID of the peering connection. Create Network Load Balancers (NLB) in each VPC and use the NLB DNS names for services integration</strong> is incorrect. Although VPC peering will only work if the overlapping IP ranges are fixed, managing peering connections between multiple VPCs can be very complex and difficult to manage as the number of VPCs increases. For each <code>x</code> number of VPCs, <code>x*(x-1)/2</code> number of peering connections needs to be created to establish connectivity across each VPC.</p><p>The option that says: <strong>Create a new AWS account and set up a new VPC. Create resource shares in AWS Resource Access Manager (AWS RAM) to share the private subnets to specified accounts. Advise the application teams to host the services in EC2 instances deployed in the shared private subnets. Create a Network Load Balancer (NLB) for each account and share the DNS names for the services integration</strong> is incorrect. Sharing private subnets across accounts using AWS Resource Access Manager (AWS RAM) can simply add to the management overhead. Each time a new subnet is created, it needs to be shared manually with the specified AWS accounts.</p><p>The option that says: <strong>Create a Network Load Balancer (NLB) in each of the account VPCs. Create an interface type VPC endpoint for </strong><code><strong>com.amazonaws.us-east-1.elasticloadbalancing</strong></code><strong> service and choose the VPC and subnets. In the AWS PrivateLink console, create a subscription for the VPC endpoint and choose the AWS account that will be allowed access to the NLB. Use the VPC endpoint DNS names for communication between services</strong> is incorrect. Although using AWS PrivateLink and VPC endpoints can provide connectivity between VPCs, it would require managing individual VPC endpoints and subscriptions, making management complex. It is better to implement a Transit Gateway solution for the long term.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/\">https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/</a><br></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html\">https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html\">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-share.html\">https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-share.html</a></p><p><br></p><p>Check out this AWS Transit Gateway Cheat Sheet:</p><p><a href=\"https://tutorialsdojo.com/aws-transit-gateway/?src=udemy\">https://tutorialsdojo.com/aws-transit-gateway/</a></p>"
        }
      },
      {
        "id": 99528233,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company plans to deploy a high-performance computing (HPC) workload on Amazon EC2 instances in a shared Amazon VPC. Developers in multiple participant accounts must be granted access to the cluster to perform analytics. The cluster requires a shared file system that supports file-based access to objects stored in Amazon S3 buckets.</p><p>Which deployment steps should be implemented to support the required features and access control?</p>",
          "answers": [
            "<p>Deploy an Amazon FSx for Lustre file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>",
            "<p>Deploy an Amazon FSx for NetApp ONTAP file system with NFS access. Create a resource based policy that allows cross-account access for members of the participant accounts. Use access keys to authenticate users and use security groups to enable file system access.</p>",
            "<p>Deploy an Amazon FSx for Windows file server file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>",
            "<p>Deploy and Amazon FSx for OpenZFS file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>"
          ],
          "explanation": "<p>Amazon FSx for Lustre is the only option that can present objects stored in S3 buckets as files. The question specifically asks for file-based access for objects stored in buckets, so this requirement is satisfied by using FSx for Lustre. The access control requirements can be implemented through an AWS IAM role that can be assumed by cross-account participants. Permissions should be assigned through an identity-based permissions policy assigned to the role.</p><p><strong>CORRECT: </strong>\"Deploy an Amazon FSx for Lustre file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon FSx for NetApp ONTAP file system with NFS access. Create a resource based policy that allows cross-account access for members of the participant accounts. Use access keys to authenticate users and use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects. You also cannot use resource based policies with FSx file systems and access keys would be unsuitable for cross-account access.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon FSx for Windows file server file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects. This file system should be used with Windows servers that need an NTFS file system.</p><p><strong>INCORRECT:</strong> \"Deploy and Amazon FSx for OpenZFS file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/LustreGuide/security_iam_service-with-iam.html\">https://docs.aws.amazon.com/fsx/latest/LustreGuide/security_iam_service-with-iam.html</a></p><p><a href=\"https://aws.amazon.com/fsx/lustre/faqs/\">https://aws.amazon.com/fsx/lustre/faqs/</a></p>"
        }
      },
      {
        "id": 115961521,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A gaming startup company is finishing its migration to AWS and realizes that many DevOps engineers have permissions to delete Amazon DynamoDB tables.</p><p>A solution is required to receive near real-time notifications when the API call DeleteTable is invoked in DynamoDB.</p><p>Which actions should be taken to achieve this requirement most cost-effectively?</p>",
          "answers": [
            "<p>Enable DynamoDB Streams and configure an AWS Lambda function to process events from the stream. Send alerts using Amazon SNS.</p>",
            "<p>Create an AWS CloudTrail trail. Create an Amazon EventBridge rule to track an AWS API call via CloudTrail and use Amazon SNS as a target.</p>",
            "<p>Create an AWS CloudTrail event filter and use an AWS Lambda function to send an Amazon SNS notification.</p>",
            "<p>Create an Amazon CloudWatch event filter that monitors for DeleteTable API actions and sends an alert via Amazon SNS.</p>"
          ],
          "explanation": "<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. You must create an AWS CloudTrail trail. For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p><p><strong>CORRECT: </strong>\"Create an AWS CloudTrail trail. Create an Amazon EventBridge rule to track an AWS API call via CloudTrail and use Amazon SNS as a target\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable DynamoDB Streams and configure an AWS Lambda function to process events from the stream. Send alerts using Amazon SNS\" is incorrect.</p><p>This would be less cost-effective compared to using AWS CloudTrail and Amazon EventBridge.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail event filter and use an AWS Lambda function to send an Amazon SNS notification\" is incorrect.</p><p>Event filters are used with CloudWatch, not with CloudTrail.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch event filter that monitors for DeleteTable API actions and sends an alert via Amazon SNS\" is incorrect.</p><p>API actions are tracked by AWS CloudTrail but not by Amazon CloudWatch.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 138248131,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A dynamic Node.js-based photo sharing application hosted in four Amazon EC2 web servers is using a DynamoDB table for session management and an S3 bucket for storing media files. The users can upload, view, organize, and share their photos using the content management system of the application. When a user uploads an image, a Lambda function will be invoked to process the media file then store it in Amazon S3. Due to the recent growth of the application\u2019s user base in the country, they decided to manually add another six EC2 instances for the web tier to handle the peak load. However, each of the instances took more than half an hour to download the required application libraries and become fully configured.&nbsp; </p><p>Which of the following is the MOST resilient and highly available solution that will also lessen the deployment time of the new servers?</p>",
          "answers": [
            "<p>Migrate the application to Amazon ECS with Fargate launch type. Create a task definition for the Node.js application that includes all required dependencies. Set up a DynamoDB table with Auto Scaling enabled and configure an Application Load Balancer to distribute traffic to the ECS service. Use Amazon Route 53 to point the application DNS record to the Application Load Balancer.</p>",
            "Deploy a Spot Fleet of EC2 instances with a target capacity of 20 then place them behind an Application Load Balancer. Configure Amazon Route 53 to point the application DNS record to the Application Load Balancer. Increase the RCU and WCU of the DynamoDB table. ",
            "Host the entire Node.js application to Amazon S3 as a static website. Create an Amazon CloudFront web distribution with the S3 bucket as its origin. Enable Auto Scaling in the Amazon DynamoDB table. In Route 53, point the application DNS record to the CloudFront URL.",
            "<p>Host the entire application in Elastic Beanstalk. Create a custom AMI using AWS Systems Manager Automation which includes all of the required dependencies and web components. Configure the Elastic Beanstalk environment to have an Auto Scaling group of EC2 instances across multiple Availability Zones with a load balancer in front that balances the incoming traffic. Enable Amazon DynamoDB Auto Scaling and point the application DNS record to the Elastic Beanstalk load balancer using Amazon Route 53.</p>"
          ],
          "explanation": "<p>When you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn't included in the standard AMIs.</p><p>Using <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">configuration files</a> is great for configuring and customizing your environment quickly and consistently. Applying configurations, however, can start to take a long time during environment creation and updates. If you do a lot of server configuration in configuration files, you can reduce this time by making a custom AMI that already has the software and configuration that you need.</p><p><img src=\"https://media.tutorialsdojo.com/public/aeb-architecture2.png\"></p><p>A custom AMI also allows you to make changes to low-level components, such as the Linux kernel, that are difficult to implement or take a long time to apply in configuration files. To create a custom AMI, launch an Elastic Beanstalk platform AMI in Amazon EC2, customize the software and configuration to your needs, and then stop the instance and save an AMI from it.</p><p>Hence, the correct solution is: <strong>Host the entire application in Elastic Beanstalk. Create a custom AMI using AWS Systems Manager Automation which includes all of the required dependencies and web components. Configure the Elastic Beanstalk environment to have an Auto Scaling group of EC2 instances across multiple Availability Zones with a load balancer in front that balances the incoming traffic. Enable Amazon DynamoDB Auto Scaling and point the application DNS record to the Elastic Beanstalk load balancer using Amazon Route 53.</strong></p><p>The option that says: <strong>Migrate the application to Amazon ECS with Fargate launch type. Create a task definition for the Node.js application that includes all required dependencies. Set up a DynamoDB table with Auto Scaling enabled and configure an Application Load Balancer to distribute traffic to the ECS service. Use Amazon Route 53 to point the application DNS record to the Application Load Balancer</strong> is incorrect. While ECS with Fargate can simplify container management, this solution requires the application to be containerized, which might involve significant changes to the existing application architecture. Elastic Beanstalk offers a simpler, more direct approach to this scenario.</p><p>The option that says: <strong>Deploy a Spot Fleet of EC2 instances with a target capacity of 20 then place them behind an Application Load Balancer. Configure Amazon Route 53 to point the application DNS record to the Application Load Balancer. Increase the RCU and WCU of the DynamoDB table</strong> is incorrect because using Spot Instances is susceptible to interruptions and could lead to outages of your application. Moreover, setting an exact number of target capacity is not recommended since your servers won't scale up or scale down based on the actual demand.</p><p>The option that says: <strong>Host the entire Node.js application to Amazon S3 as a static website. Create an Amazon CloudFront web distribution with the S3 bucket as its origin. Enable Auto Scaling in the Amazon DynamoDB table. In Route 53, point the application DNS record to the CloudFront URL </strong>is incorrect because the web application is a dynamic site and cannot be migrated to a static S3 website hosting.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/beanstalk-environment-configuration-advanced.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/beanstalk-environment-configuration-advanced.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 134588463,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A leading telecommunications company is migrating a multi-tier enterprise application to AWS, which must be hosted on a single Amazon EC2 Dedicated Instance with an Elastic Fabric Adapter (EFA) and instance store volumes. The app cannot use Auto Scaling due to server licensing constraints.</p><p>For its database tier, Amazon Aurora will be used to store the application's data and transactions. Automatic recovery must be configured to ensure high availability even in the event of EC2 or Aurora database outages.</p><p>Which of the following options provides the MOST cost-effective solution for this migration task?</p>",
          "answers": [
            "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch a single-instance Aurora database. Create an AWS Config conformance pack containing the Config rules and remediation actions to automatically recover the EC2 instance during system outages.</p>",
            "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch a single-instance Aurora database. Create an AWS Config conformance pack containing the Config rules and remediation actions to automatically recover the EC2 instance during system outages.</p>",
            "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Set up an Amazon EventBridge rule to trigger an AWS Lambda function to start a new EC2 instance in an available Availability Zone when the instance status reaches a failure state. Configure an Aurora database with a Read Replica in the other Availability Zone. In the event that the primary database instance fails, promote the read replica to a primary database instance.</p>",
            "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch an Elastic IP address and attach it to the dedicated instance. Set up a second EC2 instance in the other Availability Zone. Create an Amazon EventBridge rule to trigger an AWS Lambda function to move the EIP to the second instance when the first instance fails. Set up a single-instance Aurora database.</p>"
          ],
          "explanation": "<p>You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group.</p><p>When the <code>StatusCheckFailed_System</code> alarm is triggered, and the recovery action is initiated, you will be notified by the Amazon SNS topic that you selected when you created the alarm and associated the recovery action. During instance recovery, the instance is migrated during an instance reboot, and any in-memory data is lost. When the process is complete, information is published to the SNS topic you've configured for the alarm. Anyone subscribed to this SNS topic will receive an email notification that includes the status of the recovery attempt and any further instructions. You will notice an instance reboot on the recovered instance.<br>Examples of problems that cause system status checks to fail include:</p><p>- Loss of network connectivity</p><p>- Loss of system power</p><p>- Software issues on the physical host</p><p>- Hardware issues on the physical host that impact network reachability</p><p>If your instance has a public IPv4 address, it retains the public IPv4 address after recovery.</p><p><img alt=\"Amazon EventBridge\" height=\"593\" src=\"https://media.tutorialsdojo.com/aws_eventbridge_13AUG2023.png\" width=\"1000\"></p><p>You can configure a CloudWatch alarm to automatically recover impaired EC2 instances and notify you through Amazon SNS. However, the SNS notification doesn't include the results of the automatic recovery action.</p><p>You must also configure an Amazon EventBridge rule to monitor AWS Personal Health Dashboard (AWS Health) events for your instance. Then, you are notified of the results of automatic recovery actions, for example.</p><p>While Amazon Aurora Read Replicas incur additional costs because each replica is a separate database instance, they are necessary to provide high availability and failover capability, critical for enterprise applications requiring auto-healing and minimal downtime.</p><p>Hence, the correct answer is: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Set up an Amazon EventBridge rule to trigger an AWS Lambda function to start a new EC2 instance in an available Availability Zone when the instance status reaches a failure state. Configure an Aurora database with a Read Replica in the other Availability Zone. In the event that the primary database instance fails, promote the read replica to a primary database instance.</strong></p><p>The option that says: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch a single-instance Aurora database. Create an AWS Config conformance pack containing the Config rules and remediation actions to automatically recover the EC2 instance during system outages</strong> is incorrect because launching a single-instance Aurora database is simply not a highly available architecture. You have to set up at least a Read Replica that you can configure to be the new primary instance during outages. In addition, AWS Config rules alone cannot recover your EC2 instances automatically. This must be integrated with the AWS Systems Manager Automation first.</p><p>The option that says: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Set up an EC2 instance and enable the built-in instance recovery feature. Create an Aurora database with a Read Replica on the other Availability Zone. Promote the replica as the primary in the event that the primary database instance fails</strong> is incorrect because the built-in instance recovery failure feature for Amazon EC2 doesn't apply to instances that use Elastic Fabric Adapter (EFA) and instance store volumes. The scenario explicitly mentioned that the application uses a Dedicated Instance with both EFA and instance store volumes, so EC2 auto-recovery won't take place. You have to use a combination of Amazon EventBridge and a Lambda function to recover the EC2 instance from failure automatically.</p><p>The option that says: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch an Elastic IP address and attach it to the dedicated instance. Set up a second EC2 instance in the other Availability Zone. Create an Amazon EventBridge rule to trigger an AWS Lambda function to move the EIP to the second instance when the first instance fails. Set up a single-instance Aurora database</strong> is incorrect because setting up a second EC2 instance in another Availability Zone will only entail an additional cost. Launching a single-instance Aurora database is not a highly available architecture as well.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-sns-ec2-automatic-recovery/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-sns-ec2-automatic-recovery/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/automatic-recovery-ec2-cloudwatch/\">https://aws.amazon.com/premiumsupport/knowledge-center/automatic-recovery-ec2-cloudwatch/</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 82921458,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A Big Data analytics company is operating a distributed Cassandra cluster on EC2. Each instance in the cluster must have a list of all the other instance's IP to function correctly, store in a configuration file. As a Devops Engineer at the company, you would like this solution to adapt automatically when newer EC2 instances join the cluster, or when some EC2 instances are terminated.</p>\n\n<p>Which of the following solutions would you recommend for the given requirement?</p>\n",
          "answers": [
            "<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</p>",
            "<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</p>",
            "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</p>",
            "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</strong></p>\n\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n\n<p>A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p>\n\n<p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer's instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes.</p>\n\n<p>The lifecycle hook that is called on ALL instances, whenever an instance comes up or another one goes down, is the <code>configure</code> hook. So this option is the best fit for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</strong> - As mentioned in the explanation above, the <code>setup</code> hook is only used when an instance is first created, so this option is incorrect.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</strong> - Lifecycle hooks on Auto Scaling Groups may seem like a good idea at first, but using AWS Lambda, the solution is not practicable as SSH'ing into the instance via Lambda will not work.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</strong> - EC2 user-data scripts are only triggered on an instance's first launch, so this option just acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/opsworks/\">https://aws.amazon.com/opsworks/</a></p>\n"
        }
      },
      {
        "id": 138248141,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A government-sponsored health service is running its web application containing information about the clinics, hospitals, medical specialists, and other medical services in the country. The organization also has a set of public web services which enable third-party companies to search medical data for its respective applications and clients. AWS Lambda functions are used for the public APIs. For its database-tier, an Amazon DynamoDB table stores all of the data with an Amazon OpenSearch Service domain, which supports the search feature and stores the indexes. A DevOps engineer has been instructed to ensure that in the event of a failed deployment, there should be no downtime and a system should be in place to prevent subsequent deployments. The service must strictly maintain full capacity during API deployment without any reduced capacity to avoid degradation of service.</p><p>How can the engineer meet the above requirements in the MOST efficient way?</p>",
          "answers": [
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>All at Once</code>.</p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Rolling</code>.</p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Immutable</code></p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication.</p>"
          ],
          "explanation": "<p><strong>AWS Elastic Beanstalk</strong> provides several options for how <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">deployments</a> are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's an automatically scaling environment (you didn't specify the <code>--single</code> option), it uses rolling deployments.</p><p>With rolling deployments, Elastic Beanstalk splits the environment's EC2 instances into batches and deploys the new version of the application to one batch at a time, leaving the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p><p><img src=\"https://media.tutorialsdojo.com/public/environments-mgmt-updates-immutable.png\"></p><p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.''</p><p>Immutable deployments perform an <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">immutable update</a> to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.</p><p>Hence, the correct answer is: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>Immutable.</strong></code></p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>All at Once</strong></code><strong><em> </em></strong>is incorrect because this policy only deploys the new version to all instances simultaneously, which means that the instances in your environment are out of service for a short time while the deployment occurs.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to</strong> <code><strong>Rolling</strong></code><em> </em>is incorrect because this policy will just deploy the new version in batches where each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication</strong> is incorrect because you can't host a dynamic web application in Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      }
    ],
    "answers": {
      "75949064": [
        "b",
        "d",
        "e"
      ],
      "75949112": [
        "b"
      ],
      "75949146": [
        "b"
      ],
      "75949148": [
        "c"
      ],
      "82921334": [
        "a"
      ],
      "82921458": [
        "a"
      ],
      "99528233": [
        "a"
      ],
      "115961521": [
        "b"
      ],
      "134588451": [
        "d"
      ],
      "134588463": [
        "c"
      ],
      "138248131": [
        "d"
      ],
      "138248141": [
        "c"
      ],
      "138248159": [
        "b"
      ],
      "138248177": [
        "c"
      ],
      "138248181": [
        "c"
      ],
      "138248227": [
        "d"
      ],
      "138248229": [
        "b"
      ],
      "138248237": [
        "a"
      ],
      "138248245": [
        "b"
      ],
      "143860765": [
        "b"
      ]
    }
  },
  {
    "id": "1769968219110",
    "date": "2026-02-01T17:50:19.110Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 16,
    "incorrect": 4,
    "unanswered": 0,
    "total": 20,
    "percent": 80,
    "duration": 4942951,
    "questions": [
      {
        "id": 82921318,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A graphics design company is experimenting with a new feature for an API and the objective is to pass the field <code>\"color\"</code> in the JSON payload to enable this feature. The new Lambda function should treat <code>\"color\": \"none\"</code> as a request from an older client. The company would like to only have to manage one Lambda function in the back-end while being able to support both old and new clients. The API Gateway API is currently deployed on the <code>v1</code> stage. Old clients include Android applications which may take time to be updated. The technical requirements mandate that the solution should support the old clients for years to come.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you recommend as the best fit for the given use-case?</p>\n",
          "answers": [
            "<p>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</p>",
            "<p>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</p>",
            "<p>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</p>",
            "<p>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway handles tasks such as traffic management, authorization and access control, monitoring, and API version management. API Gateway acts as a \"front door\" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications. Simply creating and developing an API Gateway API doesn't automatically make it callable by your users. To make it callable, you must deploy your API to a stage. A stage is a named reference to a deployment, which is a snapshot of the API.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>For the given use-case, API Gateway mappings must be used. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. As such, you must deploy a v2 API alongside the v1 API backed by the same Lambda function. Old clients will hit the v1 API, which will use a mapping template to add the static missing field <code>\"color\": \"none\"</code>. Newer clients will hit the v2 API and will have that field value included.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</strong> - If we release two separate Lambda functions (named lambda v1 and lambda v2), then we have to maintain them both and that would be going against the requirements of the given use-case.</p>\n\n<p><strong>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</strong> -  API Gateway mapping templates do not support adding default values for fields as these only support static fields.</p>\n\n<p><strong>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. For the given use-case, API Gateway caching is a distractor and should be disregarded.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n"
        }
      },
      {
        "id": 75949056,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company is using AWS CodeCommit for version control and AWS CodePipeline for orchestration of software deployments. The development team are using a remote main branch as the trigger for the pipeline. A developer noticed that the CodePipeline pipeline was not triggered after the developer pushed code changes to the CodeCommit repository.</p><p>Which of the following actions should be taken to troubleshoot this issue?</p>",
          "answers": [
            "<p>Check that the developer's IAM role has permission to push to the CodeCommit repository.</p>",
            "<p>Check that an AWS Lambda function has been created to check for code commits and trigger the pipeline.</p>",
            "<p>Check that the CodePipeline service role has permission to access the CodeCommit repository.</p>",
            "<p>Check that an Amazon CloudWatch Events rule has been created for the main branch to trigger the pipeline.</p>"
          ],
          "explanation": "<p>An Amazon CloudWatch Events rule must be created to trigger the pipeline when changes are committed to the CodeCommit repository. If you use the console to create or edit your pipeline, the CloudWatch Events rule is created for you. In this case, the developer should check to make sure that the rule has been created and is correctly configured.</p><p>The following is a sample CodeCommit event pattern for a MyTestRepo repository with a branch named master:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-34-08-dc82125abf42153048e88229777e35c5.jpg\"><p><strong>CORRECT: </strong>\"Check that an Amazon CloudWatch Events rule has been created for the main branch to trigger the pipeline\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check that the CodePipeline service role has permission to access the CodeCommit repository\" is incorrect.</p><p>The issue is that the pipeline was not triggered. If the service role does not have permissions the pipeline should still be triggered by the CloudWatch Events rule but then an error would be generated if insufficient permissions are assigned for accessing the CodeCommit repository.</p><p><strong>INCORRECT:</strong> \"Check that the developer's IAM role has permission to push to the CodeCommit repository\" is incorrect.</p><p>The developer already committed the code to the repository and did not experience any errors.</p><p><strong>INCORRECT:</strong> \"Check that an AWS Lambda function has been created to check for code commits and trigger the pipeline\" is incorrect.</p><p>An AWS Lambda function is not used to check for commits or to trigger the pipeline. A CloudWatch Events rule must be created for this purpose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 143860761,
        "correct_response": [
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A web application runs on a custom port. The application has been deployed in an Auto Scaling group with an Application Load Balancer (ALB). After launching instances the Auto Scaling and Target Group health checks are returning a healthy status. However, users report that the application is not accessible.</p><p>Which steps should a DevOps engineer take to troubleshoot the issue? (Select TWO.)</p>",
          "answers": [
            "<p>Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port.</p>",
            "<p>Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address.</p>",
            "<p>Modify the Target Group health check configuration to check the application process on the custom port and path.</p>",
            "<p>Modify the Auto Scaling group health check configuration to check the application process on the custom port and path.</p>",
            "<p>Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances.</p>"
          ],
          "explanation": "<p>By default health checks are configured to use the HTTP protocol and port 80. For an ALB the traffic protocol must be HTTP or HTTPS, but the port can be customized. The most likely cause of the issue is that the web service is running on the instances and the default protocol/port is used for health checks on the ALB and ASG. This will result in instances becoming \u201chealthy\u201d despite the actual application service not functioning correctly.</p><p>The engineer should check the ASG health check configuration and the target group health check configuration. If the default values are used then the correct custom port number should be entered instead. The path may also be updated if a specific web page should be checked.</p><p>The image below shows how you can override the default port number and path in a target group health check:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-08-15-0585684e22874b6c255bd441d63a099e.jpg\"><p><strong>CORRECT: </strong>\"Modify the Target Group health check configuration to check the application process on the custom port and path\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Modify the Auto Scaling group health check configuration to check the application process on the custom port and path\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port\" is incorrect.</p><p>You cannot use a TCP listener with an ALB. It must be HTTP or HTTPS though a custom port number can certainly be used.</p><p><strong>INCORRECT:</strong> \"Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address\" is incorrect.</p><p>The issue is not related to the IP addresses traffic is being directed to on the instances; it is related to the port number and path the health checks are configured to check.</p><p><strong>INCORRECT:</strong> \"Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances\" is incorrect.</p><p>Path-based routing rules are used to route traffic to different target groups based on the path in the HTTP request. In this case there is only one target group, so a path-based routing rule is useless. Instead, the ALB must direct traffic to a custom port number (configured in the listener) and validate the application is healthy be running health checks against the appropriate port and path.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 75949118,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A DevOps team is building a pipeline in AWS CodePipeline that will build, stage, test, and then deploy an application on Amazon EC2. The team will add a manual approval stage between the test stage and the deployment stage. The development team uses a custom chat tool that offers a webhook interface for sending notifications.</p><p>The DevOps team require status updates for pipeline activity and approval requests to be posted to the chat tool. How can this be achieved?</p>",
          "answers": [
            "<p>Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic.</p>",
            "<p>Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change events. Publish subscription events to an Amazon SNS topic and subscribe the chat webhook URL to the SNS topic and complete the subscription validation.</p>",
            "<p>Create an AWS Lambda function that is invoked by AWS CloudTrail API events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details directly to the chat webhook URL.</p>",
            "<p>Create an AWS Config rule that checks for CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic.</p>"
          ],
          "explanation": "<p>You can monitor CodePipeline events in EventBridge, which delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services. EventBridge routes that data to targets such as AWS Lambda and Amazon Simple Notification Service.</p><p>Events are composed of rules that include an event pattern and event target. Each type of execution state change event in CodePipeline emits notifications with specific message content. In this case the team should filter for the \u201cCodePipeline Pipeline Execution State Change\u201d events and route to an SNS Topic as a target. The Lambda function can then be subscribed to the topic.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change events. Publish subscription events to an Amazon SNS topic and subscribe the chat webhook URL to the SNS topic and complete the subscription validation\" is incorrect.</p><p>CloudWatch Logs subscription filters can be used to publish to Kinesis Data Streams, Kinesis Data Firehose, or AWS Lambda. You cannot publish directly to SNS. Also, the log will not contain execution state change events for CodePipeline.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that is invoked by AWS CloudTrail API events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details directly to the chat webhook URL\" is incorrect.</p><p>You cannot use API events to look for this specific event and then send directly from CloudTrail to the chat webhook URL.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config rule that checks for CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic\" is incorrect.</p><p>AWS Config is used for configuration compliance and cannot check for events that relate to pipeline execution state changes in AWS CodePipeline.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 138248203,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A financial company has several accounting applications that are hosted in AWS and used by thousands of small and medium businesses. As part of its Business Continuity Plan, the company is required to set up an automatic DNS failover for its applications to a disaster recovery (DR) environment. The DevOps team was instructed to configure Amazon Route 53 to automatically route to an alternate endpoint when the primary application stack in the us-west-1 region experiences an outage or degradation of service.</p><p>What steps should the team take to satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Set up health checks in Route 53 for non-alias records to each service endpoint. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks.</p>",
            "<p>Use a Failover routing policy configuration. Set up alias records in Route 53 that route traffic to AWS resources. Set the <code>Evaluate Target Health</code> option to <code>Yes</code>, then create all of the required non-alias records.</p>",
            "<p>Set up a record in Route 53 with a Weighted routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints.</p>",
            "<p>Set up an Amazon CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom AWS Lambda function. Execute the <code>ChangeResourceRecordSets</code> API call using the function to initiate the failover to the secondary DNS record.</p>",
            "<p>Set up a record in Route 53 with a latency routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints.</p>"
          ],
          "explanation": "<p>Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.</p><p>To create an active-passive failover configuration with one primary record and one secondary record, you just create the records and specify <strong>Failover</strong> for the routing policy. When the primary resource is healthy, Route 53 responds to DNS queries using the primary record. When the primary resource is unhealthy, Route 53 responds to DNS queries using the secondary record.</p><p>You can configure a health check that monitors an endpoint that you specify either by IP address or by domain name. At regular intervals that you specify, Route 53 submits automated requests over the Internet to your application, server, or other resources to verify that it's reachable, available, and functional. Optionally, you can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-route53-evaluate-target-health.png\"></p><p>When Route 53 checks the health of an endpoint, it sends an HTTP, HTTPS, or TCP request to the IP address and port that you specified when you created the health check. For a health check to succeed, your router and firewall rules must allow inbound traffic from the IP addresses that the Route 53 health checkers use.</p><p>Hence, the correct answers are:</p><p><strong>- Set up health checks in Route 53 for non-alias records to each service endpoint. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks.</strong></p><p><strong>- Use a Failover routing policy configuration. Set up alias records in Route 53 that route traffic to AWS resources. Set the </strong><code><strong>Evaluate Target Health</strong></code><strong> option to </strong><code><strong>Yes</strong></code><strong>, then create all of the required non-alias records.</strong></p><p>The option that says: <strong>Set up a record in Route 53 with a Weighted routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints</strong> is incorrect because Weighted routing simply lets you associate multiple resources with a single domain name (pasigcity.com) or subdomain name (blog.pasigcity.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.</p><p>The option that says:<strong> Set up an Amazon CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom AWS Lambda function. Execute the </strong><code><strong>ChangeResourceRecordSets</strong></code><strong> API call using the function to initiate the failover to the secondary DNS record</strong> is incorrect because you only have to use a Failover routing policy. Calling the Route 53 API is not applicable nor useful at all in this scenario.</p><p>The option that says: <strong>Set up a record in Route 53 with a latency routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints</strong> is incorrect because the Latency routing policy simply improves the application performance for your users by serving their requests from the AWS Region that provides the lowest latency. You have to use a Failover routing policy instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>"
        }
      },
      {
        "id": 134588431,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A mobile phone manufacturer hosts a suite of enterprise resource planning (ERP) solutions to several Amazon EC2 instances in their AWS VPC. Its DevOps team is using AWS CloudFormation templates to design, launch, and deploy resources to their cloud infrastructure. Each template is manually updated to map the latest AMI IDs of the ERP solution. This process takes a significant amount of time to execute, which is why the team was tasked to automate this process. </p><p>In this scenario, which of the following options is the MOST suitable solution that can satisfy the requirement?</p>",
          "answers": [
            "<p>Set up and configure the Systems Manager State Manager service to store the latest AMI IDs and integrate it with your AWS CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.</p>",
            "<p>Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.</p>",
            "<p>Integrate AWS CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.</p>",
            "<p>Use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template. Call the update-stack API in CloudFormation in your template whenever you decide to update the Amazon EC2 instances.</p>"
          ],
          "explanation": "<p>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.</p><p>If the parameter being referenced in the template does not exist in Systems Manager, a synchronous validation error is thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.</p><p><img src=\"https://media.tutorialsdojo.com/public/Systems-Manager-parameters_6AUG2023.png\"></p><p>Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The <a href=\"http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html\">Parameters</a> section in the output for Describe API will show an additional \u2018ResolvedValue\u2019 field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation.</p><p>Hence, the correct answer is: <strong>Use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template. Call the update-stack API in CloudFormation in your template whenever you decide to update the Amazon EC2 instances.</strong></p><p>The option that says: <strong>Set up and configure the Systems Manager State Manager service to store the latest AMI IDs and integrate it with your AWS CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template</strong> is incorrect because the Systems Manager State Manager service simply automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This can't be used as a parameter store that refers to the latest AMI of your application.</p><p>The option that says: <strong>Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments</strong> is incorrect because using AWS Service Catalog is not suitable in this scenario. This service just allows organizations to create and manage catalogs of IT services that are approved for use on AWS.</p><p>The option that says: <strong>Integrate AWS CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments</strong> is incorrect because AWS Service Catalog just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A better solution is to use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949156,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A DevOps Engineer needs a scalable Node.js application in AWS with a MySQL database. There should be no downtime during deployments and if issues occur rollback to a previous version must be easy to implement. The database may also be used by other applications.</p><p>Which solution meets these requirements?</p>",
          "answers": [
            "<p>Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier.</p>",
            "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack.</p>",
            "<p>Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier.</p>",
            "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack.</p>"
          ],
          "explanation": "<p>AWS Elastic Beanstalk offers automatic rollback options for deployment updates. This coupled with auto scaling and the ALB meets the requirements for a scalable compute and web tier. The RDS database provides a managed solution for the MySQL database. The RDS MySQL database should be created outside of the Elastic Beanstalk environment as it may be used by other applications. If it is created within the Elastic Beanstalk environment it could be automatically deleted if the environment is deleted.</p><p><strong>CORRECT: </strong>\"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack\" is incorrect.</p><p>As explained above the RDS database should be created outside of the Elastic Beanstalk environment.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>ECS does not offer automatic rollback so Elastic Beanstalk is a better solution to meet the requirements.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>Automating the creation of snapshots is not a suitable solution for rollback. Elastic Beanstalk offers several deployment options which offer automatic rollback.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 138248207,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A software development company is using GitHub, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline for its CI/CD process. To further improve their systems, they need to implement a solution that automatically detects and reacts to changes in the state of their deployments in AWS CodeDeploy. Any changes must be rolled back automatically if the deployment process fails, and a notification must be sent to the DevOps Team's Slack channel for easy monitoring.</p><p>Which of the following is the MOST suitable configuration that you should implement to satisfy this requirement?</p>",
          "answers": [
            "<p>Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team's Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when a deployment fails</code> setting.</p>",
            "<p>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when alarm thresholds are met</code> setting.</p>",
            "<p>Configure a CodeDeploy agent to send notification to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful.</p>",
            "<p>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team's Slack Channel when the <code>PutLifecycleEventHookExecutionStatus</code> API call has been detected. Rollback the changes by using the AWS CLI.</p>"
          ],
          "explanation": "<p>You can monitor <strong>CodeDeploy</strong> deployments using the following CloudWatch tools: Amazon EventBridge, CloudWatch alarms, and Amazon CloudWatch Logs.</p><p>Reviewing the logs created by the CodeDeploy agent and deployments can help you troubleshoot the causes of deployment failures. As an alternative to reviewing CodeDeploy logs on one instance at a time, you can use CloudWatch Logs to monitor all logs in a central location.</p><p>You can use <strong>Amazon EventBridge </strong>(formerly known as Amazon CloudWatch Events) to detect and react to changes in the state of an instance or a deployment (an \"event\") in your CodeDeploy operations. Then, based on the rules you create, EventBridge will invoke one or more target actions when a deployment or instance enters the state you specify in a rule. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-rule-12-09-2024.png\"></p><p>You can select the following types of targets when using EventBridge as part of your CodeDeploy operations:</p><p>- AWS Lambda functions</p><p>- Kinesis streams</p><p>- Amazon SQS queues</p><p>- Built-in targets (CloudWatch alarm actions)</p><p>- Amazon SNS topics</p><p>The following are some use cases:</p><p>- Use a Lambda function to pass a notification to a Slack channel whenever deployments fail.</p><p>- Push data about deployments or instances to a Kinesis stream to support comprehensive, real-time status monitoring.</p><p>- Use CloudWatch alarm actions to automatically stop, terminate, reboot, or recover Amazon EC2 instances when a deployment or instance event you specify occurs.</p><p>Hence, the correct answer is:<strong> Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team's Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when a deployment fails</strong></code><strong> setting.</strong></p><p>The option that says: <strong>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when alarm thresholds are met</strong></code><strong> setting</strong> is incorrect because CloudWatch Alarm can't directly send a message to a Slack Channel. You have to use an EventBridge with an associated Lambda function to notify the DevOps Team via Slack.</p><p>The option that says:<strong> Configure a CodeDeploy agent to send a notification to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful</strong> is incorrect because a CodeDeploy agent is primarily used for deployment and not for sending custom messages to non-AWS resources such as a Slack Channel.</p><p>The option that says: <strong>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team's Slack Channel when the </strong><code><strong>PutLifecycleEventHookExecutionStatus</strong></code><strong> API call has been detected. Rollback the changes by using the AWS CLI</strong> is incorrect because this API simply sets the result of a Lambda validation function. This is not a suitable solution since invoking various API calls is not necessary at all. You simply have to integrate an EventBridge rule with an associated Lambda function to your CodeDeploy project in order to meet the specified requirement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 115961527,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>When deploying a newly developed application on AWS, a DevOps team notices an intermittent error when attempting to make a connection to the application.</p><p>The application has a two-tier architecture with an AWS Lambda function backed by an Amazon API gateway and a NoSQL database as the data store.</p><p>The DevOps team noticed that sometime after deployment the error stops occurring. This application is deployed by AWS CodeDeploy and the Lambda function is deployed as the last step of pipeline.</p><p>What is the most efficient way for a DevOps engineer to resolve the issue?</p>",
          "answers": [
            "<p>Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function.</p>",
            "<p>Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond.</p>",
            "<p>Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed.</p>",
            "<p>Use the ValidateService hook to validate that the deployment was completed successfully.</p>"
          ],
          "explanation": "<p>An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Here are descriptions of the hooks available for use in your AppSpec file.</p><p>\u00b7 <strong>BeforeAllowTraffic</strong> \u2013 Use to run tasks before traffic is shifted to the deployed Lambda function version.</p><p>\u00b7 <strong>AfterAllowTraffic</strong> \u2013 Use to run tasks after all traffic is shifted to the deployed Lambda function version.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-44-54-3b27da721fdb31cd0114ba6bbff8f1d5.jpg\"><p><strong>CORRECT: </strong>\"Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond\" is incorrect.</p><p>You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><strong>INCORRECT:</strong> \"Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed\" is incorrect.</p><p>Since the error resolves after some time, the issue will most likely be resolved by ensuring the application is not brought online until it is ready.</p><p><strong>INCORRECT:</strong> \"Use the ValidateService hook to validate that the deployment was completed successfully\" is incorrect.</p><p>This is used to verify the deployment was completed successfully, this will only detect deployment status and will not help in this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 134588393,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. </p><p>As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?</p>",
          "answers": [
            "<p>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/ssm-patch-baselines.jpg\"></p><p><strong>Patch Manager</strong> uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p><strong>AWS Config</strong> provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements. Hence, the correct option is: <strong>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</strong></p><p>The option that says: <strong>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, it is stated in the scenario that the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC </strong>is incorrect because Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise, however, it does not check if your EC2 instances are using an approved AMI or not.</p><p>The option that says: <strong>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect because the AWS Shield Advanced service is more suitable in preventing DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248213,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A global cryptocurrency trading company has a suite of web applications hosted in an Auto Scaling group of Amazon EC2 instances across multiple Available Zones behind an Application Load Balancer to distribute the incoming traffic. The Auto Scaling group is configured to use Elastic Load Balancing health checks for scaling instead of the default EC2 status checks. However, there are several occasions when some instances are automatically terminated after failing the HTTPS health checks in the ALB that purges all the logs stored in the instance.</p><p>To improve system monitoring, a DevOps Engineer must implement a solution that collects all of the application and server logs effectively. The Operations team should be able to perform a root cause analysis based on the logs, even if the Auto Scaling group immediately terminated the instance.</p><p>How can the DevOps Engineer automate the log collection from the EC2 instances with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Pending:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance-terminate Lifecycle Action</code> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Automation to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent.</p>",
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Use AWS Step Functions to collect the application logs and send them to an Amazon CloudWatch Log group. Resume the instance termination once all the logs are sent to CloudWatch Logs.</p>",
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance-terminate Lifecycle Action</code> Auto Scaling Event with an associated AWS Systems Manager Automation document. Trigger the Amazon CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.</p>",
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance Terminate Successful</code> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Run Command to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent.</p>"
          ],
          "explanation": "<p>The EC2 instances in an Auto Scaling group have a path, or lifecycle, that differs from that of other EC2 instances. The lifecycle starts when the Auto Scaling group launches an instance and puts it into service. The lifecycle ends when you terminate the instance, or the Auto Scaling group takes the instance out of service and terminates it.</p><p>You can add a lifecycle hook to your Auto Scaling group so that you can perform custom actions when instances launch or terminate.</p><p>When Amazon EC2 Auto Scaling responds to a scale out event, it launches one or more instances. These instances start in the <code>Pending</code> state. If you added an <code>autoscaling:EC2_INSTANCE_LAUNCHING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Pending</code> state to the <code>Pending:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Pending:Proceed</code> state. When the instances are fully configured, they are attached to the Auto Scaling group and they enter the <code>InService</code> state.</p><p>When Amazon EC2 Auto Scaling responds to a scale in event, it terminates one or more instances. These instances are detached from the Auto Scaling group and enter the <code>Terminating</code> state. If you added an <code>autoscaling:EC2_INSTANCE_TERMINATING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Terminating:Proceed</code> state. When the instances are fully terminated, they enter the <code>Terminated</code> state.</p><p><img src=\"https://media.tutorialsdojo.com/public/auto_scaling_lifecycle.png\"></p><p>Using CloudWatch agent is the most suitable tool to use to collect the logs. The unified CloudWatch agent enables you to do the following:</p><p>- Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html\">Metrics Collected by the CloudWatch Agent</a>.</p><p>- Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p><p>- Retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collectd</code> protocols. <code>StatsD</code> is supported on both Linux servers and servers running Windows Server. On the other hand, <code>collectd</code> is supported only on Linux servers.</p><p>- Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-event-pattern-27Jan2025.png\"></p><p>You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is <code>CWAgent</code>, although you can specify a different namespace when you configure the agent.</p><p>Hence, the correct answer is: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance-terminate Lifecycle Action</strong></code><strong> Auto Scaling Event with an associated AWS Systems Manager Automation document. Trigger the Amazon CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.</strong></p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Pending:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance-terminate Lifecycle Action</strong></code><strong> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Automation to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent</strong> is incorrect because the <code><strong><em>Pending:Wait</em></strong></code><strong><em> </em></strong>state simply refers to the scale-out action in Amazon EC2 Auto Scaling and not for scale-in or for terminating the instances.</p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Use AWS Step Functions to collect the application logs and send them to an Amazon CloudWatch Log group. Resume the instance termination once all the logs are sent to CloudWatch Logs</strong> is incorrect because using AWS Step Functions is inappropriate when collecting the logs from your EC2 instances. You should use a CloudWatch agent instead.</p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance Terminate Successful</strong></code><strong> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Run Command to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent</strong> is incorrect. The <code><strong>EC2 Instance Terminate Successful</strong></code> indicates that the ASG has terminated an instance. The automated solution won't just work because the target instance is already deleted when the Lambda function is triggered.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/</a></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/cloud-watch-events.html#terminate-successful</a></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p>"
        }
      },
      {
        "id": 138248217,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An American tech company used an AWS CloudFormation template to deploy its static corporate website hosted on Amazon S3 in the US East (N. Virginia) region. The template defines an Amazon S3 bucket with a Lambda-backed custom resource that downloads the content from a file server into the bucket. There is a new task for the DevOps Engineer to move the website to the US West (Oregon) region to better serve its customers on the West Coast with lower latency. However, the application stack could not be deleted successfully in CloudFormation. </p><p>Which among the following options shows the root cause of this issue, and how can the DevOps Engineer mitigate this problem for current and future versions of the website?</p>",
          "answers": [
            "<p>The CloudFormation stack deletion fails for an S3 bucket that is used as a static web hosting. To fix the issue, modify the CloudFormation template to remove the website configuration for the S3 bucket.</p>",
            "<p>The CloudFormation stack deletion fails for an S3 bucket that still has contents. To fix the issue, modify the Lambda function code of the custom resource to recursively empty the bucket if the stack is selected for deletion.</p>",
            "<p>The CloudFormation stack deletion fails for an S3 bucket because the <code>DeletionPolicy</code> attribute is set to <code>Snapshot</code>. To fix the issue, set the <code>DeletionPolicy</code> to <code>Delete</code> instead.</p>",
            "<p>The CloudFormation stack deletion fails for an S3 bucket because it is not yet empty. To fix the issue, set the <code>DeletionPolicy</code> to <code>ForceDelete</code> instead.</p>"
          ],
          "explanation": "<p>When you associate a Lambda function with a custom resource, the function is invoked whenever the custom resource is created, updated, or deleted. AWS CloudFormation calls a Lambda API to invoke the function and to pass all the request data (such as the request type and resource properties) to the function. The power and customizability of Lambda functions in combination with AWS CloudFormation enable a wide range of scenarios, such as dynamically looking up AMI IDs during stack creation, or implementing and using utility functions, such as string reversal functions.</p><p>AWS CloudFormation templates that declare an Amazon Elastic Compute Cloud (Amazon EC2) instance must also specify an Amazon Machine Image (AMI) ID, which includes an operating system and other software and configuration information used to launch the instance. The correct AMI ID depends on the instance type and region in which you're launching your stack. And IDs can change regularly, such as when an AMI is updated with software updates.</p><p>Normally, you might map AMI IDs to specific instance types and regions. To update the IDs, you manually change them in each of your templates. By using custom resources and AWS Lambda (Lambda), you can create a function that gets the IDs of the latest AMIs for the region and instance type that you're using so that you don't have to maintain mappings.</p><p><img src=\"https://media.tutorialsdojo.com/public/CloudFormation-AMIManager-Flow.png\"></p><p>You can also run the custom resource to recursively empty the bucket when the CloudFormation stack is triggered for deletion. In CloudFormation, you can only delete empty buckets. Any request for deletion will fail for buckets that still have contents. To control how AWS CloudFormation handles the bucket when the stack is deleted, you can set a deletion policy for your bucket. You can choose to retain the bucket or to delete the bucket.</p><p>Hence, the correct answer is: <strong>The CloudFormation stack deletion fails for an S3 bucket that still has contents. To fix the issue, modify the Lambda function code of the custom resource to recursively empty the bucket if the stack is selected for deletion.</strong></p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket that is used as a static web hosting. To fix the issue, modify the CloudFormation template to remove the website configuration for the S3 bucket </strong>is incorrect because the CloudFormation deletion process will not be hindered simply because your S3 bucket is configured for static web hosting. The primary root cause of this issue is that the CloudFormation stack deletion fails for an S3 bucket that still has contents.</p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket because the </strong><code><strong>DeletionPolicy</strong></code><strong> attribute is set to </strong><code><strong>Snapshot</strong></code><strong>. To fix the issue, set the </strong><code><strong>DeletionPolicy</strong></code><strong> to </strong><code><strong>Delete</strong></code><strong> instead </strong>is incorrect because you can only set the <code><strong><em>DeletionPolicy</em></strong></code> to either <code><strong><em>Retain </em></strong></code>or <code><strong><em>Delete</em></strong></code> for an Amazon S3 resource. In addition, the CloudFormation deletion will still fail as long as the S3 bucket is not empty, even if the <code>DeletionPolicy</code> attribute is already set to <code>Delete</code>.</p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket is not yet empty. To fix the issue, set the </strong><code><strong>DeletionPolicy</strong></code><strong> to </strong><code><strong>ForceDelete</strong></code><strong> instead</strong> is incorrect. Although the provided root cause is accurate, the configuration for <code><strong><em>DeletionPolicy </em></strong></code>remains invalid. <code><strong><em>ForceDelete</em></strong></code> is not a valid value for the deletion policy attribute.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/\">https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 138248159,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company has a suite of applications that are hosted in AWS and each app has its own AMI. Currently, a new AMI must be manually created and deployed to the server if there is a new application version. A DevOps engineer was instructed to automate the process of generating the AMIs to streamline the company's CI/CD workflow. The ID of the newly created AMI must be stored in a centralized location where other build pipelines can programmatically access it.</p><p>Which of the following is the MOST cost-effective way to accomplish this requirement with the LEAST amount of overhead?</p>",
          "answers": [
            "<p>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store.</p>",
            "<p>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</p>",
            "<p>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket.</p>",
            "<p>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table.</p>"
          ],
          "explanation": "<p><strong>Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p><br></p><p>Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machine Images (AMIs) and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for various reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img alt=\"Custom AMI\" height=\"771\" src=\"https://media.tutorialsdojo.com/public/custom_ami_1.gif\" width=\"1000\"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that the Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</strong></p><p>The option that says: <strong>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store </strong>is incorrect because manually customizing the image using an interactive shell and downloading each application image in an OVF file will simply entails a lot of effort. It is also better to use the AWS Systems Manager Automation instead of creating a new pipeline in AWS CodePipeline.</p><p>The option that says: <strong>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket </strong>is incorrect. Although you can technically generate an AMI using an EBS volume snapshot, this process is still tedious and entails a lot of configuration. Using the AWS Systems Manager Automation to generate the AMIs is a more suitable solution.</p><p>The option that says: <strong>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table </strong>is incorrect. Although this may work, this solution will only costs more to maintain than other options since it uses an EC2 instance and an Amazon DynamoDB table. There is also an associated overhead in configuring and using Packer for generating the AMIs and preparing the Jenkins pipeline.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/\">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949174,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
          "answers": [
            "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
            "<p>Add test stages before and after the deploy action.</p>",
            "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
            "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"
          ],
          "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921464,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a social media company, you have implemented a CICD pipeline that takes code from a CodeCommit repository, builds it using CodeBuild thanks to the instructions in the local <code>Dockerfile</code>, and then pushes to ECR at <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code>. The last step of your CICD pipeline is to deploy to the application to your ECS cluster. It seems that while you do so, the application is only partly updated on some ECS instances which are running an older version of your image. You have found that terminating the instance or clearing the local Docker cache fixes the issue, but would like to implement something more robust that provides visibility and identification to track where container images are deployed.</p>\n\n<p>How should you implement a solution to address this issue?</p>\n",
          "answers": [
            "<p>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</p>",
            "<p>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</strong></p>\n\n<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. You can use the <code>IMAGEID</code> property, which is the SHA digest for the Docker image used to start the container.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n\n<p>The issue here is that the ECS instances do not detect that a newer image version is available, because the name <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is re-used. Therefore, by specifying the sha256 e.g.: <code>aws_account_id.dkr.ecr.region.amazonaws.com/my-web-app@sha256:94afd1f2e64d908bc90dbca0035a5b567EXAMPLE</code>, we are certain that newer versions of the Docker image will have a different hash value and therefore the ECS cluster will always pull the newest image at the end of our CICD Pipeline.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</strong> - SSM Run Command lets you remotely and securely manage the configuration of your managed instances. A managed instance is any EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. SSM Run Command may work but it's not an elegant solution.</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</strong> - Lambda Functions can't SSH into EC2 instances, so this option is incorrect.</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</strong> - Adding the <code>latest</code> tag won't help because <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is same as <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app:latest</code>. The <code>latest</code> tag cannot provide visibility and identification to track where container images are deployed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/\">https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n"
        }
      },
      {
        "id": 138248239,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A government agency recently decided to modernize its network infrastructure using AWS. They are developing a solution to store confidential files containing Personally Identifiable Information (PII) and other sensitive financial records of its citizens. All data in the storage solution must be encrypted both at rest and in transit. In addition, all of its data must also be replicated in two locations that are at least 450 miles apart from each other. </p><p>As a DevOps Engineer, what solution should you implement to meet these requirements?</p>",
          "answers": [
            "<p>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3 SSE-C encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
            "<p>Set up primary and secondary S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
            "<p>Set up primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create an IAM role to enforce access to the buckets only through HTTPS. Set up a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
            "<p>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce AWS KMS encryption on all objects uploaded to the bucket. Enable Transfer Acceleration between the two buckets. Set up a KMS Key in the primary region for encrypting objects.</p>"
          ],
          "explanation": "<p><strong>Availability Zones</strong> give customers the ability to operate production applications and databases that are more highly available, fault-tolerant, and scalable than would be possible from a single data center. AWS maintains multiple AZs around the world and more zones are added at a fast pace. Each AZ can be multiple data centers (typically 3), and at full scale can be hundreds of thousands of servers. They are fully isolated partitions of the AWS Global Infrastructure. With their own power infrastructure, the AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles of each other).</p><p>All AZs are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. The network performance is sufficient to accomplish synchronous replication between AZs. AWS Availability Zones are also powerful tools for helping build highly available applications. AZs make partitioning applications about as easy as it can be. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more.</p><p><img src=\"https://media.tutorialsdojo.com/public/Amazon-S3.png\"></p><p>By default, Amazon S3 allows both HTTP and HTTPS requests. To comply with the <strong>s3-bucket-ssl-requests-only</strong> rule, confirm that your bucket policies explicitly deny access to HTTP requests. Bucket policies that allow HTTPS requests without explicitly denying HTTP requests might not comply with the rule.</p><p>To determine HTTP or HTTPS requests in a bucket policy, use a condition that checks for the key <strong>\"aws:SecureTransport\"</strong>. When this key is <strong>true</strong>, this means that the request is sent through HTTPS. To be sure to comply with the <strong>s3-bucket-ssl-requests-only</strong> rule, create a bucket policy that explicitly denies access when the request meets the condition <strong>\"aws:SecureTransport\": \"false\"</strong>. This policy explicitly denies access to HTTP requests.</p><p>In this scenario, you should use AWS Regions since AZs are physically separated by only 100 km (60 miles) from each other. Within each AWS Region, S3 operates in a minimum of three AZs, each separated by miles to protect against local events like fires, floods et cetera. Take note that you can't launch an AZ-based S3 bucket.</p><p>Hence, the correct answer is: <strong>Set up primary and secondary S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</strong></p><p>The option that says: <strong>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3 SSE-C encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets</strong> is incorrect. You can't create Amazon S3 buckets in two separate Availability Zones since this is a regional service.</p><p>The option that says: <strong>Set up primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create an IAM role to enforce access to the buckets only through HTTPS. Set up a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets</strong> is incorrect. You have to use the bucket policy to enforce access to the bucket using HTTPS only and not an IAM role.</p><p>The option that says: <strong>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce AWS KMS encryption on all objects uploaded to the bucket. Enable Transfer Acceleration between the two buckets. Set up a KMS Key in the primary region for encrypting objects</strong> is incorrect. You have to enable Cross-Region replication and not Transfer Acceleration. This feature simply enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket but not data replication.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html \">https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/ \">https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/</a></p><p><a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">https://aws.amazon.com/about-aws/global-infrastructure/regions_az/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>"
        }
      },
      {
        "id": 82921352,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A Big Data analytics company has deployed a stream processing application using KCL to read records from Kinesis Data Streams configured with multiple shards. The application is running on one EC2 instance. It seems that the consuming application is lagging under a large load and therefore records are not processed in time and eventually dropped from the stream.</p>\n\n<p>As a DevOps Engineer, you have been tasked with improving the reliability of this application with minimal changes, what should you do? (Select two)</p>\n",
          "answers": [
            "<p>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></p>",
            "<p>Increase the stream data retention period</p>",
            "<p>Migrate the application to AWS Lambda</p>",
            "<p>Increase the number of shards in Kinesis to increase throughput</p>",
            "<p>Decrease the numbers of shards in Kinesis to decrease the load</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></strong></p>\n\n<p>In a typical Kinesis Data Streams architecture, you have producers that continually push data to Kinesis Data Streams, and the consumers process the data in real-time. Consumers (such as a custom application running on Amazon EC2 or an Amazon Kinesis Data Firehose delivery stream) can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>Key concepts for Kinesis Data Streams:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>The Kinesis Client Library (KCL) ensures that for every shard, there is a record processor running and processing that shard. The library also simplifies reading data from the stream. The Kinesis Client Library uses an Amazon DynamoDB table to store control data.</p>\n\n<p>For the given use-case, you need to run KCL on multiple EC2 instances behind an ASG. Running more KCL processes is the key here, and we need for that to have an Auto Scaling Group based on the metric <code>MillisBehindLatest</code>, which represents the time that the current iterator is behind from the latest record (tip) in the shard. The Kinesis Client Library (KCL) for Amazon Kinesis Data Streams publishes custom Amazon CloudWatch metrics on your behalf, using the name of your KCL application as the namespace.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n\n<p><strong>Increase the stream data retention period</strong></p>\n\n<p>The retention period is the length of time that data records are accessible after they are added to the stream. A stream\u2019s retention period is set to a default of 24 hours after creation. To avoid records being dropped, it's good to increase the stream retention time and allow ourselves a higher margin to process the records. The maximum retention you can set is 7 days.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to AWS Lambda</strong> - Migrating the application to AWS Lambda will not help with the processing time, as eventually, the same processing code would run under EC2 or Lambda.</p>\n\n<p><strong>Increase the number of shards in Kinesis to increase throughput</strong> - Increasing the number of shards in Kinesis can increase the total throughput of the stream, but this does not impact the processing performance of your processes (which is bound by what you do with the messages). Increasing the number of shards though would help you increase the number of processing processes in KCL if that was already an upper bound (but currently we only have one KCL process running so it's not running at capacity).</p>\n\n<p><strong>Decrease the numbers of shards in Kinesis to decrease the load</strong> - Decrease the number of shards would decrease the throughput but again would have no effect on processing applications regarding their performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n"
        }
      },
      {
        "id": 138248111,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company is planning to deploy a new version of their legacy application in AWS which is deployed to an Auto Scaling group of EC2 instances with an Application Load Balancer in front. To avoid any disruption of their services, they need to implement canary testing first before all of the traffic is shifted to the new application version.</p><p>Which of the following solutions can meet this requirement?</p>",
          "answers": [
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</p>",
            "<p>Do a Canary deployment using CodeDeploy with a <code>CodeDeployDefault.LambdaCanary10Percent30Minutes</code> deployment configuration.</p>",
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers.</p>",
            "<p>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment.</p>"
          ],
          "explanation": "<p>The purpose of a canary deployment is to reduce the risk of deploying a new version that impacts the <a href=\"https://wa.aws.amazon.com/wat.concept.workload.en.html\" title=\"The set of components that together deliver business value.\">workload</a>. The method will incrementally deploy the new version, making it visible to new users in a slow fashion. As you gain confidence in the deployment, you will deploy it to replace the current version in its entirety.</p><p><img src=\"https://media.tutorialsdojo.com/public/Upgrades_Image1.jpeg\"></p><p>To properly implement the canary deployment, you should do the following steps:</p><p>- Use a router or load balancer that allows you to send a small percentage of users to the new version.</p><p>- Use a dimension on your KPIs to indicate which version is reporting the metrics.</p><p>- Use the metric to measure the success of the deployment; this indicates whether the deployment should continue or rollback.</p><p>- Increase the load on the new version until either all users are on the new version or you have fully rolled back.</p><p><br></p><p>Hence, the correct answer is: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</strong></p><p>The option that says: <strong>Do a Canary deployment using CodeDeploy with a </strong><code><strong>CodeDeployDefault.LambdaCanary10Percent30Minutes</strong></code><strong> deployment configuration</strong> is incorrect because this specific configuration type is only applicable for Lambda functions and for the applications hosted in an Auto Scaling group.</p><p>The option that says: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers</strong> is incorrect because you can't use CloudFront to adjust the weight of the incoming traffic to your application. You should use Route 53 instead.</p><p>The option that says: <strong>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment</strong> is incorrect because you can only integrate a Network Load Balancer to your Amazon API Gateway. Moreover, this service is only applicable for APIs, not full-fledged web applications.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html\">https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/\">https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/</a></p>"
        }
      },
      {
        "id": 138248229,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A multinational investment bank is implementing regulatory compliance checks over its AWS accounts. All API calls made on each AWS resource across the company's accounts must be monitored and tracked for auditing purposes. AWS CloudTrail will be used to monitor all API activities and detect sensitive security issues in the company's AWS accounts. The DevOps Team was assigned to come up with a solution to prevent CloudTrail from being disabled on some AWS accounts automatically.</p><p>What solution should be applied to ensure CloudTrail log deliveries experience the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of <code>Configuration changes</code>. By default, this managed rule will automatically remediate the accounts that disabled its CloudTrail.</p>",
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a <code>StopLogging</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>StartLogging</code> API on the resource ARN.</p>",
            "<p>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications.</p>",
            "<p>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a <code>DeleteTrail</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>CreateTrail</code> API on the resource ARN.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides <em>AWS managed rules</em>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly assess whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p><strong><img src=\"https://media.tutorialsdojo.com/public/td-aws-config-diagram-13Jan2025.png\"></strong>You can customize the behavior of a managed rule to suit your needs. For example, you can define the rule's scope to constrain which resources trigger an evaluation for the rule, such as EC2 instances or volumes. You can customize the rule's parameters to define attributes that your resources must have to comply with the rule. For example, you can customize a parameter to specify that your security group should block incoming traffic to a specific port number.</p><p>After you activate a rule, AWS Config compares your resources to the rule's conditions. After this initial evaluation, AWS Config continues to run evaluations each time one is triggered. The evaluation triggers are defined as part of the rule, and they can include the following types:</p><p><strong>Configuration changes</strong> \u2013 AWS Config triggers the evaluation when any resource that matches the rule's scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p><p><strong>Periodic</strong> \u2013 AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p><p>The cloudtrail-enabled checks whether AWS CloudTrail is enabled in your AWS account. Optionally, you can specify which S3 bucket, SNS topic, and Amazon CloudWatch Logs ARN to use.</p><p><img src=\"https://media.tutorialsdojo.com/aws-config-cloudtrail-enabled.JPG\"></p><p>Hence, the correct answer is: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a </strong><code><strong>StopLogging</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>StartLogging</strong></code><strong> API on the resource ARN.</strong></p><p>The option that says: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of </strong><code><strong>Configuration changes</strong></code><strong>. This managed rule will automatically remediate the accounts that disabled its CloudTrail </strong>is incorrect because, by default, AWS Config will not automatically remediate the accounts that disabled its CloudTrail. You must manually set this up using an Amazon EventBridge rule and a custom Lambda function that calls the StartLogging API to enable CloudTrail back again. Furthermore, the <code><strong>cloudtrail-enabled</strong></code> AWS Config managed rule is only available for the <code>periodic trigger</code> type and not <code>Configuration changes</code>.</p><p>The option that says: <strong>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications</strong> is incorrect. AWS Cloud Development Kit (AWS CDK) is only an open-source software development framework for building cloud applications and infrastructure using programming languages. It isn't used to check whether the CloudTrail is enabled in an AWS account.</p><p>The option that says: <strong>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a </strong><code><strong>DeleteTrail</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>CreateTrail</strong></code><strong> API on the resource ARN</strong> is incorrect. Instead, you should detect the <code>StopLogging</code> event and call the StartLogging API to enable CloudTrail again. The <code>DeleteTrail</code> and <code>CreateTrail</code> events, as their name implies, are simply for deleting and creating the trails respectively.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/\">https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/v2/guide/home.html\">https://docs.aws.amazon.com/cdk/v2/guide/home.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 99528227,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>An application is being deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The security team requires that the traffic is secured with SSL/TLS certificates. Protection against common web exploits must also be implemented. The solution should not have a performance impact on the EC2 instances.</p><p>What steps should be taken to secure the web application? (Select TWO.)</p>",
          "answers": [
            "<p>Create an AWS WAF web ACL and attach it to the ALB.</p>",
            "<p>Add an SSL/TLS certificate to a secure listener on the ALB.</p>",
            "<p>Install SSL/TLS certificates on the EC2 instances.</p>",
            "<p>Configure Server-Side Encryption with KMS managed keys.</p>",
            "<p>Enable EBS encryption for the EC2 volumes to encrypt all traffic.</p>"
          ],
          "explanation": "<p>To secure the traffic in transit an SSL/TLS certificate should be attached to a secure listener on the ALB. This will not affect the performance of the EC2 instances as the encryption takes place only between the client and the ALB. The certificate can be issued through AWS Certificate Manager (ACM).</p><p>The AWS Web Application Firewall (AWS WAF) protects against common web exploits. The company can create a web ACL with a rule and action and then attach it to the ALB. This will protect against web exploits.</p><p><strong>CORRECT: </strong>\"Add an SSL/TLS certificate to a secure listener on the ALB\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL and attach it to the ALB\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install SSL/TLS certificates on the EC2 instances\" is incorrect.</p><p>Encryption on the EC2 instances would impact the performance of those instances.</p><p><strong>INCORRECT:</strong> \"Configure Server-Side Encryption with KMS managed keys\" is incorrect.</p><p>This is not relevant to in transit encryption, this is used to encrypt data at rest on services such as Amazon S3.</p><p><strong>INCORRECT:</strong> \"Enable EBS encryption for the EC2 volumes to encrypt all traffic\" is incorrect.</p><p>EBS encryption is used for encrypting data at rest. The question requires encryption using SSL/TLS certificates which is encryption in transit.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\">https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\">https://aws.amazon.com/elasticloadbalancing/application-load-balancer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      }
    ],
    "answers": {
      "75949056": [
        "d"
      ],
      "75949118": [
        "a"
      ],
      "75949156": [
        "d"
      ],
      "75949174": [
        "c"
      ],
      "82921318": [
        "b"
      ],
      "82921352": [
        "a",
        "b"
      ],
      "82921464": [
        "a"
      ],
      "99528227": [
        "a",
        "b"
      ],
      "115961527": [
        "a"
      ],
      "134588393": [
        "d"
      ],
      "134588431": [
        "d"
      ],
      "138248111": [
        "b"
      ],
      "138248159": [
        "b"
      ],
      "138248203": [
        "a",
        "b"
      ],
      "138248207": [
        "a"
      ],
      "138248213": [
        "c"
      ],
      "138248217": [
        "d"
      ],
      "138248229": [
        "a"
      ],
      "138248239": [
        "b"
      ],
      "143860761": [
        "c",
        "d"
      ]
    }
  },
  {
    "id": "1769846401359",
    "date": "2026-01-31T08:00:01.359Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 7,
    "incorrect": 3,
    "unanswered": 0,
    "total": 10,
    "percent": 70,
    "duration": 5057695,
    "questions": [
      {
        "id": 134588405,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A business wants to leverage AWS CloudFormation to deploy its infrastructure. The business would like to restrict deployment to two particular regions and wants to implement a strict tagging requirement. Developers are expected to deploy various versions of the same application and want to guarantee that resources are deployed in compliance with the business policy while still enabling developers to deploy different versions of the application.</p><p>Which of the following is the MOST suitable solution?</p>",
          "answers": [
            "<p>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</p>",
            "<p>Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks.</p>",
            "<p>Utilize approved CloudFormation templates and launch CloudFormation StackSets.</p>",
            "<p>Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation.</p>"
          ],
          "explanation": "<p>With <strong>AWS Service Catalog</strong>, cloud resources can be centrally managed to achieve infrastructure as code (IaC) template governance at scale, whether written in CloudFormation or Terraform. Compliance requirements can be met while ensuring customers can efficiently deploy the necessary cloud resources.</p><p><img alt=\"Service Catalog\" height=\"540\" src=\"https://media.tutorialsdojo.com/public/dop-c02-service-catalog.png\" width=\"1000\"></p><p>Template constraints can be applied when limiting end-users' options during a product launch. This ensures that the organization's compliance requirements are not breached.</p><p>A product must be present within a Service Catalog portfolio to apply template constraints. A template constraint includes rules that narrow the allowable values for parameters in the underlying AWS CloudFormation template of the product. These parameters define the set of values available to users when creating a stack. For instance, an instance type parameter can be specified to limit the types of instances that users can choose from when launching a stack containing EC2 instances.</p><p>Hence, the correct answer is: <strong>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</strong></p><p>The option that says:<strong> Utilize approved CloudFormation templates and launch CloudFormation StackSets </strong>is incorrect because StackSets manage deployments across accounts and regions but do not enforce tagging or region restrictions. They do not typically provide governance to prevent non-compliant implementations.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks </strong>is incorrect because Trusted Advisor does not support checks for unauthorized StackSets or enforce CloudFormation template compliance.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation </strong>is incorrect because drift detection only identifies changes after deployment, but cannot prevent non-compliant resource creation or enforce policies before deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/servicecatalog/\">https://aws.amazon.com/servicecatalog/</a></p><p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/\">https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/</a></p><p><br></p><p><strong>Check out this AWS Service Catalog Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-service-catalog/?src=udemy\">https://tutorialsdojo.com/aws-service-catalog/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/</a></p>"
        }
      },
      {
        "id": 115961529,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An online sales application is being migrated to AWS with the application layer hosted on Amazon EC2 instances and the database layer on a PostgreSQL database. It is mandated that the application must have minimal downtime as it receives traffic 24/7 and any downtime may reduce business revenue. The application must also be fault tolerant including the data layer.</p><p>Concerns have been raised around performance of the database layer during sales events and other peak periods. The application must also be continually scanned for vulnerabilities.</p><p>Which option will meet the above requirements?</p>",
          "answers": [
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use RDS PostgreSQL for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments.</p>",
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon Macie to perform automatic security assessments.</p>",
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon GuardDuty to perform automatic security assessments.</p>",
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments.</p>"
          ],
          "explanation": "<p>The above question clearly mandates three requirements:</p><p>1. Performance- Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group</p><p>2. Database performance- Amazon Aurora will perform better than PostgreSQL since it provides multi-master configuration and can be scaled better than RDS on a global scale.</p><p>3. Vulnerability assessment- Amazon Inspector is the right fit for the scanning. The difference between Amazon Inspector and Amazon GuardDuty is that the former \"checks what happens when you actually get an attack\" and the latter \"analyzes the actual logs to check if a threat exists\". The purpose of Amazon Inspector is to test whether you are addressing common security risks in the target AWS.</p><p>Database categorization and selection parameters:</p><p>\u00b7 If your scaling needs are for standard/ general purpose applications, RDS is the better option. You can auto-scale the database to max capacity with just a few clicks on the AWS console.</p><p>\u00b7 You also have the option of Aurora Serverless that can scale up or scale down well, you have to be aware of several <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations\">restrictions that apply in the Serverless mode</a>.</p><p>\u00b7 If you must handle a very high volume of read/write requests, DynamoDB is a better choice. It scales seamlessly with no impact on performance. You can run these database servers in on-demand or provisioned capacity mode.</p><p>If you have heavy write workloads and require more than five read replicas, Aurora is a better choice. Since Aurora uses shared storage for writer and readers, there is minimal replica lag. RDS allows only up to five replicas and the replication process is slower than Aurora.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-46-18-8cd4e56a381e9f1f934f96761a29cae7.jpg\"><p><strong>CORRECT: </strong>\"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use RDS PostgreSQL for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments\" is incorrect.</p><p>Amazon Aurora is a better fit for this use case as described above.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon Macie to perform automatic security assessments\" is incorrect.</p><p>Amazon Inspector should be used for performing the security assessments.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon GuardDuty to perform automatic security assessments\" is incorrect.</p><p>Amazon Inspector should be used for performing the security assessments.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p>https://aws.amazon.com/blogs/database/is-amazon-rds-for-postgresql-or-amazon-aurora-postgresql-a-better-choice-for-me/</p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>"
        }
      },
      {
        "id": 82921406,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A multi-national retail company has defined tagging guidelines and standard for all its resources in AWS and would like to create a dashboard to visualize the compliance of all the resources with the ability to find out the non-compliant resources. The company has hired you as an AWS Certified DevOps Engineer Professional to develop a solution for this requirement.</p>\n\n<p>Which of the following options would you suggest to address the use-case?</p>\n",
          "answers": [
            "<p>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</p>",
            "<p>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</p>",
            "<p>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</p>",
            "<p>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</strong></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p>Here, we can use AWS Config to track resource configuration, and we could create a rule to track the tagging of these resources. All the changes to resource configuration as well as tagging of resources are streamed to an SNS topic.</p>\n\n<p>A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags make it easier to manage, search for, and filter resources. You can use tags to categorize your AWS resources in different ways, for example, by purpose, owner, or environment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p>You can set up the Required-Tag managed rule for Config which requires up to 6 tags with optional values in a single rule. Previously, each rule accepted only a single tag/value combo. Additionally, the Required-Tag managed rule now accepts a comma-separated list of values for each checked tag. This allows for a rule to be compliant if any one of a supplied list of tags is present on the resource.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</strong> - AWS Service Catalog enabling AWS customers to create and deliver standardized services that provide the necessary control, while still empowering developers to choose the services that best fit their needs. You cannot use Service Catalog to get an inventory of all the resources in your account.</p>\n\n<p><strong>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</strong> - SSM inventory will only help with understanding what is installed on your managed instances. To view Systems Manager Inventory history and change tracking for all of your managed instances, you need to use AWS Config itself.</p>\n\n<p><strong>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</strong> - CloudTrail is used to track API calls, not resources. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n"
        }
      },
      {
        "id": 75949108,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>",
          "answers": [
            "<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"
          ],
          "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921460,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A gaming company would like to be able to receive near real-time notifications when the API call <code>DeleteTable</code> is invoked in DynamoDB.</p>\n\n<p>As a DevOps Engineer at the company, how would you implement this at a minimal cost?</p>\n",
          "answers": [
            "<p>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</p>",
            "<p>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</p>",
            "<p>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</p>",
            "<p>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. Rules in CloudWatch Events work only in the Region in which they are created. If you configure CloudTrail to track API calls in multiple Regions, and you want a rule-based on CloudTrail to trigger in each of those Regions, you must create a separate rule in each Region that you want to track.</p>\n\n<p>For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q40-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</strong> - A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. DynamoDB Streams do not capture <code>DeleteTable</code> API calls, they only capture item-level events.</p>\n\n<p><strong>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</strong> - Sending CloudTrail Logs to CloudWatch Logs and creating a filter on those will work but will be expensive, as we're streaming all the logs from CloudTrail just to extract a single event.</p>\n\n<p><strong>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</strong> - CloudTrail trails do not have event filters and cannot be directly sent to a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p>\n"
        }
      },
      {
        "id": 75949166,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An application sits behind a Network Load Balancer (NLB) that is configured with a TLS listener. The DevOps team must analyze traffic patterns and require information about the connections made by clients. The data that is captured must be stored securely with encryption at rest and should only be accessible to the DevOps team members.</p><p>Which actions should a DevOps engineer take?</p>",
          "answers": [
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing provides access logs that capture detailed information about the TLS requests sent to your Network Load Balancer. You can use these access logs to analyze traffic patterns and troubleshoot issues. The logs are sent to an Amazon S3 bucket you configure as the logging destination. This bucket can be encrypted using one of the available server-side encryption options.</p><p>When you enable access logging, you must specify an S3 bucket for the access logs. The policy must grant permission to the AWS service account \u2018delivery.logs.amazonaws.com\u2019. In this case, the DevOps team also require permissions to access the bucket, and this can be granted through an IAM policy attached to the team members, most likely via an IAM group.</p><p><strong>CORRECT: </strong>\"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account\" is incorrect.</p><p>This will not allow read access for the DevOps team as the only permission is write access.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 82921334,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.</p>\n\n<p>How can you do this most securely?</p>\n",
          "answers": [
            "<p>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</p>",
            "<p>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>",
            "<p>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</p>",
            "<p>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</strong></p>\n\n<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.</p>\n\n<p>You can use Secrets Manager to natively rotate credentials for Amazon Relational Database Service (RDS), Amazon DocumentDB, and Amazon Redshift. You can extend Secrets Manager to rotate other secrets, such as credentials for Oracle databases hosted on EC2 or OAuth refresh tokens, by modifying sample AWS Lambda functions available in the Secrets Manager documentation.</p>\n\n<p>To access PostgreSQL, you can use database credentials and they're best stored in Secrets Manager from a security best-practices perspective. Access to Secrets Manager itself is regulated using an IAM role with the requisite policy. You must write this IAM policy permitting your application on EC2 instances to access specific secrets. Then, in the application source code, you can replace secrets in plain text with code to retrieve these secrets programmatically using the Secrets Manager APIs. To access the DynamoDB table, you should also add the appropriate policy to this IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - As mentioned in the explanation above, Secrets Manager does NOT support DynamoDB, so this option is incorrect.</p>\n\n<p><strong>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</strong> - It's not recommended to store RDS credentials in a DynamoDB, as it can be accessed by everyone who has access to the underlying table. This constitutes a serious security threat.</p>\n\n<p><strong>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - Storing IAM user credentials in Secrets Manager is a distractor as IAM user credentials are not required to build a solution for this use-case. You can just use an IAM instance role for EC2 to access Secrets Manager.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/faqs/\">https://aws.amazon.com/secrets-manager/faqs/</a></p>\n"
        }
      },
      {
        "id": 75949116,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company has deployed a web service that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company has deployed the application in us-east-1. The web service uses Amazon Route 53 records for DNS requests for example.com. The records are configured with health checks that assess the availability of the web service.</p><p>A second environment has been deployed into eu-west-1. The company requires traffic to be routed to the environment that provides the lowest latency for user requests. In the event of a regional outage, traffic should be directed to the alternate Region.</p><p>Which configuration will achieve these requirements?</p>",
          "answers": [
            "<p>Create a subdomain named us.example.com with failover routing. Configure the US ALB as primary and the EU ALB as secondary. Create another subdomain named eu.example.com with failover routing. Configure the EU ALB as primary and the US ALB as secondary. Create latency-based routing records for example.com that are aliased to us.example.com and eu.example.com.</p>",
            "<p>Create a subdomain named us.example.com with weighted routing. Configure the US ALB with weight 2 and the EU ALB with weight 1. Create another subdomain named eu.example.com with weighted routing. Configure the EU ALB with weight 2 and the US ALB with weight 1. Create geolocation routing records for example.com with North America aliased to us.example.com and Europe aliased to eu.example.com.</p>",
            "<p>Create a subdomain named us.example.com with latency-based routing. Configure the US ALB as the first target and the EU ALB as the second target. Create another subdomain named eu.example.com with latency-based routing. Configure the EU ALB as the first target and the US ALB as the second target. Create failover routing records for example.com aliased to us.example.com as the first target and eu.example.com as the second target.</p>",
            "<p>Create a subdomain named us.example.com with multivalue answer routing. Configure the US ALB first and the EU ALB second. Create another subdomain named eu.example.com with multivalue answer routing. Configure the EU ALB first and the US ALB second. Create failover routing records for example.com that are aliased to us.example.com and eu.example.com.</p>"
          ],
          "explanation": "<p>There are two key requirements that inform the design. Firstly, the solution must route based on latency. Secondly, failover across Regions must be automatic. This is a more complex DNS routing configuration. To meet both requirements the company will need to use a combination of latency-based routing records and failover routing records.</p><p>The solution is to create subdomains for each Region that can be used for the failover records and pointing the secondary to the alternate Region. Then, on top of those records the solution includes a latency-based routing record for example.com.</p><p>With this solution example.com will resolve to the subdomain that represents the lowest latency from the user request location. If the environment in that Region is not available (has failed health checks) then the request will be failed over to the alternate Region.</p><p><strong>CORRECT: </strong>\"Create a subdomain named us.example.com with failover routing. Configure the US ALB as primary and the EU ALB as secondary. Create another subdomain named eu.example.com with failover routing. Configure the EU ALB as primary and the US ALB as secondary. Create latency-based routing records for example.com that are aliased to us.example.com and eu.example.com\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with weighted routing. Configure the US ALB with weight 2 and the EU ALB with weight 1. Create another subdomain named eu.example.com with weighted routing. Configure the EU ALB with weight 2 and the US ALB with weight 1. Create geolocation routing records for example.com with North America aliased to us.example.com and Europe aliased to eu.example.com\" is incorrect.</p><p>The solution should use failover routing and latency routing, not weighted routing and geolocation routing.</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with latency-based routing. Configure the US ALB as the first target and the EU ALB as the second target. Create another subdomain named eu.example.com with latency-based routing. Configure the EU ALB as the first target and the US ALB as the second target. Create failover routing records for example.com aliased to us.example.com as the first target and eu.example.com as the second target\" is incorrect.</p><p>This solution gets the failover and latency records the wrong way around. Failover routing should be used for the subdomain and latency routing for the apex domain (example.com).</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with multivalue answer routing. Configure the US ALB first and the EU ALB second. Create another subdomain named eu.example.com with multivalue answer routing. Configure the EU ALB first and the US ALB second. Create failover routing records for example.com that are aliased to us.example.com and eu.example.com\" is incorrect.</p><p>Multivalue routing is a form of DNS load balancing and will simply route records across all registered and available targets. This does not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>"
        }
      },
      {
        "id": 75949158,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A financial services company requires that DevOps engineers should not log directly into Amazon EC2 instances that process highly sensitive data except in exceptional circumstances. The security team requires a notification within 15 minutes if a DevOps engineer does log in to an instance.</p><p>Which solution will meet these requirements with the least operational overhead?</p>",
          "answers": [
            "<p>Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>"
          ],
          "explanation": "<p>The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. The agent includes the following components:</p><p>\u00b7 A plug-in to the AWS CLI that pushes log data to CloudWatch Logs.</p><p>\u00b7 A script (daemon) that initiates the process to push data to CloudWatch Logs.</p><p>\u00b7 A cron job that ensures that the daemon is always running.</p><p>You can create metric filters to match terms in your log events and convert log data into metrics. When a metric filter matches a term, it increments the metric's count. For example, you can create a metric filter that counts the number of times the word <strong><em>ERROR</em></strong> occurs in your log events.</p><p>In this case the metric filter can search for user login data and then if this information is found it can send an SNS notification to the security team.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>The Systems Manager agent will not gather this information from EC2 instances. The CloudWatch Logs agent must be installed.</p><p><strong>INCORRECT:</strong> \"Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>CloudTrail will only report on API activity, and this does not include login data from an Amazon EC2 instance.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>This is possible though it is not the best solution as it requires the script to be rerun on a regular basis and requires more operational overhead to create and maintain.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 75949120,
        "correct_response": [
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>An application running on an Amazon EC2 instance stores sensitive data on an attached Amazon EBS volume. The volume is not encrypted. A DevOps engineer must enable encryption at rest for the data.</p><p>Which actions should the engineer take? (Select TWO.)</p>",
          "answers": [
            "<p>Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume.</p>",
            "<p>Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data.</p>",
            "<p>Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted.</p>",
            "<p>Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume.</p>",
            "<p>Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume.</p>"
          ],
          "explanation": "<p>You cannot enable encryption for an existing EBS volume. You must enable encryption of the volume at creation time. There are a couple of ways to enable encryption of data stored on an unencrypted volume:</p><p>1) Create a snapshot of the volume. The snapshot will also be unencrypted, but you can then copy it and enable encryption. Then, you can create an encrypted volume from the snapshot and attach it to the instance.</p><p>2) Create and mount a new, encrypted EBS volume. The engineer would then need to move data onto the volume.</p><p>In both cases the engineer will need to update the application to use the new volume.</p><p><strong>CORRECT: </strong>\"Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume\" is incorrect.</p><p>This has not resulted in any change to the EBS volume, it is still unencrypted.</p><p><strong>INCORRECT:</strong> \"Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data\" is incorrect.</p><p>You cannot enable encryption for existing volumes through any AWS tools.</p><p><strong>INCORRECT:</strong> \"Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume\" is incorrect.</p><p>SSL/TLS certificates are used for enabling encryption in-transit, not encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>"
        }
      }
    ],
    "answers": {
      "75949108": [
        "c"
      ],
      "75949116": [
        "a"
      ],
      "75949120": [
        "c",
        "e"
      ],
      "75949158": [
        "a"
      ],
      "75949166": [
        "a"
      ],
      "82921334": [
        "a"
      ],
      "82921406": [
        "a"
      ],
      "82921460": [
        "d"
      ],
      "115961529": [
        "d"
      ],
      "134588405": [
        "a"
      ]
    }
  },
  {
    "id": "1769688092801",
    "date": "2026-01-29T12:01:32.801Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 12,
    "incorrect": 8,
    "unanswered": 0,
    "total": 20,
    "percent": 60,
    "duration": 5297548,
    "questions": [
      {
        "id": 82921412,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>An IT company is deploying a Python Flask based application and would like to ensure that it has a base AMI that contains the necessary Python runtime, as well as OS patches. That AMI must be used able to be referenced programmatically from across all regions in your account in a scalable way. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>Which of the following options would you recommend for this use-case? (Select two)</p>\n",
          "answers": [
            "<p>Create an SSM Automation document to create the AMI in a repeatable manner</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Use AWS Inspector to create a patched AMI using the latest working AMI</p>",
            "<p>Use AWS Lambda to create a patched AMI using the latest working AMI</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an SSM Automation document to create the AMI in a repeatable manner</strong></p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>You can use AWS Systems Manager to create a maintenance window, and then register an Automation task to automate the creation of the AMIs. This process is applicable for both Windows and Linux instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q46-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</strong></p>\n\n<p>The AMI ID is region-scoped, so the AMI must be copied across regions and therefore each SSM parameter store will have different AMI ID values. But you can still use the same SSM Parameter Store key across all regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</strong> - The AMI ID is region-scoped and the AMI must be copied across regions for the solution to work. This option only copies the value of the AMI ID across regions but the AMI itself stays in one region. So this option is incorrect.</p>\n\n<p><strong>Use AWS Inspector to create a patched AMI using the latest working AMI</strong> - AWS Inspector can be leveraged to analyze EC2 instance OS and network vulnerabilities. You cannot use Inspector to create a patched AMI.</p>\n\n<p><strong>Use AWS Lambda to create a patched AMI using the latest working AMI</strong> - AWS Lambda cannot be used to create AMIs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n"
        }
      },
      {
        "id": 138248241,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A commercial bank has a hybrid cloud architecture in AWS where its online banking platform is hosted. The CTO instructed its Lead DevOps Engineer to implement a suitable deployment set up that minimizes the impact on their production environment. The CI/CD process should be configured as follows: </p><p>- A new fleet of Amazon EC2 instances should be automatically launched first before the actual production deployment. The additional instances will serve traffic during the deployment. </p><p>- All available EC2 instances across various Availability Zones must be load-balanced and must automatically heal if it becomes impaired due to an underlying hardware failure in Amazon EC2. </p><p>- At least half of the incoming traffic must be rerouted to the new application version that is hosted to the new instances. </p><p>- The deployment should be considered successful if traffic is rerouted to at least half of the available EC2 instances. </p><p>- All temporary files must be deleted before routing traffic to the new fleet of instances. Ensure that any other files that were automatically generated during the deployment process are removed. </p><p>- To reduce costs, the EC2 instances that host the old version in the deployment group must be terminated immediately. </p><p>What should the Engineer do to satisfy these requirements?</p>",
          "answers": [
            "<p>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the <code>CodeDeployDefault.OneAtAtime</code> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the <code>AllowTraffic</code> hook within the <code>appspec.yml</code> configuration file to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the <code>BeforeAllowTraffic</code> Traffic hook within <code>appspec.yml</code> to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the <code>CodeDeployDefault.HalfAtAtime</code> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the &lt;code&gt;CodeDeployDefault AllatOnce&lt;/code&gt; as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>"
          ],
          "explanation": "<p>The content in the <code>'hooks'</code> section of the AppSpec file varies, depending on the compute platform for your deployment. The <code>'hooks'</code> section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The <code>'hooks'</code> section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line. Here are descriptions of the hooks available for use in your AppSpec file.</p><p><code><strong>ApplicationStop</strong></code> \u2013 This deployment lifecycle event occurs even before the application revision is downloaded. You can specify scripts for this event to gracefully stop the application or remove currently installed packages in preparation for a deployment. The AppSpec file and scripts used for this deployment lifecycle event are from the previous successfully deployed application revision.</p><p><code><strong>DownloadBundle</strong></code> \u2013 During this deployment lifecycle event, the CodeDeploy agent copies the application revision files to a temporary location:</p><p><code>/opt/codedeploy-agent/deployment-root/<em>deployment-group-id</em>/<em>deployment-id</em>/deployment-archive</code> folder on Amazon Linux, Ubuntu Server, and RHEL Amazon EC2 instances.</p><p><code>C:\\ProgramData\\Amazon\\CodeDeploy\\<em>deployment-group-id</em>\\<em>deployment-id</em>\\deployment-archive</code> folder on Windows Server Amazon EC2 instances.</p><p>This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>BeforeInstall</strong></code> \u2013 You can use this deployment lifecycle event for preinstall tasks, such as decrypting files and creating a backup of the current version.</p><p><code><strong>Install</strong></code> \u2013 During this deployment lifecycle event, the CodeDeploy agent copies the revision files from the temporary location to the final destination folder. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterInstall</strong></code> \u2013 You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions.</p><p><code><strong>ApplicationStart</strong></code> \u2013 You typically use this deployment lifecycle event to restart services that were stopped during <code>ApplicationStop</code>.</p><p><code><strong>ValidateService</strong></code> \u2013 This is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.</p><p><code><strong>BeforeBlockTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances before they are deregistered from a load balancer.</p><p><code><strong>BlockTraffic</strong></code> \u2013 During this deployment lifecycle event, internet traffic is blocked from accessing instances that are currently serving traffic. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterBlockTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><code><strong>BeforeAllowTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances before they are registered with a load balancer.</p><p><code><strong>AllowTraffic</strong></code> \u2013 During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterAllowTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances after they are registered with a load balancer.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2019-12-16_03-04-56-691d69e91f667f1c9bb8b0554c2da933.png\"></p><p>Hence, the correct answer is: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BeforeAllowTraffic</strong></code><strong> Traffic hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files.</strong></p><p>The option that says: <strong>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the </strong><code><strong>CodeDeployDefault.OneAtAtime</strong></code><strong> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the </strong><code><strong>AllowTraffic</strong></code><strong> hook within the </strong><code><strong>appspec.yml</strong></code><strong> configuration file to purge the temporary files</strong> is incorrect because you should use blue/green deployment instead of in-place. In addition, the <code><strong><em>AllowTraffic</em></strong></code> event just allows the incoming traffic to the instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p>The option that says: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.HalfAtAtime</strong></code><strong> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appspec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because the <code>BlockTraffic</code> event is reserved for the CodeDeploy agent and cannot be used to run custom scripts such as deleting the temporary files.</p><p>The option that says: <strong>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.AllatOnce</strong></code><strong> as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because you should use a blue/green deployment instead of in-place. It is also incorrect to use the <code>CodeDeployDefault AllatOnce</code> deployment configuration as this attempts to deploy the application revision to as many instances as possible at once.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 82921416,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>The compliance department at a Wall Street trading firm has hired you as an AWS Certified DevOps Engineer Professional to help with several strategic DevOps initiatives. The department has asked you to regularly generate the list of all the software packages installed on the EC2 instances. The solution needs to be able to extend to future instances in the AWS account and send notifications if the instances are not set up correctly to track their software.</p>\n\n<p>Which of the following options are the best-fit solutions that require the least effort to meet the given requirements? (Select two)</p>\n",
          "answers": [
            "<p>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</p>",
            "<p>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</p>",
            "<p>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</p>",
            "<p>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</p>",
            "<p>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</strong></p>\n\n<p>SSM Agent is an Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources. The agent processes requests from the Systems Manager service in the AWS Cloud, and then run them as specified in the request. SSM Agent then sends status and execution information back to the Systems Manager service by using the Amazon Message Delivery Service (service prefix: ec2messages).</p>\n\n<p>SSM Inventory provides visibility into your Amazon EC2 and on-premises computing environment. You can use Inventory to collect metadata from your managed instances. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated.</p>\n\n<p>Sample Inventory Cards:\n<img src=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/images/inventory-cards.png\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><strong>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</strong></p>\n\n<p>Since SSM does not have any native capability to find out which instances are not currently tracked by it, so here we would need to create a custom Lambda function for this and send notifications if any new untracked instances are detected. We can trigger the Lambda function using CloudWatch Events.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</strong> - SSM does not have any native capability to find out which instances are not currently tracked by the SSM service.</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</strong> - You can use SSM Automation to build Automation workflows to configure and manage instances and AWS resources. You can also create custom workflows or use pre-defined workflows maintained by AWS. For the given requirement, SSM Automation could be used to get the list of packages but it would require a lot of manual work, so it is not the best fit for the given use-case.</p>\n\n<p><strong>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</strong> - Inspector is meant to find security vulnerabilities on EC2 instances, not to get a metadata list of your installed packages.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p>\n"
        }
      },
      {
        "id": 143860745,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company is using AWS Storage Gateway for a branch office location. The gateway is configured in file gateway mode in front of an Amazon S3 bucket that contains files that must be processed by workers in the branch office. Each night a batch process uploads many files to the S3 bucket. Users have reported that the new files are not visible in the morning though they do exist in the S3 bucket.</p><p>How can a DevOps engineer ensure that the files become visible?</p>",
          "answers": [
            "<p>Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration.</p>",
            "<p>Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded.</p>",
            "<p>Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway.</p>",
            "<p>Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command.</p>"
          ],
          "explanation": "<p>In file gateway mode the users and applications in the branch office can access the storage using either NFS or SMB protocols. The new objects in the bucket may sometimes not be visible. The RefreshCache operation refreshes the cached inventory of objects for the specified file share. This operation finds objects in the Amazon S3 bucket that were added, removed, or replaced since the gateway last listed the bucket's contents and cached the results.</p><p><strong>CORRECT: </strong>\"Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded\" is incorrect.</p><p>Volume Gateway mode uses block storage targets (iSCSI) rather than file storage targets (NFS/SMB) and would therefore be a major change to the architecture. Event notifications cannot be used to refresh the cache in a storage gateway.</p><p><strong>INCORRECT:</strong> \"Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway\" is incorrect. S3 replication cannot be used to replicate to storage gateway.</p><p><strong>INCORRECT:</strong> \"Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration\" is incorrect. Transfer acceleration is used to improve upload speeds and does not assist at all in this situation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html\">https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>"
        }
      },
      {
        "id": 82921384,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.</p>\n\n<p>What can be done to improve the performance of the setup?</p>\n",
          "answers": [
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</p>",
            "<p>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong></p>\n\n<p>A golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can also add the web application as part of the golden AMI. You can think of it as an input base AMI for creating a standardized application-specific golden AMI.</p>\n\n<p>Once you create a golden AMI for a product (a product can be a standardized OS-AMI that you want to distribute to accounts in your organization or an application-specific AMI you want to let your business unit(s) deploy in their environment), you can validate whether the AMI meets your expectations, and choose to approve or reject the AMI.</p>\n\n<p>About the golden AMI pipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q66-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</strong> - The local cache warmup can unfortunately not be improved, as caching is dynamic and data may change over time. So creating an AMI with a copy of the local data cache just serves as a distractor.</p>\n\n<p><strong>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong> - You cannot migrate from ElastiCache to DynamoDB for the given use-case, as it's primarily a NoSQL database and not a caching solution (You could use DAX as a caching solution with DynamoDB). Besides, the existing database is RDS MySQL which is a relational database, so DynamoDB does not really fit into this mix.</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</strong> - You cannot use Lambda to configure the instance local cache at boot time as caching is dynamic and data may change over time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n"
        }
      },
      {
        "id": 75949124,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps team manages an application that consists of four separate AWS Lambda functions. A DevOps Engineer on the team has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule that executes the pipeline after a change is made to the application source code. During testing, the engineer noticed that the pipeline takes a long time to complete.</p><p><br></p><p>What should the DevOps Engineer do to improve the speed of the pipeline?</p>",
          "answers": [
            "<p>Modify the CodeBuild projects within the pipeline to use a compute type with more available vCPUs.</p>",
            "<p>Configure parallel executions of the Lambda functions by specifying the same runOrder in the CodePipeline configuration for the stage.</p>",
            "<p>Modify CodeBuild execute the builds in batches using a build graph deployment and specify the dependency chain.</p>",
            "<p>Configure the CodeBuild projects to run within a VPC and use dedicated instances to increase the build throughput.</p>"
          ],
          "explanation": "<p>The best way to speed up the pipeline will be to run the builds in parallel. This can be achieved through the pipeline configuration by specifying the runOrder to be the same for the build of each function within the action structure.</p><p>To specify parallel actions, you use the same integer for each action you want to run in parallel.</p><p><strong>CORRECT: </strong>\"Configure parallel executions of the Lambda functions by specifying the same runOrder in the CodePipeline configuration for the stage\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the CodeBuild projects to run within a VPC and use dedicated instances to increase the build throughput\" is incorrect.</p><p>Connecting to a VPC does not help and using dedicated instances is not the best way to improve the speed of the pipeline. Without specifying other changes the builds will still run sequentially.</p><p><strong>INCORRECT:</strong> \"Modify the CodeBuild projects within the pipeline to use a compute type with more available vCPUs\" is incorrect.</p><p>This may offer some improvement in speed but not as much as running the builds in parallel.</p><p><strong>INCORRECT:</strong> \"Modify CodeBuild execute the builds in batches using a build graph deployment and specify the dependency chain\" is incorrect.</p><p>CodeBuild can be configured to run builds in batches but a build list or build matrix should be used for running the builds in parallel. The build graph deployment runs the builds sequentially with dependencies mapped out.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921450,
        "correct_response": [
          "a",
          "c",
          "e"
        ],
        "prompt": {
          "question": "<p>As the Lead DevOps Engineer at a retail company, you have a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. You must collect the logs before an instance is terminated to perform log analytics later on. It's also necessary to collect all the access logs. The analysis of these logs should be performed at a minimal cost, and only need to be run from time to time.</p>\n\n<p>Which of the following options would you suggest to implement the MOST cost-optimal solution for this requirement? (Select three)</p>\n",
          "answers": [
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</p>",
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</p>",
            "<p>Enable Access Logs at the Application Load Balancer level</p>",
            "<p>Enable Access Logs at the Target Group level</p>",
            "<p>Analyze the logs using AWS Athena</p>",
            "<p>Analyze the logs using an EMR cluster</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</strong></p>\n\n<p>Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. When a scale-in event occurs, the terminating instance is first deregistered from the load balancer and while the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q71-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p>For the given use-case, you can configure a lifecycle hook to invoke the CloudWatch Event rule to trigger a Lambda function that launches an SSM Run Command to extract the application logs and store them in S3.</p>\n\n<p><strong>Enable Access Logs at the Application Load Balancer level</strong></p>\n\n<p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p>\n\n<p><strong>Analyze the logs using AWS Athena</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don\u2019t even need to load your data into Athena, it works directly with data stored in S3. You can analyze the access logs stored in S3 via Athena.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</strong> - CloudWatch Logs agent can only be used for continuous log streaming, and NOT for a one-time log extract to S3.</p>\n\n<p><strong>Enable Access Logs at the Target Group level</strong> - Please note that access logs are enabled at the ALB level and NOT at the target group level.</p>\n\n<p><strong>Analyze the logs using an EMR cluster</strong> - Analyzing logs at a low cost and in a serverless fashion should be done using AWS Athena. EMR clusters are usually long-running and cost a lot of money, and don't have serverless scaling capabilities.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n"
        }
      },
      {
        "id": 75949148,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer needs to stream data from Amazon CloudWatch Logs to a VPC-based Amazon OpenSearch Service cluster. The DevOps engineer creates an OpenSearch subscription filter for the OpenSearch cluster. However, the DevOps engineer notices that the logs are not visible in Amazon OpenSearch.</p><p>What should the DevOps engineer do to solve this problem?</p>",
          "answers": [
            "<p>Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role.</p>",
            "<p>Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch.</p>",
            "<p>Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role.</p>",
            "<p>Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch.</p>"
          ],
          "explanation": "<p>The configuration requires that the Lambda IAM execution role that is specified in the CloudWatch Logs configuration must have the trust relationship configured to allow lambda.amazonaws.com to assume the role. The AWSLambdaVPCAccessExecutionRole policy must also be added to the function to allow access to the VPC-based OpenSearch cluster.</p><p><strong>CORRECT: </strong>\"Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role\" is incorrect.</p><p>The trust relationship is defined in the role attached to the CloudWatch Logs configuration.</p><p><strong>INCORRECT:</strong> \"Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch\" is incorrect.</p><p>This policy should be attached to the Lambda IAM execution role, not the OpenSearch cluster.</p><p><strong>INCORRECT:</strong> \"Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch\" is incorrect.</p><p>The subscription filter is used instead of using an export which is a better and more automated solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/\">https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-opensearch/\">https://digitalcloud.training/amazon-opensearch/</a></p>"
        }
      },
      {
        "id": 82921374,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.</p>\n\n<p>How can you improve the user experience with the least effort?</p>\n",
          "answers": [
            "<p>Enhance the health check so that the return status code corresponds to the connectivity to the database</p>",
            "<p>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</p>",
            "<p>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</p>",
            "<p>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enhance the health check so that the return status code corresponds to the connectivity to the database</strong></p>\n\n<p>Configuring health checks for the Application Load Balancer (ALB) is an important step to ensure that your AWS Cloud application runs smoothly. The ALB Health Check is configured with a protocol and port number to call on the target instances. A healthy EC2 instance is one that issues a response to a health check call with an HTTP 200 response code. Instances that return a status code that is other than the 2XX range or which time out are designated as being unhealthy and will not receive traffic from the ELB.</p>\n\n<p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q69-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>You could just add a simple health check endpoint to the ALB which accepts a request and immediately responds with an HTTP status of 200. This approach provides for a fast health check, but would not meet the requirement for the given use-case. You need to improve the quality of the health check and make sure it returns a proper status code. As the application depends on the database, you need to ensure that you include health checks for these components when determining the health of your service.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</strong> - Migrating to Beanstalk would require significant effort and even then it won't help gather detailed database-specific health checks.</p>\n\n<p><strong>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</strong> - Health Checks for the ALB are pretty basic and only work with the HTTP return status code, not the payload itself.</p>\n\n<p><strong>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</strong> - Route53 health checks can only be used to prevent DNS records from being returned from a DNS query, so it won't help for routing to specific instances behind an ALB (that's why we have health checks at the ALB level).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf\">https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf</a></p>\n"
        }
      },
      {
        "id": 138248103,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A fast-growing company has multiple AWS accounts which are consolidated using AWS Organizations, and the company expects to add new accounts soon. The DevOps engineer was instructed to design a centralized logging solution to deliver all Amazon VPC Flow Logs and Amazon CloudWatch Logs across all sub-accounts to its dedicated Audit account for compliance purposes. The logs should also be properly indexed in order to perform search, retrieval, and analysis.</p><p>Which of the following is the MOST suitable solution that the engineer should implement to meet the above requirements?</p>",
          "answers": [
            "<p>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account.</p>",
            "<p>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</p>",
            "<p>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account.</p>",
            "<p>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account.</p>"
          ],
          "explanation": "<p>You can load streaming data into your Amazon OpenSearch Service domain from many different sources in AWS. Some sources, like Amazon Data Firehose and Amazon CloudWatch Logs, have built-in support for Amazon OpenSearch Service (successor to Amazon ElasticSearch). Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers. The Lambda functions respond to new data by processing it and streaming it to your domain.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-Amazon-OpenSearch-Service-02-04-2025.png\"></p><p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, Amazon Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. To begin subscribing to log events, create the receiving source, such as a Kinesis stream, where the events will be delivered. A subscription filter defines the filter pattern to use for filtering which log events get delivered to your AWS resource, as well as information about where to send matching log events to.</p><p>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Amazon Kinesis stream to perform custom processing and analysis. Custom processing is especially useful when you collaborate and analyze data across many accounts. For example, a company's information security group might want to analyze data for real-time intrusion detection or anomalous behaviors so it could conduct an audit of accounts in all divisions in the company by collecting their federated production logs for central processing.</p><p>A real-time stream of event data across those accounts can be assembled and delivered to the information security groups, who can use Kinesis to attach the data to their existing security analytic systems. Kinesis streams are currently the only resource supported as a destination for cross-account subscriptions.</p><p>Hence, the correct solution is: <strong>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</strong></p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account</strong> is incorrect. While Lambda functions can be used to process logs, relying solely on a Lambda function to handle a large volume of logs might only lead to performance bottlenecks and scalability issues. Lambda has concurrency limits, and processing logs from multiple sub-accounts may exceed these limits, especially as the company grows and more accounts are added.</p><p>The option that says: <strong>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account </strong>is incorrect because the CloudWatch subscription filter doesn't directly support SQS. You should use a Kinesis Data Stream, Kinesis Firehose, or Lambda function.</p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account</strong> is incorrect. Although this approach typically works, using a self-hosted OpenSearch cluster on an EC2 instance introduces unnecessary complexity and overhead regarding management, scaling, and availability. AWS-managed OpenSearch Service would be a more efficient and cost-effective option. Additionally, relying on EC2 for hosting OpenSearch adds management and operational burdens, whereas using AWS-managed services like Kinesis Data Streams or Firehose with OpenSearch is more reliable and easier to scale.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html</a></p><p><a href=\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 82921402,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An IT company is creating an online booking system for hotels. The booking workflow that the company has implemented can take over 3 hours to complete as a manual verification step is required by a 3rd party provider to ensure big transactions are not fraudulent.</p>\n\n<p>As a DevOps Engineer, you need to expose this as a secure API for the end customers. The website must be able to sustain 5000 requests at the same time. How should you implement this in the simplest possible way?</p>\n",
          "answers": [
            "<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</strong></p>\n\n<p>API Gateway APIs can directly invoke an AWS service and pass in a payload. It's a common way to provide a publicly available and secure API for your chosen AWS services.</p>\n\n<p>Amazon API Gateway integrates with AWS Step Functions, allowing you to call Step Functions with APIs that you create to simplify and customize interfaces to your applications. Step Functions makes it easy to coordinate the components of distributed applications and microservices as a series of steps in a visual workflow. You create state machines in the Step Functions Console or through the Step Functions API to specify and execute the steps of your application at scale. API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to implement the payment workflow using Step Functions. A key reason you need this integration is that AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</strong> - AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you. So there is no need to use Lambda as an intermediary for this workflow.</p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</strong></p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</strong></p>\n\n<p>Lambda functions cannot process the booking workflow as it may take 3 hours, which is more than the 15 minutes max timeout limit that Lambda supports. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html\">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/faqs/\">https://aws.amazon.com/step-functions/faqs/</a></p>\n"
        }
      },
      {
        "id": 99528237,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A development team is running a project that will involve deploying applications across several Amazon VPCs. The applications will require fully meshed network connectivity to enable transitive routing between VPCs. The development lead is concerned about security and has requested centralized control over network access controls.</p><p>Which deployment will satisfy the requirements with the most operational efficiency?</p>",
          "answers": [
            "<p>Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs.</p>",
            "<p>Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs.</p>",
            "<p>Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs.</p>",
            "<p>Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPCs.</p>"
          ],
          "explanation": "<p>AWS Transit Gateway can be configured to enable a fully meshed network topology which allows transitive routing between all the VPCs. With AWS Network Firewall, you can define firewall rules that provide fine-grained control over network traffic.</p><p>AWS Firewall Manager allows you to build policies based on Network Firewall rules and then centrally apply those policies across your virtual private clouds (VPCs) and accounts.</p><p><strong>CORRECT: </strong>\"Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs\" is incorrect.</p><p>VPC peering requires creating complex peering relationships and does not support transitive routing (though this can be achieved through a mesh of peering connections). VPC peering is less operationally efficient compared to using a Transit Gateway. AWS WAF is not the best solution for enforcing centralized network access controls, it is used for preventing web based attacks.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs\" is incorrect.</p><p>You cannot connect Amazon VPCs using AWS S2S VPNs. You can only use an AWS VPN to connect on-premises networks.</p><p><strong>INCORRECT:</strong> \"Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPC\" is incorrect.</p><p>PrivateLink is not used for creating this kind of network deployment. It is used for private access to AWS services using private IP addresses.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/network-firewall/\">https://aws.amazon.com/network-firewall/</a></p><p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>"
        }
      },
      {
        "id": 138248235,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company is developing a serverless application that uses AWS Lambda, AWS SAM, and Amazon API Gateway. There is a requirement to fully automate the backend Lambda deployment in such a way that the deployment will automatically run whenever a new commit is pushed to a GitHub repository. There should also be a separate environment pipeline for TEST and PROD environments. In addition, the TEST environment should be the only one that allows automatic deployment.</p><p>How can a DevOps Engineer satisfy these requirements?</p>",
          "answers": [
            "<p>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>"
          ],
          "explanation": "<p>In <strong>AWS CodePipeline</strong>, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p>If the action is approved, the pipeline execution resumes. If the action is rejected\u2014or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping\u2014the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p><img src=\"https://media.tutorialsdojo.com/public/PipelineFlow.png\"></p><p>Hence, the correct answer is: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</strong></p><p>The option that says: <strong>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline</strong> is incorrect. You should add a manual approval step on the PROD pipeline as mentioned in the requirements of the scenario.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline</strong> is incorrect. You don't need to create separate GitHub repositories for the two environments. You just need to create two different branches from a single repository.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline </strong>is incorrect. You should add the manual approval step on the PROD pipeline only, excluding the TEST pipeline. Moreover, you don't need to create separate GitHub repositories for the two environments. You only need to create two different branches from a single repository.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html \">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 134588393,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. </p><p>As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?</p>",
          "answers": [
            "<p>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/ssm-patch-baselines.jpg\"></p><p><strong>Patch Manager</strong> uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p><strong>AWS Config</strong> provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements. Hence, the correct option is: <strong>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</strong></p><p>The option that says: <strong>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, it is stated in the scenario that the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC </strong>is incorrect because Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise, however, it does not check if your EC2 instances are using an approved AMI or not.</p><p>The option that says: <strong>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect because the AWS Shield Advanced service is more suitable in preventing DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 82921348,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>Your company has adopted CodeCommit and forces developers to create new branches and create pull requests before merging the code to master. The development team lead reviewing the pull request needs high confidence in the quality of the code and therefore would like the CICD system to automatically build a Pull Request to provide a testing badge with a pass/fail status.</p>\n\n<p>How can you implement the validation of Pull Requests by CodeBuild efficiently?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>",
            "<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>",
            "<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>",
            "<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong></p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p>\n\n<p>CloudWatch Events Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>For the given use-case, we need to create two CloudWatch Event Rules. The first rule would trigger on CodeCommit Pull Request and have the target as CodeBuild.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i2.jpg\"></p>\n\n<p>The second rule would trigger on CodeBuild build success or failure event and have the target as a Lambda function that will update the pull request with the Build outcome</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong> - Using a scheduled rate of 5 minutes would work but would be inefficient. It is much better to configure a CloudWatch Event Rule that would trigger on CodeCommit Pull Request and carry out the rest of the solution workflow as outlined earlier.</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p>For both these options, invoking a Lambda function to start CodeBuild would work, but having the function wait on CodeBuild has two issues:\n1) The Lambda function may timeout and has a maximum timeout of 15 minutes. What if the test suite takes longer to run?\n2) You will be billed for the Lambda function wait time.</p>\n\n<p>Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n"
        }
      },
      {
        "id": 75949068,
        "correct_response": [
          "b",
          "c"
        ],
        "prompt": {
          "question": "<p>To increase security, a company plans to use AWS Systems Manager Session Manager to managed Amazon EC2 instances rather than SSH. The connectivity to Session Manager should also use a private network connection.</p><p>Which configuration actions should be taken to implement this? (Select TWO.)</p>",
          "answers": [
            "<p>Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range.</p>",
            "<p>Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager.</p>",
            "<p>Create a VPC endpoint for Systems Manager in the Region where the instances are running.</p>",
            "<p>Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables.</p>",
            "<p>Establish private Session Manager connectivity using the instance IDs of EC2 instances.</p>"
          ],
          "explanation": "<p>Systems Manager Session Manager enables secure remote access to EC2 instances without the need to open ports for SSH or create bastion hosts. You can connect to instances through Session Manager privately by establishing a VPC endpoint in your VPC. This ensures that all connectivity takes place using private addresses.</p><p>The EC2 instances must have the Systems Manager agent running and they must have permissions to be able to communicate with the Systems Manager service. An instance profile can be easily attached to provide these permissions to instances.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-59-28-dac1a7fafdbe3507d5ca4b1fe07ed213.jpg\"><p><strong>CORRECT: </strong>\"Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Systems Manager in the Region where the instances are running\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range\" is incorrect.</p><p>Port 22 is used by the SSH protocol and is not required by Systems Manager Session Manager.</p><p><strong>INCORRECT:</strong> \"Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables\" is incorrect.</p><p>You cannot connect to EC2 instances via a NAT gateway. These gateways are used for outbound (internet) connectivity from the EC2 instances in private subnets.</p><p><strong>INCORRECT:</strong> \"Establish private Session Manager connectivity using the instance IDs of EC2 instances\" is incorrect.</p><p>You cannot connect privately to instances simply by connecting via the instance ID. You must establish a VPC endpoint and attach the necessary permissions to instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 134588381,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company has a CI/CD pipeline built in AWS CodePipeline for deployment updates. Part of the deployment process is to perform database schema updates and is performed via AWS CodeBuild. A recent security audit has discovered that the AWS CodeBuild is downloading the database scripts via Amazon S3 in an unauthenticated manner. The security team requires a solution that will enhance the security of the company's CI/CD pipeline.</p><p>What action should the DevOps Engineer take to address the issue in the MOST secure way?</p>",
          "answers": [
            "<p>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>",
            "<p>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>"
          ],
          "explanation": "<p><strong>Service role</strong>: A service role is an AWS Identity and Access Management (IAM) that grants permissions to an AWS service so that the service can access AWS resources. You need an AWS CodeBuild service role so that CodeBuild can interact with dependent AWS services on your behalf. You can create a CodeBuild service role by using the CodeBuild or AWS CodePipeline consoles.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-codebuild-service-role.png\"></p><p>In this scenario, the S3 bucket will be safeguarded from unauthorized access by utilizing a bucket policy. Moreover, CodeBuild leverages the service role for executing S3 actions on your behalf.</p><p>Hence, the correct answer is: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</strong></p><p>The option that says: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI</strong> is incorrect. While the use of IAM access key and secret access key can provide S3 access to CodeBuild, it is not the most secure approach to address the issue.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because an IAM policy alone cannot secure an S3 bucket from unauthorized access. A bucket policy must be used instead. Furthermore, this option uses IAM access key and secret access key, which is not the most secure way.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. It is not used for removing unauthenticated access to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role</a></p><p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>"
        }
      },
      {
        "id": 82921330,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As the Lead DevOps Engineer at an e-commerce company, you would like to upgrade the major version of your MySQL database, which is managed by CloudFormation with <code>AWS::RDS::DBInstance</code> and setup using Multi-AZ.</p>\n\n<p>You have a requirement to minimize the downtime as much as possible, what steps should you take to achieve this?</p>\n",
          "answers": [
            "<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>",
            "<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>",
            "<p>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</p>",
            "<p>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong></p>\n\n<p>You can minimize downtime on an upgrade by using a rolling upgrade using read replicas. Amazon RDS doesn\u2019t fully automate one-click rolling upgrades. However, you can still perform a rolling upgrade by creating a read replica, upgrading the replica by using the property <code>EngineVersion</code>, promoting the replica, and then routing traffic to the promoted replica. If you want to create a Read Replica DB instance, specify the ID of the source DB instance. The SourceDBInstanceIdentifier property determines whether a DB instance is a Read Replica.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p>You should also note that a Multi-AZ configuration does not prevent downtime during an upgrade. Multi-AZ is only recommended for a high availability use-case. However, in the case of a MySQL or MariaDB engine upgrade, Multi-AZ doesn\u2019t eliminate downtime. The slow shutdown and the physical changes made on the active server by the mysql_upgrade program require this downtime.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong> - You should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p><strong>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - If you update the <code>EngineVersion</code> property of an AWS::RDS::DBInstance resource type, AWS CloudFormation creates a new resource and replaces the current DB instance resource with the new one, so this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n\n<p><strong>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - Also, you should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n"
        }
      },
      {
        "id": 138248125,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company wants to implement a continuous delivery workflow that will facilitate the process of source code promotion in the development, staging, and production environments in AWS. In the event of system degradation or failure, the company should also have the ability to roll back the recent deployment of the application.</p><p>Which of the following CI/CD designs is the MOST suitable one to implement and will incur the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production.</p>",
            "Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy."
          ],
          "explanation": "<p>A repository is the fundamental version control object in GitHub or GitLab. It\u2019s where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. In GitHub and GitLab, you can set up notifications to configure notifications so that repository users receive emails about events (for example, another user commenting on code). You can change default settings to customize the default settings for your repository. You can browse contents to easily navigate and browse the contents of your repository. You can create triggers to set up triggers so that code pushes or other events trigger actions, such as emails or code functions. You can also configure a repository on your local computer (a local repo) to push your changes to more than one repository.</p><p><img src=\"https://media.tutorialsdojo.com/public/setup-cicd-pipeline-100324.png\"></p><p>In designing your CI/CD process in AWS, you can use a single repository in GitHub (or GitLab) and create different branches for development, master, and release. You can use CodeBuild to build your application and run tests to verify that all of the core features of your application are working. For deployment, you can either select an in-place or blue/green deployment using CodeDeploy.</p><p>Hence, the correct answer is: <strong>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</strong></p><p>The option that says: <strong>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy </strong>is incorrect because creating a separate repository for each developer is absurd since they can simply clone the code instead. A single repository will suffice in this scenario which can have several branches for development and production deployment purposes.</p><p>The option that says: <strong>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production</strong> is incorrect because you don't need to create two repositories for one application. Instead, you can just create at least two different branches to separate your development and production code.</p><p>The option that says: <strong>Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy</strong> is incorrect because Amazon ECR is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. This is not a suitable service to be used to store your application code.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/devops/continuous-integration/\">https://aws.amazon.com/devops/continuous-integration/</a></p><p><a href=\"https://aws.amazon.com/devops/continuous-delivery/\">https://aws.amazon.com/devops/continuous-delivery/</a></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html</a></p><p><br></p><p><strong>Check out this AWS CodePipelin Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 134588407,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A multinational company is using multiple AWS accounts for its global cloud architecture. The AWS resources in their production account are shared among various business units of the company. A single business unit may have one or more AWS accounts that have resources in the production account. Recently, there were a lot of incidents in which the developers from a specific business unit accidentally terminated the Amazon EC2 instances owned by another business unit. A DevOps Engineer was tasked to come up with a solution to only allow a specific business unit who owns the EC2 instances and other AWS resources to terminate their own resources. </p><p>How should the Engineer implement a multi-account strategy to satisfy this requirement?</p>",
          "answers": [
            "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access.</p>",
            "<p>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources.</p>",
            "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</p>",
            "<p>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts.</p>"
          ],
          "explanation": "<p><strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an <em>organization</em> that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.</p><p>You can use organizational units (OUs) to group accounts together to administer as a single unit. This greatly simplifies the management of your accounts. For example, you can attach a policy-based control to an OU, and all accounts within the OU automatically inherit the policy. You can create multiple OUs within a single organization, and you can create OUs within other OUs. Each OU can contain multiple accounts, and you can move accounts from one OU to another. However, OU names must be unique within a parent OU or root.</p><p><img src=\"https://media.tutorialsdojo.com/aws-organizations.jpg\"></p><p>Resource-level permissions refer to the ability to specify which resources users are allowed to perform actions on. Amazon EC2 has partial support for resource-level permissions. This means that for certain Amazon EC2 actions, you can control when users are allowed to use those actions based on conditions that have to be fulfilled or specific resources that users are allowed to use. For example, you can grant users permissions to launch instances, but only of a specific type and only using a specific AMI.</p><p>Hence, the correct answer is: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</strong></p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access</strong> is incorrect. The AWSServiceRoleForOrganizations service-linked role is primarily used to only allow AWS Organizations to create service-linked roles for other AWS services. This service-linked role is present in all organizations and not just in a specific OU.</p><p>The option that says: <strong>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources<em> </em></strong>is incorrect. Although the use of the AWS Control Tower is right, the aggregator feature is simply an AWS Config resource type that collects AWS Config configuration and compliance data from the following various AWS accounts. In addition, you have to launch the Customizations for AWS Control Tower (CfCT) on the same AWS region where your AWS Control Tower landing zone is deployed, and not on a different account, to put it in effect.</p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts</strong> is incorrect. AWS Service Catalog simply allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A more suitable service to use here is AWS Organizations.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>"
        }
      }
    ],
    "answers": {
      "75949068": [
        "b",
        "e"
      ],
      "75949124": [
        "b"
      ],
      "75949148": [
        "b"
      ],
      "82921330": [
        "a"
      ],
      "82921348": [
        "a"
      ],
      "82921374": [
        "a"
      ],
      "82921384": [
        "a"
      ],
      "82921402": [
        "b"
      ],
      "82921412": [
        "a",
        "b"
      ],
      "82921416": [
        "a",
        "d"
      ],
      "82921450": [
        "b",
        "c",
        "e"
      ],
      "99528237": [
        "a"
      ],
      "134588381": [
        "d"
      ],
      "134588393": [
        "d"
      ],
      "134588407": [
        "b"
      ],
      "138248103": [
        "a"
      ],
      "138248125": [
        "a"
      ],
      "138248235": [
        "c"
      ],
      "138248241": [
        "c"
      ],
      "143860745": [
        "d"
      ]
    }
  },
  {
    "id": "1769619233089",
    "date": "2026-01-28T16:53:53.089Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 15,
    "incorrect": 5,
    "unanswered": 0,
    "total": 20,
    "percent": 75,
    "duration": 2519968,
    "questions": [
      {
        "id": 75949046,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company manages a continuous integration and continuous delivery (CI/CD) pipeline that includes a Jenkins implementation that runs on Amazon EC2. The security team has requested that all build artifacts are encrypted as they contain company sensitive data.</p><p>Which changes should a DevOps engineer make to meet these requirements whilst reducing operational overhead for ongoing management?</p>",
          "answers": [
            "<p>Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes.</p>",
            "<p>Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption.</p>",
            "<p>Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group.</p>",
            "<p>Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM).</p>"
          ],
          "explanation": "<p>The existing Jenkins implementation runs on an Amazon EC2 instance, and this requires more operational management compared to using AWS CodeBuild which is a managed service. The DevOps engineer should replace Jenkins with AWS CodeBuild which is a fully managed build service.</p><p>CodeBuild compiles source code, runs unit tests, and produces artifacts that are ready to deploy. Encryption for build artifacts such as a cache, logs, exported raw test report data files, and build results, is enabled by default, and uses AWS managed keys (AWS KMS).</p><p><strong>CORRECT: </strong>\"Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes\" is incorrect.</p><p>This solution may be secure, but it does require more operational management compared to using AWS CodeBuild and is therefore not the best option.</p><p><strong>INCORRECT:</strong> \"Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group\" is incorrect.</p><p>Moving Jenkins to an Auto Scaling group does not reduce the operational management. This is also an option that requires more overhead to manage compared to using a managed service such as AWS CodeBuild.</p><p><strong>INCORRECT:</strong> \"Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM)\" is incorrect.</p><p>Artifacts cannot be encrypted using ACM as that is a service that issues SSL/TLS certificates which are used for encryption in-transit rather than encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 67357100,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.</p>\n\n<p>As a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?</p>\n",
          "answers": [
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>AWS::EC2::Volume</code> that returns a compliance failure if the custom tag is not applied on the EBS volume. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong></p>\n\n<p>Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type.</p>\n\n<p>AWS Config managed rule that checks required-tags:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p>This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes.</p>\n\n<p>AWS Config rules remediation with automation runbooks architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it's possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n"
        }
      },
      {
        "id": 75949146,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
          "answers": [
            "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
            "<p>The new AZ has not been enabled for the ALB.</p>",
            "<p>The AMI has not been updated to include the new AZ.</p>",
            "<p>There are no subnets associated with the new AZ.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 99528239,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company runs a Java application in an on-premises data center. The application is not currently containerized but the company plans to migrate it to the AWS Cloud and modernize the application into a containerized deployment. A CI/CD pipeline should also be created to automate application updates.</p><p>Which solution can a DevOps engineer use to meet all these requirements?</p>",
          "answers": [
            "<p>Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates.</p>",
            "<p>Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy.</p>",
            "<p>Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository.</p>",
            "<p>Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy.</p>"
          ],
          "explanation": "<p>App2Container (A2C) is a command-line tool for modernizing .NET and Java applications into containerized applications. A2C analyzes and builds an inventory of all applications running in virtual machines on-premises or in the cloud.</p><p>With A2C you can simply select the application you want to containerize, and A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the ECS task and Kubernetes pod definitions.</p><p>App2Container creates CI/CD pipelines for Amazon DevOps services such as CodeBuild and CodeDeploy to build and deploy containers. If you have existing CI/CD tooling (for example, Azure DevOps and Jenkins), then you can integrate A2C provided artifacts \u2013 dockerfile, ECS task definition, EKS deployment YAML - into your existing CI/CD workflows.</p><p><strong>CORRECT: </strong>\"Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates\" is incorrect.</p><p>AWS Copilot is a command line interface (CLI) that you can use to quickly launch and manage containerized applications on AWS. It simplifies running applications on Amazon Elastic Container Service (ECS), AWS Fargate, and AWS App Runner. It is not able to inventory and analyze existing applications or migrate them to containers.</p><p><strong>INCORRECT:</strong> \"Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository\" is incorrect.</p><p>AWS Proton is used to create application stack templates and build infrastructure for applications using the CLI, API, or UI. It is not used for refactoring applications or migrating them to containerized deployments.</p><p><strong>INCORRECT:</strong> \"Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy\" is incorrect.</p><p>Amazon AppFlow is used to automate bi-directional data flows between SaaS applications and AWS services. It is not used to migrate applications between on-premises deployments and Amazon ECS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/app2container/features/\">https://aws.amazon.com/app2container/features/</a></p>"
        }
      },
      {
        "id": 75949108,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>",
          "answers": [
            "<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"
          ],
          "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949138,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
            "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
            "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
            "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
            "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
            "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"
          ],
          "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949100,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company is deploying a new serverless application that uses AWS Lambda functions. A DevOps engineer must create a continuous deployment pipeline for the application. The deployment preferences must be configured to minimize the impact of failed deployments.</p><p>Which deployment configuration will meet these requirements?</p>",
          "answers": [
            "<p>Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type.</p>",
            "<p>Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch.</p>",
            "<p>Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version.</p>",
            "<p>Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version.</p>"
          ],
          "explanation": "<p>The benefits of using AWS SAM to create the serverless application include that it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM can perform the following actions:</p><ul><li><p>Deploys new versions of the Lambda function, and automatically creates aliases that point to the new version.</p></li><li><p>Gradually shifts customer traffic to the new version until you\u2019re satisfied that it's working as expected, or you roll back the update.</p></li><li><p>Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly, and your application operates as expected.</p></li><li><p>Rolls back the deployment if CloudWatch alarms are triggered.</p></li></ul><p>The DevOps engineer can choose the Deployment Preference Type. The following options are available:</p><ul><li><p><strong>Canary:</strong> Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p></li><li><p><strong>Linear:</strong> Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment.</p></li><li><p><strong>All-at-once:</strong> All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</p></li></ul><p>The best option to minimize the impact of failed deployments is to use the canary deployment type. This will ensure that only a small amount of traffic reaches the new Lambda function in the first shift and if any issues occur the deployment can be stopped.</p><p><strong>CORRECT: </strong>\"Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch\" is incorrect.</p><p>The all-at-once deployment preference type would shift all traffic across to the new functions which would increase the impact of failed deployments. Also, CloudFormation is not a continuous deployment tool and is not a suitable substitute for CodePipeline and CodeDeploy.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version\" is incorrect.</p><p>As above, CloudFormation is not designed for continuous delivery and should not be used in this scenario. Creating a CodePipeline that leverages CodeDeploy and a source control service such as CodeCommit would be a better solution.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version\" is incorrect.</p><p>This option requires more operational overhead and testing on behalf of the DevOps team. The better solution is an automated pipeline that shifts some traffic across to the new functions so actual end users are attempting to use the application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949142,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.</p><p>How can the company meet these requirements with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns.</p>",
            "<p>Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket.</p>"
          ],
          "explanation": "<p>The simplest solution for this request is to enable server access logging on the bucket and then analyze the data that is logged using Amazon Athena. Server access logging provides detailed records for the requests that are made to a bucket. This includes the information requested by the company.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p><strong>CORRECT: </strong>\"Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket\" is incorrect $</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/\">https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>"
        }
      },
      {
        "id": 75949172,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A service provider has created business relationships with several companies. The service provider plans to deploy an application to multiple AWS accounts managed by these partner companies using AWS CloudFormation. Each partner company has granted the permissions to create IAM roles with permissions for the deployment in their respective accounts. The organization must minimize operational overhead and stack management.</p><p>Which actions should be taken to deploy the application across these accounts?</p>",
          "answers": [
            "<p>Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template.</p>",
            "<p>Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application.</p>"
          ],
          "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf</p><p>In this case the best solution is to use self-managed permissions as the partner companies have only granted the ability to create IAM roles. They would certainly not want their entire account to be controlled by the service provider through AWS Organizations just so they can deploy a single application.</p><p><strong>CORRECT: </strong>\"Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>You cannot have a role in one account that has permissions in another account. The roles must be created in each AWS account and self-managed permissions must be used with AWS CloudFormation stack sets.</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>The partner companies would not want the service provider to control their accounts with AWS Organizations just for a single application. They have granted the ability to create roles which is enough for the service provider to deploy the application using stack sets with self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template\" is incorrect.</p><p>The partner companies have granted the ability to create roles, not user accounts. Also, this would be inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 75949092,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is developing an application that calls AWS Lambda functions. The functions must connect to a database and credentials must not be stored in source code. The credentials for connection to the database must be regularly rotated to meet security policy requirements.</p><p>What should a DevOps engineer do to meet these requirements?</p>",
          "answers": [
            "<p>Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID.</p>",
            "<p>Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID.</p>",
            "<p>Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution.</p>",
            "<p>Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID.</p>"
          ],
          "explanation": "<p>AWS Secrets Manager is ideal for this scenario as it can be used to securely store the secrets. Automatic rotation can be enabled for several AWS databases and can be configured through custom Lambda functions for other databases.</p><p><strong>CORRECT: </strong>\"Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID\" is incorrect.</p><p>CloudHSM is used for storing encryption keys, not connection credentials.</p><p><strong>INCORRECT:</strong> \"Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution\" is incorrect.</p><p>This is not a secure method of storing the credentials and Secrets Manager is more secure.</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID\" is incorrect.</p><p>KMS is used for storing encryption keys, not connection credentials.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>"
        }
      },
      {
        "id": 115961513,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>",
          "answers": [
            "<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>",
            "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>",
            "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>",
            "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"
          ],
          "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 99528229,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.</p><p>Which actions should the engineer take?</p>",
          "answers": [
            "<p>Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL.</p>",
            "<p>Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield.</p>",
            "<p>Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL.</p>",
            "<p>Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty.</p>"
          ],
          "explanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website.</p><p>AWS Web Application Firewall (WAF) can be used to protect the static website from web exploits. To do this, the engineer should create an AWS WAF web ACL.</p><p><strong>CORRECT: </strong>\"Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield\" is incorrect.</p><p>Both EC2 and ElastiCache use EC2 instances so they are not serverless services. AWS Shield protects against DDoS attacks but AWS WAF should be used for web exploits.</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL\" is incorrect.</p><p>ECS with Fargate is serverless but you cannot use a Redis cluster to cache content served by a task running on Amazon ECS. ElastiCache is used for caching database content.</p><p><strong>INCORRECT:</strong> \"Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty\" is incorrect.</p><p>Lambda with a REST API as the frontend is definitely possible, however the static website on S3 would be easier to deploy and manage. AWS GuardDuty does not protect against web exploits.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>"
        }
      },
      {
        "id": 75949128,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>The DevOps team at an e-commerce company introduced multiple stages of security to the code release process. As an additional measure, they want to add additional SAST &amp; DAST tools into an automated pipeline. These tools should be invoked for every code push in an AWS CodeCommit repository. The code must be sent via an external API.</p><p>Which actions should a DevOps engineer take to achieve these requirements MOST efficiently?</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API.</p>",
            "<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected.</p>",
            "<p>Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API.</p>",
            "<p>Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API.</p>"
          ],
          "explanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that can be set up, you can match events and route them to one or more target functions or streams. Custom application-level events can be generated and published to CloudWatch Events. Scheduled events can be set up that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event Rules cannot have S3 buckets as a target. It is more efficient to directly invoke the Lambda function from the CloudWatch Event rule than use S3 as a temporary store and then trigger a function execution.</p><p><strong>INCORRECT:</strong> \"Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected\" is incorrect.</p><p>You cannot create a CodeCommit hook on an EC2 instance, so this is not possible</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So, this option is ruled out.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/\">https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949174,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
          "answers": [
            "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
            "<p>Add test stages before and after the deploy action.</p>",
            "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
            "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"
          ],
          "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528205,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team has prohibited unauthenticated requests to S3 buckets for this project.</p><p>How can this issue be resolved in the <strong>MOST secure</strong> manner?</p>",
          "answers": [
            "<p>Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.</p>",
            "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</p>"
          ],
          "explanation": "<p>Removing unauthenticated access from the bucket through a bucket policy ensures that the S3 bucket is protected from unauthorized access. Granting the CodeBuild project's service role explicit permissions to access the S3 bucket aligns with the principle of least privilege. Using the AWS CLI within the build spec ensures secure and seamless access to the bucket.<br><br></p><p><strong>CORRECT: </strong>\"Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script\" is incorrect.</p><p>Adding the bucket name to the AllowedBuckets section does not address the security issue. The CodeBuild project needs proper IAM permissions to access the S3 bucket securely. Relying on unauthenticated access remains insecure and against the security team's requirements.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script\" is incorrect.</p><p>Enabling HTTPS basic authentication for the S3 bucket is not supported by AWS. Amazon S3 uses IAM policies and bucket policies for access control, not basic authentication. Additionally, using cURL to pass tokens does not align with AWS's recommended best practices.</p><p><strong>INCORRECT:</strong> \"Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key\" is incorrect.</p><p>Using IAM access keys and secret access keys directly is less secure than granting a CodeBuild service role the necessary permissions. Hardcoding access keys in the build spec or environment can lead to security vulnerabilities, such as accidental exposure of credentials.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html</a></p>"
        }
      },
      {
        "id": 75949134,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An automotive organization is planning to migrate their website into AWS across multiple accounts. The current infrastructure uses an on-premises Microsoft IIS web server and Microsoft SQL server for the data persistence layer.</p><p>They want to be able to scale their infrastructure based on demand. Along with the current website, they also want to collect user interest data from ad clicks that occur on the website. Amazon RedShift has been chosen for the consumption and aggregation of data.</p><p>Which of the below architectures best suits their needs?</p>",
          "answers": [
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.</p>"
          ],
          "explanation": "<p>Amazon Kinesis Data Firehose is used to reliably load streaming data into data lakes, data stores, and analytics tools like Amazon Redshift. Process the incoming data from Firehose with Kinesis Data Analytics to provide real-time dashboarding of website activity.</p><p><strong>CORRECT: </strong>\"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>To load data into S3, Kinesis Firehose is a more suitable tool.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>As per the option above, Kinesis Data Firehose is a better service for this use case.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>Athena is more suited to data analysis within S3 buckets and cannot be used for loading data into S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/\">https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>"
        }
      },
      {
        "id": 75949074,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A retail company has hired a DevOps engineer to provide consultancy services. The company runs Oracle and PostgreSQL services on Amazon RDS for storing large quantities of data generated by manufacturing equipment.</p><p>The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team.</p><p>To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p><p>Which solution should the DevOps engineer recommend to achieve these requirements?</p>",
          "answers": [
            "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift.</p>"
          ],
          "explanation": "<p>The AWS Database Migration Service (DMS) helps migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS DMS the data can be continuously replicated with high availability and the multiple databases can be consolidated into a petabyte-scale data warehouse.</p><p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance. During a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-02-48-34cbd839e896555130df72dfa20efc35.jpg\"><p><strong>CORRECT: </strong>\u201cUse AWS Database Migration Service to replicate the data from the databases into Amazon Redshift\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>AWS Glue is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.</p><p><strong>INCORRECT:</strong> \"Use Amazon EMR to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon EMR (previously called Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon Kinesis Data Streams is a fully managed, serverless data streaming service that stores and ingests various streaming data in real time at any scale. It is not suitable for migrating data between databases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>"
        }
      },
      {
        "id": 99528223,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A data intelligence and analytics company has implemented a CI/CD pipeline using AWS CodePipeline which takes code from an AWS CodeCommit repository and then builds it using AWS CodeBuild. During the deploy stage, the application is deployed onto an Amazon ECS cluster. During deployment, the application is only partly updated on some ECS tasks which are running an older version of the image.</p><p>A DevOps engineer investigated and found that terminating the task or clearing the local Docker cache fixes the issue, but a more robust solution is required that provides visibility and identification to track where container images are deployed. Also, the start-up time of the containers needs to be optimized.</p><p>Which actions should the DevOps engineer take to achieve these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Move all the dependencies into a single image and pull them from a single container registry.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task.</p>",
            "<p>Move secondary dependencies to be downloaded at application startup rather than including them within a static container image.</p>"
          ],
          "explanation": "<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. The IMAGEID property is the SHA digest for the Docker image used to start the container.</p><p>Ideally, a container image is intended to be a complete snapshot of everything that the application requires to function. With a complete container image, the application could be run by downloading one container image from one place.</p><p>There is no need to download several separate pieces from different locations. Therefore, as a best practice, store all application dependencies as static files inside the container image. This will improve performance and start up time.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-00-17-d1b2e063cff66b89f7caee81435b0971.jpg\"><p><strong>CORRECT: </strong>\"Move all the dependencies into a single image and pull them from a single container registry\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time\" is incorrect.</p><p>The exact image tag needs to be pulled because the latest tag will only pull the image that was pushed last.</p><p><strong>INCORRECT:</strong> \"After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task\" is incorrect.</p><p>Using Lambda to SSH onto instances is not a workable solution. SSM Run Command may be better but it\u2019s also not ideal.</p><p><strong>INCORRECT:</strong> \"Move secondary dependencies to be downloaded at application startup rather than including them within a static container image\" is incorrect.</p><p>This would delay the image spin up time rather than improving it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html\">https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528211,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. It should not be possible to create IAM users unless the user account making the requires is specific on an exception list. A DevOps engineer must apply these restrictions.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>",
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>"
          ],
          "explanation": "<p>With AWS Organizations Service Control Policies (SCPs) you can restrict access to specific resources or define conditions for when SCPs are in effect. The condition element is supported when the Effect element has a value of Deny.</p><p>The StringNotLike condition will check the exception list for the aws:username that made the request. If the username is not present the request will be denied.</p><p><strong>CORRECT: </strong>\"Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list\" is incorrect.</p><p>The API action iam:CreateLoginProfile creates a password for the specified IAM user. This is not used to create the user account.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The API action iam:GetUser retrieves information about the specified IAM user, including the user's creation date, path, unique ID, and ARN.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The function should be used to validate that the iam:CreateUser request is being initiated by a user on the exception list. It should not delete the requesting user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>"
        }
      },
      {
        "id": 75949048,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company has several AWS accounts and an on-premises data center. Several microservices applications run across the accounts and data center. The distributed architecture results in challenges with investigating application issues as the logs are saved in a variety of locations. A DevOps engineer must configure a solution that centralizes and aggregates the logs for analytics.</p><p>What is the MOST efficient and cost-effective solution?</p>",
          "answers": [
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account.</p>"
          ],
          "explanation": "<p>The most efficient and cost-effective solution is to use the CloudWatch agent to collect log files from both AWS resources and on-premises servers and save that data to a centralized Amazon S3 bucket. S3 event notifications can be used to trigger an AWS Lambda function that analyzes the data looking for anomalies.</p><p>Amazon Athena is ideal for running ad-hoc SQL queries on data stored in S3. This can be used by the company when they have specific queries they need to run against the data.</p><p><strong>CORRECT: </strong>\"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center\" is incorrect.</p><p>The best solution is not to store the data on-premises, it should be in the AWS Cloud. You would then be able to use the ELK stack to analyze the data.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket\" is incorrect.</p><p>You cannot use S3 APIs to export log files from on-premises servers. You also cannot use KDA to analyze data in S3, it is used for analyzing data in other Kinesis services.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account\" is incorrect.</p><p>As above, you cannot use the S3 API with on-premises resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p><p><a href=\"https://aws.amazon.com/solutions/implementations/centralized-logging/\">https://aws.amazon.com/solutions/implementations/centralized-logging/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      }
    ],
    "answers": {
      "67357100": [
        "d"
      ],
      "75949046": [
        "b"
      ],
      "75949048": [
        "d"
      ],
      "75949074": [
        "c"
      ],
      "75949092": [
        "b"
      ],
      "75949100": [
        "a"
      ],
      "75949108": [
        "a"
      ],
      "75949128": [
        "c"
      ],
      "75949134": [
        "d"
      ],
      "75949138": [
        "b",
        "c",
        "d"
      ],
      "75949142": [
        "c"
      ],
      "75949146": [
        "a"
      ],
      "75949172": [
        "b"
      ],
      "75949174": [
        "b"
      ],
      "99528205": [
        "c"
      ],
      "99528211": [
        "c"
      ],
      "99528223": [
        "b",
        "d"
      ],
      "99528229": [
        "a"
      ],
      "99528239": [
        "b"
      ],
      "115961513": [
        "b"
      ]
    }
  }
]