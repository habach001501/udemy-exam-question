[
  {
    "id": "1769619233089",
    "date": "2026-01-28T16:53:53.089Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 15,
    "incorrect": 5,
    "unanswered": 0,
    "total": 20,
    "percent": 75,
    "duration": 2519968,
    "questions": [
      {
        "id": 75949046,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company manages a continuous integration and continuous delivery (CI/CD) pipeline that includes a Jenkins implementation that runs on Amazon EC2. The security team has requested that all build artifacts are encrypted as they contain company sensitive data.</p><p>Which changes should a DevOps engineer make to meet these requirements whilst reducing operational overhead for ongoing management?</p>",
          "answers": [
            "<p>Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes.</p>",
            "<p>Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption.</p>",
            "<p>Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group.</p>",
            "<p>Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM).</p>"
          ],
          "explanation": "<p>The existing Jenkins implementation runs on an Amazon EC2 instance, and this requires more operational management compared to using AWS CodeBuild which is a managed service. The DevOps engineer should replace Jenkins with AWS CodeBuild which is a fully managed build service.</p><p>CodeBuild compiles source code, runs unit tests, and produces artifacts that are ready to deploy. Encryption for build artifacts such as a cache, logs, exported raw test report data files, and build results, is enabled by default, and uses AWS managed keys (AWS KMS).</p><p><strong>CORRECT: </strong>\"Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes\" is incorrect.</p><p>This solution may be secure, but it does require more operational management compared to using AWS CodeBuild and is therefore not the best option.</p><p><strong>INCORRECT:</strong> \"Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group\" is incorrect.</p><p>Moving Jenkins to an Auto Scaling group does not reduce the operational management. This is also an option that requires more overhead to manage compared to using a managed service such as AWS CodeBuild.</p><p><strong>INCORRECT:</strong> \"Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM)\" is incorrect.</p><p>Artifacts cannot be encrypted using ACM as that is a service that issues SSL/TLS certificates which are used for encryption in-transit rather than encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 67357100,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.</p>\n\n<p>As a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?</p>\n",
          "answers": [
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>AWS::EC2::Volume</code> that returns a compliance failure if the custom tag is not applied on the EBS volume. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong></p>\n\n<p>Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type.</p>\n\n<p>AWS Config managed rule that checks required-tags:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p>This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes.</p>\n\n<p>AWS Config rules remediation with automation runbooks architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it's possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n"
        }
      },
      {
        "id": 75949146,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
          "answers": [
            "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
            "<p>The new AZ has not been enabled for the ALB.</p>",
            "<p>The AMI has not been updated to include the new AZ.</p>",
            "<p>There are no subnets associated with the new AZ.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 99528239,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company runs a Java application in an on-premises data center. The application is not currently containerized but the company plans to migrate it to the AWS Cloud and modernize the application into a containerized deployment. A CI/CD pipeline should also be created to automate application updates.</p><p>Which solution can a DevOps engineer use to meet all these requirements?</p>",
          "answers": [
            "<p>Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates.</p>",
            "<p>Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy.</p>",
            "<p>Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository.</p>",
            "<p>Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy.</p>"
          ],
          "explanation": "<p>App2Container (A2C) is a command-line tool for modernizing .NET and Java applications into containerized applications. A2C analyzes and builds an inventory of all applications running in virtual machines on-premises or in the cloud.</p><p>With A2C you can simply select the application you want to containerize, and A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the ECS task and Kubernetes pod definitions.</p><p>App2Container creates CI/CD pipelines for Amazon DevOps services such as CodeBuild and CodeDeploy to build and deploy containers. If you have existing CI/CD tooling (for example, Azure DevOps and Jenkins), then you can integrate A2C provided artifacts \u2013 dockerfile, ECS task definition, EKS deployment YAML - into your existing CI/CD workflows.</p><p><strong>CORRECT: </strong>\"Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates\" is incorrect.</p><p>AWS Copilot is a command line interface (CLI) that you can use to quickly launch and manage containerized applications on AWS. It simplifies running applications on Amazon Elastic Container Service (ECS), AWS Fargate, and AWS App Runner. It is not able to inventory and analyze existing applications or migrate them to containers.</p><p><strong>INCORRECT:</strong> \"Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository\" is incorrect.</p><p>AWS Proton is used to create application stack templates and build infrastructure for applications using the CLI, API, or UI. It is not used for refactoring applications or migrating them to containerized deployments.</p><p><strong>INCORRECT:</strong> \"Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy\" is incorrect.</p><p>Amazon AppFlow is used to automate bi-directional data flows between SaaS applications and AWS services. It is not used to migrate applications between on-premises deployments and Amazon ECS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/app2container/features/\">https://aws.amazon.com/app2container/features/</a></p>"
        }
      },
      {
        "id": 75949108,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>",
          "answers": [
            "<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"
          ],
          "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949138,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
            "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
            "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
            "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
            "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
            "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"
          ],
          "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949100,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company is deploying a new serverless application that uses AWS Lambda functions. A DevOps engineer must create a continuous deployment pipeline for the application. The deployment preferences must be configured to minimize the impact of failed deployments.</p><p>Which deployment configuration will meet these requirements?</p>",
          "answers": [
            "<p>Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type.</p>",
            "<p>Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch.</p>",
            "<p>Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version.</p>",
            "<p>Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version.</p>"
          ],
          "explanation": "<p>The benefits of using AWS SAM to create the serverless application include that it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM can perform the following actions:</p><ul><li><p>Deploys new versions of the Lambda function, and automatically creates aliases that point to the new version.</p></li><li><p>Gradually shifts customer traffic to the new version until you\u2019re satisfied that it's working as expected, or you roll back the update.</p></li><li><p>Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly, and your application operates as expected.</p></li><li><p>Rolls back the deployment if CloudWatch alarms are triggered.</p></li></ul><p>The DevOps engineer can choose the Deployment Preference Type. The following options are available:</p><ul><li><p><strong>Canary:</strong> Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p></li><li><p><strong>Linear:</strong> Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment.</p></li><li><p><strong>All-at-once:</strong> All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</p></li></ul><p>The best option to minimize the impact of failed deployments is to use the canary deployment type. This will ensure that only a small amount of traffic reaches the new Lambda function in the first shift and if any issues occur the deployment can be stopped.</p><p><strong>CORRECT: </strong>\"Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch\" is incorrect.</p><p>The all-at-once deployment preference type would shift all traffic across to the new functions which would increase the impact of failed deployments. Also, CloudFormation is not a continuous deployment tool and is not a suitable substitute for CodePipeline and CodeDeploy.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version\" is incorrect.</p><p>As above, CloudFormation is not designed for continuous delivery and should not be used in this scenario. Creating a CodePipeline that leverages CodeDeploy and a source control service such as CodeCommit would be a better solution.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version\" is incorrect.</p><p>This option requires more operational overhead and testing on behalf of the DevOps team. The better solution is an automated pipeline that shifts some traffic across to the new functions so actual end users are attempting to use the application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949142,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.</p><p>How can the company meet these requirements with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns.</p>",
            "<p>Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket.</p>"
          ],
          "explanation": "<p>The simplest solution for this request is to enable server access logging on the bucket and then analyze the data that is logged using Amazon Athena. Server access logging provides detailed records for the requests that are made to a bucket. This includes the information requested by the company.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p><strong>CORRECT: </strong>\"Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket\" is incorrect $</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/\">https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>"
        }
      },
      {
        "id": 75949172,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A service provider has created business relationships with several companies. The service provider plans to deploy an application to multiple AWS accounts managed by these partner companies using AWS CloudFormation. Each partner company has granted the permissions to create IAM roles with permissions for the deployment in their respective accounts. The organization must minimize operational overhead and stack management.</p><p>Which actions should be taken to deploy the application across these accounts?</p>",
          "answers": [
            "<p>Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template.</p>",
            "<p>Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application.</p>"
          ],
          "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf</p><p>In this case the best solution is to use self-managed permissions as the partner companies have only granted the ability to create IAM roles. They would certainly not want their entire account to be controlled by the service provider through AWS Organizations just so they can deploy a single application.</p><p><strong>CORRECT: </strong>\"Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>You cannot have a role in one account that has permissions in another account. The roles must be created in each AWS account and self-managed permissions must be used with AWS CloudFormation stack sets.</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>The partner companies would not want the service provider to control their accounts with AWS Organizations just for a single application. They have granted the ability to create roles which is enough for the service provider to deploy the application using stack sets with self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template\" is incorrect.</p><p>The partner companies have granted the ability to create roles, not user accounts. Also, this would be inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 75949092,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is developing an application that calls AWS Lambda functions. The functions must connect to a database and credentials must not be stored in source code. The credentials for connection to the database must be regularly rotated to meet security policy requirements.</p><p>What should a DevOps engineer do to meet these requirements?</p>",
          "answers": [
            "<p>Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID.</p>",
            "<p>Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID.</p>",
            "<p>Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution.</p>",
            "<p>Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID.</p>"
          ],
          "explanation": "<p>AWS Secrets Manager is ideal for this scenario as it can be used to securely store the secrets. Automatic rotation can be enabled for several AWS databases and can be configured through custom Lambda functions for other databases.</p><p><strong>CORRECT: </strong>\"Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID\" is incorrect.</p><p>CloudHSM is used for storing encryption keys, not connection credentials.</p><p><strong>INCORRECT:</strong> \"Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution\" is incorrect.</p><p>This is not a secure method of storing the credentials and Secrets Manager is more secure.</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID\" is incorrect.</p><p>KMS is used for storing encryption keys, not connection credentials.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>"
        }
      },
      {
        "id": 115961513,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>",
          "answers": [
            "<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>",
            "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>",
            "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>",
            "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"
          ],
          "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 99528229,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.</p><p>Which actions should the engineer take?</p>",
          "answers": [
            "<p>Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL.</p>",
            "<p>Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield.</p>",
            "<p>Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL.</p>",
            "<p>Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty.</p>"
          ],
          "explanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website.</p><p>AWS Web Application Firewall (WAF) can be used to protect the static website from web exploits. To do this, the engineer should create an AWS WAF web ACL.</p><p><strong>CORRECT: </strong>\"Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield\" is incorrect.</p><p>Both EC2 and ElastiCache use EC2 instances so they are not serverless services. AWS Shield protects against DDoS attacks but AWS WAF should be used for web exploits.</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL\" is incorrect.</p><p>ECS with Fargate is serverless but you cannot use a Redis cluster to cache content served by a task running on Amazon ECS. ElastiCache is used for caching database content.</p><p><strong>INCORRECT:</strong> \"Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty\" is incorrect.</p><p>Lambda with a REST API as the frontend is definitely possible, however the static website on S3 would be easier to deploy and manage. AWS GuardDuty does not protect against web exploits.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>"
        }
      },
      {
        "id": 75949128,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>The DevOps team at an e-commerce company introduced multiple stages of security to the code release process. As an additional measure, they want to add additional SAST &amp; DAST tools into an automated pipeline. These tools should be invoked for every code push in an AWS CodeCommit repository. The code must be sent via an external API.</p><p>Which actions should a DevOps engineer take to achieve these requirements MOST efficiently?</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API.</p>",
            "<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected.</p>",
            "<p>Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API.</p>",
            "<p>Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API.</p>"
          ],
          "explanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that can be set up, you can match events and route them to one or more target functions or streams. Custom application-level events can be generated and published to CloudWatch Events. Scheduled events can be set up that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event Rules cannot have S3 buckets as a target. It is more efficient to directly invoke the Lambda function from the CloudWatch Event rule than use S3 as a temporary store and then trigger a function execution.</p><p><strong>INCORRECT:</strong> \"Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected\" is incorrect.</p><p>You cannot create a CodeCommit hook on an EC2 instance, so this is not possible</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So, this option is ruled out.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/\">https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949174,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
          "answers": [
            "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
            "<p>Add test stages before and after the deploy action.</p>",
            "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
            "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"
          ],
          "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528205,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team has prohibited unauthenticated requests to S3 buckets for this project.</p><p>How can this issue be resolved in the <strong>MOST secure</strong> manner?</p>",
          "answers": [
            "<p>Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.</p>",
            "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</p>"
          ],
          "explanation": "<p>Removing unauthenticated access from the bucket through a bucket policy ensures that the S3 bucket is protected from unauthorized access. Granting the CodeBuild project's service role explicit permissions to access the S3 bucket aligns with the principle of least privilege. Using the AWS CLI within the build spec ensures secure and seamless access to the bucket.<br><br></p><p><strong>CORRECT: </strong>\"Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script\" is incorrect.</p><p>Adding the bucket name to the AllowedBuckets section does not address the security issue. The CodeBuild project needs proper IAM permissions to access the S3 bucket securely. Relying on unauthenticated access remains insecure and against the security team's requirements.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script\" is incorrect.</p><p>Enabling HTTPS basic authentication for the S3 bucket is not supported by AWS. Amazon S3 uses IAM policies and bucket policies for access control, not basic authentication. Additionally, using cURL to pass tokens does not align with AWS's recommended best practices.</p><p><strong>INCORRECT:</strong> \"Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key\" is incorrect.</p><p>Using IAM access keys and secret access keys directly is less secure than granting a CodeBuild service role the necessary permissions. Hardcoding access keys in the build spec or environment can lead to security vulnerabilities, such as accidental exposure of credentials.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html</a></p>"
        }
      },
      {
        "id": 75949134,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An automotive organization is planning to migrate their website into AWS across multiple accounts. The current infrastructure uses an on-premises Microsoft IIS web server and Microsoft SQL server for the data persistence layer.</p><p>They want to be able to scale their infrastructure based on demand. Along with the current website, they also want to collect user interest data from ad clicks that occur on the website. Amazon RedShift has been chosen for the consumption and aggregation of data.</p><p>Which of the below architectures best suits their needs?</p>",
          "answers": [
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.</p>"
          ],
          "explanation": "<p>Amazon Kinesis Data Firehose is used to reliably load streaming data into data lakes, data stores, and analytics tools like Amazon Redshift. Process the incoming data from Firehose with Kinesis Data Analytics to provide real-time dashboarding of website activity.</p><p><strong>CORRECT: </strong>\"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>To load data into S3, Kinesis Firehose is a more suitable tool.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>As per the option above, Kinesis Data Firehose is a better service for this use case.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>Athena is more suited to data analysis within S3 buckets and cannot be used for loading data into S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/\">https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>"
        }
      },
      {
        "id": 75949074,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A retail company has hired a DevOps engineer to provide consultancy services. The company runs Oracle and PostgreSQL services on Amazon RDS for storing large quantities of data generated by manufacturing equipment.</p><p>The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team.</p><p>To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p><p>Which solution should the DevOps engineer recommend to achieve these requirements?</p>",
          "answers": [
            "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift.</p>"
          ],
          "explanation": "<p>The AWS Database Migration Service (DMS) helps migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS DMS the data can be continuously replicated with high availability and the multiple databases can be consolidated into a petabyte-scale data warehouse.</p><p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance. During a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-02-48-34cbd839e896555130df72dfa20efc35.jpg\"><p><strong>CORRECT: </strong>\u201cUse AWS Database Migration Service to replicate the data from the databases into Amazon Redshift\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>AWS Glue is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.</p><p><strong>INCORRECT:</strong> \"Use Amazon EMR to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon EMR (previously called Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon Kinesis Data Streams is a fully managed, serverless data streaming service that stores and ingests various streaming data in real time at any scale. It is not suitable for migrating data between databases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>"
        }
      },
      {
        "id": 99528223,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A data intelligence and analytics company has implemented a CI/CD pipeline using AWS CodePipeline which takes code from an AWS CodeCommit repository and then builds it using AWS CodeBuild. During the deploy stage, the application is deployed onto an Amazon ECS cluster. During deployment, the application is only partly updated on some ECS tasks which are running an older version of the image.</p><p>A DevOps engineer investigated and found that terminating the task or clearing the local Docker cache fixes the issue, but a more robust solution is required that provides visibility and identification to track where container images are deployed. Also, the start-up time of the containers needs to be optimized.</p><p>Which actions should the DevOps engineer take to achieve these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Move all the dependencies into a single image and pull them from a single container registry.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task.</p>",
            "<p>Move secondary dependencies to be downloaded at application startup rather than including them within a static container image.</p>"
          ],
          "explanation": "<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. The IMAGEID property is the SHA digest for the Docker image used to start the container.</p><p>Ideally, a container image is intended to be a complete snapshot of everything that the application requires to function. With a complete container image, the application could be run by downloading one container image from one place.</p><p>There is no need to download several separate pieces from different locations. Therefore, as a best practice, store all application dependencies as static files inside the container image. This will improve performance and start up time.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-00-17-d1b2e063cff66b89f7caee81435b0971.jpg\"><p><strong>CORRECT: </strong>\"Move all the dependencies into a single image and pull them from a single container registry\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time\" is incorrect.</p><p>The exact image tag needs to be pulled because the latest tag will only pull the image that was pushed last.</p><p><strong>INCORRECT:</strong> \"After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task\" is incorrect.</p><p>Using Lambda to SSH onto instances is not a workable solution. SSM Run Command may be better but it\u2019s also not ideal.</p><p><strong>INCORRECT:</strong> \"Move secondary dependencies to be downloaded at application startup rather than including them within a static container image\" is incorrect.</p><p>This would delay the image spin up time rather than improving it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html\">https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528211,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. It should not be possible to create IAM users unless the user account making the requires is specific on an exception list. A DevOps engineer must apply these restrictions.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>",
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>"
          ],
          "explanation": "<p>With AWS Organizations Service Control Policies (SCPs) you can restrict access to specific resources or define conditions for when SCPs are in effect. The condition element is supported when the Effect element has a value of Deny.</p><p>The StringNotLike condition will check the exception list for the aws:username that made the request. If the username is not present the request will be denied.</p><p><strong>CORRECT: </strong>\"Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list\" is incorrect.</p><p>The API action iam:CreateLoginProfile creates a password for the specified IAM user. This is not used to create the user account.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The API action iam:GetUser retrieves information about the specified IAM user, including the user's creation date, path, unique ID, and ARN.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The function should be used to validate that the iam:CreateUser request is being initiated by a user on the exception list. It should not delete the requesting user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>"
        }
      },
      {
        "id": 75949048,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company has several AWS accounts and an on-premises data center. Several microservices applications run across the accounts and data center. The distributed architecture results in challenges with investigating application issues as the logs are saved in a variety of locations. A DevOps engineer must configure a solution that centralizes and aggregates the logs for analytics.</p><p>What is the MOST efficient and cost-effective solution?</p>",
          "answers": [
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account.</p>"
          ],
          "explanation": "<p>The most efficient and cost-effective solution is to use the CloudWatch agent to collect log files from both AWS resources and on-premises servers and save that data to a centralized Amazon S3 bucket. S3 event notifications can be used to trigger an AWS Lambda function that analyzes the data looking for anomalies.</p><p>Amazon Athena is ideal for running ad-hoc SQL queries on data stored in S3. This can be used by the company when they have specific queries they need to run against the data.</p><p><strong>CORRECT: </strong>\"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center\" is incorrect.</p><p>The best solution is not to store the data on-premises, it should be in the AWS Cloud. You would then be able to use the ELK stack to analyze the data.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket\" is incorrect.</p><p>You cannot use S3 APIs to export log files from on-premises servers. You also cannot use KDA to analyze data in S3, it is used for analyzing data in other Kinesis services.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account\" is incorrect.</p><p>As above, you cannot use the S3 API with on-premises resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p><p><a href=\"https://aws.amazon.com/solutions/implementations/centralized-logging/\">https://aws.amazon.com/solutions/implementations/centralized-logging/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      }
    ],
    "answers": {
      "67357100": [
        "d"
      ],
      "75949046": [
        "b"
      ],
      "75949048": [
        "d"
      ],
      "75949074": [
        "c"
      ],
      "75949092": [
        "b"
      ],
      "75949100": [
        "a"
      ],
      "75949108": [
        "a"
      ],
      "75949128": [
        "c"
      ],
      "75949134": [
        "d"
      ],
      "75949138": [
        "b",
        "c",
        "d"
      ],
      "75949142": [
        "c"
      ],
      "75949146": [
        "a"
      ],
      "75949172": [
        "b"
      ],
      "75949174": [
        "b"
      ],
      "99528205": [
        "c"
      ],
      "99528211": [
        "c"
      ],
      "99528223": [
        "b",
        "d"
      ],
      "99528229": [
        "a"
      ],
      "99528239": [
        "b"
      ],
      "115961513": [
        "b"
      ]
    }
  }
]