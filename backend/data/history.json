[
  {
    "id": "1769968219110",
    "date": "2026-02-01T17:50:19.110Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 16,
    "incorrect": 4,
    "unanswered": 0,
    "total": 20,
    "percent": 80,
    "duration": 4942951,
    "questions": [
      {
        "id": 82921318,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A graphics design company is experimenting with a new feature for an API and the objective is to pass the field <code>\"color\"</code> in the JSON payload to enable this feature. The new Lambda function should treat <code>\"color\": \"none\"</code> as a request from an older client. The company would like to only have to manage one Lambda function in the back-end while being able to support both old and new clients. The API Gateway API is currently deployed on the <code>v1</code> stage. Old clients include Android applications which may take time to be updated. The technical requirements mandate that the solution should support the old clients for years to come.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you recommend as the best fit for the given use-case?</p>\n",
          "answers": [
            "<p>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</p>",
            "<p>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</p>",
            "<p>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</p>",
            "<p>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway handles tasks such as traffic management, authorization and access control, monitoring, and API version management. API Gateway acts as a \"front door\" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications. Simply creating and developing an API Gateway API doesn't automatically make it callable by your users. To make it callable, you must deploy your API to a stage. A stage is a named reference to a deployment, which is a snapshot of the API.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>For the given use-case, API Gateway mappings must be used. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. As such, you must deploy a v2 API alongside the v1 API backed by the same Lambda function. Old clients will hit the v1 API, which will use a mapping template to add the static missing field <code>\"color\": \"none\"</code>. Newer clients will hit the v2 API and will have that field value included.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</strong> - If we release two separate Lambda functions (named lambda v1 and lambda v2), then we have to maintain them both and that would be going against the requirements of the given use-case.</p>\n\n<p><strong>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</strong> -  API Gateway mapping templates do not support adding default values for fields as these only support static fields.</p>\n\n<p><strong>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. For the given use-case, API Gateway caching is a distractor and should be disregarded.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n"
        }
      },
      {
        "id": 75949056,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company is using AWS CodeCommit for version control and AWS CodePipeline for orchestration of software deployments. The development team are using a remote main branch as the trigger for the pipeline. A developer noticed that the CodePipeline pipeline was not triggered after the developer pushed code changes to the CodeCommit repository.</p><p>Which of the following actions should be taken to troubleshoot this issue?</p>",
          "answers": [
            "<p>Check that the developer's IAM role has permission to push to the CodeCommit repository.</p>",
            "<p>Check that an AWS Lambda function has been created to check for code commits and trigger the pipeline.</p>",
            "<p>Check that the CodePipeline service role has permission to access the CodeCommit repository.</p>",
            "<p>Check that an Amazon CloudWatch Events rule has been created for the main branch to trigger the pipeline.</p>"
          ],
          "explanation": "<p>An Amazon CloudWatch Events rule must be created to trigger the pipeline when changes are committed to the CodeCommit repository. If you use the console to create or edit your pipeline, the CloudWatch Events rule is created for you. In this case, the developer should check to make sure that the rule has been created and is correctly configured.</p><p>The following is a sample CodeCommit event pattern for a MyTestRepo repository with a branch named master:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-34-08-dc82125abf42153048e88229777e35c5.jpg\"><p><strong>CORRECT: </strong>\"Check that an Amazon CloudWatch Events rule has been created for the main branch to trigger the pipeline\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check that the CodePipeline service role has permission to access the CodeCommit repository\" is incorrect.</p><p>The issue is that the pipeline was not triggered. If the service role does not have permissions the pipeline should still be triggered by the CloudWatch Events rule but then an error would be generated if insufficient permissions are assigned for accessing the CodeCommit repository.</p><p><strong>INCORRECT:</strong> \"Check that the developer's IAM role has permission to push to the CodeCommit repository\" is incorrect.</p><p>The developer already committed the code to the repository and did not experience any errors.</p><p><strong>INCORRECT:</strong> \"Check that an AWS Lambda function has been created to check for code commits and trigger the pipeline\" is incorrect.</p><p>An AWS Lambda function is not used to check for commits or to trigger the pipeline. A CloudWatch Events rule must be created for this purpose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 143860761,
        "correct_response": [
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A web application runs on a custom port. The application has been deployed in an Auto Scaling group with an Application Load Balancer (ALB). After launching instances the Auto Scaling and Target Group health checks are returning a healthy status. However, users report that the application is not accessible.</p><p>Which steps should a DevOps engineer take to troubleshoot the issue? (Select TWO.)</p>",
          "answers": [
            "<p>Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port.</p>",
            "<p>Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address.</p>",
            "<p>Modify the Target Group health check configuration to check the application process on the custom port and path.</p>",
            "<p>Modify the Auto Scaling group health check configuration to check the application process on the custom port and path.</p>",
            "<p>Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances.</p>"
          ],
          "explanation": "<p>By default health checks are configured to use the HTTP protocol and port 80. For an ALB the traffic protocol must be HTTP or HTTPS, but the port can be customized. The most likely cause of the issue is that the web service is running on the instances and the default protocol/port is used for health checks on the ALB and ASG. This will result in instances becoming \u201chealthy\u201d despite the actual application service not functioning correctly.</p><p>The engineer should check the ASG health check configuration and the target group health check configuration. If the default values are used then the correct custom port number should be entered instead. The path may also be updated if a specific web page should be checked.</p><p>The image below shows how you can override the default port number and path in a target group health check:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-08-15-0585684e22874b6c255bd441d63a099e.jpg\"><p><strong>CORRECT: </strong>\"Modify the Target Group health check configuration to check the application process on the custom port and path\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Modify the Auto Scaling group health check configuration to check the application process on the custom port and path\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port\" is incorrect.</p><p>You cannot use a TCP listener with an ALB. It must be HTTP or HTTPS though a custom port number can certainly be used.</p><p><strong>INCORRECT:</strong> \"Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address\" is incorrect.</p><p>The issue is not related to the IP addresses traffic is being directed to on the instances; it is related to the port number and path the health checks are configured to check.</p><p><strong>INCORRECT:</strong> \"Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances\" is incorrect.</p><p>Path-based routing rules are used to route traffic to different target groups based on the path in the HTTP request. In this case there is only one target group, so a path-based routing rule is useless. Instead, the ALB must direct traffic to a custom port number (configured in the listener) and validate the application is healthy be running health checks against the appropriate port and path.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 75949118,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A DevOps team is building a pipeline in AWS CodePipeline that will build, stage, test, and then deploy an application on Amazon EC2. The team will add a manual approval stage between the test stage and the deployment stage. The development team uses a custom chat tool that offers a webhook interface for sending notifications.</p><p>The DevOps team require status updates for pipeline activity and approval requests to be posted to the chat tool. How can this be achieved?</p>",
          "answers": [
            "<p>Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic.</p>",
            "<p>Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change events. Publish subscription events to an Amazon SNS topic and subscribe the chat webhook URL to the SNS topic and complete the subscription validation.</p>",
            "<p>Create an AWS Lambda function that is invoked by AWS CloudTrail API events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details directly to the chat webhook URL.</p>",
            "<p>Create an AWS Config rule that checks for CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic.</p>"
          ],
          "explanation": "<p>You can monitor CodePipeline events in EventBridge, which delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services. EventBridge routes that data to targets such as AWS Lambda and Amazon Simple Notification Service.</p><p>Events are composed of rules that include an event pattern and event target. Each type of execution state change event in CodePipeline emits notifications with specific message content. In this case the team should filter for the \u201cCodePipeline Pipeline Execution State Change\u201d events and route to an SNS Topic as a target. The Lambda function can then be subscribed to the topic.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change events. Publish subscription events to an Amazon SNS topic and subscribe the chat webhook URL to the SNS topic and complete the subscription validation\" is incorrect.</p><p>CloudWatch Logs subscription filters can be used to publish to Kinesis Data Streams, Kinesis Data Firehose, or AWS Lambda. You cannot publish directly to SNS. Also, the log will not contain execution state change events for CodePipeline.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that is invoked by AWS CloudTrail API events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details directly to the chat webhook URL\" is incorrect.</p><p>You cannot use API events to look for this specific event and then send directly from CloudTrail to the chat webhook URL.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config rule that checks for CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic\" is incorrect.</p><p>AWS Config is used for configuration compliance and cannot check for events that relate to pipeline execution state changes in AWS CodePipeline.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 138248203,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A financial company has several accounting applications that are hosted in AWS and used by thousands of small and medium businesses. As part of its Business Continuity Plan, the company is required to set up an automatic DNS failover for its applications to a disaster recovery (DR) environment. The DevOps team was instructed to configure Amazon Route 53 to automatically route to an alternate endpoint when the primary application stack in the us-west-1 region experiences an outage or degradation of service.</p><p>What steps should the team take to satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Set up health checks in Route 53 for non-alias records to each service endpoint. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks.</p>",
            "<p>Use a Failover routing policy configuration. Set up alias records in Route 53 that route traffic to AWS resources. Set the <code>Evaluate Target Health</code> option to <code>Yes</code>, then create all of the required non-alias records.</p>",
            "<p>Set up a record in Route 53 with a Weighted routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints.</p>",
            "<p>Set up an Amazon CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom AWS Lambda function. Execute the <code>ChangeResourceRecordSets</code> API call using the function to initiate the failover to the secondary DNS record.</p>",
            "<p>Set up a record in Route 53 with a latency routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints.</p>"
          ],
          "explanation": "<p>Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.</p><p>To create an active-passive failover configuration with one primary record and one secondary record, you just create the records and specify <strong>Failover</strong> for the routing policy. When the primary resource is healthy, Route 53 responds to DNS queries using the primary record. When the primary resource is unhealthy, Route 53 responds to DNS queries using the secondary record.</p><p>You can configure a health check that monitors an endpoint that you specify either by IP address or by domain name. At regular intervals that you specify, Route 53 submits automated requests over the Internet to your application, server, or other resources to verify that it's reachable, available, and functional. Optionally, you can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-route53-evaluate-target-health.png\"></p><p>When Route 53 checks the health of an endpoint, it sends an HTTP, HTTPS, or TCP request to the IP address and port that you specified when you created the health check. For a health check to succeed, your router and firewall rules must allow inbound traffic from the IP addresses that the Route 53 health checkers use.</p><p>Hence, the correct answers are:</p><p><strong>- Set up health checks in Route 53 for non-alias records to each service endpoint. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks.</strong></p><p><strong>- Use a Failover routing policy configuration. Set up alias records in Route 53 that route traffic to AWS resources. Set the </strong><code><strong>Evaluate Target Health</strong></code><strong> option to </strong><code><strong>Yes</strong></code><strong>, then create all of the required non-alias records.</strong></p><p>The option that says: <strong>Set up a record in Route 53 with a Weighted routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints</strong> is incorrect because Weighted routing simply lets you associate multiple resources with a single domain name (pasigcity.com) or subdomain name (blog.pasigcity.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.</p><p>The option that says:<strong> Set up an Amazon CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom AWS Lambda function. Execute the </strong><code><strong>ChangeResourceRecordSets</strong></code><strong> API call using the function to initiate the failover to the secondary DNS record</strong> is incorrect because you only have to use a Failover routing policy. Calling the Route 53 API is not applicable nor useful at all in this scenario.</p><p>The option that says: <strong>Set up a record in Route 53 with a latency routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints</strong> is incorrect because the Latency routing policy simply improves the application performance for your users by serving their requests from the AWS Region that provides the lowest latency. You have to use a Failover routing policy instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>"
        }
      },
      {
        "id": 134588431,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A mobile phone manufacturer hosts a suite of enterprise resource planning (ERP) solutions to several Amazon EC2 instances in their AWS VPC. Its DevOps team is using AWS CloudFormation templates to design, launch, and deploy resources to their cloud infrastructure. Each template is manually updated to map the latest AMI IDs of the ERP solution. This process takes a significant amount of time to execute, which is why the team was tasked to automate this process. </p><p>In this scenario, which of the following options is the MOST suitable solution that can satisfy the requirement?</p>",
          "answers": [
            "<p>Set up and configure the Systems Manager State Manager service to store the latest AMI IDs and integrate it with your AWS CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.</p>",
            "<p>Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.</p>",
            "<p>Integrate AWS CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.</p>",
            "<p>Use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template. Call the update-stack API in CloudFormation in your template whenever you decide to update the Amazon EC2 instances.</p>"
          ],
          "explanation": "<p>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.</p><p>If the parameter being referenced in the template does not exist in Systems Manager, a synchronous validation error is thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.</p><p><img src=\"https://media.tutorialsdojo.com/public/Systems-Manager-parameters_6AUG2023.png\"></p><p>Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The <a href=\"http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html\">Parameters</a> section in the output for Describe API will show an additional \u2018ResolvedValue\u2019 field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation.</p><p>Hence, the correct answer is: <strong>Use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template. Call the update-stack API in CloudFormation in your template whenever you decide to update the Amazon EC2 instances.</strong></p><p>The option that says: <strong>Set up and configure the Systems Manager State Manager service to store the latest AMI IDs and integrate it with your AWS CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template</strong> is incorrect because the Systems Manager State Manager service simply automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This can't be used as a parameter store that refers to the latest AMI of your application.</p><p>The option that says: <strong>Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments</strong> is incorrect because using AWS Service Catalog is not suitable in this scenario. This service just allows organizations to create and manage catalogs of IT services that are approved for use on AWS.</p><p>The option that says: <strong>Integrate AWS CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments</strong> is incorrect because AWS Service Catalog just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A better solution is to use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949156,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A DevOps Engineer needs a scalable Node.js application in AWS with a MySQL database. There should be no downtime during deployments and if issues occur rollback to a previous version must be easy to implement. The database may also be used by other applications.</p><p>Which solution meets these requirements?</p>",
          "answers": [
            "<p>Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier.</p>",
            "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack.</p>",
            "<p>Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier.</p>",
            "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack.</p>"
          ],
          "explanation": "<p>AWS Elastic Beanstalk offers automatic rollback options for deployment updates. This coupled with auto scaling and the ALB meets the requirements for a scalable compute and web tier. The RDS database provides a managed solution for the MySQL database. The RDS MySQL database should be created outside of the Elastic Beanstalk environment as it may be used by other applications. If it is created within the Elastic Beanstalk environment it could be automatically deleted if the environment is deleted.</p><p><strong>CORRECT: </strong>\"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack\" is incorrect.</p><p>As explained above the RDS database should be created outside of the Elastic Beanstalk environment.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>ECS does not offer automatic rollback so Elastic Beanstalk is a better solution to meet the requirements.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>Automating the creation of snapshots is not a suitable solution for rollback. Elastic Beanstalk offers several deployment options which offer automatic rollback.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 138248207,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A software development company is using GitHub, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline for its CI/CD process. To further improve their systems, they need to implement a solution that automatically detects and reacts to changes in the state of their deployments in AWS CodeDeploy. Any changes must be rolled back automatically if the deployment process fails, and a notification must be sent to the DevOps Team's Slack channel for easy monitoring.</p><p>Which of the following is the MOST suitable configuration that you should implement to satisfy this requirement?</p>",
          "answers": [
            "<p>Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team's Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when a deployment fails</code> setting.</p>",
            "<p>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when alarm thresholds are met</code> setting.</p>",
            "<p>Configure a CodeDeploy agent to send notification to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful.</p>",
            "<p>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team's Slack Channel when the <code>PutLifecycleEventHookExecutionStatus</code> API call has been detected. Rollback the changes by using the AWS CLI.</p>"
          ],
          "explanation": "<p>You can monitor <strong>CodeDeploy</strong> deployments using the following CloudWatch tools: Amazon EventBridge, CloudWatch alarms, and Amazon CloudWatch Logs.</p><p>Reviewing the logs created by the CodeDeploy agent and deployments can help you troubleshoot the causes of deployment failures. As an alternative to reviewing CodeDeploy logs on one instance at a time, you can use CloudWatch Logs to monitor all logs in a central location.</p><p>You can use <strong>Amazon EventBridge </strong>(formerly known as Amazon CloudWatch Events) to detect and react to changes in the state of an instance or a deployment (an \"event\") in your CodeDeploy operations. Then, based on the rules you create, EventBridge will invoke one or more target actions when a deployment or instance enters the state you specify in a rule. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-rule-12-09-2024.png\"></p><p>You can select the following types of targets when using EventBridge as part of your CodeDeploy operations:</p><p>- AWS Lambda functions</p><p>- Kinesis streams</p><p>- Amazon SQS queues</p><p>- Built-in targets (CloudWatch alarm actions)</p><p>- Amazon SNS topics</p><p>The following are some use cases:</p><p>- Use a Lambda function to pass a notification to a Slack channel whenever deployments fail.</p><p>- Push data about deployments or instances to a Kinesis stream to support comprehensive, real-time status monitoring.</p><p>- Use CloudWatch alarm actions to automatically stop, terminate, reboot, or recover Amazon EC2 instances when a deployment or instance event you specify occurs.</p><p>Hence, the correct answer is:<strong> Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team's Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when a deployment fails</strong></code><strong> setting.</strong></p><p>The option that says: <strong>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when alarm thresholds are met</strong></code><strong> setting</strong> is incorrect because CloudWatch Alarm can't directly send a message to a Slack Channel. You have to use an EventBridge with an associated Lambda function to notify the DevOps Team via Slack.</p><p>The option that says:<strong> Configure a CodeDeploy agent to send a notification to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful</strong> is incorrect because a CodeDeploy agent is primarily used for deployment and not for sending custom messages to non-AWS resources such as a Slack Channel.</p><p>The option that says: <strong>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team's Slack Channel when the </strong><code><strong>PutLifecycleEventHookExecutionStatus</strong></code><strong> API call has been detected. Rollback the changes by using the AWS CLI</strong> is incorrect because this API simply sets the result of a Lambda validation function. This is not a suitable solution since invoking various API calls is not necessary at all. You simply have to integrate an EventBridge rule with an associated Lambda function to your CodeDeploy project in order to meet the specified requirement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 115961527,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>When deploying a newly developed application on AWS, a DevOps team notices an intermittent error when attempting to make a connection to the application.</p><p>The application has a two-tier architecture with an AWS Lambda function backed by an Amazon API gateway and a NoSQL database as the data store.</p><p>The DevOps team noticed that sometime after deployment the error stops occurring. This application is deployed by AWS CodeDeploy and the Lambda function is deployed as the last step of pipeline.</p><p>What is the most efficient way for a DevOps engineer to resolve the issue?</p>",
          "answers": [
            "<p>Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function.</p>",
            "<p>Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond.</p>",
            "<p>Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed.</p>",
            "<p>Use the ValidateService hook to validate that the deployment was completed successfully.</p>"
          ],
          "explanation": "<p>An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Here are descriptions of the hooks available for use in your AppSpec file.</p><p>\u00b7 <strong>BeforeAllowTraffic</strong> \u2013 Use to run tasks before traffic is shifted to the deployed Lambda function version.</p><p>\u00b7 <strong>AfterAllowTraffic</strong> \u2013 Use to run tasks after all traffic is shifted to the deployed Lambda function version.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-44-54-3b27da721fdb31cd0114ba6bbff8f1d5.jpg\"><p><strong>CORRECT: </strong>\"Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond\" is incorrect.</p><p>You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><strong>INCORRECT:</strong> \"Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed\" is incorrect.</p><p>Since the error resolves after some time, the issue will most likely be resolved by ensuring the application is not brought online until it is ready.</p><p><strong>INCORRECT:</strong> \"Use the ValidateService hook to validate that the deployment was completed successfully\" is incorrect.</p><p>This is used to verify the deployment was completed successfully, this will only detect deployment status and will not help in this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 134588393,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. </p><p>As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?</p>",
          "answers": [
            "<p>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/ssm-patch-baselines.jpg\"></p><p><strong>Patch Manager</strong> uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p><strong>AWS Config</strong> provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements. Hence, the correct option is: <strong>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</strong></p><p>The option that says: <strong>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, it is stated in the scenario that the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC </strong>is incorrect because Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise, however, it does not check if your EC2 instances are using an approved AMI or not.</p><p>The option that says: <strong>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect because the AWS Shield Advanced service is more suitable in preventing DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248213,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A global cryptocurrency trading company has a suite of web applications hosted in an Auto Scaling group of Amazon EC2 instances across multiple Available Zones behind an Application Load Balancer to distribute the incoming traffic. The Auto Scaling group is configured to use Elastic Load Balancing health checks for scaling instead of the default EC2 status checks. However, there are several occasions when some instances are automatically terminated after failing the HTTPS health checks in the ALB that purges all the logs stored in the instance.</p><p>To improve system monitoring, a DevOps Engineer must implement a solution that collects all of the application and server logs effectively. The Operations team should be able to perform a root cause analysis based on the logs, even if the Auto Scaling group immediately terminated the instance.</p><p>How can the DevOps Engineer automate the log collection from the EC2 instances with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Pending:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance-terminate Lifecycle Action</code> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Automation to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent.</p>",
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Use AWS Step Functions to collect the application logs and send them to an Amazon CloudWatch Log group. Resume the instance termination once all the logs are sent to CloudWatch Logs.</p>",
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance-terminate Lifecycle Action</code> Auto Scaling Event with an associated AWS Systems Manager Automation document. Trigger the Amazon CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.</p>",
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance Terminate Successful</code> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Run Command to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent.</p>"
          ],
          "explanation": "<p>The EC2 instances in an Auto Scaling group have a path, or lifecycle, that differs from that of other EC2 instances. The lifecycle starts when the Auto Scaling group launches an instance and puts it into service. The lifecycle ends when you terminate the instance, or the Auto Scaling group takes the instance out of service and terminates it.</p><p>You can add a lifecycle hook to your Auto Scaling group so that you can perform custom actions when instances launch or terminate.</p><p>When Amazon EC2 Auto Scaling responds to a scale out event, it launches one or more instances. These instances start in the <code>Pending</code> state. If you added an <code>autoscaling:EC2_INSTANCE_LAUNCHING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Pending</code> state to the <code>Pending:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Pending:Proceed</code> state. When the instances are fully configured, they are attached to the Auto Scaling group and they enter the <code>InService</code> state.</p><p>When Amazon EC2 Auto Scaling responds to a scale in event, it terminates one or more instances. These instances are detached from the Auto Scaling group and enter the <code>Terminating</code> state. If you added an <code>autoscaling:EC2_INSTANCE_TERMINATING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Terminating:Proceed</code> state. When the instances are fully terminated, they enter the <code>Terminated</code> state.</p><p><img src=\"https://media.tutorialsdojo.com/public/auto_scaling_lifecycle.png\"></p><p>Using CloudWatch agent is the most suitable tool to use to collect the logs. The unified CloudWatch agent enables you to do the following:</p><p>- Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html\">Metrics Collected by the CloudWatch Agent</a>.</p><p>- Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p><p>- Retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collectd</code> protocols. <code>StatsD</code> is supported on both Linux servers and servers running Windows Server. On the other hand, <code>collectd</code> is supported only on Linux servers.</p><p>- Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-event-pattern-27Jan2025.png\"></p><p>You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is <code>CWAgent</code>, although you can specify a different namespace when you configure the agent.</p><p>Hence, the correct answer is: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance-terminate Lifecycle Action</strong></code><strong> Auto Scaling Event with an associated AWS Systems Manager Automation document. Trigger the Amazon CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.</strong></p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Pending:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance-terminate Lifecycle Action</strong></code><strong> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Automation to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent</strong> is incorrect because the <code><strong><em>Pending:Wait</em></strong></code><strong><em> </em></strong>state simply refers to the scale-out action in Amazon EC2 Auto Scaling and not for scale-in or for terminating the instances.</p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Use AWS Step Functions to collect the application logs and send them to an Amazon CloudWatch Log group. Resume the instance termination once all the logs are sent to CloudWatch Logs</strong> is incorrect because using AWS Step Functions is inappropriate when collecting the logs from your EC2 instances. You should use a CloudWatch agent instead.</p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance Terminate Successful</strong></code><strong> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Run Command to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent</strong> is incorrect. The <code><strong>EC2 Instance Terminate Successful</strong></code> indicates that the ASG has terminated an instance. The automated solution won't just work because the target instance is already deleted when the Lambda function is triggered.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/</a></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/cloud-watch-events.html#terminate-successful</a></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p>"
        }
      },
      {
        "id": 138248217,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An American tech company used an AWS CloudFormation template to deploy its static corporate website hosted on Amazon S3 in the US East (N. Virginia) region. The template defines an Amazon S3 bucket with a Lambda-backed custom resource that downloads the content from a file server into the bucket. There is a new task for the DevOps Engineer to move the website to the US West (Oregon) region to better serve its customers on the West Coast with lower latency. However, the application stack could not be deleted successfully in CloudFormation. </p><p>Which among the following options shows the root cause of this issue, and how can the DevOps Engineer mitigate this problem for current and future versions of the website?</p>",
          "answers": [
            "<p>The CloudFormation stack deletion fails for an S3 bucket that is used as a static web hosting. To fix the issue, modify the CloudFormation template to remove the website configuration for the S3 bucket.</p>",
            "<p>The CloudFormation stack deletion fails for an S3 bucket that still has contents. To fix the issue, modify the Lambda function code of the custom resource to recursively empty the bucket if the stack is selected for deletion.</p>",
            "<p>The CloudFormation stack deletion fails for an S3 bucket because the <code>DeletionPolicy</code> attribute is set to <code>Snapshot</code>. To fix the issue, set the <code>DeletionPolicy</code> to <code>Delete</code> instead.</p>",
            "<p>The CloudFormation stack deletion fails for an S3 bucket because it is not yet empty. To fix the issue, set the <code>DeletionPolicy</code> to <code>ForceDelete</code> instead.</p>"
          ],
          "explanation": "<p>When you associate a Lambda function with a custom resource, the function is invoked whenever the custom resource is created, updated, or deleted. AWS CloudFormation calls a Lambda API to invoke the function and to pass all the request data (such as the request type and resource properties) to the function. The power and customizability of Lambda functions in combination with AWS CloudFormation enable a wide range of scenarios, such as dynamically looking up AMI IDs during stack creation, or implementing and using utility functions, such as string reversal functions.</p><p>AWS CloudFormation templates that declare an Amazon Elastic Compute Cloud (Amazon EC2) instance must also specify an Amazon Machine Image (AMI) ID, which includes an operating system and other software and configuration information used to launch the instance. The correct AMI ID depends on the instance type and region in which you're launching your stack. And IDs can change regularly, such as when an AMI is updated with software updates.</p><p>Normally, you might map AMI IDs to specific instance types and regions. To update the IDs, you manually change them in each of your templates. By using custom resources and AWS Lambda (Lambda), you can create a function that gets the IDs of the latest AMIs for the region and instance type that you're using so that you don't have to maintain mappings.</p><p><img src=\"https://media.tutorialsdojo.com/public/CloudFormation-AMIManager-Flow.png\"></p><p>You can also run the custom resource to recursively empty the bucket when the CloudFormation stack is triggered for deletion. In CloudFormation, you can only delete empty buckets. Any request for deletion will fail for buckets that still have contents. To control how AWS CloudFormation handles the bucket when the stack is deleted, you can set a deletion policy for your bucket. You can choose to retain the bucket or to delete the bucket.</p><p>Hence, the correct answer is: <strong>The CloudFormation stack deletion fails for an S3 bucket that still has contents. To fix the issue, modify the Lambda function code of the custom resource to recursively empty the bucket if the stack is selected for deletion.</strong></p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket that is used as a static web hosting. To fix the issue, modify the CloudFormation template to remove the website configuration for the S3 bucket </strong>is incorrect because the CloudFormation deletion process will not be hindered simply because your S3 bucket is configured for static web hosting. The primary root cause of this issue is that the CloudFormation stack deletion fails for an S3 bucket that still has contents.</p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket because the </strong><code><strong>DeletionPolicy</strong></code><strong> attribute is set to </strong><code><strong>Snapshot</strong></code><strong>. To fix the issue, set the </strong><code><strong>DeletionPolicy</strong></code><strong> to </strong><code><strong>Delete</strong></code><strong> instead </strong>is incorrect because you can only set the <code><strong><em>DeletionPolicy</em></strong></code> to either <code><strong><em>Retain </em></strong></code>or <code><strong><em>Delete</em></strong></code> for an Amazon S3 resource. In addition, the CloudFormation deletion will still fail as long as the S3 bucket is not empty, even if the <code>DeletionPolicy</code> attribute is already set to <code>Delete</code>.</p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket is not yet empty. To fix the issue, set the </strong><code><strong>DeletionPolicy</strong></code><strong> to </strong><code><strong>ForceDelete</strong></code><strong> instead</strong> is incorrect. Although the provided root cause is accurate, the configuration for <code><strong><em>DeletionPolicy </em></strong></code>remains invalid. <code><strong><em>ForceDelete</em></strong></code> is not a valid value for the deletion policy attribute.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/\">https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 138248159,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company has a suite of applications that are hosted in AWS and each app has its own AMI. Currently, a new AMI must be manually created and deployed to the server if there is a new application version. A DevOps engineer was instructed to automate the process of generating the AMIs to streamline the company's CI/CD workflow. The ID of the newly created AMI must be stored in a centralized location where other build pipelines can programmatically access it.</p><p>Which of the following is the MOST cost-effective way to accomplish this requirement with the LEAST amount of overhead?</p>",
          "answers": [
            "<p>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store.</p>",
            "<p>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</p>",
            "<p>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket.</p>",
            "<p>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table.</p>"
          ],
          "explanation": "<p><strong>Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p><br></p><p>Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machine Images (AMIs) and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for various reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img alt=\"Custom AMI\" height=\"771\" src=\"https://media.tutorialsdojo.com/public/custom_ami_1.gif\" width=\"1000\"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that the Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</strong></p><p>The option that says: <strong>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store </strong>is incorrect because manually customizing the image using an interactive shell and downloading each application image in an OVF file will simply entails a lot of effort. It is also better to use the AWS Systems Manager Automation instead of creating a new pipeline in AWS CodePipeline.</p><p>The option that says: <strong>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket </strong>is incorrect. Although you can technically generate an AMI using an EBS volume snapshot, this process is still tedious and entails a lot of configuration. Using the AWS Systems Manager Automation to generate the AMIs is a more suitable solution.</p><p>The option that says: <strong>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table </strong>is incorrect. Although this may work, this solution will only costs more to maintain than other options since it uses an EC2 instance and an Amazon DynamoDB table. There is also an associated overhead in configuring and using Packer for generating the AMIs and preparing the Jenkins pipeline.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/\">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949174,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
          "answers": [
            "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
            "<p>Add test stages before and after the deploy action.</p>",
            "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
            "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"
          ],
          "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921464,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a social media company, you have implemented a CICD pipeline that takes code from a CodeCommit repository, builds it using CodeBuild thanks to the instructions in the local <code>Dockerfile</code>, and then pushes to ECR at <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code>. The last step of your CICD pipeline is to deploy to the application to your ECS cluster. It seems that while you do so, the application is only partly updated on some ECS instances which are running an older version of your image. You have found that terminating the instance or clearing the local Docker cache fixes the issue, but would like to implement something more robust that provides visibility and identification to track where container images are deployed.</p>\n\n<p>How should you implement a solution to address this issue?</p>\n",
          "answers": [
            "<p>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</p>",
            "<p>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</strong></p>\n\n<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. You can use the <code>IMAGEID</code> property, which is the SHA digest for the Docker image used to start the container.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n\n<p>The issue here is that the ECS instances do not detect that a newer image version is available, because the name <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is re-used. Therefore, by specifying the sha256 e.g.: <code>aws_account_id.dkr.ecr.region.amazonaws.com/my-web-app@sha256:94afd1f2e64d908bc90dbca0035a5b567EXAMPLE</code>, we are certain that newer versions of the Docker image will have a different hash value and therefore the ECS cluster will always pull the newest image at the end of our CICD Pipeline.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</strong> - SSM Run Command lets you remotely and securely manage the configuration of your managed instances. A managed instance is any EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. SSM Run Command may work but it's not an elegant solution.</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</strong> - Lambda Functions can't SSH into EC2 instances, so this option is incorrect.</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</strong> - Adding the <code>latest</code> tag won't help because <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is same as <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app:latest</code>. The <code>latest</code> tag cannot provide visibility and identification to track where container images are deployed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/\">https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n"
        }
      },
      {
        "id": 138248239,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A government agency recently decided to modernize its network infrastructure using AWS. They are developing a solution to store confidential files containing Personally Identifiable Information (PII) and other sensitive financial records of its citizens. All data in the storage solution must be encrypted both at rest and in transit. In addition, all of its data must also be replicated in two locations that are at least 450 miles apart from each other. </p><p>As a DevOps Engineer, what solution should you implement to meet these requirements?</p>",
          "answers": [
            "<p>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3 SSE-C encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
            "<p>Set up primary and secondary S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
            "<p>Set up primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create an IAM role to enforce access to the buckets only through HTTPS. Set up a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
            "<p>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce AWS KMS encryption on all objects uploaded to the bucket. Enable Transfer Acceleration between the two buckets. Set up a KMS Key in the primary region for encrypting objects.</p>"
          ],
          "explanation": "<p><strong>Availability Zones</strong> give customers the ability to operate production applications and databases that are more highly available, fault-tolerant, and scalable than would be possible from a single data center. AWS maintains multiple AZs around the world and more zones are added at a fast pace. Each AZ can be multiple data centers (typically 3), and at full scale can be hundreds of thousands of servers. They are fully isolated partitions of the AWS Global Infrastructure. With their own power infrastructure, the AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles of each other).</p><p>All AZs are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. The network performance is sufficient to accomplish synchronous replication between AZs. AWS Availability Zones are also powerful tools for helping build highly available applications. AZs make partitioning applications about as easy as it can be. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more.</p><p><img src=\"https://media.tutorialsdojo.com/public/Amazon-S3.png\"></p><p>By default, Amazon S3 allows both HTTP and HTTPS requests. To comply with the <strong>s3-bucket-ssl-requests-only</strong> rule, confirm that your bucket policies explicitly deny access to HTTP requests. Bucket policies that allow HTTPS requests without explicitly denying HTTP requests might not comply with the rule.</p><p>To determine HTTP or HTTPS requests in a bucket policy, use a condition that checks for the key <strong>\"aws:SecureTransport\"</strong>. When this key is <strong>true</strong>, this means that the request is sent through HTTPS. To be sure to comply with the <strong>s3-bucket-ssl-requests-only</strong> rule, create a bucket policy that explicitly denies access when the request meets the condition <strong>\"aws:SecureTransport\": \"false\"</strong>. This policy explicitly denies access to HTTP requests.</p><p>In this scenario, you should use AWS Regions since AZs are physically separated by only 100 km (60 miles) from each other. Within each AWS Region, S3 operates in a minimum of three AZs, each separated by miles to protect against local events like fires, floods et cetera. Take note that you can't launch an AZ-based S3 bucket.</p><p>Hence, the correct answer is: <strong>Set up primary and secondary S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</strong></p><p>The option that says: <strong>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3 SSE-C encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets</strong> is incorrect. You can't create Amazon S3 buckets in two separate Availability Zones since this is a regional service.</p><p>The option that says: <strong>Set up primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create an IAM role to enforce access to the buckets only through HTTPS. Set up a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets</strong> is incorrect. You have to use the bucket policy to enforce access to the bucket using HTTPS only and not an IAM role.</p><p>The option that says: <strong>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce AWS KMS encryption on all objects uploaded to the bucket. Enable Transfer Acceleration between the two buckets. Set up a KMS Key in the primary region for encrypting objects</strong> is incorrect. You have to enable Cross-Region replication and not Transfer Acceleration. This feature simply enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket but not data replication.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html \">https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/ \">https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/</a></p><p><a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">https://aws.amazon.com/about-aws/global-infrastructure/regions_az/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>"
        }
      },
      {
        "id": 82921352,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A Big Data analytics company has deployed a stream processing application using KCL to read records from Kinesis Data Streams configured with multiple shards. The application is running on one EC2 instance. It seems that the consuming application is lagging under a large load and therefore records are not processed in time and eventually dropped from the stream.</p>\n\n<p>As a DevOps Engineer, you have been tasked with improving the reliability of this application with minimal changes, what should you do? (Select two)</p>\n",
          "answers": [
            "<p>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></p>",
            "<p>Increase the stream data retention period</p>",
            "<p>Migrate the application to AWS Lambda</p>",
            "<p>Increase the number of shards in Kinesis to increase throughput</p>",
            "<p>Decrease the numbers of shards in Kinesis to decrease the load</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></strong></p>\n\n<p>In a typical Kinesis Data Streams architecture, you have producers that continually push data to Kinesis Data Streams, and the consumers process the data in real-time. Consumers (such as a custom application running on Amazon EC2 or an Amazon Kinesis Data Firehose delivery stream) can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>Key concepts for Kinesis Data Streams:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>The Kinesis Client Library (KCL) ensures that for every shard, there is a record processor running and processing that shard. The library also simplifies reading data from the stream. The Kinesis Client Library uses an Amazon DynamoDB table to store control data.</p>\n\n<p>For the given use-case, you need to run KCL on multiple EC2 instances behind an ASG. Running more KCL processes is the key here, and we need for that to have an Auto Scaling Group based on the metric <code>MillisBehindLatest</code>, which represents the time that the current iterator is behind from the latest record (tip) in the shard. The Kinesis Client Library (KCL) for Amazon Kinesis Data Streams publishes custom Amazon CloudWatch metrics on your behalf, using the name of your KCL application as the namespace.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n\n<p><strong>Increase the stream data retention period</strong></p>\n\n<p>The retention period is the length of time that data records are accessible after they are added to the stream. A stream\u2019s retention period is set to a default of 24 hours after creation. To avoid records being dropped, it's good to increase the stream retention time and allow ourselves a higher margin to process the records. The maximum retention you can set is 7 days.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to AWS Lambda</strong> - Migrating the application to AWS Lambda will not help with the processing time, as eventually, the same processing code would run under EC2 or Lambda.</p>\n\n<p><strong>Increase the number of shards in Kinesis to increase throughput</strong> - Increasing the number of shards in Kinesis can increase the total throughput of the stream, but this does not impact the processing performance of your processes (which is bound by what you do with the messages). Increasing the number of shards though would help you increase the number of processing processes in KCL if that was already an upper bound (but currently we only have one KCL process running so it's not running at capacity).</p>\n\n<p><strong>Decrease the numbers of shards in Kinesis to decrease the load</strong> - Decrease the number of shards would decrease the throughput but again would have no effect on processing applications regarding their performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n"
        }
      },
      {
        "id": 138248111,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company is planning to deploy a new version of their legacy application in AWS which is deployed to an Auto Scaling group of EC2 instances with an Application Load Balancer in front. To avoid any disruption of their services, they need to implement canary testing first before all of the traffic is shifted to the new application version.</p><p>Which of the following solutions can meet this requirement?</p>",
          "answers": [
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</p>",
            "<p>Do a Canary deployment using CodeDeploy with a <code>CodeDeployDefault.LambdaCanary10Percent30Minutes</code> deployment configuration.</p>",
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers.</p>",
            "<p>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment.</p>"
          ],
          "explanation": "<p>The purpose of a canary deployment is to reduce the risk of deploying a new version that impacts the <a href=\"https://wa.aws.amazon.com/wat.concept.workload.en.html\" title=\"The set of components that together deliver business value.\">workload</a>. The method will incrementally deploy the new version, making it visible to new users in a slow fashion. As you gain confidence in the deployment, you will deploy it to replace the current version in its entirety.</p><p><img src=\"https://media.tutorialsdojo.com/public/Upgrades_Image1.jpeg\"></p><p>To properly implement the canary deployment, you should do the following steps:</p><p>- Use a router or load balancer that allows you to send a small percentage of users to the new version.</p><p>- Use a dimension on your KPIs to indicate which version is reporting the metrics.</p><p>- Use the metric to measure the success of the deployment; this indicates whether the deployment should continue or rollback.</p><p>- Increase the load on the new version until either all users are on the new version or you have fully rolled back.</p><p><br></p><p>Hence, the correct answer is: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</strong></p><p>The option that says: <strong>Do a Canary deployment using CodeDeploy with a </strong><code><strong>CodeDeployDefault.LambdaCanary10Percent30Minutes</strong></code><strong> deployment configuration</strong> is incorrect because this specific configuration type is only applicable for Lambda functions and for the applications hosted in an Auto Scaling group.</p><p>The option that says: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers</strong> is incorrect because you can't use CloudFront to adjust the weight of the incoming traffic to your application. You should use Route 53 instead.</p><p>The option that says: <strong>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment</strong> is incorrect because you can only integrate a Network Load Balancer to your Amazon API Gateway. Moreover, this service is only applicable for APIs, not full-fledged web applications.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html\">https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/\">https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/</a></p>"
        }
      },
      {
        "id": 138248229,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A multinational investment bank is implementing regulatory compliance checks over its AWS accounts. All API calls made on each AWS resource across the company's accounts must be monitored and tracked for auditing purposes. AWS CloudTrail will be used to monitor all API activities and detect sensitive security issues in the company's AWS accounts. The DevOps Team was assigned to come up with a solution to prevent CloudTrail from being disabled on some AWS accounts automatically.</p><p>What solution should be applied to ensure CloudTrail log deliveries experience the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of <code>Configuration changes</code>. By default, this managed rule will automatically remediate the accounts that disabled its CloudTrail.</p>",
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a <code>StopLogging</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>StartLogging</code> API on the resource ARN.</p>",
            "<p>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications.</p>",
            "<p>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a <code>DeleteTrail</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>CreateTrail</code> API on the resource ARN.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides <em>AWS managed rules</em>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly assess whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p><strong><img src=\"https://media.tutorialsdojo.com/public/td-aws-config-diagram-13Jan2025.png\"></strong>You can customize the behavior of a managed rule to suit your needs. For example, you can define the rule's scope to constrain which resources trigger an evaluation for the rule, such as EC2 instances or volumes. You can customize the rule's parameters to define attributes that your resources must have to comply with the rule. For example, you can customize a parameter to specify that your security group should block incoming traffic to a specific port number.</p><p>After you activate a rule, AWS Config compares your resources to the rule's conditions. After this initial evaluation, AWS Config continues to run evaluations each time one is triggered. The evaluation triggers are defined as part of the rule, and they can include the following types:</p><p><strong>Configuration changes</strong> \u2013 AWS Config triggers the evaluation when any resource that matches the rule's scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p><p><strong>Periodic</strong> \u2013 AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p><p>The cloudtrail-enabled checks whether AWS CloudTrail is enabled in your AWS account. Optionally, you can specify which S3 bucket, SNS topic, and Amazon CloudWatch Logs ARN to use.</p><p><img src=\"https://media.tutorialsdojo.com/aws-config-cloudtrail-enabled.JPG\"></p><p>Hence, the correct answer is: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a </strong><code><strong>StopLogging</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>StartLogging</strong></code><strong> API on the resource ARN.</strong></p><p>The option that says: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of </strong><code><strong>Configuration changes</strong></code><strong>. This managed rule will automatically remediate the accounts that disabled its CloudTrail </strong>is incorrect because, by default, AWS Config will not automatically remediate the accounts that disabled its CloudTrail. You must manually set this up using an Amazon EventBridge rule and a custom Lambda function that calls the StartLogging API to enable CloudTrail back again. Furthermore, the <code><strong>cloudtrail-enabled</strong></code> AWS Config managed rule is only available for the <code>periodic trigger</code> type and not <code>Configuration changes</code>.</p><p>The option that says: <strong>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications</strong> is incorrect. AWS Cloud Development Kit (AWS CDK) is only an open-source software development framework for building cloud applications and infrastructure using programming languages. It isn't used to check whether the CloudTrail is enabled in an AWS account.</p><p>The option that says: <strong>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a </strong><code><strong>DeleteTrail</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>CreateTrail</strong></code><strong> API on the resource ARN</strong> is incorrect. Instead, you should detect the <code>StopLogging</code> event and call the StartLogging API to enable CloudTrail again. The <code>DeleteTrail</code> and <code>CreateTrail</code> events, as their name implies, are simply for deleting and creating the trails respectively.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/\">https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/v2/guide/home.html\">https://docs.aws.amazon.com/cdk/v2/guide/home.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 99528227,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>An application is being deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The security team requires that the traffic is secured with SSL/TLS certificates. Protection against common web exploits must also be implemented. The solution should not have a performance impact on the EC2 instances.</p><p>What steps should be taken to secure the web application? (Select TWO.)</p>",
          "answers": [
            "<p>Create an AWS WAF web ACL and attach it to the ALB.</p>",
            "<p>Add an SSL/TLS certificate to a secure listener on the ALB.</p>",
            "<p>Install SSL/TLS certificates on the EC2 instances.</p>",
            "<p>Configure Server-Side Encryption with KMS managed keys.</p>",
            "<p>Enable EBS encryption for the EC2 volumes to encrypt all traffic.</p>"
          ],
          "explanation": "<p>To secure the traffic in transit an SSL/TLS certificate should be attached to a secure listener on the ALB. This will not affect the performance of the EC2 instances as the encryption takes place only between the client and the ALB. The certificate can be issued through AWS Certificate Manager (ACM).</p><p>The AWS Web Application Firewall (AWS WAF) protects against common web exploits. The company can create a web ACL with a rule and action and then attach it to the ALB. This will protect against web exploits.</p><p><strong>CORRECT: </strong>\"Add an SSL/TLS certificate to a secure listener on the ALB\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL and attach it to the ALB\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install SSL/TLS certificates on the EC2 instances\" is incorrect.</p><p>Encryption on the EC2 instances would impact the performance of those instances.</p><p><strong>INCORRECT:</strong> \"Configure Server-Side Encryption with KMS managed keys\" is incorrect.</p><p>This is not relevant to in transit encryption, this is used to encrypt data at rest on services such as Amazon S3.</p><p><strong>INCORRECT:</strong> \"Enable EBS encryption for the EC2 volumes to encrypt all traffic\" is incorrect.</p><p>EBS encryption is used for encrypting data at rest. The question requires encryption using SSL/TLS certificates which is encryption in transit.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\">https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\">https://aws.amazon.com/elasticloadbalancing/application-load-balancer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      }
    ],
    "answers": {
      "75949056": [
        "d"
      ],
      "75949118": [
        "a"
      ],
      "75949156": [
        "d"
      ],
      "75949174": [
        "c"
      ],
      "82921318": [
        "b"
      ],
      "82921352": [
        "a",
        "b"
      ],
      "82921464": [
        "a"
      ],
      "99528227": [
        "a",
        "b"
      ],
      "115961527": [
        "a"
      ],
      "134588393": [
        "d"
      ],
      "134588431": [
        "d"
      ],
      "138248111": [
        "b"
      ],
      "138248159": [
        "b"
      ],
      "138248203": [
        "a",
        "b"
      ],
      "138248207": [
        "a"
      ],
      "138248213": [
        "c"
      ],
      "138248217": [
        "d"
      ],
      "138248229": [
        "a"
      ],
      "138248239": [
        "b"
      ],
      "143860761": [
        "c",
        "d"
      ]
    }
  },
  {
    "id": "1769846401359",
    "date": "2026-01-31T08:00:01.359Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 7,
    "incorrect": 3,
    "unanswered": 0,
    "total": 10,
    "percent": 70,
    "duration": 5057695,
    "questions": [
      {
        "id": 134588405,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A business wants to leverage AWS CloudFormation to deploy its infrastructure. The business would like to restrict deployment to two particular regions and wants to implement a strict tagging requirement. Developers are expected to deploy various versions of the same application and want to guarantee that resources are deployed in compliance with the business policy while still enabling developers to deploy different versions of the application.</p><p>Which of the following is the MOST suitable solution?</p>",
          "answers": [
            "<p>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</p>",
            "<p>Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks.</p>",
            "<p>Utilize approved CloudFormation templates and launch CloudFormation StackSets.</p>",
            "<p>Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation.</p>"
          ],
          "explanation": "<p>With <strong>AWS Service Catalog</strong>, cloud resources can be centrally managed to achieve infrastructure as code (IaC) template governance at scale, whether written in CloudFormation or Terraform. Compliance requirements can be met while ensuring customers can efficiently deploy the necessary cloud resources.</p><p><img alt=\"Service Catalog\" height=\"540\" src=\"https://media.tutorialsdojo.com/public/dop-c02-service-catalog.png\" width=\"1000\"></p><p>Template constraints can be applied when limiting end-users' options during a product launch. This ensures that the organization's compliance requirements are not breached.</p><p>A product must be present within a Service Catalog portfolio to apply template constraints. A template constraint includes rules that narrow the allowable values for parameters in the underlying AWS CloudFormation template of the product. These parameters define the set of values available to users when creating a stack. For instance, an instance type parameter can be specified to limit the types of instances that users can choose from when launching a stack containing EC2 instances.</p><p>Hence, the correct answer is: <strong>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</strong></p><p>The option that says:<strong> Utilize approved CloudFormation templates and launch CloudFormation StackSets </strong>is incorrect because StackSets manage deployments across accounts and regions but do not enforce tagging or region restrictions. They do not typically provide governance to prevent non-compliant implementations.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks </strong>is incorrect because Trusted Advisor does not support checks for unauthorized StackSets or enforce CloudFormation template compliance.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation </strong>is incorrect because drift detection only identifies changes after deployment, but cannot prevent non-compliant resource creation or enforce policies before deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/servicecatalog/\">https://aws.amazon.com/servicecatalog/</a></p><p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/\">https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/</a></p><p><br></p><p><strong>Check out this AWS Service Catalog Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-service-catalog/?src=udemy\">https://tutorialsdojo.com/aws-service-catalog/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/</a></p>"
        }
      },
      {
        "id": 115961529,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An online sales application is being migrated to AWS with the application layer hosted on Amazon EC2 instances and the database layer on a PostgreSQL database. It is mandated that the application must have minimal downtime as it receives traffic 24/7 and any downtime may reduce business revenue. The application must also be fault tolerant including the data layer.</p><p>Concerns have been raised around performance of the database layer during sales events and other peak periods. The application must also be continually scanned for vulnerabilities.</p><p>Which option will meet the above requirements?</p>",
          "answers": [
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use RDS PostgreSQL for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments.</p>",
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon Macie to perform automatic security assessments.</p>",
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon GuardDuty to perform automatic security assessments.</p>",
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments.</p>"
          ],
          "explanation": "<p>The above question clearly mandates three requirements:</p><p>1. Performance- Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group</p><p>2. Database performance- Amazon Aurora will perform better than PostgreSQL since it provides multi-master configuration and can be scaled better than RDS on a global scale.</p><p>3. Vulnerability assessment- Amazon Inspector is the right fit for the scanning. The difference between Amazon Inspector and Amazon GuardDuty is that the former \"checks what happens when you actually get an attack\" and the latter \"analyzes the actual logs to check if a threat exists\". The purpose of Amazon Inspector is to test whether you are addressing common security risks in the target AWS.</p><p>Database categorization and selection parameters:</p><p>\u00b7 If your scaling needs are for standard/ general purpose applications, RDS is the better option. You can auto-scale the database to max capacity with just a few clicks on the AWS console.</p><p>\u00b7 You also have the option of Aurora Serverless that can scale up or scale down well, you have to be aware of several <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations\">restrictions that apply in the Serverless mode</a>.</p><p>\u00b7 If you must handle a very high volume of read/write requests, DynamoDB is a better choice. It scales seamlessly with no impact on performance. You can run these database servers in on-demand or provisioned capacity mode.</p><p>If you have heavy write workloads and require more than five read replicas, Aurora is a better choice. Since Aurora uses shared storage for writer and readers, there is minimal replica lag. RDS allows only up to five replicas and the replication process is slower than Aurora.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-46-18-8cd4e56a381e9f1f934f96761a29cae7.jpg\"><p><strong>CORRECT: </strong>\"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use RDS PostgreSQL for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments\" is incorrect.</p><p>Amazon Aurora is a better fit for this use case as described above.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon Macie to perform automatic security assessments\" is incorrect.</p><p>Amazon Inspector should be used for performing the security assessments.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon GuardDuty to perform automatic security assessments\" is incorrect.</p><p>Amazon Inspector should be used for performing the security assessments.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p>https://aws.amazon.com/blogs/database/is-amazon-rds-for-postgresql-or-amazon-aurora-postgresql-a-better-choice-for-me/</p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>"
        }
      },
      {
        "id": 82921406,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A multi-national retail company has defined tagging guidelines and standard for all its resources in AWS and would like to create a dashboard to visualize the compliance of all the resources with the ability to find out the non-compliant resources. The company has hired you as an AWS Certified DevOps Engineer Professional to develop a solution for this requirement.</p>\n\n<p>Which of the following options would you suggest to address the use-case?</p>\n",
          "answers": [
            "<p>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</p>",
            "<p>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</p>",
            "<p>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</p>",
            "<p>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</strong></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p>Here, we can use AWS Config to track resource configuration, and we could create a rule to track the tagging of these resources. All the changes to resource configuration as well as tagging of resources are streamed to an SNS topic.</p>\n\n<p>A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags make it easier to manage, search for, and filter resources. You can use tags to categorize your AWS resources in different ways, for example, by purpose, owner, or environment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p>You can set up the Required-Tag managed rule for Config which requires up to 6 tags with optional values in a single rule. Previously, each rule accepted only a single tag/value combo. Additionally, the Required-Tag managed rule now accepts a comma-separated list of values for each checked tag. This allows for a rule to be compliant if any one of a supplied list of tags is present on the resource.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</strong> - AWS Service Catalog enabling AWS customers to create and deliver standardized services that provide the necessary control, while still empowering developers to choose the services that best fit their needs. You cannot use Service Catalog to get an inventory of all the resources in your account.</p>\n\n<p><strong>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</strong> - SSM inventory will only help with understanding what is installed on your managed instances. To view Systems Manager Inventory history and change tracking for all of your managed instances, you need to use AWS Config itself.</p>\n\n<p><strong>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</strong> - CloudTrail is used to track API calls, not resources. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n"
        }
      },
      {
        "id": 75949108,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>",
          "answers": [
            "<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"
          ],
          "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921460,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A gaming company would like to be able to receive near real-time notifications when the API call <code>DeleteTable</code> is invoked in DynamoDB.</p>\n\n<p>As a DevOps Engineer at the company, how would you implement this at a minimal cost?</p>\n",
          "answers": [
            "<p>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</p>",
            "<p>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</p>",
            "<p>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</p>",
            "<p>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. Rules in CloudWatch Events work only in the Region in which they are created. If you configure CloudTrail to track API calls in multiple Regions, and you want a rule-based on CloudTrail to trigger in each of those Regions, you must create a separate rule in each Region that you want to track.</p>\n\n<p>For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q40-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</strong> - A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. DynamoDB Streams do not capture <code>DeleteTable</code> API calls, they only capture item-level events.</p>\n\n<p><strong>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</strong> - Sending CloudTrail Logs to CloudWatch Logs and creating a filter on those will work but will be expensive, as we're streaming all the logs from CloudTrail just to extract a single event.</p>\n\n<p><strong>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</strong> - CloudTrail trails do not have event filters and cannot be directly sent to a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p>\n"
        }
      },
      {
        "id": 75949166,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An application sits behind a Network Load Balancer (NLB) that is configured with a TLS listener. The DevOps team must analyze traffic patterns and require information about the connections made by clients. The data that is captured must be stored securely with encryption at rest and should only be accessible to the DevOps team members.</p><p>Which actions should a DevOps engineer take?</p>",
          "answers": [
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing provides access logs that capture detailed information about the TLS requests sent to your Network Load Balancer. You can use these access logs to analyze traffic patterns and troubleshoot issues. The logs are sent to an Amazon S3 bucket you configure as the logging destination. This bucket can be encrypted using one of the available server-side encryption options.</p><p>When you enable access logging, you must specify an S3 bucket for the access logs. The policy must grant permission to the AWS service account \u2018delivery.logs.amazonaws.com\u2019. In this case, the DevOps team also require permissions to access the bucket, and this can be granted through an IAM policy attached to the team members, most likely via an IAM group.</p><p><strong>CORRECT: </strong>\"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account\" is incorrect.</p><p>This will not allow read access for the DevOps team as the only permission is write access.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 82921334,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.</p>\n\n<p>How can you do this most securely?</p>\n",
          "answers": [
            "<p>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</p>",
            "<p>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>",
            "<p>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</p>",
            "<p>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</strong></p>\n\n<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.</p>\n\n<p>You can use Secrets Manager to natively rotate credentials for Amazon Relational Database Service (RDS), Amazon DocumentDB, and Amazon Redshift. You can extend Secrets Manager to rotate other secrets, such as credentials for Oracle databases hosted on EC2 or OAuth refresh tokens, by modifying sample AWS Lambda functions available in the Secrets Manager documentation.</p>\n\n<p>To access PostgreSQL, you can use database credentials and they're best stored in Secrets Manager from a security best-practices perspective. Access to Secrets Manager itself is regulated using an IAM role with the requisite policy. You must write this IAM policy permitting your application on EC2 instances to access specific secrets. Then, in the application source code, you can replace secrets in plain text with code to retrieve these secrets programmatically using the Secrets Manager APIs. To access the DynamoDB table, you should also add the appropriate policy to this IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - As mentioned in the explanation above, Secrets Manager does NOT support DynamoDB, so this option is incorrect.</p>\n\n<p><strong>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</strong> - It's not recommended to store RDS credentials in a DynamoDB, as it can be accessed by everyone who has access to the underlying table. This constitutes a serious security threat.</p>\n\n<p><strong>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - Storing IAM user credentials in Secrets Manager is a distractor as IAM user credentials are not required to build a solution for this use-case. You can just use an IAM instance role for EC2 to access Secrets Manager.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/faqs/\">https://aws.amazon.com/secrets-manager/faqs/</a></p>\n"
        }
      },
      {
        "id": 75949116,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company has deployed a web service that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company has deployed the application in us-east-1. The web service uses Amazon Route 53 records for DNS requests for example.com. The records are configured with health checks that assess the availability of the web service.</p><p>A second environment has been deployed into eu-west-1. The company requires traffic to be routed to the environment that provides the lowest latency for user requests. In the event of a regional outage, traffic should be directed to the alternate Region.</p><p>Which configuration will achieve these requirements?</p>",
          "answers": [
            "<p>Create a subdomain named us.example.com with failover routing. Configure the US ALB as primary and the EU ALB as secondary. Create another subdomain named eu.example.com with failover routing. Configure the EU ALB as primary and the US ALB as secondary. Create latency-based routing records for example.com that are aliased to us.example.com and eu.example.com.</p>",
            "<p>Create a subdomain named us.example.com with weighted routing. Configure the US ALB with weight 2 and the EU ALB with weight 1. Create another subdomain named eu.example.com with weighted routing. Configure the EU ALB with weight 2 and the US ALB with weight 1. Create geolocation routing records for example.com with North America aliased to us.example.com and Europe aliased to eu.example.com.</p>",
            "<p>Create a subdomain named us.example.com with latency-based routing. Configure the US ALB as the first target and the EU ALB as the second target. Create another subdomain named eu.example.com with latency-based routing. Configure the EU ALB as the first target and the US ALB as the second target. Create failover routing records for example.com aliased to us.example.com as the first target and eu.example.com as the second target.</p>",
            "<p>Create a subdomain named us.example.com with multivalue answer routing. Configure the US ALB first and the EU ALB second. Create another subdomain named eu.example.com with multivalue answer routing. Configure the EU ALB first and the US ALB second. Create failover routing records for example.com that are aliased to us.example.com and eu.example.com.</p>"
          ],
          "explanation": "<p>There are two key requirements that inform the design. Firstly, the solution must route based on latency. Secondly, failover across Regions must be automatic. This is a more complex DNS routing configuration. To meet both requirements the company will need to use a combination of latency-based routing records and failover routing records.</p><p>The solution is to create subdomains for each Region that can be used for the failover records and pointing the secondary to the alternate Region. Then, on top of those records the solution includes a latency-based routing record for example.com.</p><p>With this solution example.com will resolve to the subdomain that represents the lowest latency from the user request location. If the environment in that Region is not available (has failed health checks) then the request will be failed over to the alternate Region.</p><p><strong>CORRECT: </strong>\"Create a subdomain named us.example.com with failover routing. Configure the US ALB as primary and the EU ALB as secondary. Create another subdomain named eu.example.com with failover routing. Configure the EU ALB as primary and the US ALB as secondary. Create latency-based routing records for example.com that are aliased to us.example.com and eu.example.com\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with weighted routing. Configure the US ALB with weight 2 and the EU ALB with weight 1. Create another subdomain named eu.example.com with weighted routing. Configure the EU ALB with weight 2 and the US ALB with weight 1. Create geolocation routing records for example.com with North America aliased to us.example.com and Europe aliased to eu.example.com\" is incorrect.</p><p>The solution should use failover routing and latency routing, not weighted routing and geolocation routing.</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with latency-based routing. Configure the US ALB as the first target and the EU ALB as the second target. Create another subdomain named eu.example.com with latency-based routing. Configure the EU ALB as the first target and the US ALB as the second target. Create failover routing records for example.com aliased to us.example.com as the first target and eu.example.com as the second target\" is incorrect.</p><p>This solution gets the failover and latency records the wrong way around. Failover routing should be used for the subdomain and latency routing for the apex domain (example.com).</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with multivalue answer routing. Configure the US ALB first and the EU ALB second. Create another subdomain named eu.example.com with multivalue answer routing. Configure the EU ALB first and the US ALB second. Create failover routing records for example.com that are aliased to us.example.com and eu.example.com\" is incorrect.</p><p>Multivalue routing is a form of DNS load balancing and will simply route records across all registered and available targets. This does not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>"
        }
      },
      {
        "id": 75949158,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A financial services company requires that DevOps engineers should not log directly into Amazon EC2 instances that process highly sensitive data except in exceptional circumstances. The security team requires a notification within 15 minutes if a DevOps engineer does log in to an instance.</p><p>Which solution will meet these requirements with the least operational overhead?</p>",
          "answers": [
            "<p>Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>"
          ],
          "explanation": "<p>The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. The agent includes the following components:</p><p>\u00b7 A plug-in to the AWS CLI that pushes log data to CloudWatch Logs.</p><p>\u00b7 A script (daemon) that initiates the process to push data to CloudWatch Logs.</p><p>\u00b7 A cron job that ensures that the daemon is always running.</p><p>You can create metric filters to match terms in your log events and convert log data into metrics. When a metric filter matches a term, it increments the metric's count. For example, you can create a metric filter that counts the number of times the word <strong><em>ERROR</em></strong> occurs in your log events.</p><p>In this case the metric filter can search for user login data and then if this information is found it can send an SNS notification to the security team.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>The Systems Manager agent will not gather this information from EC2 instances. The CloudWatch Logs agent must be installed.</p><p><strong>INCORRECT:</strong> \"Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>CloudTrail will only report on API activity, and this does not include login data from an Amazon EC2 instance.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>This is possible though it is not the best solution as it requires the script to be rerun on a regular basis and requires more operational overhead to create and maintain.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 75949120,
        "correct_response": [
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>An application running on an Amazon EC2 instance stores sensitive data on an attached Amazon EBS volume. The volume is not encrypted. A DevOps engineer must enable encryption at rest for the data.</p><p>Which actions should the engineer take? (Select TWO.)</p>",
          "answers": [
            "<p>Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume.</p>",
            "<p>Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data.</p>",
            "<p>Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted.</p>",
            "<p>Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume.</p>",
            "<p>Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume.</p>"
          ],
          "explanation": "<p>You cannot enable encryption for an existing EBS volume. You must enable encryption of the volume at creation time. There are a couple of ways to enable encryption of data stored on an unencrypted volume:</p><p>1) Create a snapshot of the volume. The snapshot will also be unencrypted, but you can then copy it and enable encryption. Then, you can create an encrypted volume from the snapshot and attach it to the instance.</p><p>2) Create and mount a new, encrypted EBS volume. The engineer would then need to move data onto the volume.</p><p>In both cases the engineer will need to update the application to use the new volume.</p><p><strong>CORRECT: </strong>\"Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume\" is incorrect.</p><p>This has not resulted in any change to the EBS volume, it is still unencrypted.</p><p><strong>INCORRECT:</strong> \"Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data\" is incorrect.</p><p>You cannot enable encryption for existing volumes through any AWS tools.</p><p><strong>INCORRECT:</strong> \"Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume\" is incorrect.</p><p>SSL/TLS certificates are used for enabling encryption in-transit, not encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>"
        }
      }
    ],
    "answers": {
      "75949108": [
        "c"
      ],
      "75949116": [
        "a"
      ],
      "75949120": [
        "c",
        "e"
      ],
      "75949158": [
        "a"
      ],
      "75949166": [
        "a"
      ],
      "82921334": [
        "a"
      ],
      "82921406": [
        "a"
      ],
      "82921460": [
        "d"
      ],
      "115961529": [
        "d"
      ],
      "134588405": [
        "a"
      ]
    }
  },
  {
    "id": "1769688092801",
    "date": "2026-01-29T12:01:32.801Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 12,
    "incorrect": 8,
    "unanswered": 0,
    "total": 20,
    "percent": 60,
    "duration": 5297548,
    "questions": [
      {
        "id": 82921412,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>An IT company is deploying a Python Flask based application and would like to ensure that it has a base AMI that contains the necessary Python runtime, as well as OS patches. That AMI must be used able to be referenced programmatically from across all regions in your account in a scalable way. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>Which of the following options would you recommend for this use-case? (Select two)</p>\n",
          "answers": [
            "<p>Create an SSM Automation document to create the AMI in a repeatable manner</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Use AWS Inspector to create a patched AMI using the latest working AMI</p>",
            "<p>Use AWS Lambda to create a patched AMI using the latest working AMI</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an SSM Automation document to create the AMI in a repeatable manner</strong></p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>You can use AWS Systems Manager to create a maintenance window, and then register an Automation task to automate the creation of the AMIs. This process is applicable for both Windows and Linux instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q46-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</strong></p>\n\n<p>The AMI ID is region-scoped, so the AMI must be copied across regions and therefore each SSM parameter store will have different AMI ID values. But you can still use the same SSM Parameter Store key across all regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</strong> - The AMI ID is region-scoped and the AMI must be copied across regions for the solution to work. This option only copies the value of the AMI ID across regions but the AMI itself stays in one region. So this option is incorrect.</p>\n\n<p><strong>Use AWS Inspector to create a patched AMI using the latest working AMI</strong> - AWS Inspector can be leveraged to analyze EC2 instance OS and network vulnerabilities. You cannot use Inspector to create a patched AMI.</p>\n\n<p><strong>Use AWS Lambda to create a patched AMI using the latest working AMI</strong> - AWS Lambda cannot be used to create AMIs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n"
        }
      },
      {
        "id": 138248241,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A commercial bank has a hybrid cloud architecture in AWS where its online banking platform is hosted. The CTO instructed its Lead DevOps Engineer to implement a suitable deployment set up that minimizes the impact on their production environment. The CI/CD process should be configured as follows: </p><p>- A new fleet of Amazon EC2 instances should be automatically launched first before the actual production deployment. The additional instances will serve traffic during the deployment. </p><p>- All available EC2 instances across various Availability Zones must be load-balanced and must automatically heal if it becomes impaired due to an underlying hardware failure in Amazon EC2. </p><p>- At least half of the incoming traffic must be rerouted to the new application version that is hosted to the new instances. </p><p>- The deployment should be considered successful if traffic is rerouted to at least half of the available EC2 instances. </p><p>- All temporary files must be deleted before routing traffic to the new fleet of instances. Ensure that any other files that were automatically generated during the deployment process are removed. </p><p>- To reduce costs, the EC2 instances that host the old version in the deployment group must be terminated immediately. </p><p>What should the Engineer do to satisfy these requirements?</p>",
          "answers": [
            "<p>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the <code>CodeDeployDefault.OneAtAtime</code> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the <code>AllowTraffic</code> hook within the <code>appspec.yml</code> configuration file to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the <code>BeforeAllowTraffic</code> Traffic hook within <code>appspec.yml</code> to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the <code>CodeDeployDefault.HalfAtAtime</code> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the &lt;code&gt;CodeDeployDefault AllatOnce&lt;/code&gt; as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>"
          ],
          "explanation": "<p>The content in the <code>'hooks'</code> section of the AppSpec file varies, depending on the compute platform for your deployment. The <code>'hooks'</code> section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The <code>'hooks'</code> section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line. Here are descriptions of the hooks available for use in your AppSpec file.</p><p><code><strong>ApplicationStop</strong></code> \u2013 This deployment lifecycle event occurs even before the application revision is downloaded. You can specify scripts for this event to gracefully stop the application or remove currently installed packages in preparation for a deployment. The AppSpec file and scripts used for this deployment lifecycle event are from the previous successfully deployed application revision.</p><p><code><strong>DownloadBundle</strong></code> \u2013 During this deployment lifecycle event, the CodeDeploy agent copies the application revision files to a temporary location:</p><p><code>/opt/codedeploy-agent/deployment-root/<em>deployment-group-id</em>/<em>deployment-id</em>/deployment-archive</code> folder on Amazon Linux, Ubuntu Server, and RHEL Amazon EC2 instances.</p><p><code>C:\\ProgramData\\Amazon\\CodeDeploy\\<em>deployment-group-id</em>\\<em>deployment-id</em>\\deployment-archive</code> folder on Windows Server Amazon EC2 instances.</p><p>This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>BeforeInstall</strong></code> \u2013 You can use this deployment lifecycle event for preinstall tasks, such as decrypting files and creating a backup of the current version.</p><p><code><strong>Install</strong></code> \u2013 During this deployment lifecycle event, the CodeDeploy agent copies the revision files from the temporary location to the final destination folder. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterInstall</strong></code> \u2013 You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions.</p><p><code><strong>ApplicationStart</strong></code> \u2013 You typically use this deployment lifecycle event to restart services that were stopped during <code>ApplicationStop</code>.</p><p><code><strong>ValidateService</strong></code> \u2013 This is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.</p><p><code><strong>BeforeBlockTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances before they are deregistered from a load balancer.</p><p><code><strong>BlockTraffic</strong></code> \u2013 During this deployment lifecycle event, internet traffic is blocked from accessing instances that are currently serving traffic. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterBlockTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><code><strong>BeforeAllowTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances before they are registered with a load balancer.</p><p><code><strong>AllowTraffic</strong></code> \u2013 During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterAllowTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances after they are registered with a load balancer.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2019-12-16_03-04-56-691d69e91f667f1c9bb8b0554c2da933.png\"></p><p>Hence, the correct answer is: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BeforeAllowTraffic</strong></code><strong> Traffic hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files.</strong></p><p>The option that says: <strong>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the </strong><code><strong>CodeDeployDefault.OneAtAtime</strong></code><strong> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the </strong><code><strong>AllowTraffic</strong></code><strong> hook within the </strong><code><strong>appspec.yml</strong></code><strong> configuration file to purge the temporary files</strong> is incorrect because you should use blue/green deployment instead of in-place. In addition, the <code><strong><em>AllowTraffic</em></strong></code> event just allows the incoming traffic to the instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p>The option that says: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.HalfAtAtime</strong></code><strong> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appspec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because the <code>BlockTraffic</code> event is reserved for the CodeDeploy agent and cannot be used to run custom scripts such as deleting the temporary files.</p><p>The option that says: <strong>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.AllatOnce</strong></code><strong> as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because you should use a blue/green deployment instead of in-place. It is also incorrect to use the <code>CodeDeployDefault AllatOnce</code> deployment configuration as this attempts to deploy the application revision to as many instances as possible at once.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 82921416,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>The compliance department at a Wall Street trading firm has hired you as an AWS Certified DevOps Engineer Professional to help with several strategic DevOps initiatives. The department has asked you to regularly generate the list of all the software packages installed on the EC2 instances. The solution needs to be able to extend to future instances in the AWS account and send notifications if the instances are not set up correctly to track their software.</p>\n\n<p>Which of the following options are the best-fit solutions that require the least effort to meet the given requirements? (Select two)</p>\n",
          "answers": [
            "<p>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</p>",
            "<p>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</p>",
            "<p>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</p>",
            "<p>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</p>",
            "<p>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</strong></p>\n\n<p>SSM Agent is an Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources. The agent processes requests from the Systems Manager service in the AWS Cloud, and then run them as specified in the request. SSM Agent then sends status and execution information back to the Systems Manager service by using the Amazon Message Delivery Service (service prefix: ec2messages).</p>\n\n<p>SSM Inventory provides visibility into your Amazon EC2 and on-premises computing environment. You can use Inventory to collect metadata from your managed instances. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated.</p>\n\n<p>Sample Inventory Cards:\n<img src=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/images/inventory-cards.png\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><strong>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</strong></p>\n\n<p>Since SSM does not have any native capability to find out which instances are not currently tracked by it, so here we would need to create a custom Lambda function for this and send notifications if any new untracked instances are detected. We can trigger the Lambda function using CloudWatch Events.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</strong> - SSM does not have any native capability to find out which instances are not currently tracked by the SSM service.</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</strong> - You can use SSM Automation to build Automation workflows to configure and manage instances and AWS resources. You can also create custom workflows or use pre-defined workflows maintained by AWS. For the given requirement, SSM Automation could be used to get the list of packages but it would require a lot of manual work, so it is not the best fit for the given use-case.</p>\n\n<p><strong>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</strong> - Inspector is meant to find security vulnerabilities on EC2 instances, not to get a metadata list of your installed packages.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p>\n"
        }
      },
      {
        "id": 143860745,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company is using AWS Storage Gateway for a branch office location. The gateway is configured in file gateway mode in front of an Amazon S3 bucket that contains files that must be processed by workers in the branch office. Each night a batch process uploads many files to the S3 bucket. Users have reported that the new files are not visible in the morning though they do exist in the S3 bucket.</p><p>How can a DevOps engineer ensure that the files become visible?</p>",
          "answers": [
            "<p>Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration.</p>",
            "<p>Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded.</p>",
            "<p>Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway.</p>",
            "<p>Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command.</p>"
          ],
          "explanation": "<p>In file gateway mode the users and applications in the branch office can access the storage using either NFS or SMB protocols. The new objects in the bucket may sometimes not be visible. The RefreshCache operation refreshes the cached inventory of objects for the specified file share. This operation finds objects in the Amazon S3 bucket that were added, removed, or replaced since the gateway last listed the bucket's contents and cached the results.</p><p><strong>CORRECT: </strong>\"Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded\" is incorrect.</p><p>Volume Gateway mode uses block storage targets (iSCSI) rather than file storage targets (NFS/SMB) and would therefore be a major change to the architecture. Event notifications cannot be used to refresh the cache in a storage gateway.</p><p><strong>INCORRECT:</strong> \"Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway\" is incorrect. S3 replication cannot be used to replicate to storage gateway.</p><p><strong>INCORRECT:</strong> \"Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration\" is incorrect. Transfer acceleration is used to improve upload speeds and does not assist at all in this situation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html\">https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>"
        }
      },
      {
        "id": 82921384,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.</p>\n\n<p>What can be done to improve the performance of the setup?</p>\n",
          "answers": [
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</p>",
            "<p>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong></p>\n\n<p>A golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can also add the web application as part of the golden AMI. You can think of it as an input base AMI for creating a standardized application-specific golden AMI.</p>\n\n<p>Once you create a golden AMI for a product (a product can be a standardized OS-AMI that you want to distribute to accounts in your organization or an application-specific AMI you want to let your business unit(s) deploy in their environment), you can validate whether the AMI meets your expectations, and choose to approve or reject the AMI.</p>\n\n<p>About the golden AMI pipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q66-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</strong> - The local cache warmup can unfortunately not be improved, as caching is dynamic and data may change over time. So creating an AMI with a copy of the local data cache just serves as a distractor.</p>\n\n<p><strong>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong> - You cannot migrate from ElastiCache to DynamoDB for the given use-case, as it's primarily a NoSQL database and not a caching solution (You could use DAX as a caching solution with DynamoDB). Besides, the existing database is RDS MySQL which is a relational database, so DynamoDB does not really fit into this mix.</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</strong> - You cannot use Lambda to configure the instance local cache at boot time as caching is dynamic and data may change over time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n"
        }
      },
      {
        "id": 75949124,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps team manages an application that consists of four separate AWS Lambda functions. A DevOps Engineer on the team has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule that executes the pipeline after a change is made to the application source code. During testing, the engineer noticed that the pipeline takes a long time to complete.</p><p><br></p><p>What should the DevOps Engineer do to improve the speed of the pipeline?</p>",
          "answers": [
            "<p>Modify the CodeBuild projects within the pipeline to use a compute type with more available vCPUs.</p>",
            "<p>Configure parallel executions of the Lambda functions by specifying the same runOrder in the CodePipeline configuration for the stage.</p>",
            "<p>Modify CodeBuild execute the builds in batches using a build graph deployment and specify the dependency chain.</p>",
            "<p>Configure the CodeBuild projects to run within a VPC and use dedicated instances to increase the build throughput.</p>"
          ],
          "explanation": "<p>The best way to speed up the pipeline will be to run the builds in parallel. This can be achieved through the pipeline configuration by specifying the runOrder to be the same for the build of each function within the action structure.</p><p>To specify parallel actions, you use the same integer for each action you want to run in parallel.</p><p><strong>CORRECT: </strong>\"Configure parallel executions of the Lambda functions by specifying the same runOrder in the CodePipeline configuration for the stage\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the CodeBuild projects to run within a VPC and use dedicated instances to increase the build throughput\" is incorrect.</p><p>Connecting to a VPC does not help and using dedicated instances is not the best way to improve the speed of the pipeline. Without specifying other changes the builds will still run sequentially.</p><p><strong>INCORRECT:</strong> \"Modify the CodeBuild projects within the pipeline to use a compute type with more available vCPUs\" is incorrect.</p><p>This may offer some improvement in speed but not as much as running the builds in parallel.</p><p><strong>INCORRECT:</strong> \"Modify CodeBuild execute the builds in batches using a build graph deployment and specify the dependency chain\" is incorrect.</p><p>CodeBuild can be configured to run builds in batches but a build list or build matrix should be used for running the builds in parallel. The build graph deployment runs the builds sequentially with dependencies mapped out.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921450,
        "correct_response": [
          "a",
          "c",
          "e"
        ],
        "prompt": {
          "question": "<p>As the Lead DevOps Engineer at a retail company, you have a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. You must collect the logs before an instance is terminated to perform log analytics later on. It's also necessary to collect all the access logs. The analysis of these logs should be performed at a minimal cost, and only need to be run from time to time.</p>\n\n<p>Which of the following options would you suggest to implement the MOST cost-optimal solution for this requirement? (Select three)</p>\n",
          "answers": [
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</p>",
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</p>",
            "<p>Enable Access Logs at the Application Load Balancer level</p>",
            "<p>Enable Access Logs at the Target Group level</p>",
            "<p>Analyze the logs using AWS Athena</p>",
            "<p>Analyze the logs using an EMR cluster</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</strong></p>\n\n<p>Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. When a scale-in event occurs, the terminating instance is first deregistered from the load balancer and while the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q71-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p>For the given use-case, you can configure a lifecycle hook to invoke the CloudWatch Event rule to trigger a Lambda function that launches an SSM Run Command to extract the application logs and store them in S3.</p>\n\n<p><strong>Enable Access Logs at the Application Load Balancer level</strong></p>\n\n<p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p>\n\n<p><strong>Analyze the logs using AWS Athena</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don\u2019t even need to load your data into Athena, it works directly with data stored in S3. You can analyze the access logs stored in S3 via Athena.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</strong> - CloudWatch Logs agent can only be used for continuous log streaming, and NOT for a one-time log extract to S3.</p>\n\n<p><strong>Enable Access Logs at the Target Group level</strong> - Please note that access logs are enabled at the ALB level and NOT at the target group level.</p>\n\n<p><strong>Analyze the logs using an EMR cluster</strong> - Analyzing logs at a low cost and in a serverless fashion should be done using AWS Athena. EMR clusters are usually long-running and cost a lot of money, and don't have serverless scaling capabilities.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n"
        }
      },
      {
        "id": 75949148,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer needs to stream data from Amazon CloudWatch Logs to a VPC-based Amazon OpenSearch Service cluster. The DevOps engineer creates an OpenSearch subscription filter for the OpenSearch cluster. However, the DevOps engineer notices that the logs are not visible in Amazon OpenSearch.</p><p>What should the DevOps engineer do to solve this problem?</p>",
          "answers": [
            "<p>Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role.</p>",
            "<p>Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch.</p>",
            "<p>Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role.</p>",
            "<p>Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch.</p>"
          ],
          "explanation": "<p>The configuration requires that the Lambda IAM execution role that is specified in the CloudWatch Logs configuration must have the trust relationship configured to allow lambda.amazonaws.com to assume the role. The AWSLambdaVPCAccessExecutionRole policy must also be added to the function to allow access to the VPC-based OpenSearch cluster.</p><p><strong>CORRECT: </strong>\"Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role\" is incorrect.</p><p>The trust relationship is defined in the role attached to the CloudWatch Logs configuration.</p><p><strong>INCORRECT:</strong> \"Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch\" is incorrect.</p><p>This policy should be attached to the Lambda IAM execution role, not the OpenSearch cluster.</p><p><strong>INCORRECT:</strong> \"Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch\" is incorrect.</p><p>The subscription filter is used instead of using an export which is a better and more automated solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/\">https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-opensearch/\">https://digitalcloud.training/amazon-opensearch/</a></p>"
        }
      },
      {
        "id": 82921374,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.</p>\n\n<p>How can you improve the user experience with the least effort?</p>\n",
          "answers": [
            "<p>Enhance the health check so that the return status code corresponds to the connectivity to the database</p>",
            "<p>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</p>",
            "<p>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</p>",
            "<p>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enhance the health check so that the return status code corresponds to the connectivity to the database</strong></p>\n\n<p>Configuring health checks for the Application Load Balancer (ALB) is an important step to ensure that your AWS Cloud application runs smoothly. The ALB Health Check is configured with a protocol and port number to call on the target instances. A healthy EC2 instance is one that issues a response to a health check call with an HTTP 200 response code. Instances that return a status code that is other than the 2XX range or which time out are designated as being unhealthy and will not receive traffic from the ELB.</p>\n\n<p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q69-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>You could just add a simple health check endpoint to the ALB which accepts a request and immediately responds with an HTTP status of 200. This approach provides for a fast health check, but would not meet the requirement for the given use-case. You need to improve the quality of the health check and make sure it returns a proper status code. As the application depends on the database, you need to ensure that you include health checks for these components when determining the health of your service.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</strong> - Migrating to Beanstalk would require significant effort and even then it won't help gather detailed database-specific health checks.</p>\n\n<p><strong>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</strong> - Health Checks for the ALB are pretty basic and only work with the HTTP return status code, not the payload itself.</p>\n\n<p><strong>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</strong> - Route53 health checks can only be used to prevent DNS records from being returned from a DNS query, so it won't help for routing to specific instances behind an ALB (that's why we have health checks at the ALB level).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf\">https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf</a></p>\n"
        }
      },
      {
        "id": 138248103,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A fast-growing company has multiple AWS accounts which are consolidated using AWS Organizations, and the company expects to add new accounts soon. The DevOps engineer was instructed to design a centralized logging solution to deliver all Amazon VPC Flow Logs and Amazon CloudWatch Logs across all sub-accounts to its dedicated Audit account for compliance purposes. The logs should also be properly indexed in order to perform search, retrieval, and analysis.</p><p>Which of the following is the MOST suitable solution that the engineer should implement to meet the above requirements?</p>",
          "answers": [
            "<p>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account.</p>",
            "<p>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</p>",
            "<p>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account.</p>",
            "<p>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account.</p>"
          ],
          "explanation": "<p>You can load streaming data into your Amazon OpenSearch Service domain from many different sources in AWS. Some sources, like Amazon Data Firehose and Amazon CloudWatch Logs, have built-in support for Amazon OpenSearch Service (successor to Amazon ElasticSearch). Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers. The Lambda functions respond to new data by processing it and streaming it to your domain.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-Amazon-OpenSearch-Service-02-04-2025.png\"></p><p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, Amazon Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. To begin subscribing to log events, create the receiving source, such as a Kinesis stream, where the events will be delivered. A subscription filter defines the filter pattern to use for filtering which log events get delivered to your AWS resource, as well as information about where to send matching log events to.</p><p>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Amazon Kinesis stream to perform custom processing and analysis. Custom processing is especially useful when you collaborate and analyze data across many accounts. For example, a company's information security group might want to analyze data for real-time intrusion detection or anomalous behaviors so it could conduct an audit of accounts in all divisions in the company by collecting their federated production logs for central processing.</p><p>A real-time stream of event data across those accounts can be assembled and delivered to the information security groups, who can use Kinesis to attach the data to their existing security analytic systems. Kinesis streams are currently the only resource supported as a destination for cross-account subscriptions.</p><p>Hence, the correct solution is: <strong>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</strong></p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account</strong> is incorrect. While Lambda functions can be used to process logs, relying solely on a Lambda function to handle a large volume of logs might only lead to performance bottlenecks and scalability issues. Lambda has concurrency limits, and processing logs from multiple sub-accounts may exceed these limits, especially as the company grows and more accounts are added.</p><p>The option that says: <strong>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account </strong>is incorrect because the CloudWatch subscription filter doesn't directly support SQS. You should use a Kinesis Data Stream, Kinesis Firehose, or Lambda function.</p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account</strong> is incorrect. Although this approach typically works, using a self-hosted OpenSearch cluster on an EC2 instance introduces unnecessary complexity and overhead regarding management, scaling, and availability. AWS-managed OpenSearch Service would be a more efficient and cost-effective option. Additionally, relying on EC2 for hosting OpenSearch adds management and operational burdens, whereas using AWS-managed services like Kinesis Data Streams or Firehose with OpenSearch is more reliable and easier to scale.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html</a></p><p><a href=\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 82921402,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An IT company is creating an online booking system for hotels. The booking workflow that the company has implemented can take over 3 hours to complete as a manual verification step is required by a 3rd party provider to ensure big transactions are not fraudulent.</p>\n\n<p>As a DevOps Engineer, you need to expose this as a secure API for the end customers. The website must be able to sustain 5000 requests at the same time. How should you implement this in the simplest possible way?</p>\n",
          "answers": [
            "<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</strong></p>\n\n<p>API Gateway APIs can directly invoke an AWS service and pass in a payload. It's a common way to provide a publicly available and secure API for your chosen AWS services.</p>\n\n<p>Amazon API Gateway integrates with AWS Step Functions, allowing you to call Step Functions with APIs that you create to simplify and customize interfaces to your applications. Step Functions makes it easy to coordinate the components of distributed applications and microservices as a series of steps in a visual workflow. You create state machines in the Step Functions Console or through the Step Functions API to specify and execute the steps of your application at scale. API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to implement the payment workflow using Step Functions. A key reason you need this integration is that AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</strong> - AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you. So there is no need to use Lambda as an intermediary for this workflow.</p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</strong></p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</strong></p>\n\n<p>Lambda functions cannot process the booking workflow as it may take 3 hours, which is more than the 15 minutes max timeout limit that Lambda supports. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html\">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/faqs/\">https://aws.amazon.com/step-functions/faqs/</a></p>\n"
        }
      },
      {
        "id": 99528237,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A development team is running a project that will involve deploying applications across several Amazon VPCs. The applications will require fully meshed network connectivity to enable transitive routing between VPCs. The development lead is concerned about security and has requested centralized control over network access controls.</p><p>Which deployment will satisfy the requirements with the most operational efficiency?</p>",
          "answers": [
            "<p>Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs.</p>",
            "<p>Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs.</p>",
            "<p>Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs.</p>",
            "<p>Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPCs.</p>"
          ],
          "explanation": "<p>AWS Transit Gateway can be configured to enable a fully meshed network topology which allows transitive routing between all the VPCs. With AWS Network Firewall, you can define firewall rules that provide fine-grained control over network traffic.</p><p>AWS Firewall Manager allows you to build policies based on Network Firewall rules and then centrally apply those policies across your virtual private clouds (VPCs) and accounts.</p><p><strong>CORRECT: </strong>\"Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs\" is incorrect.</p><p>VPC peering requires creating complex peering relationships and does not support transitive routing (though this can be achieved through a mesh of peering connections). VPC peering is less operationally efficient compared to using a Transit Gateway. AWS WAF is not the best solution for enforcing centralized network access controls, it is used for preventing web based attacks.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs\" is incorrect.</p><p>You cannot connect Amazon VPCs using AWS S2S VPNs. You can only use an AWS VPN to connect on-premises networks.</p><p><strong>INCORRECT:</strong> \"Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPC\" is incorrect.</p><p>PrivateLink is not used for creating this kind of network deployment. It is used for private access to AWS services using private IP addresses.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/network-firewall/\">https://aws.amazon.com/network-firewall/</a></p><p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>"
        }
      },
      {
        "id": 138248235,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company is developing a serverless application that uses AWS Lambda, AWS SAM, and Amazon API Gateway. There is a requirement to fully automate the backend Lambda deployment in such a way that the deployment will automatically run whenever a new commit is pushed to a GitHub repository. There should also be a separate environment pipeline for TEST and PROD environments. In addition, the TEST environment should be the only one that allows automatic deployment.</p><p>How can a DevOps Engineer satisfy these requirements?</p>",
          "answers": [
            "<p>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>"
          ],
          "explanation": "<p>In <strong>AWS CodePipeline</strong>, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p>If the action is approved, the pipeline execution resumes. If the action is rejected\u2014or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping\u2014the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p><img src=\"https://media.tutorialsdojo.com/public/PipelineFlow.png\"></p><p>Hence, the correct answer is: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</strong></p><p>The option that says: <strong>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline</strong> is incorrect. You should add a manual approval step on the PROD pipeline as mentioned in the requirements of the scenario.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline</strong> is incorrect. You don't need to create separate GitHub repositories for the two environments. You just need to create two different branches from a single repository.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline </strong>is incorrect. You should add the manual approval step on the PROD pipeline only, excluding the TEST pipeline. Moreover, you don't need to create separate GitHub repositories for the two environments. You only need to create two different branches from a single repository.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html \">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 134588393,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. </p><p>As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?</p>",
          "answers": [
            "<p>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/ssm-patch-baselines.jpg\"></p><p><strong>Patch Manager</strong> uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p><strong>AWS Config</strong> provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements. Hence, the correct option is: <strong>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</strong></p><p>The option that says: <strong>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, it is stated in the scenario that the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC </strong>is incorrect because Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise, however, it does not check if your EC2 instances are using an approved AMI or not.</p><p>The option that says: <strong>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect because the AWS Shield Advanced service is more suitable in preventing DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 82921348,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>Your company has adopted CodeCommit and forces developers to create new branches and create pull requests before merging the code to master. The development team lead reviewing the pull request needs high confidence in the quality of the code and therefore would like the CICD system to automatically build a Pull Request to provide a testing badge with a pass/fail status.</p>\n\n<p>How can you implement the validation of Pull Requests by CodeBuild efficiently?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>",
            "<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>",
            "<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>",
            "<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong></p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p>\n\n<p>CloudWatch Events Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>For the given use-case, we need to create two CloudWatch Event Rules. The first rule would trigger on CodeCommit Pull Request and have the target as CodeBuild.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i2.jpg\"></p>\n\n<p>The second rule would trigger on CodeBuild build success or failure event and have the target as a Lambda function that will update the pull request with the Build outcome</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong> - Using a scheduled rate of 5 minutes would work but would be inefficient. It is much better to configure a CloudWatch Event Rule that would trigger on CodeCommit Pull Request and carry out the rest of the solution workflow as outlined earlier.</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p>For both these options, invoking a Lambda function to start CodeBuild would work, but having the function wait on CodeBuild has two issues:\n1) The Lambda function may timeout and has a maximum timeout of 15 minutes. What if the test suite takes longer to run?\n2) You will be billed for the Lambda function wait time.</p>\n\n<p>Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n"
        }
      },
      {
        "id": 75949068,
        "correct_response": [
          "b",
          "c"
        ],
        "prompt": {
          "question": "<p>To increase security, a company plans to use AWS Systems Manager Session Manager to managed Amazon EC2 instances rather than SSH. The connectivity to Session Manager should also use a private network connection.</p><p>Which configuration actions should be taken to implement this? (Select TWO.)</p>",
          "answers": [
            "<p>Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range.</p>",
            "<p>Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager.</p>",
            "<p>Create a VPC endpoint for Systems Manager in the Region where the instances are running.</p>",
            "<p>Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables.</p>",
            "<p>Establish private Session Manager connectivity using the instance IDs of EC2 instances.</p>"
          ],
          "explanation": "<p>Systems Manager Session Manager enables secure remote access to EC2 instances without the need to open ports for SSH or create bastion hosts. You can connect to instances through Session Manager privately by establishing a VPC endpoint in your VPC. This ensures that all connectivity takes place using private addresses.</p><p>The EC2 instances must have the Systems Manager agent running and they must have permissions to be able to communicate with the Systems Manager service. An instance profile can be easily attached to provide these permissions to instances.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-59-28-dac1a7fafdbe3507d5ca4b1fe07ed213.jpg\"><p><strong>CORRECT: </strong>\"Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Systems Manager in the Region where the instances are running\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range\" is incorrect.</p><p>Port 22 is used by the SSH protocol and is not required by Systems Manager Session Manager.</p><p><strong>INCORRECT:</strong> \"Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables\" is incorrect.</p><p>You cannot connect to EC2 instances via a NAT gateway. These gateways are used for outbound (internet) connectivity from the EC2 instances in private subnets.</p><p><strong>INCORRECT:</strong> \"Establish private Session Manager connectivity using the instance IDs of EC2 instances\" is incorrect.</p><p>You cannot connect privately to instances simply by connecting via the instance ID. You must establish a VPC endpoint and attach the necessary permissions to instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 134588381,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company has a CI/CD pipeline built in AWS CodePipeline for deployment updates. Part of the deployment process is to perform database schema updates and is performed via AWS CodeBuild. A recent security audit has discovered that the AWS CodeBuild is downloading the database scripts via Amazon S3 in an unauthenticated manner. The security team requires a solution that will enhance the security of the company's CI/CD pipeline.</p><p>What action should the DevOps Engineer take to address the issue in the MOST secure way?</p>",
          "answers": [
            "<p>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>",
            "<p>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>"
          ],
          "explanation": "<p><strong>Service role</strong>: A service role is an AWS Identity and Access Management (IAM) that grants permissions to an AWS service so that the service can access AWS resources. You need an AWS CodeBuild service role so that CodeBuild can interact with dependent AWS services on your behalf. You can create a CodeBuild service role by using the CodeBuild or AWS CodePipeline consoles.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-codebuild-service-role.png\"></p><p>In this scenario, the S3 bucket will be safeguarded from unauthorized access by utilizing a bucket policy. Moreover, CodeBuild leverages the service role for executing S3 actions on your behalf.</p><p>Hence, the correct answer is: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</strong></p><p>The option that says: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI</strong> is incorrect. While the use of IAM access key and secret access key can provide S3 access to CodeBuild, it is not the most secure approach to address the issue.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because an IAM policy alone cannot secure an S3 bucket from unauthorized access. A bucket policy must be used instead. Furthermore, this option uses IAM access key and secret access key, which is not the most secure way.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. It is not used for removing unauthenticated access to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role</a></p><p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>"
        }
      },
      {
        "id": 82921330,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As the Lead DevOps Engineer at an e-commerce company, you would like to upgrade the major version of your MySQL database, which is managed by CloudFormation with <code>AWS::RDS::DBInstance</code> and setup using Multi-AZ.</p>\n\n<p>You have a requirement to minimize the downtime as much as possible, what steps should you take to achieve this?</p>\n",
          "answers": [
            "<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>",
            "<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>",
            "<p>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</p>",
            "<p>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong></p>\n\n<p>You can minimize downtime on an upgrade by using a rolling upgrade using read replicas. Amazon RDS doesn\u2019t fully automate one-click rolling upgrades. However, you can still perform a rolling upgrade by creating a read replica, upgrading the replica by using the property <code>EngineVersion</code>, promoting the replica, and then routing traffic to the promoted replica. If you want to create a Read Replica DB instance, specify the ID of the source DB instance. The SourceDBInstanceIdentifier property determines whether a DB instance is a Read Replica.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p>You should also note that a Multi-AZ configuration does not prevent downtime during an upgrade. Multi-AZ is only recommended for a high availability use-case. However, in the case of a MySQL or MariaDB engine upgrade, Multi-AZ doesn\u2019t eliminate downtime. The slow shutdown and the physical changes made on the active server by the mysql_upgrade program require this downtime.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong> - You should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p><strong>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - If you update the <code>EngineVersion</code> property of an AWS::RDS::DBInstance resource type, AWS CloudFormation creates a new resource and replaces the current DB instance resource with the new one, so this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n\n<p><strong>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - Also, you should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n"
        }
      },
      {
        "id": 138248125,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company wants to implement a continuous delivery workflow that will facilitate the process of source code promotion in the development, staging, and production environments in AWS. In the event of system degradation or failure, the company should also have the ability to roll back the recent deployment of the application.</p><p>Which of the following CI/CD designs is the MOST suitable one to implement and will incur the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production.</p>",
            "Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy."
          ],
          "explanation": "<p>A repository is the fundamental version control object in GitHub or GitLab. It\u2019s where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. In GitHub and GitLab, you can set up notifications to configure notifications so that repository users receive emails about events (for example, another user commenting on code). You can change default settings to customize the default settings for your repository. You can browse contents to easily navigate and browse the contents of your repository. You can create triggers to set up triggers so that code pushes or other events trigger actions, such as emails or code functions. You can also configure a repository on your local computer (a local repo) to push your changes to more than one repository.</p><p><img src=\"https://media.tutorialsdojo.com/public/setup-cicd-pipeline-100324.png\"></p><p>In designing your CI/CD process in AWS, you can use a single repository in GitHub (or GitLab) and create different branches for development, master, and release. You can use CodeBuild to build your application and run tests to verify that all of the core features of your application are working. For deployment, you can either select an in-place or blue/green deployment using CodeDeploy.</p><p>Hence, the correct answer is: <strong>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</strong></p><p>The option that says: <strong>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy </strong>is incorrect because creating a separate repository for each developer is absurd since they can simply clone the code instead. A single repository will suffice in this scenario which can have several branches for development and production deployment purposes.</p><p>The option that says: <strong>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production</strong> is incorrect because you don't need to create two repositories for one application. Instead, you can just create at least two different branches to separate your development and production code.</p><p>The option that says: <strong>Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy</strong> is incorrect because Amazon ECR is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. This is not a suitable service to be used to store your application code.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/devops/continuous-integration/\">https://aws.amazon.com/devops/continuous-integration/</a></p><p><a href=\"https://aws.amazon.com/devops/continuous-delivery/\">https://aws.amazon.com/devops/continuous-delivery/</a></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html</a></p><p><br></p><p><strong>Check out this AWS CodePipelin Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 134588407,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A multinational company is using multiple AWS accounts for its global cloud architecture. The AWS resources in their production account are shared among various business units of the company. A single business unit may have one or more AWS accounts that have resources in the production account. Recently, there were a lot of incidents in which the developers from a specific business unit accidentally terminated the Amazon EC2 instances owned by another business unit. A DevOps Engineer was tasked to come up with a solution to only allow a specific business unit who owns the EC2 instances and other AWS resources to terminate their own resources. </p><p>How should the Engineer implement a multi-account strategy to satisfy this requirement?</p>",
          "answers": [
            "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access.</p>",
            "<p>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources.</p>",
            "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</p>",
            "<p>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts.</p>"
          ],
          "explanation": "<p><strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an <em>organization</em> that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.</p><p>You can use organizational units (OUs) to group accounts together to administer as a single unit. This greatly simplifies the management of your accounts. For example, you can attach a policy-based control to an OU, and all accounts within the OU automatically inherit the policy. You can create multiple OUs within a single organization, and you can create OUs within other OUs. Each OU can contain multiple accounts, and you can move accounts from one OU to another. However, OU names must be unique within a parent OU or root.</p><p><img src=\"https://media.tutorialsdojo.com/aws-organizations.jpg\"></p><p>Resource-level permissions refer to the ability to specify which resources users are allowed to perform actions on. Amazon EC2 has partial support for resource-level permissions. This means that for certain Amazon EC2 actions, you can control when users are allowed to use those actions based on conditions that have to be fulfilled or specific resources that users are allowed to use. For example, you can grant users permissions to launch instances, but only of a specific type and only using a specific AMI.</p><p>Hence, the correct answer is: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</strong></p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access</strong> is incorrect. The AWSServiceRoleForOrganizations service-linked role is primarily used to only allow AWS Organizations to create service-linked roles for other AWS services. This service-linked role is present in all organizations and not just in a specific OU.</p><p>The option that says: <strong>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources<em> </em></strong>is incorrect. Although the use of the AWS Control Tower is right, the aggregator feature is simply an AWS Config resource type that collects AWS Config configuration and compliance data from the following various AWS accounts. In addition, you have to launch the Customizations for AWS Control Tower (CfCT) on the same AWS region where your AWS Control Tower landing zone is deployed, and not on a different account, to put it in effect.</p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts</strong> is incorrect. AWS Service Catalog simply allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A more suitable service to use here is AWS Organizations.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>"
        }
      }
    ],
    "answers": {
      "75949068": [
        "b",
        "e"
      ],
      "75949124": [
        "b"
      ],
      "75949148": [
        "b"
      ],
      "82921330": [
        "a"
      ],
      "82921348": [
        "a"
      ],
      "82921374": [
        "a"
      ],
      "82921384": [
        "a"
      ],
      "82921402": [
        "b"
      ],
      "82921412": [
        "a",
        "b"
      ],
      "82921416": [
        "a",
        "d"
      ],
      "82921450": [
        "b",
        "c",
        "e"
      ],
      "99528237": [
        "a"
      ],
      "134588381": [
        "d"
      ],
      "134588393": [
        "d"
      ],
      "134588407": [
        "b"
      ],
      "138248103": [
        "a"
      ],
      "138248125": [
        "a"
      ],
      "138248235": [
        "c"
      ],
      "138248241": [
        "c"
      ],
      "143860745": [
        "d"
      ]
    }
  },
  {
    "id": "1769619233089",
    "date": "2026-01-28T16:53:53.089Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 15,
    "incorrect": 5,
    "unanswered": 0,
    "total": 20,
    "percent": 75,
    "duration": 2519968,
    "questions": [
      {
        "id": 75949046,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company manages a continuous integration and continuous delivery (CI/CD) pipeline that includes a Jenkins implementation that runs on Amazon EC2. The security team has requested that all build artifacts are encrypted as they contain company sensitive data.</p><p>Which changes should a DevOps engineer make to meet these requirements whilst reducing operational overhead for ongoing management?</p>",
          "answers": [
            "<p>Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes.</p>",
            "<p>Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption.</p>",
            "<p>Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group.</p>",
            "<p>Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM).</p>"
          ],
          "explanation": "<p>The existing Jenkins implementation runs on an Amazon EC2 instance, and this requires more operational management compared to using AWS CodeBuild which is a managed service. The DevOps engineer should replace Jenkins with AWS CodeBuild which is a fully managed build service.</p><p>CodeBuild compiles source code, runs unit tests, and produces artifacts that are ready to deploy. Encryption for build artifacts such as a cache, logs, exported raw test report data files, and build results, is enabled by default, and uses AWS managed keys (AWS KMS).</p><p><strong>CORRECT: </strong>\"Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes\" is incorrect.</p><p>This solution may be secure, but it does require more operational management compared to using AWS CodeBuild and is therefore not the best option.</p><p><strong>INCORRECT:</strong> \"Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group\" is incorrect.</p><p>Moving Jenkins to an Auto Scaling group does not reduce the operational management. This is also an option that requires more overhead to manage compared to using a managed service such as AWS CodeBuild.</p><p><strong>INCORRECT:</strong> \"Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM)\" is incorrect.</p><p>Artifacts cannot be encrypted using ACM as that is a service that issues SSL/TLS certificates which are used for encryption in-transit rather than encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 67357100,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.</p>\n\n<p>As a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?</p>\n",
          "answers": [
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>AWS::EC2::Volume</code> that returns a compliance failure if the custom tag is not applied on the EBS volume. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong></p>\n\n<p>Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type.</p>\n\n<p>AWS Config managed rule that checks required-tags:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p>This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes.</p>\n\n<p>AWS Config rules remediation with automation runbooks architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it's possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n"
        }
      },
      {
        "id": 75949146,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
          "answers": [
            "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
            "<p>The new AZ has not been enabled for the ALB.</p>",
            "<p>The AMI has not been updated to include the new AZ.</p>",
            "<p>There are no subnets associated with the new AZ.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 99528239,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company runs a Java application in an on-premises data center. The application is not currently containerized but the company plans to migrate it to the AWS Cloud and modernize the application into a containerized deployment. A CI/CD pipeline should also be created to automate application updates.</p><p>Which solution can a DevOps engineer use to meet all these requirements?</p>",
          "answers": [
            "<p>Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates.</p>",
            "<p>Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy.</p>",
            "<p>Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository.</p>",
            "<p>Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy.</p>"
          ],
          "explanation": "<p>App2Container (A2C) is a command-line tool for modernizing .NET and Java applications into containerized applications. A2C analyzes and builds an inventory of all applications running in virtual machines on-premises or in the cloud.</p><p>With A2C you can simply select the application you want to containerize, and A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the ECS task and Kubernetes pod definitions.</p><p>App2Container creates CI/CD pipelines for Amazon DevOps services such as CodeBuild and CodeDeploy to build and deploy containers. If you have existing CI/CD tooling (for example, Azure DevOps and Jenkins), then you can integrate A2C provided artifacts \u2013 dockerfile, ECS task definition, EKS deployment YAML - into your existing CI/CD workflows.</p><p><strong>CORRECT: </strong>\"Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates\" is incorrect.</p><p>AWS Copilot is a command line interface (CLI) that you can use to quickly launch and manage containerized applications on AWS. It simplifies running applications on Amazon Elastic Container Service (ECS), AWS Fargate, and AWS App Runner. It is not able to inventory and analyze existing applications or migrate them to containers.</p><p><strong>INCORRECT:</strong> \"Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository\" is incorrect.</p><p>AWS Proton is used to create application stack templates and build infrastructure for applications using the CLI, API, or UI. It is not used for refactoring applications or migrating them to containerized deployments.</p><p><strong>INCORRECT:</strong> \"Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy\" is incorrect.</p><p>Amazon AppFlow is used to automate bi-directional data flows between SaaS applications and AWS services. It is not used to migrate applications between on-premises deployments and Amazon ECS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/app2container/features/\">https://aws.amazon.com/app2container/features/</a></p>"
        }
      },
      {
        "id": 75949108,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>",
          "answers": [
            "<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"
          ],
          "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949138,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
            "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
            "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
            "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
            "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
            "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"
          ],
          "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949100,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company is deploying a new serverless application that uses AWS Lambda functions. A DevOps engineer must create a continuous deployment pipeline for the application. The deployment preferences must be configured to minimize the impact of failed deployments.</p><p>Which deployment configuration will meet these requirements?</p>",
          "answers": [
            "<p>Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type.</p>",
            "<p>Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch.</p>",
            "<p>Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version.</p>",
            "<p>Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version.</p>"
          ],
          "explanation": "<p>The benefits of using AWS SAM to create the serverless application include that it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM can perform the following actions:</p><ul><li><p>Deploys new versions of the Lambda function, and automatically creates aliases that point to the new version.</p></li><li><p>Gradually shifts customer traffic to the new version until you\u2019re satisfied that it's working as expected, or you roll back the update.</p></li><li><p>Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly, and your application operates as expected.</p></li><li><p>Rolls back the deployment if CloudWatch alarms are triggered.</p></li></ul><p>The DevOps engineer can choose the Deployment Preference Type. The following options are available:</p><ul><li><p><strong>Canary:</strong> Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p></li><li><p><strong>Linear:</strong> Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment.</p></li><li><p><strong>All-at-once:</strong> All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</p></li></ul><p>The best option to minimize the impact of failed deployments is to use the canary deployment type. This will ensure that only a small amount of traffic reaches the new Lambda function in the first shift and if any issues occur the deployment can be stopped.</p><p><strong>CORRECT: </strong>\"Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch\" is incorrect.</p><p>The all-at-once deployment preference type would shift all traffic across to the new functions which would increase the impact of failed deployments. Also, CloudFormation is not a continuous deployment tool and is not a suitable substitute for CodePipeline and CodeDeploy.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version\" is incorrect.</p><p>As above, CloudFormation is not designed for continuous delivery and should not be used in this scenario. Creating a CodePipeline that leverages CodeDeploy and a source control service such as CodeCommit would be a better solution.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version\" is incorrect.</p><p>This option requires more operational overhead and testing on behalf of the DevOps team. The better solution is an automated pipeline that shifts some traffic across to the new functions so actual end users are attempting to use the application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949142,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.</p><p>How can the company meet these requirements with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns.</p>",
            "<p>Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket.</p>"
          ],
          "explanation": "<p>The simplest solution for this request is to enable server access logging on the bucket and then analyze the data that is logged using Amazon Athena. Server access logging provides detailed records for the requests that are made to a bucket. This includes the information requested by the company.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p><strong>CORRECT: </strong>\"Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket\" is incorrect $</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/\">https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>"
        }
      },
      {
        "id": 75949172,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A service provider has created business relationships with several companies. The service provider plans to deploy an application to multiple AWS accounts managed by these partner companies using AWS CloudFormation. Each partner company has granted the permissions to create IAM roles with permissions for the deployment in their respective accounts. The organization must minimize operational overhead and stack management.</p><p>Which actions should be taken to deploy the application across these accounts?</p>",
          "answers": [
            "<p>Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template.</p>",
            "<p>Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application.</p>"
          ],
          "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf</p><p>In this case the best solution is to use self-managed permissions as the partner companies have only granted the ability to create IAM roles. They would certainly not want their entire account to be controlled by the service provider through AWS Organizations just so they can deploy a single application.</p><p><strong>CORRECT: </strong>\"Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>You cannot have a role in one account that has permissions in another account. The roles must be created in each AWS account and self-managed permissions must be used with AWS CloudFormation stack sets.</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>The partner companies would not want the service provider to control their accounts with AWS Organizations just for a single application. They have granted the ability to create roles which is enough for the service provider to deploy the application using stack sets with self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template\" is incorrect.</p><p>The partner companies have granted the ability to create roles, not user accounts. Also, this would be inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 75949092,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is developing an application that calls AWS Lambda functions. The functions must connect to a database and credentials must not be stored in source code. The credentials for connection to the database must be regularly rotated to meet security policy requirements.</p><p>What should a DevOps engineer do to meet these requirements?</p>",
          "answers": [
            "<p>Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID.</p>",
            "<p>Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID.</p>",
            "<p>Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution.</p>",
            "<p>Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID.</p>"
          ],
          "explanation": "<p>AWS Secrets Manager is ideal for this scenario as it can be used to securely store the secrets. Automatic rotation can be enabled for several AWS databases and can be configured through custom Lambda functions for other databases.</p><p><strong>CORRECT: </strong>\"Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID\" is incorrect.</p><p>CloudHSM is used for storing encryption keys, not connection credentials.</p><p><strong>INCORRECT:</strong> \"Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution\" is incorrect.</p><p>This is not a secure method of storing the credentials and Secrets Manager is more secure.</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID\" is incorrect.</p><p>KMS is used for storing encryption keys, not connection credentials.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>"
        }
      },
      {
        "id": 115961513,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>",
          "answers": [
            "<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>",
            "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>",
            "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>",
            "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"
          ],
          "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 99528229,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.</p><p>Which actions should the engineer take?</p>",
          "answers": [
            "<p>Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL.</p>",
            "<p>Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield.</p>",
            "<p>Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL.</p>",
            "<p>Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty.</p>"
          ],
          "explanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website.</p><p>AWS Web Application Firewall (WAF) can be used to protect the static website from web exploits. To do this, the engineer should create an AWS WAF web ACL.</p><p><strong>CORRECT: </strong>\"Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield\" is incorrect.</p><p>Both EC2 and ElastiCache use EC2 instances so they are not serverless services. AWS Shield protects against DDoS attacks but AWS WAF should be used for web exploits.</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL\" is incorrect.</p><p>ECS with Fargate is serverless but you cannot use a Redis cluster to cache content served by a task running on Amazon ECS. ElastiCache is used for caching database content.</p><p><strong>INCORRECT:</strong> \"Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty\" is incorrect.</p><p>Lambda with a REST API as the frontend is definitely possible, however the static website on S3 would be easier to deploy and manage. AWS GuardDuty does not protect against web exploits.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>"
        }
      },
      {
        "id": 75949128,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>The DevOps team at an e-commerce company introduced multiple stages of security to the code release process. As an additional measure, they want to add additional SAST &amp; DAST tools into an automated pipeline. These tools should be invoked for every code push in an AWS CodeCommit repository. The code must be sent via an external API.</p><p>Which actions should a DevOps engineer take to achieve these requirements MOST efficiently?</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API.</p>",
            "<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected.</p>",
            "<p>Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API.</p>",
            "<p>Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API.</p>"
          ],
          "explanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that can be set up, you can match events and route them to one or more target functions or streams. Custom application-level events can be generated and published to CloudWatch Events. Scheduled events can be set up that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event Rules cannot have S3 buckets as a target. It is more efficient to directly invoke the Lambda function from the CloudWatch Event rule than use S3 as a temporary store and then trigger a function execution.</p><p><strong>INCORRECT:</strong> \"Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected\" is incorrect.</p><p>You cannot create a CodeCommit hook on an EC2 instance, so this is not possible</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So, this option is ruled out.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/\">https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949174,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
          "answers": [
            "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
            "<p>Add test stages before and after the deploy action.</p>",
            "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
            "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"
          ],
          "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528205,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team has prohibited unauthenticated requests to S3 buckets for this project.</p><p>How can this issue be resolved in the <strong>MOST secure</strong> manner?</p>",
          "answers": [
            "<p>Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.</p>",
            "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</p>"
          ],
          "explanation": "<p>Removing unauthenticated access from the bucket through a bucket policy ensures that the S3 bucket is protected from unauthorized access. Granting the CodeBuild project's service role explicit permissions to access the S3 bucket aligns with the principle of least privilege. Using the AWS CLI within the build spec ensures secure and seamless access to the bucket.<br><br></p><p><strong>CORRECT: </strong>\"Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script\" is incorrect.</p><p>Adding the bucket name to the AllowedBuckets section does not address the security issue. The CodeBuild project needs proper IAM permissions to access the S3 bucket securely. Relying on unauthenticated access remains insecure and against the security team's requirements.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script\" is incorrect.</p><p>Enabling HTTPS basic authentication for the S3 bucket is not supported by AWS. Amazon S3 uses IAM policies and bucket policies for access control, not basic authentication. Additionally, using cURL to pass tokens does not align with AWS's recommended best practices.</p><p><strong>INCORRECT:</strong> \"Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key\" is incorrect.</p><p>Using IAM access keys and secret access keys directly is less secure than granting a CodeBuild service role the necessary permissions. Hardcoding access keys in the build spec or environment can lead to security vulnerabilities, such as accidental exposure of credentials.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html</a></p>"
        }
      },
      {
        "id": 75949134,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An automotive organization is planning to migrate their website into AWS across multiple accounts. The current infrastructure uses an on-premises Microsoft IIS web server and Microsoft SQL server for the data persistence layer.</p><p>They want to be able to scale their infrastructure based on demand. Along with the current website, they also want to collect user interest data from ad clicks that occur on the website. Amazon RedShift has been chosen for the consumption and aggregation of data.</p><p>Which of the below architectures best suits their needs?</p>",
          "answers": [
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.</p>"
          ],
          "explanation": "<p>Amazon Kinesis Data Firehose is used to reliably load streaming data into data lakes, data stores, and analytics tools like Amazon Redshift. Process the incoming data from Firehose with Kinesis Data Analytics to provide real-time dashboarding of website activity.</p><p><strong>CORRECT: </strong>\"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>To load data into S3, Kinesis Firehose is a more suitable tool.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>As per the option above, Kinesis Data Firehose is a better service for this use case.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>Athena is more suited to data analysis within S3 buckets and cannot be used for loading data into S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/\">https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>"
        }
      },
      {
        "id": 75949074,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A retail company has hired a DevOps engineer to provide consultancy services. The company runs Oracle and PostgreSQL services on Amazon RDS for storing large quantities of data generated by manufacturing equipment.</p><p>The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team.</p><p>To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p><p>Which solution should the DevOps engineer recommend to achieve these requirements?</p>",
          "answers": [
            "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift.</p>"
          ],
          "explanation": "<p>The AWS Database Migration Service (DMS) helps migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS DMS the data can be continuously replicated with high availability and the multiple databases can be consolidated into a petabyte-scale data warehouse.</p><p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance. During a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-02-48-34cbd839e896555130df72dfa20efc35.jpg\"><p><strong>CORRECT: </strong>\u201cUse AWS Database Migration Service to replicate the data from the databases into Amazon Redshift\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>AWS Glue is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.</p><p><strong>INCORRECT:</strong> \"Use Amazon EMR to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon EMR (previously called Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon Kinesis Data Streams is a fully managed, serverless data streaming service that stores and ingests various streaming data in real time at any scale. It is not suitable for migrating data between databases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>"
        }
      },
      {
        "id": 99528223,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A data intelligence and analytics company has implemented a CI/CD pipeline using AWS CodePipeline which takes code from an AWS CodeCommit repository and then builds it using AWS CodeBuild. During the deploy stage, the application is deployed onto an Amazon ECS cluster. During deployment, the application is only partly updated on some ECS tasks which are running an older version of the image.</p><p>A DevOps engineer investigated and found that terminating the task or clearing the local Docker cache fixes the issue, but a more robust solution is required that provides visibility and identification to track where container images are deployed. Also, the start-up time of the containers needs to be optimized.</p><p>Which actions should the DevOps engineer take to achieve these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Move all the dependencies into a single image and pull them from a single container registry.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task.</p>",
            "<p>Move secondary dependencies to be downloaded at application startup rather than including them within a static container image.</p>"
          ],
          "explanation": "<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. The IMAGEID property is the SHA digest for the Docker image used to start the container.</p><p>Ideally, a container image is intended to be a complete snapshot of everything that the application requires to function. With a complete container image, the application could be run by downloading one container image from one place.</p><p>There is no need to download several separate pieces from different locations. Therefore, as a best practice, store all application dependencies as static files inside the container image. This will improve performance and start up time.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-00-17-d1b2e063cff66b89f7caee81435b0971.jpg\"><p><strong>CORRECT: </strong>\"Move all the dependencies into a single image and pull them from a single container registry\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time\" is incorrect.</p><p>The exact image tag needs to be pulled because the latest tag will only pull the image that was pushed last.</p><p><strong>INCORRECT:</strong> \"After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task\" is incorrect.</p><p>Using Lambda to SSH onto instances is not a workable solution. SSM Run Command may be better but it\u2019s also not ideal.</p><p><strong>INCORRECT:</strong> \"Move secondary dependencies to be downloaded at application startup rather than including them within a static container image\" is incorrect.</p><p>This would delay the image spin up time rather than improving it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html\">https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528211,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. It should not be possible to create IAM users unless the user account making the requires is specific on an exception list. A DevOps engineer must apply these restrictions.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>",
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>"
          ],
          "explanation": "<p>With AWS Organizations Service Control Policies (SCPs) you can restrict access to specific resources or define conditions for when SCPs are in effect. The condition element is supported when the Effect element has a value of Deny.</p><p>The StringNotLike condition will check the exception list for the aws:username that made the request. If the username is not present the request will be denied.</p><p><strong>CORRECT: </strong>\"Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list\" is incorrect.</p><p>The API action iam:CreateLoginProfile creates a password for the specified IAM user. This is not used to create the user account.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The API action iam:GetUser retrieves information about the specified IAM user, including the user's creation date, path, unique ID, and ARN.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The function should be used to validate that the iam:CreateUser request is being initiated by a user on the exception list. It should not delete the requesting user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>"
        }
      },
      {
        "id": 75949048,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company has several AWS accounts and an on-premises data center. Several microservices applications run across the accounts and data center. The distributed architecture results in challenges with investigating application issues as the logs are saved in a variety of locations. A DevOps engineer must configure a solution that centralizes and aggregates the logs for analytics.</p><p>What is the MOST efficient and cost-effective solution?</p>",
          "answers": [
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account.</p>"
          ],
          "explanation": "<p>The most efficient and cost-effective solution is to use the CloudWatch agent to collect log files from both AWS resources and on-premises servers and save that data to a centralized Amazon S3 bucket. S3 event notifications can be used to trigger an AWS Lambda function that analyzes the data looking for anomalies.</p><p>Amazon Athena is ideal for running ad-hoc SQL queries on data stored in S3. This can be used by the company when they have specific queries they need to run against the data.</p><p><strong>CORRECT: </strong>\"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center\" is incorrect.</p><p>The best solution is not to store the data on-premises, it should be in the AWS Cloud. You would then be able to use the ELK stack to analyze the data.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket\" is incorrect.</p><p>You cannot use S3 APIs to export log files from on-premises servers. You also cannot use KDA to analyze data in S3, it is used for analyzing data in other Kinesis services.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account\" is incorrect.</p><p>As above, you cannot use the S3 API with on-premises resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p><p><a href=\"https://aws.amazon.com/solutions/implementations/centralized-logging/\">https://aws.amazon.com/solutions/implementations/centralized-logging/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      }
    ],
    "answers": {
      "67357100": [
        "d"
      ],
      "75949046": [
        "b"
      ],
      "75949048": [
        "d"
      ],
      "75949074": [
        "c"
      ],
      "75949092": [
        "b"
      ],
      "75949100": [
        "a"
      ],
      "75949108": [
        "a"
      ],
      "75949128": [
        "c"
      ],
      "75949134": [
        "d"
      ],
      "75949138": [
        "b",
        "c",
        "d"
      ],
      "75949142": [
        "c"
      ],
      "75949146": [
        "a"
      ],
      "75949172": [
        "b"
      ],
      "75949174": [
        "b"
      ],
      "99528205": [
        "c"
      ],
      "99528211": [
        "c"
      ],
      "99528223": [
        "b",
        "d"
      ],
      "99528229": [
        "a"
      ],
      "99528239": [
        "b"
      ],
      "115961513": [
        "b"
      ]
    }
  }
]