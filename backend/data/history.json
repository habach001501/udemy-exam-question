[
  {
    "id": "1769600054664",
    "date": "2026-01-28T11:34:14.664Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 0,
    "incorrect": 0,
    "unanswered": 20,
    "total": 20,
    "percent": 0,
    "duration": 2968,
    "questions": [
      {
        "id": 67357178,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An e-commerce company has a serverless application stack that consists of CloudFront, API Gateway and Lambda functions. The company has hired you to improve the current deployment process which creates a new version of the Lambda function and then runs an AWS CLI script for deployment. In case the new version errors out, then another CLI script is invoked to deploy the previous working version of the Lambda function. The company has mandated you to decrease the time to deploy new versions of the Lambda functions and also reduce the time to detect and roll back when errors are identified.</p>\n\n<p>Which of the following solutions would you suggest for the given use case?</p>\n",
          "answers": [
            "<p>Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered</p>",
            "<p>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version</p>",
            "<p>Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint</p>",
            "<p>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered</strong></p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities.</p>\n\n<p>To address the given use case, you can use the traffic shifting feature of SAM to easily test the new version of the Lambda function without having to manually move 100% of the traffic to the new version in one shot.</p>\n\n<p>You can use CodeDeploy to create a deployment process that publishes the new Lambda version but does not send any traffic to it. Then it executes a PreTraffic test to ensure that your new function works as expected. After the test succeeds, CodeDeploy automatically shifts traffic gradually to the new version of the Lambda function. This workflow addresses one of the key requirements of reducing the time to detect errors. You can roll back to the previous version in case the new version errors out.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/\">https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version</strong> - You can use CloudFormation change sets to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set.</p>\n\n<p>This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n\n<p>Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered.</p>\n\n<p><strong>Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint</strong> - This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack and point to the new endpoint.</p>\n\n<p>Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered.</p>\n\n<p><strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered</strong> - This option has been added as a distractor since CloudFormation change sets do not have pre-traffic and post-traffic test functions. Therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/\">https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p>\n"
        }
      },
      {
        "id": 138248193,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A business has a Go-based on-premises application that needs to be migrated to AWS. The business was excited about the migration and wanted to take advantage of the new opportunities that AWS provided. Its development team desires to enable blue/green deployments. This would allow them to have two identical environments running side-by-side, with one being the current production environment (blue) and the other being a new environment (green) where they could test new features and changes via A/B testing.</p><p><br>Which of the following is the MOST appropriate solution that the DevOps Engineer should implement to meet the requirements?</p>",
          "answers": [
            "<p>Host the application using Amazon Lightsail. Upload a zipped version of the application on an Amazon S3 bucket. Utilize the bucket to implement new versions of the application. Use Lightsail deployment options to manage the deployment.</p>",
            "<p>Host the application code in AWS CodeArtifact. Deploy the application on a fleet of Amazon EC2 instances using AWS CodeDeploy. To distribute the traffic to the EC2 instances, utilize the Application Load Balancer. When modifying the application, store a new version to CodeArtifact and create a new CodeDeploy deployment.</p>",
            "<p>Host the application using AWS Elastic Beanstalk. Upload a zipped version of the application on an Amazon S3 bucket. Utilize the bucket to implement new versions of the application. Use Elastic Beanstalk to manage the deployment options.</p>",
            "<p>Host the application on an Amazon EC2 instance and make an AMI of the instance. Create a Launch Template based on this AMI to be used in an Auto Scaling group. Provision an Application Load Balancer to distribute the traffic. When changes are made to the application, create a new AMI, then trigger an EC2 instance refresh.</p>"
          ],
          "explanation": "<p><strong>Elastic Beanstalk</strong> enables users to deploy and administer applications in the AWS Cloud promptly without the need to comprehend the underlying infrastructure. It streamlines management complexities while still granting flexibility and authority to users. The user is only required to upload their application, and Elastic Beanstalk will automatically take care of tasks such as capacity provisioning, load balancing, scaling, and application health monitoring.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-elastic-beanstalk-blue-green.png\"></p><p>A diverse selection of platforms is available on AWS Elastic Beanstalk for building applications. Users choose a platform for their web application, and Elastic Beanstalk deploys their code to the chosen platform version, creating a functional application environment.</p><p><strong>Elastic Beanstalk</strong> provides platforms for programming languages (Go, Java, Node.js, PHP, Python, Ruby), application servers (Tomcat, Passenger, Puma), and Docker containers.</p><p>In the scenario, the application is built in Go which is supported by Elastic Beanstalk and can leverage the built-in deployment options for blue/green deployment for less overhead.</p><p>Hence, the correct answer is the option that says: <strong>Host the application using AWS Elastic Beanstalk. Upload a zipped version of the application on an Amazon S3 bucket. Utilize the bucket to implement new versions of the application. Use Elastic Beanstalk to manage the deployment options.</strong></p><p>The option that says: <strong>Host the application using Amazon Lightsail. Upload a zipped version of the application on an Amazon S3 bucket. Utilize the bucket to implement new versions of the application. Use Lightsail deployment options to manage the deployment </strong>is incorrect because using Amazon Lightsail only provides compute, storage, and networking capacity and capabilities to deploy and manage websites and web applications in the cloud. While this approach may be viable, extra configuration is required to establish the Go application, which entails more work.</p><p>The option that says: <strong>Host the application code in AWS CodeArtifact. Deploy the application on a fleet of Amazon EC2 instances using AWS CodeDeploy. To distribute the traffic to the EC2 instances, utilize the Application Load Balancer. When modifying the application, store a new version to CodeArtifact and create a new CodeDeploy deployment </strong>is incorrect because AWS CodeArtifact is primarily used to store artifacts using popular package managers and build tools like Maven, Gradle, npm, Yarn, Twine, pip, and NuGet.</p><p>The option that says: <strong>Host the application on an Amazon EC2 instance and make an AMI of the instance. Create a Launch Template based on this AMI to be used in an Auto Scaling group. Provision an Application Load Balancer to distribute the traffic. When changes are made to the application, create a new AMI, then trigger an EC2 instance refresh </strong>is incorrect because this option will not satisfy the A/B testing requirement since triggering an EC2 instance refresh will replace the instances running the old AMI.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html\">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 82921358,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A retail company uses the open-source tool Jenkins on its on-premise infrastructure to perform CICD. It has decided to move to AWS and take advantage of the elasticity properties of the cloud provider to have more efficient workloads. It needs to ensure the Jenkins setup is highly available, fault-tolerant and also elastic to perform builds. The company has hired you as an AWS Certified DevOps Engineer Professional to build the most cost-effective solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend?</p>\n",
          "answers": [
            "<p>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>",
            "<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>",
            "<p>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</p>",
            "<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p>In the AWS Cloud, a web-accessible application like Jenkins is typically designed for high availability and fault tolerance by spreading instances across multiple AZs and fronting them with an Elastic Load Balancing (ELB) load balancer. Elastic Load Balancing automatically distributes incoming application traffic across multiple Amazon EC2 instances in the cloud. It enables you to achieve greater levels of fault tolerance in your applications and seamlessly provides the required amount of load balancing capacity needed to distribute application traffic. If your business requirements demand a fault-tolerant Jenkins environment, your preferred setup might be a scenario in which multiple masters with their own workers are placed in separate Availability Zones.</p>\n\n<p>You can use the Jenkins plugin for AWS CodeBuild to integrate CodeBuild with your Jenkins build jobs. Instead of sending your build jobs to Jenkins build nodes, you use the plugin to send your build jobs to CodeBuild. This eliminates the need for you to provision, configure, and manage Jenkins build nodes.</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2019/10/20/Diagram2.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/\">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p>For the given use-case, Jenkins must be deployed as a multi-master across multi-AZ to be highly available and fault-tolerant. The Jenkins CodeBuild plugin allows to elastically start CodeBuild builds that run a special docker image that works as a Jenkins slave. It allows you to be fully elastic in the cloud with Jenkins, and only pay exactly for the resources you have used.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p>As mentioned in the explanation above, if configured with EC2 instances in an Auto Scaling Group, the setup will be elastic in some ways, but probably expensive if the EC2 instances are not fully utilized at capacity. So these three options are not the best fit for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/\">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf\">https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf</a></p>\n"
        }
      },
      {
        "id": 67357138,
        "correct_response": [
          "a",
          "e"
        ],
        "prompt": {
          "question": "<p>An AWS managed <code>cloudformation-stack-drift-detection-check</code> rule is defined in AWS Config for drift detection in AWS CloudFormation resources. The DevOps team is facing two issues:</p>\n\n<p>a) How to detect drifts of Cloudformation custom resources\nb) Drift status of the stack shows as IN_SYNC in the CloudFormation console, the following is the drift detection error  - 'While AWS CloudFormation failed to detect drift, defaulting to NON_COMPLIANT. Re-evaluate the rule and try again. If the problem persists contact AWS CloudFormation support'</p>\n\n<p>As a DevOps Engineer, which steps will you combine to fix the aforementioned issues? (Select two)</p>\n",
          "answers": [
            "<p>AWS Config rule depends on the availability of <code>DetectStackDrift</code> action of CloudFormation API. AWS Config defaults the rule to NON_COMPLIANT when throttling occurs</p>",
            "<p>You receive the error when the AWS Identity and Access Management (IAM) role for the required <code>cloudformationRoleArn</code> parameter doesn't have sufficient service permissions</p>",
            "<p>This error is a false positive and can be ignored for this scenario</p>",
            "<p>AWS CloudFormation only determines drift for property values that are explicitly set. Explicitly set the property values for your custom resource to be included in drift</p>",
            "<p>AWS CloudFormation does not support drift detection of custom resources</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p>The <code>cloudformation-stack-drift-detection-check</code> rule checks if the actual configuration of a Cloud Formation stack differs, or has drifted, from the expected configuration. A stack is considered to have drifted if one or more of its resources differ from their expected configuration. The rule and the stack are COMPLIANT when the stack drift status is IN_SYNC. The rule is NON_COMPLIANT if the stack drift status is DRIFTED.</p>\n\n<p>CloudFormation offers a drift detection feature to detect unmanaged configuration changes to stacks and resources. This will let you take corrective action to put the stack resources back in sync with their definitions in the stack template. To return a resource to compliance, the resource definition changes can be reverted directly.</p>\n\n<p><strong>AWS Config rule depends on the availability of <code>DetectStackDrift</code> action of CloudFormation API. AWS Config defaults the rule to NON_COMPLIANT when throttling occurs</strong></p>\n\n<p>AWS Config rule depends on the availability of <code>DetectStackDrift</code>. You receive a throttling or \"Rate Exceeded\" error because AWS Config defaults the rule to NON_COMPLIANT when throttling occurs.</p>\n\n<p>Resolve the error resulting from the availability of DetectStackDrift:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q22-i1.jpg\">\nvia - <a href=\"https://repost.aws/knowledge-center/config-cloudformation-drift-detection\">https://repost.aws/knowledge-center/config-cloudformation-drift-detection</a></p>\n\n<p><strong>AWS CloudFormation does not support drift detection of custom resources</strong></p>\n\n<p>AWS CloudFormation supports resource import and drift detection operations for only supported resource types. Custom resource types are not currently supported.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You receive the error when the AWS Identity and Access Management (IAM) role for the required <code>cloudformationRoleArn</code> parameter doesn't have sufficient service permissions</strong> - Any issues with permissions will not result in the shown error. Permissions error looks something like this: \"Your stack drift detection operation for the specific stack has failed. Check your existing AWS CloudFormation role permissions and add the missing permissions.\"</p>\n\n<p><strong>This error is a false positive and can be ignored for this scenario</strong> - This option just acts as a distractor.</p>\n\n<p><strong>AWS CloudFormation only determines drift for property values that are explicitly set. Explicitly set the property values for your custom resource to be included in drift</strong> - It is true that CloudFormation only determines drift for property values that are explicitly set, either through the stack template or by specifying template parameters. However, custom resources are not currently supported for drift.</p>\n\n<p>References:</p>\n\n<p>[[https://repost.aws/questions/QUolH-EWnHRNGDbnUiu3chXw/how-to-detect-drifts-of-cloudformation-custom-resource](https://repost.aws/questions/QUolH-EWnHRNGDbnUiu3chXw/how-to-detect-drifts-of-cloudformation-custom-resource)]</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/config-cloudformation-drift-detection\">https://repost.aws/knowledge-center/config-cloudformation-drift-detection</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html</a></p>\n"
        }
      },
      {
        "id": 134588385,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company is planning to host their enterprise application in an ECS Cluster which uses the Fargate launch type. The database credentials should be provided to the AMI by using environment variables for security purposes. A DevOps engineer was instructed to ensure that the credentials are secure when passed to the image and that the sensitive passwords cannot be viewed on the cluster itself. In addition, the credentials must be kept in a dedicated storage with lifecycle management and key rotation. </p><p>Which of the following is the MOST suitable solution that the engineer should implement with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Upload and manage the database credentials using AWS Systems Manager Parameter Store then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container.</p>",
            "<p>Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt them with KMS. Store the task definition JSON file in a private Amazon S3 bucket. Ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Set up an IAM role to the ECS task definiton script that allows access to the specific S3 bucket and then pass the <code>--cli-input-json</code> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials.</p>",
            "<p>Store the database credentials using the AWS Secrets Manager. Encrypt the credentials using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret, which contains the sensitive data, to present to the container.</p>",
            "<p>Store the database credentials using Docker Secrets in the ECS task definition file of the ECS Cluster where you can centrally manage sensitive data and securely transmit it to only those containers that need access to it. Ensure that the secrets are encrypted during transit and at rest.</p>"
          ],
          "explanation": "<p><strong>Amazon ECS</strong> enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</p><p><img src=\"https://media.tutorialsdojo.com/public/diagram3-1_2AUG2023.png\"></p><p>Within your container definition, specify <code>secrets</code> with the name of the environment variable to set in the container and the full ARN of either the Secrets Manager secret or Systems Manager Parameter Store parameter containing the sensitive data to present to the container. The parameter that you reference can be from a different Region than the container using it, but must be from within the same account.</p><p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises.</p><p>If you want a single store for configuration and secrets, you can use Parameter Store. If you want a dedicated secrets store with lifecycle management, use Secrets Manager.</p><p>Hence, the correct answer is the option that says: <strong>Store the database credentials using the AWS Secrets Manager. Encrypt the credentials using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret, which contains the sensitive data, to present to the container<em>.</em></strong></p><p>The option that says: <strong>Upload and manage the database credentials using AWS Systems Manager Parameter Store then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container </strong>is incorrect. Although the use of Systems Manager Parameter Store in securing sensitive data in ECS is valid, this service doesn't provide dedicated storage with lifecycle management and key rotation, unlike Secrets Manager.</p><p>The option that says: <strong>Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt them with KMS. Store the task definition JSON file in a private Amazon S3 bucket. Ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Set up an IAM role to the ECS task definition script that allows access to the specific S3 bucket and then pass the </strong><code><strong>--cli-input-json</strong></code><strong> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials</strong><em> </em>is incorrect. Although the solution may work, it is not recommended to store sensitive credentials in S3. This entails a lot of overhead and manual configuration steps which can be simplified by simply using the Secrets Manager or Systems Manager Parameter Store.</p><p>The option that says: <strong>Store the database credentials using Docker Secrets in the ECS task definition file of the ECS Cluster where you can centrally manage sensitive data and securely transmit it to only those containers that need access to it. Ensure that the secrets are encrypted during transit and at rest<em> </em></strong>is incorrect. Although you can use Docker Secrets to secure the sensitive database credentials, this feature is only applicable in Docker Swarm. In AWS, the recommended way to secure sensitive data is either through the use of Secrets Manager or Systems Manager Parameter Store.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/\">https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><br></p><p><strong>Check out this AWS Secrets Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager/</a></p>"
        }
      },
      {
        "id": 67357140,
        "correct_response": [
          "a",
          "d"
        ],
        "prompt": {
          "question": "<p>A media company extensively uses Amazon S3 buckets for storing images files, documents, and other business-specific data. The company has mandated enabling logging for all Amazon S3 buckets. The audit team publishes the reports of all AWS resources failing company security standards. Until recently, the security team would pick the list of noncompliant Amazon S3 buckets from the audit list and execute remediation actions manually for each resource. This process is not only time-consuming but also leaves noncompliant resources vulnerable for a long duration.</p>\n\n<p>Which combination of steps should a DevOps Engineer take to meet these requirements using an automated solution? (Select two)</p>\n",
          "answers": [
            "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></p>",
            "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</p>",
            "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</p>",
            "<p>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</p>",
            "<p>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></strong></p>\n\n<p><strong>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</strong></p>\n\n<p>The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. You can:</p>\n\n<ol>\n<li>Choose the remediation action you want to associate from a pre-populated list.</li>\n<li>Create your own custom remediation actions using AWS Systems Manager Automation documents.</li>\n</ol>\n\n<p>If a resource is still non-compliant after auto-remediation, you can set the rule to try auto-remediation again.</p>\n\n<p>For the above use case: You must have AWS Config enabled in your AWS account. The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config, and that role must have whatever permissions the SSM document requires.</p>\n\n<p>Steps to set up Auto Remediation for s3-bucket-logging-enabled:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q23-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</strong> - While you can create a custom remediation action using SSM, for this particular use case, it is not required since auto-remediation for S3 server logging is already present in the remediation action list of AWS Config rules.</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</strong> - As discussed above, the AWS config remediation action pre-populated list already has an action defined for enabling S3 logging. Hence, custom code is unnecessary for this use case.</p>\n\n<p><strong>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</strong>- While setting up remediation action if you want to pass the resource ID of non-compliant resources to the remediation action, choose the Resource ID parameter. If selected at runtime, the parameter is substituted with the ID of the resource to be remediated. This is not mandatory though.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n"
        }
      },
      {
        "id": 75949138,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
            "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
            "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
            "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
            "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
            "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"
          ],
          "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 134588405,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A business wants to leverage AWS CloudFormation to deploy its infrastructure. The business would like to restrict deployment to two particular regions and wants to implement a strict tagging requirement. Developers are expected to deploy various versions of the same application and want to guarantee that resources are deployed in compliance with the business policy while still enabling developers to deploy different versions of the application.</p><p>Which of the following is the MOST suitable solution?</p>",
          "answers": [
            "<p>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</p>",
            "<p>Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks.</p>",
            "<p>Utilize approved CloudFormation templates and launch CloudFormation StackSets.</p>",
            "<p>Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation.</p>"
          ],
          "explanation": "<p>With <strong>AWS Service Catalog</strong>, cloud resources can be centrally managed to achieve infrastructure as code (IaC) template governance at scale, whether written in CloudFormation or Terraform. Compliance requirements can be met while ensuring customers can efficiently deploy the necessary cloud resources.</p><p><img alt=\"Service Catalog\" height=\"540\" src=\"https://media.tutorialsdojo.com/public/dop-c02-service-catalog.png\" width=\"1000\"></p><p>Template constraints can be applied when limiting end-users' options during a product launch. This ensures that the organization's compliance requirements are not breached.</p><p>A product must be present within a Service Catalog portfolio to apply template constraints. A template constraint includes rules that narrow the allowable values for parameters in the underlying AWS CloudFormation template of the product. These parameters define the set of values available to users when creating a stack. For instance, an instance type parameter can be specified to limit the types of instances that users can choose from when launching a stack containing EC2 instances.</p><p>Hence, the correct answer is: <strong>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</strong></p><p>The option that says:<strong> Utilize approved CloudFormation templates and launch CloudFormation StackSets </strong>is incorrect because StackSets manage deployments across accounts and regions but do not enforce tagging or region restrictions. They do not typically provide governance to prevent non-compliant implementations.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks </strong>is incorrect because Trusted Advisor does not support checks for unauthorized StackSets or enforce CloudFormation template compliance.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation </strong>is incorrect because drift detection only identifies changes after deployment, but cannot prevent non-compliant resource creation or enforce policies before deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/servicecatalog/\">https://aws.amazon.com/servicecatalog/</a></p><p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/\">https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/</a></p><p><br></p><p><strong>Check out this AWS Service Catalog Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-service-catalog/?src=udemy\">https://tutorialsdojo.com/aws-service-catalog/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/</a></p>"
        }
      },
      {
        "id": 82921388,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As part of your CodePipeline, you are running multiple test suites. Two are bundled as Docker containers and run directly on CodeBuild, while another one runs as a Lambda function executing Python code. All these test suites are based on HTTP requests and upon analyzing, these are found to be network bound, not CPU bound. Right now, the CodePipeline takes a long time to execute as these actions happen one after the other. They prevent the company from adding further tests. The whole pipeline is managed by CloudFormation.</p>\n\n<p>As a DevOps Engineer, which of the following would you recommend improving the completion time of your pipeline?</p>\n",
          "answers": [
            "<p>Change the <code>runOrder</code> of your actions so that they have the same value</p>",
            "<p>Increase the number of vCPU assigned to the CodeBuild builds and the RAM assigned to your Lambda function</p>",
            "<p>Enable CloudFormation StackSets to run the actions in parallel</p>",
            "<p>Migrate all the test suites to Jenkins and use the ECS plugin</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the <code>runOrder</code> of your actions so that they have the same value</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/images/PipelineFlow.png\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p>The pipeline structure format is used to build actions and stages in a pipeline. An action type consists of an action category and provider type. Valid action providers for each action category:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p>\n\n<p>You can use the <code>runOrder</code> to specify parallel actions and use the same integer for each action you want to run in parallel. The default runOrder value for an action is 1. The value must be a positive integer (natural number). You cannot use fractions, decimals, negative numbers, or zero. Here, you need to specify a common <code>runOrder</code> value in your CloudFormation template so that all the stage actions happen in parallel.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q14-i2.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q14-i3.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the number of vCPU assigned to the CodeBuild builds and the RAM assigned to your Lambda function</strong> - As the test suites are HTTP and network-bound, increasing the RAM for Lambda and vCPU capacity of CodeBuild won't affect the performance (the bottleneck remains the network latency between each HTTP calls).</p>\n\n<p><strong>Enable CloudFormation StackSets to run the actions in parallel</strong> - CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p>CloudFormation StackSets is a distractor here, as they do not enable parallel actions.</p>\n\n<p><strong>Migrate all the test suites to Jenkins and use the ECS plugin</strong> - Migrating to Jenkins also would not solve the problem, as the test suites would still happen sequentially.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p>\n"
        }
      },
      {
        "id": 138248181,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company would like to set up an audit process to ensure that the enterprise application is running exclusively on Amazon EC2 Dedicated Hosts. The company is also concerned about the increasing costs of its application software licensing from its third-party vendor. To meet the compliance requirement, a DevOps Engineer must create a workflow to audit the enterprise applications hosted in its Amazon VPC.</p><p>Which of the following options should the Engineer implement to satisfy the requirement with the LEAST administrative overhead?</p>",
          "answers": [
            "<p>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the <code>PutComplianceItems</code> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the <code>ListComplianceSummaries</code> API action.</p>",
            "<p>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the <code>inspector-scheduled-run</code> blueprint.</p>",
            "<p>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the <code>config-rule-change-triggered</code> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</p>",
            "<p>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data.</p>"
          ],
          "explanation": "<p>You can use <strong>AWS Config</strong> to record configuration changes for Dedicated Hosts, and instances that are launched, stopped, or terminated on them. You can then use the information captured by AWS Config as a data source for license reporting.</p><p>AWS Config records configuration information for Dedicated Hosts and instances individually and pairs this information through relationships. There are three reporting conditions:</p><p>- AWS Config recording status \u2014 When On, AWS Config is recording one or more AWS resource types, which can include Dedicated Hosts and Dedicated Instances. To capture the information required for license reporting, verify that hosts and instances are being recorded with the following fields.</p><p>- Host recording status \u2014 When Enabled, the configuration information for Dedicated Hosts is recorded.</p><p>- Instance recording status \u2014 When Enabled, the configuration information for Dedicated Instances is recorded.</p><p>If any of these three conditions are disabled, the icon in the Edit Config Recording button is red. To derive the full benefit of this tool, ensure that all three recording methods are enabled. When all three are enabled, the icon is green. To edit the settings, choose Edit Config Recording. You are directed to the <em>Set up AWS Config </em>page in the AWS Config console, where you can set up AWS Config and start recording for your hosts, instances, and other supported resource types. AWS Config records your resources after it discovers them, which might take several minutes.</p><p>After AWS Config starts recording configuration changes to your hosts and instances, you can get the configuration history of any host that you have allocated or released and any instance that you have launched, stopped, or terminated. For example, at any point in the configuration history of a Dedicated Host, you can look up how many instances are launched on that host, along with the number of sockets and cores on the host. For any of those instances, you can also look up the ID of its Amazon Machine Image (AMI). You can use this information to report on licensing for your own server-bound software that is licensed per-socket or per-core.</p><p>You can view configuration histories in any of the following ways.</p><p>- By using the AWS Config console. For each recorded resource, you can view a timeline page, which provides a history of configuration details. To view this page, choose the gray icon in the Config Timeline column of the Dedicated Hosts page.</p><p>- By running AWS CLI commands. First, you can use the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/configservice/list-discovered-resources.html\">list-discovered-resources</a> command to get a list of all hosts and instances. Then, you can use the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/configservice/get-resource-config-history.html#get-resource-config-history\">get-resource-config-history</a> command to get the configuration details of a host or instance for a specific time interval.</p><p>- By using the AWS Config API in your applications. First, you can use the <a href=\"https://docs.aws.amazon.com/config/latest/APIReference/API_ListDiscoveredResources.html\">ListDiscoveredResources</a> action to get a list of all hosts and instances. Then, you can use the <a href=\"https://docs.aws.amazon.com/config/latest/APIReference/API_GetResourceConfigHistory.html\">GetResourceConfigHistory</a> action to get the configuration details of a host or instance for a specific time interval.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-AWS-Config-Status-02-05-2025.png\"></p><p>Hence, the correct answer is: <strong>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the </strong><code><strong>config-rule-change-triggered</strong></code><strong> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</strong></p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the </strong><code><strong>PutComplianceItems</strong></code><strong> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the </strong><code><strong>ListComplianceSummaries</strong></code><strong> API action </strong>is incorrect because the AWS Systems Manager Configuration Compliance service is primarily used to scan your fleet of managed instances for patch compliance and configuration inconsistencies. A better solution is to use AWS Config to record the status of your Dedicated Hosts.</p><p>The option that says: <strong>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the </strong><code><strong>inspector-scheduled-run</strong></code><strong> blueprint</strong> is incorrect because Amazon Inspector is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not capable of recording the status of your EC2 instances nor detect if they are configured as a Dedicated Host.</p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data </strong>is incorrect. Although this may be a possible solution, it entails a lot of administrative effort in comparison to just using AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html \">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom \">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/\">https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and AWS Config Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-config/\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 134588413,
        "correct_response": [
          "a",
          "e"
        ],
        "prompt": {
          "question": "<p>A leading IT consulting firm is building a GraphQL API service and a mobile application that lets people post photos and videos of the traffic situations and other issues in the city's public roads. Users can include a text report and constructive feedback to the authorities. The department of public works shall rectify the problems based on the data gathered by the system. In order for the mobile app to run on various mobile and tablet devices, the firm decided to develop it using the React Native mobile framework, which will consume and send data to the GraphQL API. The backend service will be responsible for storing the photos and videos in an Amazon S3 bucket. The API will also need access to the Amazon DynamoDB database to store the text reports. The firm has recently deployed the mobile app prototype, however, during testing, the GraphQL API showed a lot of issues. The team decided to remove the API to proceed with the project and refactor the mobile application instead so that it will directly connect to both DynamoDB and S3 as well as handle user authentication. </p><p>Which of the following options provides a cost-effective and scalable architecture for this project? (Select TWO.)</p>",
          "answers": [
            "<p>Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with social identity providers like Facebook, Google or any other OpenID Connect (OIDC)-compatible IdP. Create a new IAM Role and grant permissions to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table.</p>",
            "<p>Create an identity pool in AWS Identity and Access Management (IAM) that will be used to store the end-user identities organized for your mobile app. IAM will automatically create the required IAM roles for authenticated identities as well as for unauthenticated \"guest\" identities that define permissions for the users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use IAM. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client.</p>",
            "<p>Using the STS AssumeRoleWithSAML API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile app to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table.</p>",
            "<p>Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the text-based report to a DynamoDB table.</p>",
            "<p>Create an identity pool in Amazon Cognito that will be used to store the end-user identities organized for your mobile app. Amazon Cognito will automatically create the required IAM roles for authenticated identities as well as for unauthenticated \"guest\" identities that define permissions for Amazon Cognito users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use Amazon Cognito. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client.</p>"
          ],
          "explanation": "<p>With web identity federation, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any of your OpenID Connect (OIDC)-compatible IdP. They can receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Using an IdP helps you keep your AWS account secure because you don't have to embed and distribute long-term security credentials with your application.</p><p>The preferred way to use web identity federation is to use <a href=\"https://aws.amazon.com/cognito/\">Amazon Cognito</a>. For example, You are a developer that builds a game for a mobile device where each user data such as scores and profiles are stored in Amazon S3 and Amazon DynamoDB. You could also store this data locally on the device and use Amazon Cognito to keep it synchronized across devices. You know that for security and maintenance reasons, long-term AWS security credentials should not be distributed with the game. You might also know that the game might have a large number of users. For all of these reasons, you don't want to create new user identities in IAM for each player. Instead, you build the game so that users can sign in using an identity that they've already established with a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible IdP. Your game can take advantage of the authentication mechanism from one of these providers to validate the user's identity.</p><p><img src=\"https://media.tutorialsdojo.com/aws-cognito-diagram-tutorialsdojo.png\"></p><p>To enable the mobile app to access your AWS resources, you should first register for a developer ID with your chosen IdPs. You can also configure the application with each of these providers. In your AWS account that contains the Amazon S3 bucket and DynamoDB table for the game, you should use Amazon Cognito to create IAM roles that precisely define permissions that the game needs. If you are using an OIDC IdP, you can also create an IAM OIDC identity provider entity to establish trust between your AWS account and the IdP.</p><p>In the app's code, you can call the sign-in interface for the IdP that you configured previously. The IdP handles all the details of letting the user sign in, and the app gets an OAuth access token or OIDC ID token from the provider. Your mobile app can trade this authentication information for a set of temporary security credentials that consist of an AWS access key ID, a secret access key, and a session token. The app can then use these credentials to access web services offered by AWS. The app is limited to the permissions that are defined in the role that it assumes.</p><p>In this scenario, you have a mobile app that needs to have access to the DynamoDB and S3 bucket. You can achieve this by using Web Identity Federation with AssumeRoleWithWebIdentity API which provides temporary security credentials and an IAM role. You can also use Amazon Cognito to simplify the process.</p><p>Hence, the correct answers are:</p><p><strong>- Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with social identity providers like Facebook, Google or any other OpenID Connect (OIDC)-compatible IdP. Create a new IAM Role and grant permissions to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table.</strong></p><p><strong>- Create an identity pool in Amazon Cognito that will be used to store the end-user identities organized for your mobile app. Amazon Cognito will automatically create the required IAM roles for authenticated identities as well as for unauthenticated \"guest\" identities that define permissions for Amazon Cognito users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use Amazon Cognito. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client.</strong></p><p>The option that says: <strong>Create an identity pool in AWS Identity and Access Management (IAM) that will be used to store the end-user identities organized for your mobile app. IAM will automatically create the required IAM roles for authenticated identities as well as for unauthenticated \"guest\" identities that define permissions for the users. Download and integrate the AWS SDK for React Native with your app, and import the files required to use IAM. Configure your app to pass the credentials provider instance to the client object, which passes the temporary security credentials to the client</strong> is incorrect because you cannot create identity pools with guest identities using the AWS Identity and Access Management (IAM) service. You can only implement this using Amazon Cognito.</p><p>The option that says: <strong>Using the STS AssumeRoleWithSAML API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile app to use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the text-based reports to the DynamoDB table</strong> is incorrect because you should have used the AssumRoleWithWebIdentity API instead of <em>AssumeRoleWithSAML</em>, as this is used in SAML authentication response and not for web identity authentication.</p><p>The option that says: <strong>Using the STS AssumeRoleWithWebIdentity API, set up a web identity federation and register with various social identity providers like Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Set up an IAM role for that provider and grant permissions for the IAM role to allow access to Amazon S3 and DynamoDB. Configure the mobile application to use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the text-based report to a DynamoDB table</strong> is incorrect because even though the use of Amazon Cognito is valid, it is wrong to store and use the AWS access and secret keys from the mobile app itself. This is a security risk and you should use the temporary security credentials instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p>"
        }
      },
      {
        "id": 138248163,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A leading commercial bank has its online banking application hosted in AWS. It uses an encrypted Amazon S3 bucket to store the confidential files of its customers. The DevOps team has configured federated access to a particular Active Directory user group from the bank's on-premises network to allow access to the S3 bucket. For audit purposes, there is a new requirement to automatically detect any policy changes that are related to the restricted federated access of the bucket and to have the ability to revert any accidental changes made by the administrators.</p><p>Which of the following options provides the FASTEST way to detect configuration changes?</p>",
          "answers": [
            "<p>Using Amazon EventBridge, integrate an Event Bus with AWS CloudTrail API in order to trigger an AWS Lambda function that will detect and revert any particular changes.</p>",
            "<p>Set up an AWS Config rule with a configuration change trigger that will detect any changes in the S3 bucket configuration and which will also invoke an AWS Systems Manager Automation document with an AWS Lambda function that will revert any changes.</p>",
            "<p>Integrate Amazon EventBridge and an AWS Lambda function to create a scheduled job that runs every hour to scan the IAM policy attached to the federated access role. Configure the function to detect as well as revert any recent changes made in the current configuration.</p>",
            "<p>Set up an AWS Config rule with a periodic trigger that runs every hour which will detect any changes in the S3 bucket configuration. Associate an AWS Lambda function in the rule that will revert any recent changes made in the bucket.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.</p><p>When you add a rule to your account, you can specify when you want AWS Config to run the rule; this is called a <em>trigger</em>. AWS Config evaluates your resource configurations against the rule when the trigger occurs.</p><p><img src=\"https://media.tutorialsdojo.com/public/2019-11-22_22-41-48-8712a29506a6524f89ee82708c97854b.png\"></p><p>There are two types of triggers:</p><ol><li><p>Configuration changes</p></li><li><p>Periodic</p></li></ol><p>If you choose both <em>configuration changes</em> and <em>periodic </em>triggers, AWS Config invokes your Lambda function when it detects a configuration change and also at the frequency that you specify.</p><p><strong>Configuration changes</strong></p><p>AWS Config runs evaluations for the rule when certain types of resources are created, changed, or deleted. You choose which resources trigger the evaluation by defining the rule's <em>scope</em>. The scope can include the following:</p><p>- One or more resource types</p><p>- A combination of a resource type and a resource ID</p><p>- A combination of a tag key and value</p><p>- When any recorded resource is created, updated, or deleted</p><p>AWS Config runs the evaluation when it detects a change to a resource that matches the rule's scope. You can use the scope to constrain which resources trigger evaluations. Otherwise, evaluations are triggered when any recorded resource changes.</p><p><strong>Periodic</strong></p><p>AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p><p>Hence, the correct answer is: <strong>Set up an AWS Config rule with a configuration change trigger that will detect any changes in the S3 bucket configuration and which will also invoke an AWS Systems Manager Automation document with an AWS Lambda function that will revert any changes.</strong></p><p>The option that says: <strong>Using Amazon EventBridge, integrate an Event Bus with AWS CloudTrail API in order to trigger an AWS Lambda function that will detect and revert any particular changes</strong> is incorrect. Although you can track all changes to your configuration using CloudTrail API, it would be difficult to integrate it with EventBridge in order to monitor the changes. There is no direct way of integrating these two services and you have to create a custom mapping in order for this to work.</p><p>The option that says: <strong>Integrate Amazon EventBridge and an AWS Lambda function to create a scheduled job that runs every hour to scan the IAM policy attached to the federated access role. Configure the function to detect as well as revert any recent changes made in the current configuration</strong> is incorrect. Although this solution may work, there would be a significant delay since the Lambda function is only run every one hour. So if the new S3 bucket configuration was applied at 12:05 PM, the change will only be detected at 1:00 PM. Moreover, this entails a lot of overhead since you have to develop a custom function that will scan your IAM policies.</p><p>The option that says: <strong>Set up an AWS Config rule with a periodic trigger that runs every hour which will detect any changes in the S3 bucket configuration. Associate an AWS Lambda function in the rule that will revert any recent changes made in the bucket<em> </em></strong>is incorrect. Although this may work, it is not the fastest way of detecting a change in your resource configurations in AWS. Since the rule is using a periodic trigger, the rule will run every hour and not in near real-time, unlike the <strong><em>Configuration changes</em></strong> trigger. So, say a new configuration was applied at 12:01 PM, the change will only be detected at 1:00 PM after the rule has been run.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_getting-started.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules_getting-started.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 134588509,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An Amazon S3 bucket named &lt;code&gt;team-cebu-devops&lt;/code&gt; is shared by all the DevOps administrators in the team. It is used to store artifact files for several CI/CD pipelines. A junior developer accidentally modified the S3 bucket policy, denying all pipelines from downloading the required artifact files and causing all deployments to halt.</p><p>To prevent similar issues in the future, the team wants to be notified of any S3 bucket policy changes to identify and take action if any problem occurs quickly.</p><p>Which of the following options will help achieve this?</p>",
          "answers": [
            "<p>Enable S3 server access logging on your bucket. Create an Amazon CloudWatch Metric Filter for bucket policy events. Create an Alarm for this metric to notify you whenever an event is matched.</p>",
            "<p>Create an AWS CloudTrail trail that sends logs to an Amazon CloudWatch log group. Create a CloudWatch Metric Filter on the log group for S3 bucket policy events. Create an Alarm that will notify you whenever this metric threshold is reached.</p>",
            "<p>Enable S3 server access logging on your bucket. Send the access logs to an Amazon CloudWatch log group. Create an Amazon CloudWatch Metric Filter for bucket policy events on the log group. Create an Alarm for this metric to notify you whenever the threshold is reached.</p>",
            "<p>Create an AWS CloudTrail trail that sends logs to an Amazon CloudWatch log group. Create an Amazon EventBridge rule for S3 bucket policy events on the log group. Create an Alarm based on the event that will send you a notification whenever an event is matched.</p>"
          ],
          "explanation": "<p>You can configure alarms for several scenarios in CloudTrail events. In this case, you can create an Amazon CloudWatch alarm that is triggered when an Amazon S3 API call is made to PUT or DELETE bucket policy, bucket lifecycle, bucket replication, or to PUT a bucket ACL. A CloudTrail trail is required since it will send its logs to a CloudWatch Log group. To create an alarm, you must first create a metric filter and then configure an alarm based on the filter.</p><p><img src=\"https://media.tutorialsdojo.com/AWS-CloudTrail-to-CWLogs.jpg\"></p><p>For this scenario, since all CloudTrail events will be sent to the Log group, you will need to create a Metric to filter specific S3 events that change the bucket policy of your <code>team-cebu-devops</code> bucket. For notification, you will then create a CloudWatch Alarm for this Metric with a threshold of &gt;=1 and set your email as a notification recipient. Even a single S3 bucket event on the log group will trigger this alarm and should send you a notification.</p><p>Hence, the correct answer is: <strong>Create an AWS CloudTrail trail that sends logs to an Amazon CloudWatch log group. Create a CloudWatch Metric Filter on the log group for S3 bucket policy events. Create an Alarm that will notify you whenever this metric threshold is reached.</strong></p><p>The option that says: <strong>Enable S3 server access logging on your bucket. Create an Amazon CloudWatch Metric Filter for bucket policy events. Create an Alarm for this metric to notify you whenever an event is matched<em> </em></strong>is incorrect because S3 Server access logging is primarily used to provide detailed records for the requests that are made to a bucket. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant. It is more appropriate to use CloudWatch or CloudTrail to track the S3 bucket policy changes.</p><p>The option that says: <strong>Enable S3 server access logging on your bucket. Send the access logs to an Amazon CloudWatch log group. Create an Amazon CloudWatch Metric Filter for bucket policy events on the log group. Create an Alarm for this metric to notify you whenever the threshold is reached </strong>is incorrect because you can\u2019t directly send the S3 server access logs to CloudWatch logs. You need to use CloudTrail to send the events to a log group before you can create a metric and alarm for those events.</p><p>The option that says: <strong>Create an AWS CloudTrail trail that sends logs to an Amazon CloudWatch log group. Create an Amazon EventBridge rule for S3 bucket policy events on the log group. Create an Alarm based on the event that will send you a notification whenever an event is matched<em> </em></strong>is incorrect because you can\u2019t simply use an EventBridge rule to filter your log groups directly.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ConsoleAlarms.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ConsoleAlarms.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source-console.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source-console.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p>"
        }
      },
      {
        "id": 134588471,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A leading digital consultancy company has two teams in its IT department: the DevOps team and the Security team, that are working together on different components of its cloud architecture. AWS CloudFormation is used to manage its resources across all of its AWS accounts, including AWS Config for configuration management. The Security team applies the operating system-level updates and patches while the DevOps team manages application-level dependencies and updates. The DevOps team must use the latest AMI when launching new EC2 instances and deploying its flagship application. </p><p>Which of the following options is the MOST scalable method for integrating the two processes and teams?</p>",
          "answers": [
            "<p>Instruct the Security team to set up an AWS CloudFormation template that creates new versions of their AMIs and lists the Amazon Resource names (ARNs) of the AMIs in an encrypted S3 object as part of the stack output section. Direct the DevOps team to use the cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs.</p>",
            "<p>Instruct the Security team to set up an AWS CloudFormation stack that creates an AWS CodePipeline pipeline that builds new Amazon Machine Images. Then, store the AMI ARNs as parameters in AWS Systems Manager Parameter Store as part of the pipeline output. Order the DevOps team to use the <code>AWS::SSM::Parameter</code> section in their CloudFormation stack to obtain the most recent AMI ARN from the Parameter Store.</p>",
            "<p>Instruct the Security team to maintain a nested stack in AWS CloudFormation that includes both the OS and the templates from the DevOps team. Order the Security team to use the stack update action to deploy updates to the application stack whenever the DevOps team changes the application code.</p>",
            "<p>Instruct the Security team to use a CloudFormation stack that launches an AWS CodePipeline pipeline that builds new AMIs then store the latest AMI ARNs in an encrypted S3 object as part of the pipeline output. Order the DevOps team to use a cross-stack reference within their own CloudFormation template to get that S3 object location and obtain the most recent AMI ARNs to use when deploying their application.</p>"
          ],
          "explanation": "<p><strong>Dynamic references</strong> provide a compact, powerful way for you to specify external values that are stored and managed in other services, such as the Systems Manager Parameter Store, in your stack templates. When you use a dynamic reference, CloudFormation retrieves the value of the specified reference when necessary during stack and change set operations.</p><p>CloudFormation currently supports the following dynamic reference patterns:</p><p>- ssm, for plaintext values stored in AWS Systems Manager Parameter Store</p><p>- ssm-secure, for secure strings stored in AWS Systems Manager Parameter Store</p><p>- secretsmanager, for entire secrets or specific secret values that are stored in AWS Secrets Manager</p><p><img src=\"https://media.tutorialsdojo.com/public/AWS-Systems-Manager-Parameter-Store_6AUG2023.png\"></p><p>Some considerations when using dynamic references:</p><p>- You can include up to 60 dynamic references in a stack template.</p><p>- For transforms, such as AWS::Include and AWS::Serverless, AWS CloudFormation does not resolve dynamic references prior to invoking any transforms. Rather, AWS CloudFormation passes the literal string of the dynamic reference to the transform. Dynamic references (including those inserted into the processed template as the result of a transform) are resolved when you execute the change set using the template.</p><p>- Dynamic references for secure values, such as <code>ssm-secure</code> and <code>secretsmanager</code>, are not currently supported in custom resources.</p><p>Hence, the correct answer is: <strong>Instruct the Security team to set up an AWS CloudFormation stack that creates an AWS CodePipeline pipeline that builds new Amazon Machine Images. Then, store the AMI ARNs as parameters in AWS Systems Manager Parameter Store as part of the pipeline output. Order the DevOps team to use the </strong><code><strong>AWS::SSM::Parameter</strong></code><strong> section in their CloudFormation stack to obtain the most recent AMI ARN from the Parameter Store.</strong></p><p>The option that says: <strong>Instruct the Security team to set up an AWS CloudFormation template that creates new versions of their AMIs and lists the Amazon Resource names (ARNs) of the AMIs in an encrypted S3 object as part of the stack output section. Direct the DevOps team to use the cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs </strong>is incorrect because it is better to store the parameters in AWS Systems Manager Parameter Store.</p><p>The option that says: <strong>Instruct the Security team to maintain a nested stack in AWS CloudFormation that includes both the OS and the templates from the DevOps team. Order the Security team to use the stack update action to deploy updates to the application stack whenever the DevOps team changes the application code</strong> is incorrect because using a nested stack will not decouple the responsibility of the two teams. Integrating AWS Systems Manager Parameter Store to store the ARN of the AMIs is a better solution.</p><p>The option that says: <strong>Instruct the Security team to use a CloudFormation stack that launches an AWS CodePipeline pipeline that builds new AMIs, then store the latest AMI ARNs in an encrypted S3 object as part of the pipeline output. Order the DevOps team to use a cross-stack reference within their own CloudFormation template to get that S3 object location and obtain the most recent AMI ARNs to use when deploying their application</strong> is incorrect. Although this is a valid solution, it entails a lot of effort to set up a cross-stack reference within the DevOps team's own CloudFormation template to get that S3 object location and obtain the most recent AMI ARNs. You also have to ensure that the AMI ARN on Amazon S3 is the latest one.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 67357136,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A multi-national company with hundreds of AWS accounts has slowly adopted AWS Organizations with all features enabled. The company has also configured a few Organization Units (OUs) to serve its business objectives. The company has some AWS Identity and Access Management (IAM) roles that need to be configured for every new AWS account created for the company. Also, the security policy mandates enabling AWS CloudTrail for all AWS accounts. The company is looking for an automated solution that can add the mandatory IAM Roles and CloudTrail configurations to all newly created accounts and also delete the resources/configurations when an account leaves the organization without manual intervention.</p>\n\n<p>What should a DevOps engineer do to meet these requirements with the minimal overhead?</p>\n",
          "answers": [
            "<p>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</p>",
            "<p>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</p>",
            "<p>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</p>",
            "<p>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</strong></p>\n\n<p>You can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. For example, you can deploy your centralized AWS Identity and Access Management (IAM) roles, provision Amazon Elastic Compute Cloud (Amazon EC2) instances or AWS Lambda functions across AWS Regions and accounts in your organization. CloudFormation StackSets simplify the configuration of cross-account permissions and allow for the automatic creation and deletion of resources when accounts are joined or removed from your Organization.</p>\n\n<p>You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the AWS Organizations master account to deploy stacks to all accounts in your organization or specific organizational units (OUs). A new service-managed permission model is available with these StackSets. Choosing <code>Service managed permissions</code> allows StackSets to automatically configure the necessary IAM permissions required to deploy your stack to the accounts in your organization.</p>\n\n<p>In addition to setting permissions, CloudFormation StackSets offer the option for automatically creating or removing your CloudFormation stacks when a new AWS account joins or quits your Organization. You do not need to remember to manually connect to the new account to deploy your common infrastructure or to delete infrastructure when an account is removed from your Organization. When an account leaves the organization, the stack will be removed from the management of StackSets. However, you can choose to either delete or retain the resources managed by the stack.</p>\n\n<p>Lastly, you choose whether to deploy a stack to your entire organization or just to one or more Organization Units (OU). You also choose a couple of deployment options: how many accounts will be prepared in parallel, and how many failures you tolerate before stopping the entire deployment.</p>\n\n<p>Use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by an AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</strong> - This option involves using too many services which unnecessarily adds to the complexity and cost of the overall solution. So, this option is incorrect.</p>\n\n<p><strong>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</strong> - While you can use AWS Systems Manager to automate tasks across multiple accounts in AWS Organization, the other details in this option are irrelevant to the given use case.</p>\n\n<p>When you run automation across multiple Regions and accounts, you target resources by using tags or the name of an AWS resource group. The resource group must exist in each target account and Region. The resource group name must be the same in each target account and Region. The automation fails to run on those resources that don't have the specified tag or that aren't included in the specified resource group.</p>\n\n<p><strong>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</strong> - It is possible to enable CloudTrail logging from the management account of AWS Organizations, and is referred to as the organization trail. The use case is to be able to log all Trail events to a commonplace. Also, creating an IAM role in the management account and sharing it across all member accounts is not straightforward and requires manual work. Hence, this option is incorrect for the given use case.</p>\n\n<p>IAM policies usage in AWS Organizations:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>\n"
        }
      },
      {
        "id": 75949062,
        "correct_response": [
          "a",
          "d",
          "f"
        ],
        "prompt": {
          "question": "<p>A company needs is deploying a new application in AWS and requires a CI/CD pipeline to automate process. The company requires that the entire CI/CD pipeline can be re-provisioned in different AWS accounts or Regions within minutes.</p><p>The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. A DevOps engineer has already created an AWS CodeCommit repository to store the source code.</p><p>Which combination of actions should the DevOps engineer take to meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Configure an AWS CodePipeline pipeline with a build stage using AWS CodeBuild.</p>",
            "<p>Copy the build artifact from CodeCommit to Amazon S3.</p>",
            "<p>Launch Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline.</p>",
            "<p>Launch Amazon EC2 instances in an AWS Elastic Beanstalk environment and configure the environment as the deployment target in AWS CodePipeline.</p>",
            "<p>Implement an Amazon SQS queue to decouple the pipeline components.</p>",
            "<p>Provision all resources using AWS CloudFormation.</p>"
          ],
          "explanation": "<p>The DevOps engineer should create a pipeline using AWS CodePipeline to automate the entire deployment. The CodeCommit repository can be used as the source. The combinations of CodeBuild with Elastic Beanstalk provides a way to build, test, and deploy with automatic rollback upon failure.</p><p>The question also requires that the entire CI/CD pipeline can be recreated in different accounts and Regions. For this reason the pipeline should be deployed using AWS CloudFormation. The templates can then be easily used to recreate the entire stack.</p><p><strong>CORRECT: </strong>\"Configure an AWS CodePipeline pipeline with a build stage using AWS CodeBuild\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Launch Amazon EC2 instances in an AWS Elastic Beanstalk environment and configure the environment as the deployment target in AWS CodePipeline\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Provision all resources using AWS CloudFormation\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Copy the build artifact from CodeCommit to Amazon S3\" is incorrect.</p><p>There is no need to do this, CodeCommit can be used directly as a source for source code and build artifacts.</p><p><strong>INCORRECT:</strong> Implement an Amazon SQS queue to decouple the pipeline components\" is incorrect.</p><p>CodePipeline has its own built-in capabilities for passing information durably between stages and does not require decoupling using Amazon SQS.</p><p><strong>INCORRECT:</strong> \"Launch Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline\" is incorrect.</p><p>This solution would not provide the automatic rollback upon failure requested. Automatic rollback can be implemented when using Elastic Beanstalk with CodeBuild. Otherwise, you would need CodeDeploy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 115961511,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company uses a tagging strategy to allocate usage costs for AWS resources. An application runs on Amazon EC2 instances in an Auto scaling group. The Amazon EBS volumes that are attached to instances are being created without the correct cost center tags. A DevOps engineer must correct the configuration to ensure the EBS volumes are tagged appropriately.</p><p>What is the MOST efficient solution that meets this requirement?</p>",
          "answers": [
            "<p>Update the Auto Scaling group launch template to include the cost center tags for EBS volumes.</p>",
            "<p>Update the Auto Scaling group to include the cost center tags. Set the PropagateAtLaunch property to true.</p>",
            "<p>Use AWS Config to enforce tagging at EBS volume creation time and deny creation of any volumes that do not have the appropriate cost center tags.</p>",
            "<p>Use Tag Editor to scan the account for EBS volumes that are missing the tags and then add the cost center tags to the volumes.</p>"
          ],
          "explanation": "<p>You can tag new or existing Auto Scaling groups. You can also propagate tags from an Auto Scaling group to the EC2 instances that it launches.</p><p>Tags are not propagated to Amazon EBS volumes. To add tags to Amazon EBS volumes, specify the tags in a launch template.</p><p><strong>CORRECT: </strong>\"Update the Auto Scaling group launch template to include the cost center tags for EBS volumes\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update the Auto Scaling group to include the cost center tags. Set the PropagateAtLaunch property to true\" is incorrect.</p><p>As noted above, you cannot propagate tags from an Auto Scaling group to EBS volumes.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to enforce tagging at EBS volume creation time and deny creation of any volumes that do not have the appropriate cost center tags\" is incorrect.</p><p>AWS Config can be used to report on compliance but cannot stop volume creation.</p><p><strong>INCORRECT:</strong> \"Use Tag Editor to scan the account for EBS volumes that are missing the tags and then add the cost center tags to the volumes\" is incorrect.</p><p>Tag Editor is not an efficient solution as it would involve manual work. The correct solution is fully automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-tagging.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-tagging.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 82921440,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.</p>\n\n<p>Which of the following options would you suggest? (Select two)</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</p>",
            "<p>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</p>",
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>Create a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p><strong>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>Understand how a pipeline execution state change rule works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Here we are only interested in pipeline failures, so we need to choose <code>CodePipeline Pipeline Execution State Change</code>.\nFinally, CloudWatch Event rules do not support Slack as a target, therefore we must create a Lambda function for it.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i2.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</strong> - CloudWatch Event rules do not support Slack as a target, so this option is incorrect.</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>**\nCreate a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>Here we are only interested in pipeline failures, so we just need to choose <code>CodePipeline Pipeline Execution State Change</code>. Therefore both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n"
        }
      },
      {
        "id": 99528217,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>The launch template that is used by an Auto Scaling group has been modified to use a new instance type and AMI. The Auto Scaling group is deployed using AWS CloudFormation. There are 8 production EC2 instances running in the Auto Scaling group.</p><p>A DevOps engineer needs to modify the Auto Scaling group to use the new template version without causing any interruption to the application and must ensure that at least 4 instances are always running.</p>",
          "answers": [
            "<p>Use the AutoScalingScheduledAction attribute with the MaxBatchSize property.</p>",
            "<p>Use the AutoScalingReplacingUpdate attribute and specify the WaitOnResourceSignals and PauseTime properties.</p>",
            "<p>Use the UpdateReplacePolicy attribute and specify the MinSuccessfulInstancesPercent and MaxBatchSize properties.</p>",
            "<p>Use the AutoScalingRollingUpdate attribute with the MinInstancesInService property.</p>"
          ],
          "explanation": "<p>You can use the UpdatePolicy attribute to specify how AWS CloudFormation handles updates to several types of resource including Auto Scaling groups. This attribute can be configured with several properties to determine how the update process works. This is important to ensure that the correct number of instances are still available to service requests whilst the update process occurs.</p><p>The MinInstancesInService property specifies the minimum number of instances that must be in service within the Auto Scaling group while CloudFormation updates old instances. You can also specify the MaxBatchSize which specifies the maximum number of instances that CloudFormation updates.</p><p><strong>CORRECT: </strong>\"Use the AutoScalingRollingUpdate attribute with the MinInstancesInService property\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AutoScalingScheduledAction attribute with the MaxBatchSize property\" is incorrect.</p><p>This is the wrong attribute to use; the engineer must use the AutoScalingRollingUpdate attribute instead.</p><p><strong>INCORRECT:</strong> \"Use the AutoScalingReplacingUpdate attribute and specify the WaitOnResourceSignals and PauseTime properties\" is incorrect.</p><p>A rolling update should be used to specify the number of instances to update in a batch. The replacing update either replaces all instances or creates a new ASG. The properties specified are also inappropriate for the situation (check the link below for more detail).</p><p><strong>INCORRECT:</strong> \"Use the UpdateReplacePolicy attribute and specify the MinSuccessfulInstancesPercent and MaxBatchSize properties\" is incorrect.</p><p>This attribute is used to retain or, in some cases, backup the existing physical instance of a resource when it's replaced during a stack update operation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 134588459,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>You are deploying a critical web application with Elastic Beanstalk using the \u201cRolling\u201d deployment policy. Your Elastic Beanstalk environment configuration has an RDS DB instance attached to it and used by your application servers. The deployment failed when you deployed a major version. And it took even more time to rollback changes because you have to manually redeploy the old version. </p><p>Which of the following options will you implement to prevent this from happening in future deployments?</p>",
          "answers": [
            "<p>Configure <code>Rolling with additional batch</code> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</p>",
            "<p>Configure <code>All at once</code> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</p>",
            "<p>Configure <code>Immutable</code> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</p>",
            "<p>Implement a Blue/green deployment strategy in your Elastic Beanstalk environment for future deployments of your web application. Ensure that the RDS DB instance is still tightly coupled with the environment.</p>"
          ],
          "explanation": "<p><strong>Immutable</strong> environment updates are an alternative to rolling updates. Immutable environment updates ensure that configuration changes that require replacing instances are applied efficiently and safely. If an immutable environment update fails, the rollback process requires only terminating an Auto Scaling group. A failed rolling update, on the other hand, requires performing an additional rolling update to roll back the changes.</p><p>To perform an immutable environment update, Elastic Beanstalk creates a second, temporary Auto Scaling group behind your environment's load balancer to contain the new instances. First, Elastic Beanstalk launches a single instance with the new configuration in the new group. This instance serves traffic alongside all of the instances in the original Auto Scaling group that are running the previous configuration.</p><p>When the first instance passes health checks, Elastic Beanstalk launches additional instances with the new configuration, matching the number of instances running in the original Auto Scaling group. When all of the new instances pass health checks, Elastic Beanstalk transfers them to the original Auto Scaling group, and terminates the temporary Auto Scaling group and old instances.</p><p>Refer to the table below for the characteristics of each deployment method as well as the amount of time it takes to do the deployment, as seen in the <strong>Deploy Time</strong> column:</p><p><img src=\"https://media.tutorialsdojo.com/public/DeploymentMethods_2AUG2023.png\"></p><p>Hence, the correct answer is: <strong>Configure </strong><code><strong>Immutable</strong></code><strong> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application.</strong></p><p>The option that says: <strong>Configure </strong><code><strong>Rolling with additional batch</strong></code><strong> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application </strong>is incorrect because this deployment type is just similar to rolling deployments and hence, this will not help alleviate the root cause of the issue in this scenario.</p><p>The option that says: <strong>Configure </strong><code><strong>All at once</strong></code><strong> as the deployment policy in your Elastic Beanstalk environment for future deployments of your web application<em> </em></strong>is incorrect because this will cause a brief downtime during deployment and hence, this is not ideal for deploying your critical production applications.</p><p>The option that says: <strong>Implement a Blue/green deployment strategy in your Elastic Beanstalk environment for future deployments of your web application. Ensure that the RDS DB instance is still tightly coupled with the environment</strong> is incorrect because a Blue/green deployment requires that your environment runs independently of your production database. This means that you have to decouple your RDS database from your environment. If your Elastic Beanstalk environment has an attached Amazon RDS DB instance, the data will be lost if you terminate the original (blue) environment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rollingupdates.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rollingupdates.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      }
    ],
    "answers": {}
  }
]