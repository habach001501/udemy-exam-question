[
  {
    "id": "1770033194197",
    "date": "2026-02-02T11:53:14.197Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 9,
    "incorrect": 1,
    "unanswered": 0,
    "total": 10,
    "percent": 90,
    "duration": 899192,
    "questions": [
      {
        "id": 134588393,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. </p><p>As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?</p>",
          "answers": [
            "<p>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/ssm-patch-baselines.jpg\"></p><p><strong>Patch Manager</strong> uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p><strong>AWS Config</strong> provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements. Hence, the correct option is: <strong>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</strong></p><p>The option that says: <strong>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, it is stated in the scenario that the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC </strong>is incorrect because Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise, however, it does not check if your EC2 instances are using an approved AMI or not.</p><p>The option that says: <strong>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect because the AWS Shield Advanced service is more suitable in preventing DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 115961493,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company runs an application across thousands of EBS-backed Amazon EC2 instances. The company needs to ensure availability of the application and requires that instances are restarted when an EC2 instance retirement event is scheduled.</p><p>How can this a DevOps engineer automate this task?</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch alarm for EC2 status checks. Configure the alarm to trigger an Amazon SNS notification to the operations team and have them stop and start affected instances.</p>",
            "<p>Create a rule in Amazon EventBridge with AWS Health as the source and look for instance retirement scheduled events. Run an AWS Systems Manager automation document that stops and starts affected instances.</p>",
            "<p>Enable EC2 Auto Recovery on all instances. Configure an Amazon CloudWatch alarm with the alarm action set to Recover. Specify a time for recovery that is outside of business hours.</p>",
            "<p>Create a rule in Amazon EventBridge with Amazon EC2 as the source and look for EC2 instance state-change notifications that indicate the instance is shutting down. Run an AWS Systems Manager automation document that starts the affected instances.</p>"
          ],
          "explanation": "<p>An EC2 instance is scheduled for retirement when AWS detects an irreparable failure in the infrastructure that's hosting your instance. You are required to stop and then start the instance at your preferred time before the instance retirement date. Stopping and starting the instance moves the instance to another healthy host.</p><p>The best way to automate this process is to create a rule in Amazon EventBridge that looks for AWS Health events. The specific event is:</p><p>\u201cAWS_EC2_INSTANCE_RETIREMENT_SCHEDULED\u201d</p><p>When this event occurs EventBridge can trigger an AWS Systems Manager automation document that stops and starts the EC2 instances.</p><p><strong>CORRECT: </strong>\"Create a rule in Amazon EventBridge with AWS Health as the source and look for instance retirement scheduled events. Run an AWS Systems Manager automation document that stops and starts affected instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for EC2 status checks. Configure the alarm to trigger an Amazon SNS notification to the operations team and have them stop and start affected instances\" is incorrect.</p><p>Status checks do not inform us that an instance retirement event is scheduled, they let us know if there are issues that are affecting the instances or hosts.</p><p><strong>INCORRECT:</strong> \"Enable EC2 Auto Recovery on all instances. Configure an Amazon CloudWatch alarm with the alarm action set to Recover. Specify a time for recovery that is outside of business hours\" is incorrect.</p><p>Auto Recovery will recover an instance automatically, but this is not related to retirement events. You also cannot configure a time schedule in the alarm action.</p><p><strong>INCORRECT:</strong> \"Create a rule in Amazon EventBridge with Amazon EC2 as the source and look for EC2 instance state-change notifications that indicate the instance is shutting down. Run an AWS Systems Manager automation document that starts the affected instances\" is incorrect.</p><p>This would restart all instances that are shutdown, so the scope is too broad. We specifically want to target only the instances that are affected by retirement events.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-retirement/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-retirement/</a></p><p><a href=\"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html\">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 82921452,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A multi-national retail company is in the process of capturing all of its infrastructure as code using CloudFormation. The infrastructure inventory is huge and will contain a networking stack, an application stack, a data stack, and so on. Some teams are ready to move ahead with the process while others are lagging, and there is a desire to keep all the infrastructure version controlled.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?</p>\n",
          "answers": [
            "<p>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</p>",
            "<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</p>",
            "<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</p>",
            "<p>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</strong></p>\n\n<p>While using CloudFormation, you work with templates and stacks. You create templates to describe your AWS resources and their properties. When you use AWS CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's AWS CloudFormation template.</p>\n\n<p>In CloudFormation the best practice is to separate stacks into individual, separate logical components that have dependencies on each other. To link through these dependencies, the best is to use Exports and Imports. Each individual CloudFormation template must be a separate file.</p>\n\n<p>CloudFormation best practices:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</strong></p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</strong></p>\n\n<p>The issue with both these options is that different teams are working on different pieces of the infrastructure with their own timelines, so it's difficult to combine all elements of the infrastructure into a single master template. It's much better to have one template per logical element of the infrastructure that is owned by the respective team and then use outputs and exports to reference values in the stacks. Nested Stacks can be helpful if a component configuration (such as a Load Balancer) can be reused across many stacks.</p>\n\n<p><strong>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</strong> - Using outputs and exports for individual templates is much better than collaborating via pull requests at code repository level. Using individual templates gives ownership to the contributing team to make sure that the CloudFormation templates are always functional and ready to be referenced in other stacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n"
        }
      },
      {
        "id": 82921356,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A social media company is running its flagship application via an Auto-Scaling group (ASG) which has 15 EC2 instances spanning across 3 Availability Zones (AZs). The current average CPU utilization of the group sits at 15% off-peak time. During peak time, it goes all the way to 45%, and these peak times happen predictably during business hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How can you improve the instance utilization while reducing cost and maintaining application availability?</p>\n",
          "answers": [
            "<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</p>",
            "<p>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</p>",
            "<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</p>",
            "<p>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</strong></p>\n\n<p>With target tracking scaling policies, you choose a scaling metric and set a target value. Application Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value.</p>\n\n<p>Target tracking scaling policies for Amazon EC2 Auto Scaling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size that are specified by the scaling action. For the given use-case, you can create two separate scheduled actions that take care of the required minimum capacity during both peak and off-peak times.</p>\n\n<p>Here, we need a scaling policy that tracks a good CPU usage of 75% and adjusts the minimum desired capacity through scheduled actions so it doesn't disrupt the number of EC2 instances negatively at any time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</strong></p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</strong></p>\n\n<p>If a Lambda function terminates 9 instances because they're in an ASG, the desired capacity won't have changed and the ASG will re-create instances automatically. Therefore both these options are incorrect.</p>\n\n<p><strong>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</strong> - UpdatePolicy for CloudFormation cannot help define Scheduled Actions. There's a special ScheduledActions property for that.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n"
        }
      },
      {
        "id": 82921384,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.</p>\n\n<p>What can be done to improve the performance of the setup?</p>\n",
          "answers": [
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</p>",
            "<p>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong></p>\n\n<p>A golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can also add the web application as part of the golden AMI. You can think of it as an input base AMI for creating a standardized application-specific golden AMI.</p>\n\n<p>Once you create a golden AMI for a product (a product can be a standardized OS-AMI that you want to distribute to accounts in your organization or an application-specific AMI you want to let your business unit(s) deploy in their environment), you can validate whether the AMI meets your expectations, and choose to approve or reject the AMI.</p>\n\n<p>About the golden AMI pipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q66-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</strong> - The local cache warmup can unfortunately not be improved, as caching is dynamic and data may change over time. So creating an AMI with a copy of the local data cache just serves as a distractor.</p>\n\n<p><strong>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong> - You cannot migrate from ElastiCache to DynamoDB for the given use-case, as it's primarily a NoSQL database and not a caching solution (You could use DAX as a caching solution with DynamoDB). Besides, the existing database is RDS MySQL which is a relational database, so DynamoDB does not really fit into this mix.</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</strong> - You cannot use Lambda to configure the instance local cache at boot time as caching is dynamic and data may change over time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n"
        }
      },
      {
        "id": 82921440,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.</p>\n\n<p>Which of the following options would you suggest? (Select two)</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</p>",
            "<p>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</p>",
            "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>",
            "<p>Create a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p><strong>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>Understand how a pipeline execution state change rule works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Here we are only interested in pipeline failures, so we need to choose <code>CodePipeline Pipeline Execution State Change</code>.\nFinally, CloudWatch Event rules do not support Slack as a target, therefore we must create a Lambda function for it.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i2.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</strong> - CloudWatch Event rules do not support Slack as a target, so this option is incorrect.</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>**\nCreate a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>Here we are only interested in pipeline failures, so we just need to choose <code>CodePipeline Pipeline Execution State Change</code>. Therefore both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n"
        }
      },
      {
        "id": 82921396,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at an e-commerce company is working with the in-house security team to improve the security workflow of the code release process. The DevOps team would like to initiate a 3rd party code vulnerability analysis tool for every push done to code in your CodeCommit repository. The code has to be sent via an external API.</p>\n\n<p>As an AWS Certified DevOps Engineer, how would you implement this most efficiently?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an AWS Lambda function that will request the code from CodeCommit, zip it and send it to the 3rd party API</p>",
            "<p>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an S3 bucket so that the code will be automatically zipped into S3. Create an S3 Event rule to trigger a Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API</p>",
            "<p>Create a CloudWatch Event rule on a schedule of 5 minutes that triggers a Lambda function that will check for new commits done on your CodeCommit repository. If new commits are detected, download and zip the code and then send it to the 3rd party API</p>",
            "<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an AWS Lambda function that will request the code from CodeCommit, zip it and send it to the 3rd party API</strong></p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p>\n\n<p>CloudWatch Events Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>For the given use-case, you can set up a CloudWatch Event rule for every push to the CodeCommit repository that would trigger the Lambda function configured as a target. The Lambda function would in turn request the code from CodeCommit, zip it and send it to the 3rd party API.</p>\n\n<p>CloudWatch Events Configuration:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i2.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an S3 bucket so that the code will be automatically zipped into S3. Create an S3 Event rule to trigger a Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API</strong> - CloudWatch Event Rules cannot have S3 buckets as a target. Although you can set an S3 trigger as a target, eventually you would still need to invoke the Lambda function via an S3 trigger to process the code via the API. Therefore it's efficient to directly invoke the Lambda function from the CloudWatch Event rule.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i3.jpg\"></p>\n\n<p><strong>Create a CloudWatch Event rule on a schedule of 5 minutes that triggers a Lambda function that will check for new commits done on your CodeCommit repository. If new commits are detected, download and zip the code and then send it to the 3rd party API</strong> - CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So this option is ruled out.</p>\n\n<p><strong>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected</strong> - The EC2 CodeCommit hook is a distractor and does not exist.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n"
        }
      },
      {
        "id": 99528201,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>An eCommerce company has operations in several countries around the world. The company runs an application in co-location facilities that uses Linux servers and a relational database running on MySQL. The application will be migrated to AWS and will include Amazon EC2 instances behind an Application Load Balancer in multiple AWS Regions. The database configuration has not yet been finalized.</p><p>A DevOps engineer has been asked to assist with determining the best solution for the database. The data includes product catalog information which must be served with low latency and customer purchase information which should be kept within each Region for compliance purposes.</p><p>Which database solution should the DevOps engineer recommend to meet these requirements with the LEAST changes to the application?</p>",
          "answers": [
            "<p>Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer purchase data.</p>",
            "<p>Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer purchase data.</p>",
            "<p>Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer purchase data.</p>",
            "<p>Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer purchase data.</p>"
          ],
          "explanation": "<p>The current database runs on MySQL which is a relational database. Therefore, to meet the requirement to minimize changes to the application the cloud solution for the database should also be a relational database. Otherwise significant changes to the application may be required. This rules out all answers that include DynamoDB as that service is a non-relational database.</p><p>Amazon Aurora provides read replicas for scaling read performance horizontally. These replicas can be within a Region or across Regions. The product catalog data can be provided at low latency within each AWS Region using cross-Region read replicas.</p><p>For the customer purchase data, this can be kept within each Region by implementing local Aurora database instances which can additionally have read replicas within the Region if additional read performance is required.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_22-38-09-545c686c23e14af309338aad4a3b523c.jpg\"><p><strong>CORRECT: </strong>\"Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer purchase data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer purchase data\" is incorrect.</p><p>RedShift is used for online analytics processing (OLAP) use cases as it is a data warehouse solution. In this case the solution calls for an online transaction processing (OLTP) type of database as it is processing transactions.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer purchase data\" is incorrect.</p><p>DynamoDB is a non-relational database, and the application is designed to work with a relational database. Using DynamoDB would require significant changes to the application.</p><p><strong>INCORRECT:</strong> \"Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer purchase data\" is incorrect.</p><p>As above, DynamoDB should not be used for this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>"
        }
      },
      {
        "id": 134588437,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A financial company has a total of over a hundred Amazon EC2 instances running across their development, testing, and production environments in AWS. Based on a recent IT review, the company initiated a new compliance rule that mandates a monthly audit of every Linux and Windows EC2 instances check for system performance issues. Each instance must have a logging function that collects various system details and retrieve custom metrics from installed applications or services. The DevOps team will periodically review these logs and analyze their contents using AWS Analytics tools, and the result will be stored in an S3 bucket. </p><p>Which is the MOST recommended way to collect and analyze logs from the instances with MINIMAL effort?</p>",
          "answers": [
            "<p>Configure and install the unified CloudWatch Logs agent in each Amazon EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
            "<p>Configure and install AWS SDK in each Amazon EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and analyze the log data of all instances using CloudWatch Logs Insights.</p>",
            "<p>Configure and install the AWS Systems Manager Agent (SSM Agent) in each EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>",
            "<p>Configure and install AWS Inspector Agent in each Amazon EC2 instance that will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all EC2 instances.</p>"
          ],
          "explanation": "<p>To collect logs from your Amazon EC2 instances and on-premises servers into CloudWatch Logs, AWS offers both a new unified CloudWatch agent, and an older CloudWatch Logs agent. It is recommended to use the unified CloudWatch agent, which has the following advantages:</p><p>- You can collect both logs and advanced metrics with the installation and configuration of just one agent.</p><p>- The unified agent enables the collection of logs from servers running Windows Server.</p><p>- If you are using the agent to collect CloudWatch metrics, the unified agent also enables the collection of additional system metrics, for in-guest visibility.</p><p>- The unified agent provides better performance.</p><p><img src=\"https://media.tutorialsdojo.com/public/LogsInsights-workflow_6AUG2023.png\"></p><p>CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you quickly and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p><p>CloudWatch Logs Insights includes a purpose-built query language with a few simple but powerful commands. CloudWatch Logs Insights provides sample queries, command descriptions, query autocompletion, and log field discovery to help you get started quickly. Sample queries are included for several types of AWS service logs.</p><p>Hence, the correct answer is: <strong>Configure and install the unified CloudWatch Logs agent in each Amazon EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</strong></p><p>The option that says: <strong>Configure and install AWS SDK in each Amazon EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and analyze the log data of all instances using CloudWatch Logs Insights</strong> is incorrect. Although this is a valid solution, this entails a lot of effort to implement as you have to allocate time to install the AWS SDK to each instance and develop a custom monitoring solution. Remember that the question is specifically looking for a solution that can be implemented with minimal effort. In addition, it is unnecessary and not cost-efficient to enable detailed monitoring in CloudWatch in order to meet the requirements of this scenario since this can be done using CloudWatch Logs.</p><p>The option that says: <strong>Configure and install the AWS Systems Manager Agent (SSM Agent) in each EC2 instance that will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights</strong> is incorrect. Although this is also a valid solution, it is more efficient to use CloudWatch agent than an SSM agent. Manually connecting to an instance to view log files and troubleshoot an issue with SSM Agent is time-consuming hence, for more efficient instance monitoring, you can use the CloudWatch Agent instead to send the log data to Amazon CloudWatch Logs.</p><p>The option that says: <strong>Configure and install AWS Inspector Agent in each Amazon EC2 instance that will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all EC2 instances </strong>is incorrect because AWS Inspector is simply a security assessment service that only helps you in checking for unintended network accessibility of your EC2 instances and for vulnerabilities on those EC2 instances. Furthermore, setting up an Amazon CloudWatch dashboard is not suitable since it's primarily used for scenarios where you have to monitor your resources in a single view, even those resources that are spread across different AWS Regions. It is better to use CloudWatch Logs Insights instead since it enables you to interactively search and analyze your log data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html \">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>CloudWatch Agent vs SSM Agent vs Custom Daemon Scripts:</strong></p><p><a href=\"https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/?src=udemy\">https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/</a></p>"
        }
      },
      {
        "id": 75949098,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company is concerned about the security of their Amazon EC2 instances. They require an automated solution for identifying security vulnerabilities on the instances and notifying the security team. They also require an audit trail of all login activities on the EC2 instances.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Kinesis Client Library (KCL) to capture system logs and save them to an Amazon S3 bucket.</p>",
            "<p>Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture an audit trail using system logs and view login activity in the AWS CloudTrail console.</p>",
            "<p>Use AWS GuardDuty to detect vulnerabilities on the EC2 instances. Configure the AWS X-Ray daemon to gather trace data and add metrics to Amazon CloudWatch. View the audit trail of login activities in the CloudWatch console.</p>",
            "<p>Use Amazon Inspector to automatically detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and upload them to Amazon CloudWatch Logs.</p>"
          ],
          "explanation": "<p>Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. This is the best service to use for automatic detection of security vulnerabilities on the EC2 instances.</p><p>The unified CloudWatch agent enables you to collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances.</p><p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs. The system logs that are collected will include information on all login activities on the EC2 instances.</p><p><strong>CORRECT: </strong>\"Use Amazon Inspector to automatically detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and upload them to Amazon CloudWatch Logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Kinesis Client Library (KCL) to capture system logs and save them to an Amazon S3 bucket\" is incorrect.</p><p>Systems Manager Patch Manager can install patches to resolve vulnerabilities but does not provide automated detection of vulnerabilities. Instead, it scans to see if specific patches are installed or not. The KCL is used with Kinesis Data Streams for processing streaming data and does not collect system logs from EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture an audit trail using system logs and view login activity in the AWS CloudTrail console\" is incorrect.</p><p>As above, Systems Manager is not suitable for this task and does not capture auditing information by processing system logs.</p><p><strong>INCORRECT:</strong> \"Use AWS GuardDuty to detect vulnerabilities on the EC2 instances. Configure the AWS X-Ray daemon to gather trace data and add metrics to Amazon CloudWatch. View the audit trail of login activities in the CloudWatch console\" is incorrect.</p><p>GuardDuty is a threat detection service that monitors for malicious activity. It does not detect vulnerabilities on EC2 instances. The X-Ray service is used for gathering trace data for troubleshooting and understanding application performance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/inspector/features/\">https://aws.amazon.com/inspector/features/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-inspector/\">https://digitalcloud.training/amazon-inspector/</a></p>"
        }
      }
    ],
    "answers": {
      "75949098": [
        "d"
      ],
      "82921356": [
        "a"
      ],
      "82921384": [
        "b"
      ],
      "82921396": [
        "a"
      ],
      "82921440": [
        "a",
        "b"
      ],
      "82921452": [
        "a"
      ],
      "99528201": [
        "c"
      ],
      "115961493": [
        "b"
      ],
      "134588393": [
        "d"
      ],
      "134588437": [
        "a"
      ]
    }
  },
  {
    "id": "1770030376250",
    "date": "2026-02-02T11:06:16.250Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 17,
    "incorrect": 3,
    "unanswered": 0,
    "total": 20,
    "percent": 85,
    "duration": 5395989,
    "questions": [
      {
        "id": 138248229,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A multinational investment bank is implementing regulatory compliance checks over its AWS accounts. All API calls made on each AWS resource across the company's accounts must be monitored and tracked for auditing purposes. AWS CloudTrail will be used to monitor all API activities and detect sensitive security issues in the company's AWS accounts. The DevOps Team was assigned to come up with a solution to prevent CloudTrail from being disabled on some AWS accounts automatically.</p><p>What solution should be applied to ensure CloudTrail log deliveries experience the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of <code>Configuration changes</code>. By default, this managed rule will automatically remediate the accounts that disabled its CloudTrail.</p>",
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a <code>StopLogging</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>StartLogging</code> API on the resource ARN.</p>",
            "<p>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications.</p>",
            "<p>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a <code>DeleteTrail</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>CreateTrail</code> API on the resource ARN.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides <em>AWS managed rules</em>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly assess whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p><strong><img src=\"https://media.tutorialsdojo.com/public/td-aws-config-diagram-13Jan2025.png\"></strong>You can customize the behavior of a managed rule to suit your needs. For example, you can define the rule's scope to constrain which resources trigger an evaluation for the rule, such as EC2 instances or volumes. You can customize the rule's parameters to define attributes that your resources must have to comply with the rule. For example, you can customize a parameter to specify that your security group should block incoming traffic to a specific port number.</p><p>After you activate a rule, AWS Config compares your resources to the rule's conditions. After this initial evaluation, AWS Config continues to run evaluations each time one is triggered. The evaluation triggers are defined as part of the rule, and they can include the following types:</p><p><strong>Configuration changes</strong> \u2013 AWS Config triggers the evaluation when any resource that matches the rule's scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p><p><strong>Periodic</strong> \u2013 AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p><p>The cloudtrail-enabled checks whether AWS CloudTrail is enabled in your AWS account. Optionally, you can specify which S3 bucket, SNS topic, and Amazon CloudWatch Logs ARN to use.</p><p><img src=\"https://media.tutorialsdojo.com/aws-config-cloudtrail-enabled.JPG\"></p><p>Hence, the correct answer is: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a </strong><code><strong>StopLogging</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>StartLogging</strong></code><strong> API on the resource ARN.</strong></p><p>The option that says: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of </strong><code><strong>Configuration changes</strong></code><strong>. This managed rule will automatically remediate the accounts that disabled its CloudTrail </strong>is incorrect because, by default, AWS Config will not automatically remediate the accounts that disabled its CloudTrail. You must manually set this up using an Amazon EventBridge rule and a custom Lambda function that calls the StartLogging API to enable CloudTrail back again. Furthermore, the <code><strong>cloudtrail-enabled</strong></code> AWS Config managed rule is only available for the <code>periodic trigger</code> type and not <code>Configuration changes</code>.</p><p>The option that says: <strong>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications</strong> is incorrect. AWS Cloud Development Kit (AWS CDK) is only an open-source software development framework for building cloud applications and infrastructure using programming languages. It isn't used to check whether the CloudTrail is enabled in an AWS account.</p><p>The option that says: <strong>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a </strong><code><strong>DeleteTrail</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>CreateTrail</strong></code><strong> API on the resource ARN</strong> is incorrect. Instead, you should detect the <code>StopLogging</code> event and call the StartLogging API to enable CloudTrail again. The <code>DeleteTrail</code> and <code>CreateTrail</code> events, as their name implies, are simply for deleting and creating the trails respectively.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/\">https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/v2/guide/home.html\">https://docs.aws.amazon.com/cdk/v2/guide/home.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 75949112,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A startup company is launching a web application on AWS that uses Amazon EC2 instances behind an Application Load Balancer. The EC2 instances will store data in an Amazon RDS database and an Amazon DynamoDB table. The DevOps team require separate environments for development, testing, and production.</p><p>What is the MOST secure method of obtaining password credentials?</p>",
          "answers": [
            "<p>Configure access keys to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.</p>",
            "<p>Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Retrieve the database credentials from AWS Secrets Manager.</p>",
            "<p>Configure access keys to access AWS services. Retrieve the database credentials from an AWS Secrets Manager SecretString parameter.</p>",
            "<p>Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Store the database passwords in an encrypted config file in system metadata.</p>"
          ],
          "explanation": "<p>The most secure solution is to use a combination of an AWS IAM Instance Profile with a policy attached providing permissions to Amazon DynamoDB and AWS Secrets Manager for retrieving SecretString parameters for the Amazon RDS DB.</p><p>Instance profiles are far more secure compared to access keys are they leverage the AWS STS service to obtain temporary security credentials. No security credentials are stored on the EC2 instances when using this method.</p><p>For Amazon RDS you may need to use database connection credentials for authentication depending on your configuration. If this is the case you can securely store these credentials as SecretString (encrypted) parameters in AWS Secrets Manager. Your application can then issue API calls to Secrets Manager to securely retrieve the encrypted parameters when authentication is needed.</p><p><strong>CORRECT: </strong>\"Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Retrieve the database credentials from AWS Secrets Manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Store the database passwords in an encrypted config file in system metadata\" is incorrect.</p><p>Database passwords should not be stored in system metadata as this would be very insecure.</p><p><strong>INCORRECT:</strong> \"Configure access keys to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter\" is incorrect.</p><p>Access keys are less secure compared to using instance profiles as with access keys the actual access key ID and secret access key are stored in plaintext on the EC2 instance file system.</p><p><strong>INCORRECT:</strong> \"Configure access keys to access AWS services. Retrieve the database credentials from an AWS Secrets Manager SecretString parameter\" is incorrect.</p><p>As above, access keys should not be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html</a></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_CreateSecret.html\">https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_CreateSecret.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>"
        }
      },
      {
        "id": 143860765,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is troubleshooting problems with an application which uses AWS Lambda to process messages in an Amazon SQS standard queue. The function sometimes fails to process the messages in the queue. The engineer needs to analyze the events to determine the cause of the issue and update the function code.</p><p>Which action should the engineer take to achieve this outcome?</p>",
          "answers": [
            "<p>Enable FIFO support for the queue to preserve ordering of the messages.</p>",
            "<p>Configure a redrive policy to move the messages to a dead-letter queue.</p>",
            "<p>Enable long-polling by increasing WaitTimeSeconds parameter.</p>",
            "<p>Configure a delay queue by increasing the DelaySeconds parameter.</p>"
          ],
          "explanation": "<p>You can configure a dead-letter queue (DLQ) by specifying a redrive policy on the SQS queue. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p><p><strong>CORRECT: </strong>\"Configure a redrive policy to move the messages to a dead-letter queue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable FIFO support for the queue to preserve ordering of the messages\" is incorrect.</p><p>You cannot enable FIFO on a standard queue, and it does not help with this issue anyway.</p><p><strong>INCORRECT:</strong> \"Enable long-polling by increasing WaitTimeSeconds parameter\" is incorrect.</p><p>Long polling just helps with efficiency of API calls as it waits for messages to appear in the queue rather than returning an immediate response. This does not assist with isolating messages for analysis.</p><p><strong>INCORRECT:</strong> \"Configure a delay queue by increasing the DelaySeconds parameter\" is incorrect.</p><p>A delay queue simply delays visibility of the messages for a specified time. This does not assist with this issue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>"
        }
      },
      {
        "id": 138248237,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The Development team of a leading IT consultancy company would like to add a manual approval action before their new application versions are deployed to their production environment. The approval action must be strictly enforced even if the unit and integration tests are all successful. They have set up a pipeline using CodePipeline to orchestrate the workflow of their continuous integration and continuous delivery processes. The new versions of the application are built using CodeBuild and are deployed to a fleet of Amazon EC2 instances using CodeDeploy. </p><p>Which of the following provides the SIMPLEST and the MOST cost-effective solution?</p>",
          "answers": [
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</p>",
            "<p>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>",
            "<p>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</p>"
          ],
          "explanation": "<p>You can automate your release process by using AWS CodePipeline to test your code and run your builds with CodeBuild. You can create reports in CodeBuild that contain details about tests that are run during builds.</p><p>You can create tests such as unit tests, configuration tests, and functional tests. The test file format can be JUnit XML or Cucumber JSON. Create your test cases with any test framework that can create files in one of those formats (for example, Surefire JUnit plugin, TestNG, and Cucumber). To create a test report, you add a report group name to the buildspec file of a build project with information about your test cases. When you run the build project, the test cases are run and a test report is created. You do not need to create a report group before you run your tests. If you specify a report group name, CodeBuild creates a report group for you when you run your reports. If you want to use a report group that already exists, you specify its ARN in the buildspec file.</p><p>In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p><img src=\"https://media.tutorialsdojo.com/public/pipeline.png\"></p><p>If the action is approved, the pipeline execution resumes. If the action is rejected\u2014or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping\u2014the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p>Hence, the correct answer is: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline.</strong></p><p>The option that says: <strong>In CodeBuild, add the required actions to automatically do the unit and integration tests. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a custom action with a corresponding custom job worker that performs the approval action. Inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. You can just simply set up a manual approval action instead of creating a custom action. That takes a lot of effort to configure including the development of a custom job worker.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using AWS Step Functions. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. It is tedious to automatically perform the unit and integration tests using AWS Step Functions. You can just use CodeBuild to handle all of the tests.</p><p>The option that says: <strong>Add the required action steps to automatically do the unit and integration tests using a third-party CI/CD Tool such as GitLab or Jenkins hosted in Amazon EC2. After the last deploy action of the pipeline, set up a test action to verify the application's functionality. Mark the action as successful if all of the tests have been successfully passed. Create a manual approval action and inform the team of the stage being triggered using SNS. Add a deploy action to deploy the app to the next stage at the end of the pipeline</strong> is incorrect. This solution entails an additional burden to install, configure and launch a third-party CI/CD tool in Amazon EC2. A more simple solution is to just use CodeBuild for tests.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html#how-to-create-pipeline-add-test</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p>"
        }
      },
      {
        "id": 75949064,
        "correct_response": [
          "b",
          "d",
          "e"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer builds an artifact locally and then uploads it to an Amazon S3 bucket. The application has a local cache that must be cleared as part of the deployment. The engineer executes a command to do this, retrieves the artifact from Amazon S3, and unzips the artifact to complete the deployment.</p><p>The engineer wants to migrate to an automated CI/CD solution and incorporate checks to stop and roll back the deployment in the event of a failure. This requires tracking the progression of the deployment.</p><p>Which combination of actions will accomplish this? (Select THREE.)</p>",
          "answers": [
            "<p>Check the code into a code repository. On each pull into master use Amazon CloudWatch Events to trigger an AWS Lambda function that builds the artifact and uploads it to Amazon S3.</p>",
            "<p>Write a custom script that clears the cache and specify the script in the BeforeInstall lifecycle hook in the AppSpec file.</p>",
            "<p>Add user data to the Amazon EC2 instances that contains script to clear the cache. Once deployed, test the application. If it is not successful, deploy it again.</p>",
            "<p>Set up an AWS CodePipeline pipeline to deploy the application. Check the code into a code repository as a source for the pipeline.</p>",
            "<p>Use AWS CodeBuild to build the artifact and upload it to Amazon S3. Use AWS CodeDeploy to deploy the artifact to the Amazon EC2 instances.</p>",
            "<p>Use AWS Systems Manager to download the artifact from Amazon S3 and deploy it to all the EC2 instances.</p>"
          ],
          "explanation": "<p>The engineer wants to build an automated CI/CD pipeline. Therefore, the best solution is to use a code repository such as CodeCommit for committing the code. Once committed a CodePipeline will automatically pick up the changes and initiate CodeBuild which will build the artifacts and upload the S3.</p><p>After the build artifact has been uploaded CodeDeploy can then be used to deploy the application. The AppSpec file is used by CodeDeploy during deployments. The engineer should add the script to clear the cache to the BeforeInstall lifecycle hook, so it is executed before the install occurs.</p><p><strong>CORRECT: </strong>\"Write a custom script that clears the cache and specify the script in the BeforeInstall lifecycle hook in the AppSpec file\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Set up an AWS CodePipeline pipeline to deploy the application. Check the code into a code repository as a source for the pipeline\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS CodeBuild to build the artifact and upload it to Amazon S3. Use AWS CodeDeploy to deploy the artifact to the Amazon EC2 instances\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check the code into a code repository. On each pull into master use Amazon CloudWatch Events to trigger an AWS Lambda function that builds the artifact and uploads it to Amazon S3\" is incorrect.</p><p>A better solution is to use CodePipeline which is designed for automating CI/CD pipelines and CodeBuild for building the artifact.</p><p><strong>INCORRECT:</strong> \"Add user data to the Amazon EC2 instances that contains script to clear the cache. Once deployed, test the application. If it is not successful, deploy it again\" is incorrect.</p><p>User data only runs when the instance is first started so is not useful for running any commands after that time.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to download the artifact from Amazon S3 and deploy it to all of the EC2 instances\" is incorrect.</p><p>Systems Manager is not suitable for deploying application updates and CodeDeploy should be used instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 138248181,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company would like to set up an audit process to ensure that the enterprise application is running exclusively on Amazon EC2 Dedicated Hosts. The company is also concerned about the increasing costs of its application software licensing from its third-party vendor. To meet the compliance requirement, a DevOps Engineer must create a workflow to audit the enterprise applications hosted in its Amazon VPC.</p><p>Which of the following options should the Engineer implement to satisfy the requirement with the LEAST administrative overhead?</p>",
          "answers": [
            "<p>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the <code>PutComplianceItems</code> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the <code>ListComplianceSummaries</code> API action.</p>",
            "<p>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the <code>inspector-scheduled-run</code> blueprint.</p>",
            "<p>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the <code>config-rule-change-triggered</code> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</p>",
            "<p>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data.</p>"
          ],
          "explanation": "<p>You can use <strong>AWS Config</strong> to record configuration changes for Dedicated Hosts, and instances that are launched, stopped, or terminated on them. You can then use the information captured by AWS Config as a data source for license reporting.</p><p>AWS Config records configuration information for Dedicated Hosts and instances individually and pairs this information through relationships. There are three reporting conditions:</p><p>- AWS Config recording status \u2014 When On, AWS Config is recording one or more AWS resource types, which can include Dedicated Hosts and Dedicated Instances. To capture the information required for license reporting, verify that hosts and instances are being recorded with the following fields.</p><p>- Host recording status \u2014 When Enabled, the configuration information for Dedicated Hosts is recorded.</p><p>- Instance recording status \u2014 When Enabled, the configuration information for Dedicated Instances is recorded.</p><p>If any of these three conditions are disabled, the icon in the Edit Config Recording button is red. To derive the full benefit of this tool, ensure that all three recording methods are enabled. When all three are enabled, the icon is green. To edit the settings, choose Edit Config Recording. You are directed to the <em>Set up AWS Config </em>page in the AWS Config console, where you can set up AWS Config and start recording for your hosts, instances, and other supported resource types. AWS Config records your resources after it discovers them, which might take several minutes.</p><p>After AWS Config starts recording configuration changes to your hosts and instances, you can get the configuration history of any host that you have allocated or released and any instance that you have launched, stopped, or terminated. For example, at any point in the configuration history of a Dedicated Host, you can look up how many instances are launched on that host, along with the number of sockets and cores on the host. For any of those instances, you can also look up the ID of its Amazon Machine Image (AMI). You can use this information to report on licensing for your own server-bound software that is licensed per-socket or per-core.</p><p>You can view configuration histories in any of the following ways.</p><p>- By using the AWS Config console. For each recorded resource, you can view a timeline page, which provides a history of configuration details. To view this page, choose the gray icon in the Config Timeline column of the Dedicated Hosts page.</p><p>- By running AWS CLI commands. First, you can use the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/configservice/list-discovered-resources.html\">list-discovered-resources</a> command to get a list of all hosts and instances. Then, you can use the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/configservice/get-resource-config-history.html#get-resource-config-history\">get-resource-config-history</a> command to get the configuration details of a host or instance for a specific time interval.</p><p>- By using the AWS Config API in your applications. First, you can use the <a href=\"https://docs.aws.amazon.com/config/latest/APIReference/API_ListDiscoveredResources.html\">ListDiscoveredResources</a> action to get a list of all hosts and instances. Then, you can use the <a href=\"https://docs.aws.amazon.com/config/latest/APIReference/API_GetResourceConfigHistory.html\">GetResourceConfigHistory</a> action to get the configuration details of a host or instance for a specific time interval.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-AWS-Config-Status-02-05-2025.png\"></p><p>Hence, the correct answer is: <strong>Record configuration changes for Amazon EC2 Instances and Dedicated Hosts by turning on the AWS Config Recording option in AWS Config. Set up a custom AWS Config rule that triggers an AWS Lambda function by using the </strong><code><strong>config-rule-change-triggered</strong></code><strong> blueprint. Customize the predefined evaluation logic to verify host placement to return a NON_COMPLIANT result whenever the Amazon EC2 instance is not running on a Dedicated Host. Use the AWS Config report to address noncompliant Amazon EC2 instances.</strong></p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts using the AWS Systems Manager Configuration Compliance. Utilize the </strong><code><strong>PutComplianceItems</strong></code><strong> API action to scan and populate a collection of noncompliant Amazon EC2 instances based on their host placement configuration. Store these instance IDs to Systems Manager Parameter Store and generate a report by calling the </strong><code><strong>ListComplianceSummaries</strong></code><strong> API action </strong>is incorrect because the AWS Systems Manager Configuration Compliance service is primarily used to scan your fleet of managed instances for patch compliance and configuration inconsistencies. A better solution is to use AWS Config to record the status of your Dedicated Hosts.</p><p>The option that says: <strong>Install the Amazon Inspector agent to all Amazon EC2 instances. Record configuration changes for Dedicated Hosts by using Amazon Inspector. Set up an automatic assessment runs through an AWS Lambda Function by using the </strong><code><strong>inspector-scheduled-run</strong></code><strong> blueprint</strong> is incorrect because Amazon Inspector is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not capable of recording the status of your EC2 instances nor detect if they are configured as a Dedicated Host.</p><p>The option that says: <strong>Record configuration changes for Dedicated Hosts by AWS CloudTrail. Filter all Amazon EC2 RunCommand API actions in the logs to detect any changes to the instances. Analyze the host placement of the instance using an AWS Lambda function and store the instance IDs of noncompliant resources in an Amazon S3 bucket. Generate a report by using Amazon Athena to query the Amazon S3 data </strong>is incorrect. Although this may be a possible solution, it entails a lot of administrative effort in comparison to just using AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html \">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom \">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-compliance-about.html#sysman-compliance-custom</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/\">https://aws.amazon.com/blogs/aws/now-available-ec2-dedicated-hosts/</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and AWS Config Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-config/\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 75949148,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer needs to stream data from Amazon CloudWatch Logs to a VPC-based Amazon OpenSearch Service cluster. The DevOps engineer creates an OpenSearch subscription filter for the OpenSearch cluster. However, the DevOps engineer notices that the logs are not visible in Amazon OpenSearch.</p><p>What should the DevOps engineer do to solve this problem?</p>",
          "answers": [
            "<p>Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role.</p>",
            "<p>Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch.</p>",
            "<p>Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role.</p>",
            "<p>Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch.</p>"
          ],
          "explanation": "<p>The configuration requires that the Lambda IAM execution role that is specified in the CloudWatch Logs configuration must have the trust relationship configured to allow lambda.amazonaws.com to assume the role. The AWSLambdaVPCAccessExecutionRole policy must also be added to the function to allow access to the VPC-based OpenSearch cluster.</p><p><strong>CORRECT: </strong>\"Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role\" is incorrect.</p><p>The trust relationship is defined in the role attached to the CloudWatch Logs configuration.</p><p><strong>INCORRECT:</strong> \"Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch\" is incorrect.</p><p>This policy should be attached to the Lambda IAM execution role, not the OpenSearch cluster.</p><p><strong>INCORRECT:</strong> \"Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch\" is incorrect.</p><p>The subscription filter is used instead of using an export which is a better and more automated solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/\">https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-opensearch/\">https://digitalcloud.training/amazon-opensearch/</a></p>"
        }
      },
      {
        "id": 75949146,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
          "answers": [
            "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
            "<p>The new AZ has not been enabled for the ALB.</p>",
            "<p>The AMI has not been updated to include the new AZ.</p>",
            "<p>There are no subnets associated with the new AZ.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 134588451,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A custom web dashboard has been developed for the company that displays all instances running on AWS, including the details of each instance. The web application relies on an Amazon DynamoDB table that is updated whenever a new instance is created or terminated. Several auto-scaling groups of Amazon EC2 instances are in use, and there is a need for an effective method to update the DynamoDB table whenever an instance is created or terminated.</p><p>Which of the following steps should be implemented?</p>",
          "answers": [
            "<p>Create a custom script that runs on the instances that will run on scale-in and scale-out events to update the DynamoDB table with the necessary details.</p>",
            "<p>Configure an Amazon EventBridge target for your auto scaling Group that will trigger an AWS Lambda function when a lifecycle action occurs. Configure the function to update the DynamoDB table with the necessary instance details.</p>",
            "<p>Configure an Amazon CloudWatch alarm that will monitor the number of instances on your auto scaling group and trigger an AWS Lambda function to update the DynamoDB table with the necessary details.</p>",
            "<p>Define an AWS Lambda function as a notification target for the lifecycle hook for the auto scaling group. In the event of a scale-out or scale-in, the lifecycle hook will trigger the Lambda function to update the DynamoDB table with the necessary details.</p>"
          ],
          "explanation": "<p>Adding lifecycle hooks to your Auto Scaling group gives you greater control over how instances launch and terminate. Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them.</p><p>For example, your newly launched instance completes its startup sequence, and a lifecycle hook pauses the instance. While the instance is in a wait state, you can install or configure software on it, making sure that your instance is fully ready before it starts receiving traffic. For another example of the use of lifecycle hooks, when a scale-in event occurs, the terminating instance is first deregistered from the load balancer (if the Auto Scaling group is being used with Elastic Load Balancing). Then, a lifecycle hook pauses the instance before it is terminated. While the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p><p><img alt=\"Auto Scaling group - Lifecycle hooks\" height=\"636\" src=\"https://media.tutorialsdojo.com/public/lifecycle_hooks_2AUG2023.png\" width=\"1000\"></p><p>After you add lifecycle hooks to your Auto Scaling group, the workflow is shown as follows:</p><ol><li><p>The Auto Scaling group responds to scale-out events by launching instances and scale-in events by terminating instances.</p></li><li><p>The lifecycle hook puts the instance into a wait state (<code>Pending:Wait</code> or <code>Terminating:Wait</code>). The instance is paused until you continue or the timeout period ends.</p></li><li><p>You can perform a custom action using one or more of the following options:</p></li></ol><p>- Define an Amazon EventBridge (CloudWatch Events) target to invoke a Lambda function when a lifecycle action occurs. The Lambda function is invoked when Amazon EC2 Auto Scaling submits an event for a lifecycle action to EventBridge. The event contains information about the instance that is launching or terminating, and a token that you can use to control the lifecycle action.</p><p>- Define a notification target for the lifecycle hook. Amazon EC2 Auto Scaling sends a message to the notification target. The message contains information about the instance that is launching or terminating and a token that you can use to control the lifecycle action.</p><p>- Create a script that runs on the instance as the instance starts. The script can control the lifecycle action using the ID of the instance on which it runs.</p><p>4. By default, the instance remains in a wait state for one hour, and then the Auto Scaling group continues the launch or terminate process (<code>Pending:Proceed</code> or <code>Terminating:Proceed</code>). If you need more time, you can restart the timeout period by recording a heartbeat. If you finish before the timeout period ends, you can complete the lifecycle action, which continues the launch or termination process.</p><p>Hence, the correct answer is: <strong>Configure an Amazon EventBridge target for your auto scaling Group that will trigger an AWS Lambda function when a lifecycle action occurs. Configure the function to update the DynamoDB table with the necessary instance details.</strong></p><p>The option that says: <strong>Create a custom script that runs on the instances that will run on scale-in and scale-out events to update the DynamoDB table with the necessary details </strong>is incorrect. Although this is possible, it will be harder to execute since a custom script needs to be developed first and will be run only at instance creation and termination.</p><p>The option that says: <strong>Define an AWS Lambda function as a notification target for the lifecycle hook for the auto scaling group. In the event of a scale-out or scale-in, the lifecycle hook will trigger the Lambda function to update the DynamoDB table with the necessary details</strong> is incorrect because you cannot use AWS Lambda as a notification target for the lifecycle hook of an Auto Scaling group. You can only configure Amazon EventBridge, Amazon SNS, or Amazon SQS as notification targets.</p><p>The option that says: <strong>Configure an Amazon CloudWatch alarm that will monitor the number of instances on your auto scaling group and trigger an AWS Lambda function to update the DynamoDB table with the necessary details</strong> is incorrect because this option doesn't track the lifecycle action of the Auto Scaling group. In this scenario, it's better to create a lifecycle hook integrated with Amazon EventBridge and Lambda instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#adding-lifecycle-hooks \">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#adding-lifecycle-hooks</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-overview\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-overview</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 82921334,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.</p>\n\n<p>How can you do this most securely?</p>\n",
          "answers": [
            "<p>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</p>",
            "<p>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>",
            "<p>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</p>",
            "<p>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</strong></p>\n\n<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.</p>\n\n<p>You can use Secrets Manager to natively rotate credentials for Amazon Relational Database Service (RDS), Amazon DocumentDB, and Amazon Redshift. You can extend Secrets Manager to rotate other secrets, such as credentials for Oracle databases hosted on EC2 or OAuth refresh tokens, by modifying sample AWS Lambda functions available in the Secrets Manager documentation.</p>\n\n<p>To access PostgreSQL, you can use database credentials and they're best stored in Secrets Manager from a security best-practices perspective. Access to Secrets Manager itself is regulated using an IAM role with the requisite policy. You must write this IAM policy permitting your application on EC2 instances to access specific secrets. Then, in the application source code, you can replace secrets in plain text with code to retrieve these secrets programmatically using the Secrets Manager APIs. To access the DynamoDB table, you should also add the appropriate policy to this IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - As mentioned in the explanation above, Secrets Manager does NOT support DynamoDB, so this option is incorrect.</p>\n\n<p><strong>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</strong> - It's not recommended to store RDS credentials in a DynamoDB, as it can be accessed by everyone who has access to the underlying table. This constitutes a serious security threat.</p>\n\n<p><strong>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - Storing IAM user credentials in Secrets Manager is a distractor as IAM user credentials are not required to build a solution for this use-case. You can just use an IAM instance role for EC2 to access Secrets Manager.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/faqs/\">https://aws.amazon.com/secrets-manager/faqs/</a></p>\n"
        }
      },
      {
        "id": 138248159,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company has a suite of applications that are hosted in AWS and each app has its own AMI. Currently, a new AMI must be manually created and deployed to the server if there is a new application version. A DevOps engineer was instructed to automate the process of generating the AMIs to streamline the company's CI/CD workflow. The ID of the newly created AMI must be stored in a centralized location where other build pipelines can programmatically access it.</p><p>Which of the following is the MOST cost-effective way to accomplish this requirement with the LEAST amount of overhead?</p>",
          "answers": [
            "<p>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store.</p>",
            "<p>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</p>",
            "<p>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket.</p>",
            "<p>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table.</p>"
          ],
          "explanation": "<p><strong>Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p><br></p><p>Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machine Images (AMIs) and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for various reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img alt=\"Custom AMI\" height=\"771\" src=\"https://media.tutorialsdojo.com/public/custom_ami_1.gif\" width=\"1000\"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that the Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</strong></p><p>The option that says: <strong>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store </strong>is incorrect because manually customizing the image using an interactive shell and downloading each application image in an OVF file will simply entails a lot of effort. It is also better to use the AWS Systems Manager Automation instead of creating a new pipeline in AWS CodePipeline.</p><p>The option that says: <strong>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket </strong>is incorrect. Although you can technically generate an AMI using an EBS volume snapshot, this process is still tedious and entails a lot of configuration. Using the AWS Systems Manager Automation to generate the AMIs is a more suitable solution.</p><p>The option that says: <strong>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table </strong>is incorrect. Although this may work, this solution will only costs more to maintain than other options since it uses an EC2 instance and an Amazon DynamoDB table. There is also an associated overhead in configuring and using Packer for generating the AMIs and preparing the Jenkins pipeline.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/\">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248177,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A technology company is planning to develop its custom online forum that covers various AWS-related technologies. They are planning to use AWS Fargate to host the containerized application and Amazon DynamoDB as its data store. The DevOps team is instructed to define the schema of the DynamoDB table with the required indexes, partition key, sort key, projected attributes, and others. To minimize cost, the schema must support certain search operations using the least provisioned read capacity units. A <code>Thread</code> attribute contains the user comments in JSON format. The sample data set is shown in the diagram below:</p><p><br></p><p><img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/LSI_01.png \"></p><p><br></p><p>The online forum should support searches within <code>ForumName</code> attribute for items where the <code>Subject</code> begins with a particular letter, such as 'a' or 'b'. It should allow fetches of items within the given <code>LastPostDateTime</code> time frame as well as the capability to return the threads that have been posted within the last quarter. </p><p>Which of the following schema configuration meets the above requirements?</p>",
          "answers": [
            "<p>Set the <code>Subject</code> attribute as the primary key and <code>ForumName</code> as the sort key. Create a Local Secondary Index with <code>LastPostDateTime</code> as the sort key and the <code>Thread</code> as a projected attribute.</p>",
            "<p>Set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key. Create a Local Secondary Index with <code>LastPostDateTime</code> as the sort key and the <code>Thread</code> as a projected attribute.</p>",
            "<p>Set the <code>Subject</code> attribute as the primary key and <code>ForumName</code> as the sort key. Create a Global Secondary Index with <code>Thread</code> as the sort key and <code>LastPostDateTime</code> as a projected attribute.</p>",
            "<p>Set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key. Create a Global Secondary Index with <code>Thread</code> as the sort key and fetch operations for <code>LastPostDateTime</code>.</p>"
          ],
          "explanation": "<p><strong>Amazon DynamoDB</strong> provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table and issue Query or Scan requests against these indexes.</p><p>A secondary index is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations. You can retrieve data from the index using a Query, in much the same way as you use Query with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns.</p><p>DynamoDB supports two types of secondary indexes:</p><p>- <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">Global secondary index</a> \u2014 an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions.</p><p>- <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">Local secondary index</a> \u2014 an index that has the same partition key as the base table, but a different sort key. A local secondary index is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value.</p><p>A <strong>local secondary index</strong> maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.</p><p>Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to <code>Scan</code> the entire <em>Thread </em>table and discard any posts that were not within the specified time frame. With a local secondary index, a <code>Query</code> operation could use <em>LastPostDateTime</em> as a sort key and find the data quickly.</p><p>In the provided scenario, you can create a local secondary index named <em>LastPostIndex </em>to meet the requirements. Note that the partition key is the same as that of the <em>Thread</em> table, but the sort key is <em>LastPostDateTime </em>as shown in the diagram below:</p><p><img src=\"https://media.tutorialsdojo.com/public/LSI_02.png\">With <code>LastPostIndex</code>, an application could use <code>ForumName</code> and <code>LastPostDateTime</code> as query criteria. However, to retrieve any additional attributes, DynamoDB must perform additional read operations against the <code>Thread</code> table. These extra reads are known as <em>fetches</em>, and they can increase the total amount of provisioned throughput required for a query.</p><p>Suppose that you wanted to populate a webpage with a list of all the threads in \"S3\" and the number of replies for each thread, sorted by the last reply date/time beginning with the most recent reply. To populate this list, you would need the following attributes:</p><p><code>-Subject</code></p><p><code>-Replies</code></p><p><code>-LastPostDateTime</code></p><p>The most efficient way to query this data and to avoid fetch operations would be to project the <code>Replies</code> attribute from the table into the local secondary index, as shown in this diagram.</p><p><img src=\"https://media.tutorialsdojo.com/public/LSI_03.png\">DynamoDB stores all of the items with the same partition key value contiguously. In this example, given a particular ForumName, a Query operation could immediately locate all of the threads for that forum. Within a group of items with the same partition key value, the items are sorted by sort key value. If the sort key (Subject) is also provided in the query, DynamoDB can narrow down the results that are returned \u2014 for example, returning all of the threads in the \"S3\" forum that have a Subject beginning with the letter \"a\".</p><p>A <em>projection</em> is the set of attributes that is copied from a table into a secondary index. The partition key and sort key of the table are always projected into the index; you can project other attributes to support your application's query requirements. When you query an index, Amazon DynamoDB can access any attribute in the projection as if those attributes were in a table of their own.</p><p>Hence, the correct answer is: <strong>Set the </strong><code><strong>ForumName</strong></code><strong> attribute as the primary key and </strong><code><strong>Subject</strong></code><strong> as the sort key. Create a Local Secondary Index with </strong><code><strong>LastPostDateTime</strong></code><strong> as the sort key and the </strong><code><strong>Thread</strong></code><strong> as a projected attribute.</strong></p><p>The option that says: <strong>Set the </strong><code><strong>Subject</strong></code><strong> attribute as the primary key and </strong><code><strong>ForumName</strong></code><strong> as the sort key. Create a Local Secondary Index with </strong><code><strong>LastPostDateTime</strong></code><strong> as the sort key and the </strong><code><strong>Thread</strong></code><strong> as a projected attribute </strong>is incorrect because the scenario says that the online forum should support searches within <code>ForumName</code> attribute for items where the <code>Subject</code> begins with a particular letter. DynamoDB stores all of the items with the same partition key value contiguously. In this example, given a particular ForumName, a Query operation could immediately locate all of the threads for that forum. Within a group of items with the same partition key value, the items are sorted by sort key value. If the sort key (Subject) is also provided in the query, DynamoDB can narrow down the results that are returned\u2014for example, returning all of the threads in the \"S3\" forum that have a Subject beginning with the letter \"a\". Hence, you should set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key instead.</p><p>The option that says: <strong>Set the </strong><code><strong>ForumName</strong></code><strong> attribute as the primary key and </strong><code><strong>Subject</strong></code><strong> as the sort key. Create a Global Secondary Index with </strong><code><strong>Thread</strong></code><strong> as the sort key and fetch operations for </strong><code><strong>LastPostDateTime<em> </em></strong></code>is incorrect because using a fetches operation can increase the total amount of provisioned throughput required for a query. Remember that the scenario mentioned that the schema must support certain search operations using the least provisioned read capacity units to minimize cost. In addition, you should create an LSI instead of GSI.</p><p>The option that says: <strong>Set the </strong><code><strong>Subject</strong></code><strong> attribute as the primary key and </strong><code><strong>ForumName</strong></code><strong> as the sort key. Create a Global Secondary Index with </strong><code><strong>Thread</strong></code><strong> as the sort key and </strong><code><strong>LastPostDateTime</strong></code><strong> as a projected attribute </strong>is incorrect because you should use a Local Secondary Index instead. You should also set the <code>ForumName</code> attribute as the primary key and <code>Subject</code> as the sort key instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p>"
        }
      },
      {
        "id": 138248245,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An organization has several accounts managed within AWS Organizations. The DevOps Engineer in charge of the network AWS account has established an AWS Transit Gateway to direct all of the company's outgoing traffic, which is then directed through a firewall appliance for inspection. The firewall appliance logs all types of event severities such as <code>CRITICAL</code>, <code>ERROR</code>, <code>HIGH</code>, <code>MID</code>, <code>LOW</code>, and <code>ERROR</code> which are all sent to Amazon CloudWatch Logs.</p><p>During a recent security audit, no alerts were discovered for events that could cause disruptions to business continuity. The company has required that the security team receive an alert for immediate remediation when any <code>CRITICAL</code> events occur.</p><p>Which of the following is the MOST suitable solution that the DevOps Engineer should implement?</p>",
          "answers": [
            "<p>Monitor flow logs with Amazon Inspector to the network AWS account. Set up an EventBridge event rule in CloudWatch that will be triggered by <code>CRITICAL</code> Inspector events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic.</p>",
            "<p>Set up a CloudWatch metric filter that searches for <code>CRITICAL</code> events. Create a custom metric for the findings, then associate a CloudWatch alarm that will send a notification to an SNS topic. Subscribe the security team's email address to the SNS topic.</p>",
            "<p>Utilize AWS Firewall Manager to enforce uniform policies across all accounts. Set up an EventBridge event rule in CloudWatch that will be triggered by <code>CRITICAL</code> Firewall Manager events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic.</p>",
            "<p>Set up a CloudWatch Synthetics canary to monitor the firewall status. If the firewall enters a <code>CRITICAL</code> state or logs a <code>CRITICAL</code> event, configure a CloudWatch alarm to publish to an SNS topic. Subscribe the security team's email address to the SNS topic.</p>"
          ],
          "explanation": "<p>The ability to search and filter the log data arriving at CloudWatch Logs is achieved by creating one or more <strong>metric filters</strong>.</p><p><strong>Metric filters</strong> define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-cloudwatch-metric-filter.png\"></p><p>When creating a metric from a log filter, it has the capability to assign dimensions and a unit to the metric. If specifying a unit, be sure to specify the correct one when creating the filter. Changing the unit for the filter later will have no effect.</p><p>In this scenario, the firewall appliance already pushes logs to <strong>CloudWatch Logs</strong>. We can leverage <strong>CloudWatch metric filter </strong>to search and filter for the CRITICAL events and set a <strong>CloudWatch alarm </strong>to send notifications to the security team.</p><p>Hence, the correct answer is the option that says: <strong>Set up a CloudWatch metric filter that searches for </strong><code><strong>CRITICAL</strong></code><strong> events. Create a custom metric for the findings, then associate a CloudWatch alarm that will send a notification to an SNS topic. Subscribe the security team's email address to the SNS topic.</strong></p><p>The option that says: <strong>Set up a CloudWatch Synthetics canary to monitor the firewall status. If the firewall enters a </strong><code><strong>CRITICAL</strong></code><strong> state or logs a </strong><code><strong>CRITICAL</strong></code><strong> event, configure a CloudWatch alarm to publish to an SNS topic. Subscribe the security team's email address to the SNS topic </strong>is incorrect because CloudWatch Synthetics canary is used to create canaries, configurable scripts that run on a schedule, to monitor endpoints and APIs. In the scenario, the firewall appliance is already pushing to CloudWatch Logs. Metric filter is a much more suitable solution as it can filter the CRITICAL events.</p><p>The option that says: <strong>Monitor flow logs with Amazon Inspector to the network AWS account. Set up an EventBridge event rule in CloudWatch that will be triggered by </strong><code><strong>CRITICAL</strong></code><strong> Inspector events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic </strong>is incorrect because Amazon Inspector is a vulnerability management service that is commonly used to scan AWS workloads for software vulnerabilities and unintended network exposure. This service does not filter logs.</p><p>The option that says: <strong>Utilize AWS Firewall Manager to enforce uniform policies across all accounts. Set up an EventBridge event rule in CloudWatch that will be triggered by </strong><code><strong>CRITICAL</strong></code><strong> Firewall Manager events. Create an SNS topic as the target, then subscribe the security team's email address to the SNS topic</strong> is incorrect because AWS Firewall Manager is primarily used to centrally configure and manage firewall rules across accounts and applications in AWS Organizations. It does not have a filter capability like CloudWatch metric filter.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html#search-filter-concepts\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html#search-filter-concepts</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_alarm_log_group_metric_filter.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_alarm_log_group_metric_filter.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 138248227,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A startup software company has several application teams that develop API services for its business. Each application team is responsible for services separated on different AWS accounts. The VPC of each AWS account was initially provisioned with a <code>192.168.0.0/24</code> CIDR block. The services are deployed on Amazon EC2 instances accessed on a secure HTTPS public endpoint of an Application Load Balancer. Integration between the services routes externally to the public internet.</p><p>As part of a security audit, there is a recommendation from the security team to re-architect the integration between services to communicate on HTTPS on the private network only. A solutions architect is asked to suggest a long-term solution considering the possibility of adding more VPCs in the future.</p><p>What should the solutions architect recommend?</p>",
          "answers": [
            "<p>Create peering connections between each VPC of the different AWS accounts. Add a new route to the route table for each peering connection, with the destination set to the CIDR block of the VPC and the target set to the ID of the peering connection. Create Network Load Balancers (NLB) in each VPC and use the NLB DNS names for services integration.</p>",
            "<p>Fix the overlapping IP address ranges by renumbering the IP networks. Create a transit gateway in a new AWS account in AWS Organizations, and configure the VPC attachments. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway across the organization. Create transit gateway attachments to each VPC of the AWS accounts. Add new routes with the destination set to each VPC CIDR block, and the target set to the transit gateway attachment. Create a Network Load Balancer (NLB) in each VPC and use this for communication between services.</p>",
            "<p>Create a new AWS account and set up a new VPC. Create resource shares in AWS Resource Access Manager (AWS RAM) to share the private subnets to specified accounts. Advise the application teams to host the services in EC2 instances deployed in the shared private subnets. Create a Network Load Balancer (NLB) for each account and share the DNS names for the services integration.</p>",
            "<p>Create a Network Load Balancer (NLB) in each of the account VPCs. Create an interface type VPC endpoint for <code>com.amazonaws.us-east-1.elasticloadbalancing</code> service and choose the VPC and subnets. In the AWS PrivateLink console, create a subscription for the VPC endpoint and choose the AWS account that will be allowed access to the NLB. Use the VPC endpoint DNS names for communication between services.</p>"
          ],
          "explanation": "<p>A transit gateway acts as a Regional virtual router for traffic flowing between your virtual private clouds (VPCs) and on-premises networks. A transit gateway scales elastically based on the volume of network traffic. It is a best practice to use a separate subnet for each transit gateway VPC attachment.</p><p>A transit gateway enables you to attach VPCs and VPN connections and route traffic between them. A transit gateway works across AWS accounts, and you can use AWS RAM to share your transit gateway with other accounts. After you share a transit gateway with another AWS account, the account owner can attach their VPCs to your transit gateway. A user from either account can delete the attachment at any time.</p><p><img alt=\"Transit Gateway with AWS RAM\" src=\"https://media.tutorialsdojo.com/public/TransitGatewayWithRAM.png\" width=\"1000\"></p><p>It is a high recommendation and the best option to renumber IP networks when possible, based on two reasons: cost, and simplicity. Changing network configurations is not easy, but it is beneficial in the long term because it removes the ongoing cost of running required components when connecting overlapping networks. Having non-overlapping IPs also makes troubleshooting easier when things go wrong, as resources can easily be identified to the network they are deployed to. This also removes the complexity of managing firewall rules across the organization.</p><p>Thus, the correct answer is: <strong>Fix the overlapping IP address ranges by renumbering the IP networks. Create a transit gateway in a new AWS account in AWS Organizations, and configure the VPC attachments. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway across the organization. Create transit gateway attachments to each VPC of the AWS accounts. Add new routes with the destination set to each VPC CIDR block, and the target set to the transit gateway attachment. Create a Network Load Balancer (NLB) in each VPC and use this for communication between services.</strong></p><p>The option that says: <strong>Create peering connections between each VPC of the different AWS accounts. Add a new route to the route table for each peering connection, with the destination set to the CIDR block of the VPC and the target set to the ID of the peering connection. Create Network Load Balancers (NLB) in each VPC and use the NLB DNS names for services integration</strong> is incorrect. Although VPC peering will only work if the overlapping IP ranges are fixed, managing peering connections between multiple VPCs can be very complex and difficult to manage as the number of VPCs increases. For each <code>x</code> number of VPCs, <code>x*(x-1)/2</code> number of peering connections needs to be created to establish connectivity across each VPC.</p><p>The option that says: <strong>Create a new AWS account and set up a new VPC. Create resource shares in AWS Resource Access Manager (AWS RAM) to share the private subnets to specified accounts. Advise the application teams to host the services in EC2 instances deployed in the shared private subnets. Create a Network Load Balancer (NLB) for each account and share the DNS names for the services integration</strong> is incorrect. Sharing private subnets across accounts using AWS Resource Access Manager (AWS RAM) can simply add to the management overhead. Each time a new subnet is created, it needs to be shared manually with the specified AWS accounts.</p><p>The option that says: <strong>Create a Network Load Balancer (NLB) in each of the account VPCs. Create an interface type VPC endpoint for </strong><code><strong>com.amazonaws.us-east-1.elasticloadbalancing</strong></code><strong> service and choose the VPC and subnets. In the AWS PrivateLink console, create a subscription for the VPC endpoint and choose the AWS account that will be allowed access to the NLB. Use the VPC endpoint DNS names for communication between services</strong> is incorrect. Although using AWS PrivateLink and VPC endpoints can provide connectivity between VPCs, it would require managing individual VPC endpoints and subscriptions, making management complex. It is better to implement a Transit Gateway solution for the long term.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/\">https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/</a><br></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html\">https://docs.aws.amazon.com/vpc/latest/tgw/how-transit-gateways-work.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html\">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-share.html\">https://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-share.html</a></p><p><br></p><p>Check out this AWS Transit Gateway Cheat Sheet:</p><p><a href=\"https://tutorialsdojo.com/aws-transit-gateway/?src=udemy\">https://tutorialsdojo.com/aws-transit-gateway/</a></p>"
        }
      },
      {
        "id": 99528233,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company plans to deploy a high-performance computing (HPC) workload on Amazon EC2 instances in a shared Amazon VPC. Developers in multiple participant accounts must be granted access to the cluster to perform analytics. The cluster requires a shared file system that supports file-based access to objects stored in Amazon S3 buckets.</p><p>Which deployment steps should be implemented to support the required features and access control?</p>",
          "answers": [
            "<p>Deploy an Amazon FSx for Lustre file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>",
            "<p>Deploy an Amazon FSx for NetApp ONTAP file system with NFS access. Create a resource based policy that allows cross-account access for members of the participant accounts. Use access keys to authenticate users and use security groups to enable file system access.</p>",
            "<p>Deploy an Amazon FSx for Windows file server file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>",
            "<p>Deploy and Amazon FSx for OpenZFS file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>"
          ],
          "explanation": "<p>Amazon FSx for Lustre is the only option that can present objects stored in S3 buckets as files. The question specifically asks for file-based access for objects stored in buckets, so this requirement is satisfied by using FSx for Lustre. The access control requirements can be implemented through an AWS IAM role that can be assumed by cross-account participants. Permissions should be assigned through an identity-based permissions policy assigned to the role.</p><p><strong>CORRECT: </strong>\"Deploy an Amazon FSx for Lustre file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon FSx for NetApp ONTAP file system with NFS access. Create a resource based policy that allows cross-account access for members of the participant accounts. Use access keys to authenticate users and use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects. You also cannot use resource based policies with FSx file systems and access keys would be unsuitable for cross-account access.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon FSx for Windows file server file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects. This file system should be used with Windows servers that need an NTFS file system.</p><p><strong>INCORRECT:</strong> \"Deploy and Amazon FSx for OpenZFS file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/LustreGuide/security_iam_service-with-iam.html\">https://docs.aws.amazon.com/fsx/latest/LustreGuide/security_iam_service-with-iam.html</a></p><p><a href=\"https://aws.amazon.com/fsx/lustre/faqs/\">https://aws.amazon.com/fsx/lustre/faqs/</a></p>"
        }
      },
      {
        "id": 115961521,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A gaming startup company is finishing its migration to AWS and realizes that many DevOps engineers have permissions to delete Amazon DynamoDB tables.</p><p>A solution is required to receive near real-time notifications when the API call DeleteTable is invoked in DynamoDB.</p><p>Which actions should be taken to achieve this requirement most cost-effectively?</p>",
          "answers": [
            "<p>Enable DynamoDB Streams and configure an AWS Lambda function to process events from the stream. Send alerts using Amazon SNS.</p>",
            "<p>Create an AWS CloudTrail trail. Create an Amazon EventBridge rule to track an AWS API call via CloudTrail and use Amazon SNS as a target.</p>",
            "<p>Create an AWS CloudTrail event filter and use an AWS Lambda function to send an Amazon SNS notification.</p>",
            "<p>Create an Amazon CloudWatch event filter that monitors for DeleteTable API actions and sends an alert via Amazon SNS.</p>"
          ],
          "explanation": "<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. You must create an AWS CloudTrail trail. For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p><p><strong>CORRECT: </strong>\"Create an AWS CloudTrail trail. Create an Amazon EventBridge rule to track an AWS API call via CloudTrail and use Amazon SNS as a target\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable DynamoDB Streams and configure an AWS Lambda function to process events from the stream. Send alerts using Amazon SNS\" is incorrect.</p><p>This would be less cost-effective compared to using AWS CloudTrail and Amazon EventBridge.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail event filter and use an AWS Lambda function to send an Amazon SNS notification\" is incorrect.</p><p>Event filters are used with CloudWatch, not with CloudTrail.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch event filter that monitors for DeleteTable API actions and sends an alert via Amazon SNS\" is incorrect.</p><p>API actions are tracked by AWS CloudTrail but not by Amazon CloudWatch.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 138248131,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A dynamic Node.js-based photo sharing application hosted in four Amazon EC2 web servers is using a DynamoDB table for session management and an S3 bucket for storing media files. The users can upload, view, organize, and share their photos using the content management system of the application. When a user uploads an image, a Lambda function will be invoked to process the media file then store it in Amazon S3. Due to the recent growth of the application\u2019s user base in the country, they decided to manually add another six EC2 instances for the web tier to handle the peak load. However, each of the instances took more than half an hour to download the required application libraries and become fully configured.&nbsp; </p><p>Which of the following is the MOST resilient and highly available solution that will also lessen the deployment time of the new servers?</p>",
          "answers": [
            "<p>Migrate the application to Amazon ECS with Fargate launch type. Create a task definition for the Node.js application that includes all required dependencies. Set up a DynamoDB table with Auto Scaling enabled and configure an Application Load Balancer to distribute traffic to the ECS service. Use Amazon Route 53 to point the application DNS record to the Application Load Balancer.</p>",
            "Deploy a Spot Fleet of EC2 instances with a target capacity of 20 then place them behind an Application Load Balancer. Configure Amazon Route 53 to point the application DNS record to the Application Load Balancer. Increase the RCU and WCU of the DynamoDB table. ",
            "Host the entire Node.js application to Amazon S3 as a static website. Create an Amazon CloudFront web distribution with the S3 bucket as its origin. Enable Auto Scaling in the Amazon DynamoDB table. In Route 53, point the application DNS record to the CloudFront URL.",
            "<p>Host the entire application in Elastic Beanstalk. Create a custom AMI using AWS Systems Manager Automation which includes all of the required dependencies and web components. Configure the Elastic Beanstalk environment to have an Auto Scaling group of EC2 instances across multiple Availability Zones with a load balancer in front that balances the incoming traffic. Enable Amazon DynamoDB Auto Scaling and point the application DNS record to the Elastic Beanstalk load balancer using Amazon Route 53.</p>"
          ],
          "explanation": "<p>When you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn't included in the standard AMIs.</p><p>Using <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">configuration files</a> is great for configuring and customizing your environment quickly and consistently. Applying configurations, however, can start to take a long time during environment creation and updates. If you do a lot of server configuration in configuration files, you can reduce this time by making a custom AMI that already has the software and configuration that you need.</p><p><img src=\"https://media.tutorialsdojo.com/public/aeb-architecture2.png\"></p><p>A custom AMI also allows you to make changes to low-level components, such as the Linux kernel, that are difficult to implement or take a long time to apply in configuration files. To create a custom AMI, launch an Elastic Beanstalk platform AMI in Amazon EC2, customize the software and configuration to your needs, and then stop the instance and save an AMI from it.</p><p>Hence, the correct solution is: <strong>Host the entire application in Elastic Beanstalk. Create a custom AMI using AWS Systems Manager Automation which includes all of the required dependencies and web components. Configure the Elastic Beanstalk environment to have an Auto Scaling group of EC2 instances across multiple Availability Zones with a load balancer in front that balances the incoming traffic. Enable Amazon DynamoDB Auto Scaling and point the application DNS record to the Elastic Beanstalk load balancer using Amazon Route 53.</strong></p><p>The option that says: <strong>Migrate the application to Amazon ECS with Fargate launch type. Create a task definition for the Node.js application that includes all required dependencies. Set up a DynamoDB table with Auto Scaling enabled and configure an Application Load Balancer to distribute traffic to the ECS service. Use Amazon Route 53 to point the application DNS record to the Application Load Balancer</strong> is incorrect. While ECS with Fargate can simplify container management, this solution requires the application to be containerized, which might involve significant changes to the existing application architecture. Elastic Beanstalk offers a simpler, more direct approach to this scenario.</p><p>The option that says: <strong>Deploy a Spot Fleet of EC2 instances with a target capacity of 20 then place them behind an Application Load Balancer. Configure Amazon Route 53 to point the application DNS record to the Application Load Balancer. Increase the RCU and WCU of the DynamoDB table</strong> is incorrect because using Spot Instances is susceptible to interruptions and could lead to outages of your application. Moreover, setting an exact number of target capacity is not recommended since your servers won't scale up or scale down based on the actual demand.</p><p>The option that says: <strong>Host the entire Node.js application to Amazon S3 as a static website. Create an Amazon CloudFront web distribution with the S3 bucket as its origin. Enable Auto Scaling in the Amazon DynamoDB table. In Route 53, point the application DNS record to the CloudFront URL </strong>is incorrect because the web application is a dynamic site and cannot be migrated to a static S3 website hosting.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/beanstalk-environment-configuration-advanced.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/beanstalk-environment-configuration-advanced.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 134588463,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A leading telecommunications company is migrating a multi-tier enterprise application to AWS, which must be hosted on a single Amazon EC2 Dedicated Instance with an Elastic Fabric Adapter (EFA) and instance store volumes. The app cannot use Auto Scaling due to server licensing constraints.</p><p>For its database tier, Amazon Aurora will be used to store the application's data and transactions. Automatic recovery must be configured to ensure high availability even in the event of EC2 or Aurora database outages.</p><p>Which of the following options provides the MOST cost-effective solution for this migration task?</p>",
          "answers": [
            "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch a single-instance Aurora database. Create an AWS Config conformance pack containing the Config rules and remediation actions to automatically recover the EC2 instance during system outages.</p>",
            "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch a single-instance Aurora database. Create an AWS Config conformance pack containing the Config rules and remediation actions to automatically recover the EC2 instance during system outages.</p>",
            "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Set up an Amazon EventBridge rule to trigger an AWS Lambda function to start a new EC2 instance in an available Availability Zone when the instance status reaches a failure state. Configure an Aurora database with a Read Replica in the other Availability Zone. In the event that the primary database instance fails, promote the read replica to a primary database instance.</p>",
            "<p>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch an Elastic IP address and attach it to the dedicated instance. Set up a second EC2 instance in the other Availability Zone. Create an Amazon EventBridge rule to trigger an AWS Lambda function to move the EIP to the second instance when the first instance fails. Set up a single-instance Aurora database.</p>"
          ],
          "explanation": "<p>You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group.</p><p>When the <code>StatusCheckFailed_System</code> alarm is triggered, and the recovery action is initiated, you will be notified by the Amazon SNS topic that you selected when you created the alarm and associated the recovery action. During instance recovery, the instance is migrated during an instance reboot, and any in-memory data is lost. When the process is complete, information is published to the SNS topic you've configured for the alarm. Anyone subscribed to this SNS topic will receive an email notification that includes the status of the recovery attempt and any further instructions. You will notice an instance reboot on the recovered instance.<br>Examples of problems that cause system status checks to fail include:</p><p>- Loss of network connectivity</p><p>- Loss of system power</p><p>- Software issues on the physical host</p><p>- Hardware issues on the physical host that impact network reachability</p><p>If your instance has a public IPv4 address, it retains the public IPv4 address after recovery.</p><p><img alt=\"Amazon EventBridge\" height=\"593\" src=\"https://media.tutorialsdojo.com/aws_eventbridge_13AUG2023.png\" width=\"1000\"></p><p>You can configure a CloudWatch alarm to automatically recover impaired EC2 instances and notify you through Amazon SNS. However, the SNS notification doesn't include the results of the automatic recovery action.</p><p>You must also configure an Amazon EventBridge rule to monitor AWS Personal Health Dashboard (AWS Health) events for your instance. Then, you are notified of the results of automatic recovery actions, for example.</p><p>While Amazon Aurora Read Replicas incur additional costs because each replica is a separate database instance, they are necessary to provide high availability and failover capability, critical for enterprise applications requiring auto-healing and minimal downtime.</p><p>Hence, the correct answer is: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Set up an Amazon EventBridge rule to trigger an AWS Lambda function to start a new EC2 instance in an available Availability Zone when the instance status reaches a failure state. Configure an Aurora database with a Read Replica in the other Availability Zone. In the event that the primary database instance fails, promote the read replica to a primary database instance.</strong></p><p>The option that says: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch a single-instance Aurora database. Create an AWS Config conformance pack containing the Config rules and remediation actions to automatically recover the EC2 instance during system outages</strong> is incorrect because launching a single-instance Aurora database is simply not a highly available architecture. You have to set up at least a Read Replica that you can configure to be the new primary instance during outages. In addition, AWS Config rules alone cannot recover your EC2 instances automatically. This must be integrated with the AWS Systems Manager Automation first.</p><p>The option that says: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Set up an EC2 instance and enable the built-in instance recovery feature. Create an Aurora database with a Read Replica on the other Availability Zone. Promote the replica as the primary in the event that the primary database instance fails</strong> is incorrect because the built-in instance recovery failure feature for Amazon EC2 doesn't apply to instances that use Elastic Fabric Adapter (EFA) and instance store volumes. The scenario explicitly mentioned that the application uses a Dedicated Instance with both EFA and instance store volumes, so EC2 auto-recovery won't take place. You have to use a combination of Amazon EventBridge and a Lambda function to recover the EC2 instance from failure automatically.</p><p>The option that says: <strong>Set up the AWS Personal Health Dashboard (AWS Health) events to monitor your EC2 Dedicated Instance. Launch an Elastic IP address and attach it to the dedicated instance. Set up a second EC2 instance in the other Availability Zone. Create an Amazon EventBridge rule to trigger an AWS Lambda function to move the EIP to the second instance when the first instance fails. Set up a single-instance Aurora database</strong> is incorrect because setting up a second EC2 instance in another Availability Zone will only entail an additional cost. Launching a single-instance Aurora database is not a highly available architecture as well.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-sns-ec2-automatic-recovery/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-sns-ec2-automatic-recovery/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/automatic-recovery-ec2-cloudwatch/\">https://aws.amazon.com/premiumsupport/knowledge-center/automatic-recovery-ec2-cloudwatch/</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 82921458,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A Big Data analytics company is operating a distributed Cassandra cluster on EC2. Each instance in the cluster must have a list of all the other instance's IP to function correctly, store in a configuration file. As a Devops Engineer at the company, you would like this solution to adapt automatically when newer EC2 instances join the cluster, or when some EC2 instances are terminated.</p>\n\n<p>Which of the following solutions would you recommend for the given requirement?</p>\n",
          "answers": [
            "<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</p>",
            "<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</p>",
            "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</p>",
            "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</strong></p>\n\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n\n<p>A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p>\n\n<p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer's instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes.</p>\n\n<p>The lifecycle hook that is called on ALL instances, whenever an instance comes up or another one goes down, is the <code>configure</code> hook. So this option is the best fit for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</strong> - As mentioned in the explanation above, the <code>setup</code> hook is only used when an instance is first created, so this option is incorrect.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</strong> - Lifecycle hooks on Auto Scaling Groups may seem like a good idea at first, but using AWS Lambda, the solution is not practicable as SSH'ing into the instance via Lambda will not work.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</strong> - EC2 user-data scripts are only triggered on an instance's first launch, so this option just acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/opsworks/\">https://aws.amazon.com/opsworks/</a></p>\n"
        }
      },
      {
        "id": 138248141,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A government-sponsored health service is running its web application containing information about the clinics, hospitals, medical specialists, and other medical services in the country. The organization also has a set of public web services which enable third-party companies to search medical data for its respective applications and clients. AWS Lambda functions are used for the public APIs. For its database-tier, an Amazon DynamoDB table stores all of the data with an Amazon OpenSearch Service domain, which supports the search feature and stores the indexes. A DevOps engineer has been instructed to ensure that in the event of a failed deployment, there should be no downtime and a system should be in place to prevent subsequent deployments. The service must strictly maintain full capacity during API deployment without any reduced capacity to avoid degradation of service.</p><p>How can the engineer meet the above requirements in the MOST efficient way?</p>",
          "answers": [
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>All at Once</code>.</p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Rolling</code>.</p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Immutable</code></p>",
            "<p>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication.</p>"
          ],
          "explanation": "<p><strong>AWS Elastic Beanstalk</strong> provides several options for how <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">deployments</a> are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's an automatically scaling environment (you didn't specify the <code>--single</code> option), it uses rolling deployments.</p><p>With rolling deployments, Elastic Beanstalk splits the environment's EC2 instances into batches and deploys the new version of the application to one batch at a time, leaving the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p><p><img src=\"https://media.tutorialsdojo.com/public/environments-mgmt-updates-immutable.png\"></p><p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.''</p><p>Immutable deployments perform an <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">immutable update</a> to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.</p><p>Hence, the correct answer is: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>Immutable.</strong></code></p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>All at Once</strong></code><strong><em> </em></strong>is incorrect because this policy only deploys the new version to all instances simultaneously, which means that the instances in your environment are out of service for a short time while the deployment occurs.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy in-place deployment. Host the web application in AWS Elastic Beanstalk and set the deployment policy to</strong> <code><strong>Rolling</strong></code><em> </em>is incorrect because this policy will just deploy the new version in batches where each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch.</p><p>The option that says: <strong>Deploy the DynamoDB tables, Lambda functions, and Amazon OpenSearch Service domain using AWS CloudFormation. Deploy changes with an AWS CodeDeploy blue/green deployment. Host the web application in Amazon S3 and enable cross-region replication</strong> is incorrect because you can't host a dynamic web application in Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p>"
        }
      }
    ],
    "answers": {
      "75949064": [
        "b",
        "d",
        "e"
      ],
      "75949112": [
        "b"
      ],
      "75949146": [
        "b"
      ],
      "75949148": [
        "c"
      ],
      "82921334": [
        "a"
      ],
      "82921458": [
        "a"
      ],
      "99528233": [
        "a"
      ],
      "115961521": [
        "b"
      ],
      "134588451": [
        "d"
      ],
      "134588463": [
        "c"
      ],
      "138248131": [
        "d"
      ],
      "138248141": [
        "c"
      ],
      "138248159": [
        "b"
      ],
      "138248177": [
        "c"
      ],
      "138248181": [
        "c"
      ],
      "138248227": [
        "d"
      ],
      "138248229": [
        "b"
      ],
      "138248237": [
        "a"
      ],
      "138248245": [
        "b"
      ],
      "143860765": [
        "b"
      ]
    }
  },
  {
    "id": "1769968219110",
    "date": "2026-02-01T17:50:19.110Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 16,
    "incorrect": 4,
    "unanswered": 0,
    "total": 20,
    "percent": 80,
    "duration": 4942951,
    "questions": [
      {
        "id": 82921318,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A graphics design company is experimenting with a new feature for an API and the objective is to pass the field <code>\"color\"</code> in the JSON payload to enable this feature. The new Lambda function should treat <code>\"color\": \"none\"</code> as a request from an older client. The company would like to only have to manage one Lambda function in the back-end while being able to support both old and new clients. The API Gateway API is currently deployed on the <code>v1</code> stage. Old clients include Android applications which may take time to be updated. The technical requirements mandate that the solution should support the old clients for years to come.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you recommend as the best fit for the given use-case?</p>\n",
          "answers": [
            "<p>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</p>",
            "<p>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</p>",
            "<p>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</p>",
            "<p>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway handles tasks such as traffic management, authorization and access control, monitoring, and API version management. API Gateway acts as a \"front door\" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications. Simply creating and developing an API Gateway API doesn't automatically make it callable by your users. To make it callable, you must deploy your API to a stage. A stage is a named reference to a deployment, which is a snapshot of the API.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>For the given use-case, API Gateway mappings must be used. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. As such, you must deploy a v2 API alongside the v1 API backed by the same Lambda function. Old clients will hit the v1 API, which will use a mapping template to add the static missing field <code>\"color\": \"none\"</code>. Newer clients will hit the v2 API and will have that field value included.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</strong> - If we release two separate Lambda functions (named lambda v1 and lambda v2), then we have to maintain them both and that would be going against the requirements of the given use-case.</p>\n\n<p><strong>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</strong> -  API Gateway mapping templates do not support adding default values for fields as these only support static fields.</p>\n\n<p><strong>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. For the given use-case, API Gateway caching is a distractor and should be disregarded.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n"
        }
      },
      {
        "id": 75949056,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company is using AWS CodeCommit for version control and AWS CodePipeline for orchestration of software deployments. The development team are using a remote main branch as the trigger for the pipeline. A developer noticed that the CodePipeline pipeline was not triggered after the developer pushed code changes to the CodeCommit repository.</p><p>Which of the following actions should be taken to troubleshoot this issue?</p>",
          "answers": [
            "<p>Check that the developer's IAM role has permission to push to the CodeCommit repository.</p>",
            "<p>Check that an AWS Lambda function has been created to check for code commits and trigger the pipeline.</p>",
            "<p>Check that the CodePipeline service role has permission to access the CodeCommit repository.</p>",
            "<p>Check that an Amazon CloudWatch Events rule has been created for the main branch to trigger the pipeline.</p>"
          ],
          "explanation": "<p>An Amazon CloudWatch Events rule must be created to trigger the pipeline when changes are committed to the CodeCommit repository. If you use the console to create or edit your pipeline, the CloudWatch Events rule is created for you. In this case, the developer should check to make sure that the rule has been created and is correctly configured.</p><p>The following is a sample CodeCommit event pattern for a MyTestRepo repository with a branch named master:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-34-08-dc82125abf42153048e88229777e35c5.jpg\"><p><strong>CORRECT: </strong>\"Check that an Amazon CloudWatch Events rule has been created for the main branch to trigger the pipeline\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check that the CodePipeline service role has permission to access the CodeCommit repository\" is incorrect.</p><p>The issue is that the pipeline was not triggered. If the service role does not have permissions the pipeline should still be triggered by the CloudWatch Events rule but then an error would be generated if insufficient permissions are assigned for accessing the CodeCommit repository.</p><p><strong>INCORRECT:</strong> \"Check that the developer's IAM role has permission to push to the CodeCommit repository\" is incorrect.</p><p>The developer already committed the code to the repository and did not experience any errors.</p><p><strong>INCORRECT:</strong> \"Check that an AWS Lambda function has been created to check for code commits and trigger the pipeline\" is incorrect.</p><p>An AWS Lambda function is not used to check for commits or to trigger the pipeline. A CloudWatch Events rule must be created for this purpose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 143860761,
        "correct_response": [
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A web application runs on a custom port. The application has been deployed in an Auto Scaling group with an Application Load Balancer (ALB). After launching instances the Auto Scaling and Target Group health checks are returning a healthy status. However, users report that the application is not accessible.</p><p>Which steps should a DevOps engineer take to troubleshoot the issue? (Select TWO.)</p>",
          "answers": [
            "<p>Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port.</p>",
            "<p>Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address.</p>",
            "<p>Modify the Target Group health check configuration to check the application process on the custom port and path.</p>",
            "<p>Modify the Auto Scaling group health check configuration to check the application process on the custom port and path.</p>",
            "<p>Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances.</p>"
          ],
          "explanation": "<p>By default health checks are configured to use the HTTP protocol and port 80. For an ALB the traffic protocol must be HTTP or HTTPS, but the port can be customized. The most likely cause of the issue is that the web service is running on the instances and the default protocol/port is used for health checks on the ALB and ASG. This will result in instances becoming \u201chealthy\u201d despite the actual application service not functioning correctly.</p><p>The engineer should check the ASG health check configuration and the target group health check configuration. If the default values are used then the correct custom port number should be entered instead. The path may also be updated if a specific web page should be checked.</p><p>The image below shows how you can override the default port number and path in a target group health check:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-08-15-0585684e22874b6c255bd441d63a099e.jpg\"><p><strong>CORRECT: </strong>\"Modify the Target Group health check configuration to check the application process on the custom port and path\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Modify the Auto Scaling group health check configuration to check the application process on the custom port and path\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port\" is incorrect.</p><p>You cannot use a TCP listener with an ALB. It must be HTTP or HTTPS though a custom port number can certainly be used.</p><p><strong>INCORRECT:</strong> \"Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address\" is incorrect.</p><p>The issue is not related to the IP addresses traffic is being directed to on the instances; it is related to the port number and path the health checks are configured to check.</p><p><strong>INCORRECT:</strong> \"Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances\" is incorrect.</p><p>Path-based routing rules are used to route traffic to different target groups based on the path in the HTTP request. In this case there is only one target group, so a path-based routing rule is useless. Instead, the ALB must direct traffic to a custom port number (configured in the listener) and validate the application is healthy be running health checks against the appropriate port and path.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 75949118,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A DevOps team is building a pipeline in AWS CodePipeline that will build, stage, test, and then deploy an application on Amazon EC2. The team will add a manual approval stage between the test stage and the deployment stage. The development team uses a custom chat tool that offers a webhook interface for sending notifications.</p><p>The DevOps team require status updates for pipeline activity and approval requests to be posted to the chat tool. How can this be achieved?</p>",
          "answers": [
            "<p>Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic.</p>",
            "<p>Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change events. Publish subscription events to an Amazon SNS topic and subscribe the chat webhook URL to the SNS topic and complete the subscription validation.</p>",
            "<p>Create an AWS Lambda function that is invoked by AWS CloudTrail API events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details directly to the chat webhook URL.</p>",
            "<p>Create an AWS Config rule that checks for CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic.</p>"
          ],
          "explanation": "<p>You can monitor CodePipeline events in EventBridge, which delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services. EventBridge routes that data to targets such as AWS Lambda and Amazon Simple Notification Service.</p><p>Events are composed of rules that include an event pattern and event target. Each type of execution state change event in CodePipeline emits notifications with specific message content. In this case the team should filter for the \u201cCodePipeline Pipeline Execution State Change\u201d events and route to an SNS Topic as a target. The Lambda function can then be subscribed to the topic.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change events. Publish subscription events to an Amazon SNS topic and subscribe the chat webhook URL to the SNS topic and complete the subscription validation\" is incorrect.</p><p>CloudWatch Logs subscription filters can be used to publish to Kinesis Data Streams, Kinesis Data Firehose, or AWS Lambda. You cannot publish directly to SNS. Also, the log will not contain execution state change events for CodePipeline.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that is invoked by AWS CloudTrail API events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details directly to the chat webhook URL\" is incorrect.</p><p>You cannot use API events to look for this specific event and then send directly from CloudTrail to the chat webhook URL.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config rule that checks for CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic\" is incorrect.</p><p>AWS Config is used for configuration compliance and cannot check for events that relate to pipeline execution state changes in AWS CodePipeline.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 138248203,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A financial company has several accounting applications that are hosted in AWS and used by thousands of small and medium businesses. As part of its Business Continuity Plan, the company is required to set up an automatic DNS failover for its applications to a disaster recovery (DR) environment. The DevOps team was instructed to configure Amazon Route 53 to automatically route to an alternate endpoint when the primary application stack in the us-west-1 region experiences an outage or degradation of service.</p><p>What steps should the team take to satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Set up health checks in Route 53 for non-alias records to each service endpoint. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks.</p>",
            "<p>Use a Failover routing policy configuration. Set up alias records in Route 53 that route traffic to AWS resources. Set the <code>Evaluate Target Health</code> option to <code>Yes</code>, then create all of the required non-alias records.</p>",
            "<p>Set up a record in Route 53 with a Weighted routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints.</p>",
            "<p>Set up an Amazon CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom AWS Lambda function. Execute the <code>ChangeResourceRecordSets</code> API call using the function to initiate the failover to the secondary DNS record.</p>",
            "<p>Set up a record in Route 53 with a latency routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints.</p>"
          ],
          "explanation": "<p>Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.</p><p>To create an active-passive failover configuration with one primary record and one secondary record, you just create the records and specify <strong>Failover</strong> for the routing policy. When the primary resource is healthy, Route 53 responds to DNS queries using the primary record. When the primary resource is unhealthy, Route 53 responds to DNS queries using the secondary record.</p><p>You can configure a health check that monitors an endpoint that you specify either by IP address or by domain name. At regular intervals that you specify, Route 53 submits automated requests over the Internet to your application, server, or other resources to verify that it's reachable, available, and functional. Optionally, you can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-route53-evaluate-target-health.png\"></p><p>When Route 53 checks the health of an endpoint, it sends an HTTP, HTTPS, or TCP request to the IP address and port that you specified when you created the health check. For a health check to succeed, your router and firewall rules must allow inbound traffic from the IP addresses that the Route 53 health checkers use.</p><p>Hence, the correct answers are:</p><p><strong>- Set up health checks in Route 53 for non-alias records to each service endpoint. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks.</strong></p><p><strong>- Use a Failover routing policy configuration. Set up alias records in Route 53 that route traffic to AWS resources. Set the </strong><code><strong>Evaluate Target Health</strong></code><strong> option to </strong><code><strong>Yes</strong></code><strong>, then create all of the required non-alias records.</strong></p><p>The option that says: <strong>Set up a record in Route 53 with a Weighted routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints</strong> is incorrect because Weighted routing simply lets you associate multiple resources with a single domain name (pasigcity.com) or subdomain name (blog.pasigcity.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.</p><p>The option that says:<strong> Set up an Amazon CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom AWS Lambda function. Execute the </strong><code><strong>ChangeResourceRecordSets</strong></code><strong> API call using the function to initiate the failover to the secondary DNS record</strong> is incorrect because you only have to use a Failover routing policy. Calling the Route 53 API is not applicable nor useful at all in this scenario.</p><p>The option that says: <strong>Set up a record in Route 53 with a latency routing policy configuration. Associate the record with the primary and secondary record sets to distribute traffic to healthy service endpoints</strong> is incorrect because the Latency routing policy simply improves the application performance for your users by serving their requests from the AWS Region that provides the lowest latency. You have to use a Failover routing policy instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-router-firewall-rules.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p>"
        }
      },
      {
        "id": 134588431,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A mobile phone manufacturer hosts a suite of enterprise resource planning (ERP) solutions to several Amazon EC2 instances in their AWS VPC. Its DevOps team is using AWS CloudFormation templates to design, launch, and deploy resources to their cloud infrastructure. Each template is manually updated to map the latest AMI IDs of the ERP solution. This process takes a significant amount of time to execute, which is why the team was tasked to automate this process. </p><p>In this scenario, which of the following options is the MOST suitable solution that can satisfy the requirement?</p>",
          "answers": [
            "<p>Set up and configure the Systems Manager State Manager service to store the latest AMI IDs and integrate it with your AWS CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.</p>",
            "<p>Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.</p>",
            "<p>Integrate AWS CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.</p>",
            "<p>Use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template. Call the update-stack API in CloudFormation in your template whenever you decide to update the Amazon EC2 instances.</p>"
          ],
          "explanation": "<p>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.</p><p>If the parameter being referenced in the template does not exist in Systems Manager, a synchronous validation error is thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.</p><p><img src=\"https://media.tutorialsdojo.com/public/Systems-Manager-parameters_6AUG2023.png\"></p><p>Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The <a href=\"http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html\">Parameters</a> section in the output for Describe API will show an additional \u2018ResolvedValue\u2019 field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation.</p><p>Hence, the correct answer is: <strong>Use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template. Call the update-stack API in CloudFormation in your template whenever you decide to update the Amazon EC2 instances.</strong></p><p>The option that says: <strong>Set up and configure the Systems Manager State Manager service to store the latest AMI IDs and integrate it with your AWS CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template</strong> is incorrect because the Systems Manager State Manager service simply automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This can't be used as a parameter store that refers to the latest AMI of your application.</p><p>The option that says: <strong>Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments</strong> is incorrect because using AWS Service Catalog is not suitable in this scenario. This service just allows organizations to create and manage catalogs of IT services that are approved for use on AWS.</p><p>The option that says: <strong>Integrate AWS CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments</strong> is incorrect because AWS Service Catalog just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A better solution is to use Systems Manager Parameter Store in conjunction with CloudFormation to retrieve the latest AMI IDs for your template.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949156,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A DevOps Engineer needs a scalable Node.js application in AWS with a MySQL database. There should be no downtime during deployments and if issues occur rollback to a previous version must be easy to implement. The database may also be used by other applications.</p><p>Which solution meets these requirements?</p>",
          "answers": [
            "<p>Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier.</p>",
            "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack.</p>",
            "<p>Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier.</p>",
            "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack.</p>"
          ],
          "explanation": "<p>AWS Elastic Beanstalk offers automatic rollback options for deployment updates. This coupled with auto scaling and the ALB meets the requirements for a scalable compute and web tier. The RDS database provides a managed solution for the MySQL database. The RDS MySQL database should be created outside of the Elastic Beanstalk environment as it may be used by other applications. If it is created within the Elastic Beanstalk environment it could be automatically deleted if the environment is deleted.</p><p><strong>CORRECT: </strong>\"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack\" is incorrect.</p><p>As explained above the RDS database should be created outside of the Elastic Beanstalk environment.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>ECS does not offer automatic rollback so Elastic Beanstalk is a better solution to meet the requirements.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>Automating the creation of snapshots is not a suitable solution for rollback. Elastic Beanstalk offers several deployment options which offer automatic rollback.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>"
        }
      },
      {
        "id": 138248207,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A software development company is using GitHub, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline for its CI/CD process. To further improve their systems, they need to implement a solution that automatically detects and reacts to changes in the state of their deployments in AWS CodeDeploy. Any changes must be rolled back automatically if the deployment process fails, and a notification must be sent to the DevOps Team's Slack channel for easy monitoring.</p><p>Which of the following is the MOST suitable configuration that you should implement to satisfy this requirement?</p>",
          "answers": [
            "<p>Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team's Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when a deployment fails</code> setting.</p>",
            "<p>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the <code>Roll back when alarm thresholds are met</code> setting.</p>",
            "<p>Configure a CodeDeploy agent to send notification to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful.</p>",
            "<p>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team's Slack Channel when the <code>PutLifecycleEventHookExecutionStatus</code> API call has been detected. Rollback the changes by using the AWS CLI.</p>"
          ],
          "explanation": "<p>You can monitor <strong>CodeDeploy</strong> deployments using the following CloudWatch tools: Amazon EventBridge, CloudWatch alarms, and Amazon CloudWatch Logs.</p><p>Reviewing the logs created by the CodeDeploy agent and deployments can help you troubleshoot the causes of deployment failures. As an alternative to reviewing CodeDeploy logs on one instance at a time, you can use CloudWatch Logs to monitor all logs in a central location.</p><p>You can use <strong>Amazon EventBridge </strong>(formerly known as Amazon CloudWatch Events) to detect and react to changes in the state of an instance or a deployment (an \"event\") in your CodeDeploy operations. Then, based on the rules you create, EventBridge will invoke one or more target actions when a deployment or instance enters the state you specify in a rule. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-rule-12-09-2024.png\"></p><p>You can select the following types of targets when using EventBridge as part of your CodeDeploy operations:</p><p>- AWS Lambda functions</p><p>- Kinesis streams</p><p>- Amazon SQS queues</p><p>- Built-in targets (CloudWatch alarm actions)</p><p>- Amazon SNS topics</p><p>The following are some use cases:</p><p>- Use a Lambda function to pass a notification to a Slack channel whenever deployments fail.</p><p>- Push data about deployments or instances to a Kinesis stream to support comprehensive, real-time status monitoring.</p><p>- Use CloudWatch alarm actions to automatically stop, terminate, reboot, or recover Amazon EC2 instances when a deployment or instance event you specify occurs.</p><p>Hence, the correct answer is:<strong> Set up an Amazon EventBridge rule to monitor AWS CodeDeploy operations with a Lambda function as a target. Configure the lambda function to send out a message to the DevOps Team's Slack Channel in the event that the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when a deployment fails</strong></code><strong> setting.</strong></p><p>The option that says: <strong>Set up a CloudWatch Alarm that tracks the CloudWatch metrics of the CodeDeploy project. Configure the CloudWatch Alarm to automatically send out a message to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to use the </strong><code><strong>Roll back when alarm thresholds are met</strong></code><strong> setting</strong> is incorrect because CloudWatch Alarm can't directly send a message to a Slack Channel. You have to use an EventBridge with an associated Lambda function to notify the DevOps Team via Slack.</p><p>The option that says:<strong> Configure a CodeDeploy agent to send a notification to the DevOps Team's Slack Channel when the deployment fails. Configure AWS CodeDeploy to automatically roll back whenever the deployment is not successful</strong> is incorrect because a CodeDeploy agent is primarily used for deployment and not for sending custom messages to non-AWS resources such as a Slack Channel.</p><p>The option that says: <strong>Monitor the API calls in the CodeDeploy project using AWS CloudTrail. Send a message to the DevOps Team's Slack Channel when the </strong><code><strong>PutLifecycleEventHookExecutionStatus</strong></code><strong> API call has been detected. Rollback the changes by using the AWS CLI</strong> is incorrect because this API simply sets the result of a Lambda validation function. This is not a suitable solution since invoking various API calls is not necessary at all. You simply have to integrate an EventBridge rule with an associated Lambda function to your CodeDeploy project in order to meet the specified requirement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch-events.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-cloudwatch.html</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cwe-now-eb.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 115961527,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>When deploying a newly developed application on AWS, a DevOps team notices an intermittent error when attempting to make a connection to the application.</p><p>The application has a two-tier architecture with an AWS Lambda function backed by an Amazon API gateway and a NoSQL database as the data store.</p><p>The DevOps team noticed that sometime after deployment the error stops occurring. This application is deployed by AWS CodeDeploy and the Lambda function is deployed as the last step of pipeline.</p><p>What is the most efficient way for a DevOps engineer to resolve the issue?</p>",
          "answers": [
            "<p>Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function.</p>",
            "<p>Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond.</p>",
            "<p>Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed.</p>",
            "<p>Use the ValidateService hook to validate that the deployment was completed successfully.</p>"
          ],
          "explanation": "<p>An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Here are descriptions of the hooks available for use in your AppSpec file.</p><p>\u00b7 <strong>BeforeAllowTraffic</strong> \u2013 Use to run tasks before traffic is shifted to the deployed Lambda function version.</p><p>\u00b7 <strong>AfterAllowTraffic</strong> \u2013 Use to run tasks after all traffic is shifted to the deployed Lambda function version.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-44-54-3b27da721fdb31cd0114ba6bbff8f1d5.jpg\"><p><strong>CORRECT: </strong>\"Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond\" is incorrect.</p><p>You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><strong>INCORRECT:</strong> \"Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed\" is incorrect.</p><p>Since the error resolves after some time, the issue will most likely be resolved by ensuring the application is not brought online until it is ready.</p><p><strong>INCORRECT:</strong> \"Use the ValidateService hook to validate that the deployment was completed successfully\" is incorrect.</p><p>This is used to verify the deployment was completed successfully, this will only detect deployment status and will not help in this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 134588393,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. </p><p>As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?</p>",
          "answers": [
            "<p>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/ssm-patch-baselines.jpg\"></p><p><strong>Patch Manager</strong> uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p><strong>AWS Config</strong> provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements. Hence, the correct option is: <strong>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</strong></p><p>The option that says: <strong>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, it is stated in the scenario that the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC </strong>is incorrect because Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise, however, it does not check if your EC2 instances are using an approved AMI or not.</p><p>The option that says: <strong>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect because the AWS Shield Advanced service is more suitable in preventing DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 138248213,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A global cryptocurrency trading company has a suite of web applications hosted in an Auto Scaling group of Amazon EC2 instances across multiple Available Zones behind an Application Load Balancer to distribute the incoming traffic. The Auto Scaling group is configured to use Elastic Load Balancing health checks for scaling instead of the default EC2 status checks. However, there are several occasions when some instances are automatically terminated after failing the HTTPS health checks in the ALB that purges all the logs stored in the instance.</p><p>To improve system monitoring, a DevOps Engineer must implement a solution that collects all of the application and server logs effectively. The Operations team should be able to perform a root cause analysis based on the logs, even if the Auto Scaling group immediately terminated the instance.</p><p>How can the DevOps Engineer automate the log collection from the EC2 instances with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Pending:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance-terminate Lifecycle Action</code> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Automation to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent.</p>",
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Use AWS Step Functions to collect the application logs and send them to an Amazon CloudWatch Log group. Resume the instance termination once all the logs are sent to CloudWatch Logs.</p>",
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance-terminate Lifecycle Action</code> Auto Scaling Event with an associated AWS Systems Manager Automation document. Trigger the Amazon CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.</p>",
            "<p>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. Create an Amazon EventBridge rule for the <code>EC2 Instance Terminate Successful</code> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Run Command to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent.</p>"
          ],
          "explanation": "<p>The EC2 instances in an Auto Scaling group have a path, or lifecycle, that differs from that of other EC2 instances. The lifecycle starts when the Auto Scaling group launches an instance and puts it into service. The lifecycle ends when you terminate the instance, or the Auto Scaling group takes the instance out of service and terminates it.</p><p>You can add a lifecycle hook to your Auto Scaling group so that you can perform custom actions when instances launch or terminate.</p><p>When Amazon EC2 Auto Scaling responds to a scale out event, it launches one or more instances. These instances start in the <code>Pending</code> state. If you added an <code>autoscaling:EC2_INSTANCE_LAUNCHING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Pending</code> state to the <code>Pending:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Pending:Proceed</code> state. When the instances are fully configured, they are attached to the Auto Scaling group and they enter the <code>InService</code> state.</p><p>When Amazon EC2 Auto Scaling responds to a scale in event, it terminates one or more instances. These instances are detached from the Auto Scaling group and enter the <code>Terminating</code> state. If you added an <code>autoscaling:EC2_INSTANCE_TERMINATING</code> lifecycle hook to your Auto Scaling group, the instances move from the <code>Terminating</code> state to the <code>Terminating:Wait</code> state. After you complete the lifecycle action, the instances enter the <code>Terminating:Proceed</code> state. When the instances are fully terminated, they enter the <code>Terminated</code> state.</p><p><img src=\"https://media.tutorialsdojo.com/public/auto_scaling_lifecycle.png\"></p><p>Using CloudWatch agent is the most suitable tool to use to collect the logs. The unified CloudWatch agent enables you to do the following:</p><p>- Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html\">Metrics Collected by the CloudWatch Agent</a>.</p><p>- Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.</p><p>- Retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collectd</code> protocols. <code>StatsD</code> is supported on both Linux servers and servers running Windows Server. On the other hand, <code>collectd</code> is supported only on Linux servers.</p><p>- Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p><p><img src=\"https://media.tutorialsdojo.com/public/td-amazon-eventbridge-event-pattern-27Jan2025.png\"></p><p>You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is <code>CWAgent</code>, although you can specify a different namespace when you configure the agent.</p><p>Hence, the correct answer is: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance-terminate Lifecycle Action</strong></code><strong> Auto Scaling Event with an associated AWS Systems Manager Automation document. Trigger the Amazon CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.</strong></p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Pending:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance-terminate Lifecycle Action</strong></code><strong> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Automation to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent</strong> is incorrect because the <code><strong><em>Pending:Wait</em></strong></code><strong><em> </em></strong>state simply refers to the scale-out action in Amazon EC2 Auto Scaling and not for scale-in or for terminating the instances.</p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Use AWS Step Functions to collect the application logs and send them to an Amazon CloudWatch Log group. Resume the instance termination once all the logs are sent to CloudWatch Logs</strong> is incorrect because using AWS Step Functions is inappropriate when collecting the logs from your EC2 instances. You should use a CloudWatch agent instead.</p><p>The option that says: <strong>Delay the termination of unhealthy EC2 instances by adding a lifecycle hook to your Auto Scaling group to move instances in the </strong><code><strong>Terminating</strong></code><strong> state to the </strong><code><strong>Terminating:Wait</strong></code><strong> state. Create an Amazon EventBridge rule for the </strong><code><strong>EC2 Instance Terminate Successful</strong></code><strong> Auto Scaling Event with an associated AWS Lambda function. Use the AWS Systems Manager Run Command to run a script that collects and uploads the application logs from the instance to an Amazon CloudWatch Logs group. Resume the instance termination once all the logs are sent</strong> is incorrect. The <code><strong>EC2 Instance Terminate Successful</strong></code> indicates that the ASG has terminated an instance. The automated solution won't just work because the target instance is already deleted when the Lambda function is triggered.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/</a></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/cloud-watch-events.html#terminate-successful</a></p><p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p>"
        }
      },
      {
        "id": 138248217,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An American tech company used an AWS CloudFormation template to deploy its static corporate website hosted on Amazon S3 in the US East (N. Virginia) region. The template defines an Amazon S3 bucket with a Lambda-backed custom resource that downloads the content from a file server into the bucket. There is a new task for the DevOps Engineer to move the website to the US West (Oregon) region to better serve its customers on the West Coast with lower latency. However, the application stack could not be deleted successfully in CloudFormation. </p><p>Which among the following options shows the root cause of this issue, and how can the DevOps Engineer mitigate this problem for current and future versions of the website?</p>",
          "answers": [
            "<p>The CloudFormation stack deletion fails for an S3 bucket that is used as a static web hosting. To fix the issue, modify the CloudFormation template to remove the website configuration for the S3 bucket.</p>",
            "<p>The CloudFormation stack deletion fails for an S3 bucket that still has contents. To fix the issue, modify the Lambda function code of the custom resource to recursively empty the bucket if the stack is selected for deletion.</p>",
            "<p>The CloudFormation stack deletion fails for an S3 bucket because the <code>DeletionPolicy</code> attribute is set to <code>Snapshot</code>. To fix the issue, set the <code>DeletionPolicy</code> to <code>Delete</code> instead.</p>",
            "<p>The CloudFormation stack deletion fails for an S3 bucket because it is not yet empty. To fix the issue, set the <code>DeletionPolicy</code> to <code>ForceDelete</code> instead.</p>"
          ],
          "explanation": "<p>When you associate a Lambda function with a custom resource, the function is invoked whenever the custom resource is created, updated, or deleted. AWS CloudFormation calls a Lambda API to invoke the function and to pass all the request data (such as the request type and resource properties) to the function. The power and customizability of Lambda functions in combination with AWS CloudFormation enable a wide range of scenarios, such as dynamically looking up AMI IDs during stack creation, or implementing and using utility functions, such as string reversal functions.</p><p>AWS CloudFormation templates that declare an Amazon Elastic Compute Cloud (Amazon EC2) instance must also specify an Amazon Machine Image (AMI) ID, which includes an operating system and other software and configuration information used to launch the instance. The correct AMI ID depends on the instance type and region in which you're launching your stack. And IDs can change regularly, such as when an AMI is updated with software updates.</p><p>Normally, you might map AMI IDs to specific instance types and regions. To update the IDs, you manually change them in each of your templates. By using custom resources and AWS Lambda (Lambda), you can create a function that gets the IDs of the latest AMIs for the region and instance type that you're using so that you don't have to maintain mappings.</p><p><img src=\"https://media.tutorialsdojo.com/public/CloudFormation-AMIManager-Flow.png\"></p><p>You can also run the custom resource to recursively empty the bucket when the CloudFormation stack is triggered for deletion. In CloudFormation, you can only delete empty buckets. Any request for deletion will fail for buckets that still have contents. To control how AWS CloudFormation handles the bucket when the stack is deleted, you can set a deletion policy for your bucket. You can choose to retain the bucket or to delete the bucket.</p><p>Hence, the correct answer is: <strong>The CloudFormation stack deletion fails for an S3 bucket that still has contents. To fix the issue, modify the Lambda function code of the custom resource to recursively empty the bucket if the stack is selected for deletion.</strong></p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket that is used as a static web hosting. To fix the issue, modify the CloudFormation template to remove the website configuration for the S3 bucket </strong>is incorrect because the CloudFormation deletion process will not be hindered simply because your S3 bucket is configured for static web hosting. The primary root cause of this issue is that the CloudFormation stack deletion fails for an S3 bucket that still has contents.</p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket because the </strong><code><strong>DeletionPolicy</strong></code><strong> attribute is set to </strong><code><strong>Snapshot</strong></code><strong>. To fix the issue, set the </strong><code><strong>DeletionPolicy</strong></code><strong> to </strong><code><strong>Delete</strong></code><strong> instead </strong>is incorrect because you can only set the <code><strong><em>DeletionPolicy</em></strong></code> to either <code><strong><em>Retain </em></strong></code>or <code><strong><em>Delete</em></strong></code> for an Amazon S3 resource. In addition, the CloudFormation deletion will still fail as long as the S3 bucket is not empty, even if the <code>DeletionPolicy</code> attribute is already set to <code>Delete</code>.</p><p>The option that says: <strong>The CloudFormation stack deletion fails for an S3 bucket is not yet empty. To fix the issue, set the </strong><code><strong>DeletionPolicy</strong></code><strong> to </strong><code><strong>ForceDelete</strong></code><strong> instead</strong> is incorrect. Although the provided root cause is accurate, the configuration for <code><strong><em>DeletionPolicy </em></strong></code>remains invalid. <code><strong><em>ForceDelete</em></strong></code> is not a valid value for the deletion policy attribute.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/\">https://aws.amazon.com/blogs/devops/faster-auto-scaling-in-aws-cloudformation-stacks-with-lambda-backed-custom-resources/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 138248159,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company has a suite of applications that are hosted in AWS and each app has its own AMI. Currently, a new AMI must be manually created and deployed to the server if there is a new application version. A DevOps engineer was instructed to automate the process of generating the AMIs to streamline the company's CI/CD workflow. The ID of the newly created AMI must be stored in a centralized location where other build pipelines can programmatically access it.</p><p>Which of the following is the MOST cost-effective way to accomplish this requirement with the LEAST amount of overhead?</p>",
          "answers": [
            "<p>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store.</p>",
            "<p>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</p>",
            "<p>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket.</p>",
            "<p>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table.</p>"
          ],
          "explanation": "<p><strong>Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following.</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon EventBridge.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p><br></p><p>Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machine Images (AMIs) and recovering unreachable EC2 instances. Here are some examples:</p><p>- Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p>- Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances. An instance can become unreachable for various reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p><p><img alt=\"Custom AMI\" height=\"771\" src=\"https://media.tutorialsdojo.com/public/custom_ami_1.gif\" width=\"1000\"></p><p>A Systems Manager Automation document defines the Automation workflow (the actions that the Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more Amazon EC2 instances or creating an Amazon Machine Image (AMI). Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order.</p><p>Hence, the correct answer is: <strong>Using AWS Systems Manager, create an Automation document with values that configure how the machine image should be created. Launch a new pipeline with a custom action in AWS CodePipeline and integrate it with Amazon Eventbridge to execute the automation document. Build the AMI when the process is triggered. Store the AMI IDs in Systems Manager Parameter Store.</strong></p><p>The option that says: <strong>Set up a brand new pipeline in AWS CodePipeline with several Amazon EC2 instances for processing. Configure it to download and save the latest operating system of the application in an Open Virtualization Format (OVF) image format and then store the image in an Amazon S3 bucket. Customize the image using the guestfish interactive shell or the virt-rescue shell. Convert the OVF to an AMI using the virtual machine (VM) import command and then store the AMI IDs in AWS Systems Manager Parameter Store </strong>is incorrect because manually customizing the image using an interactive shell and downloading each application image in an OVF file will simply entails a lot of effort. It is also better to use the AWS Systems Manager Automation instead of creating a new pipeline in AWS CodePipeline.</p><p>The option that says: <strong>Using AWS CodePipeline, set up a new pipeline that will take an Amazon EBS snapshot of the EBS-backed Amazon EC2 instance which runs the latest version of each application. Launch a new EC2 instance from the generated snapshot and update the running instance using a AWS Lambda function. Take a snapshot of the updated EC2 instance and then afterward, convert it to an Amazon Machine Image (AMI). Store all of the AMI IDs in an Amazon S3 bucket </strong>is incorrect. Although you can technically generate an AMI using an EBS volume snapshot, this process is still tedious and entails a lot of configuration. Using the AWS Systems Manager Automation to generate the AMIs is a more suitable solution.</p><p>The option that says: <strong>Use an open-source machine image creation tool such as Packer, then configure it with values defining how the AMI should be created. Set up a Jenkins pipeline hosted in a large Amazon EC2 instance to start Packer when triggered to build an AMI. Store the AMI IDs in an Amazon DynamoDB table </strong>is incorrect. Although this may work, this solution will only costs more to maintain than other options since it uses an EC2 instance and an Amazon DynamoDB table. There is also an associated overhead in configuring and using Packer for generating the AMIs and preparing the Jenkins pipeline.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/\">https://aws.amazon.com/blogs/big-data/create-custom-amis-and-push-updates-to-a-running-amazon-emr-cluster-using-amazon-ec2-systems-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949174,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
          "answers": [
            "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
            "<p>Add test stages before and after the deploy action.</p>",
            "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
            "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"
          ],
          "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921464,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a social media company, you have implemented a CICD pipeline that takes code from a CodeCommit repository, builds it using CodeBuild thanks to the instructions in the local <code>Dockerfile</code>, and then pushes to ECR at <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code>. The last step of your CICD pipeline is to deploy to the application to your ECS cluster. It seems that while you do so, the application is only partly updated on some ECS instances which are running an older version of your image. You have found that terminating the instance or clearing the local Docker cache fixes the issue, but would like to implement something more robust that provides visibility and identification to track where container images are deployed.</p>\n\n<p>How should you implement a solution to address this issue?</p>\n",
          "answers": [
            "<p>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</p>",
            "<p>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</strong></p>\n\n<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. You can use the <code>IMAGEID</code> property, which is the SHA digest for the Docker image used to start the container.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n\n<p>The issue here is that the ECS instances do not detect that a newer image version is available, because the name <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is re-used. Therefore, by specifying the sha256 e.g.: <code>aws_account_id.dkr.ecr.region.amazonaws.com/my-web-app@sha256:94afd1f2e64d908bc90dbca0035a5b567EXAMPLE</code>, we are certain that newer versions of the Docker image will have a different hash value and therefore the ECS cluster will always pull the newest image at the end of our CICD Pipeline.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</strong> - SSM Run Command lets you remotely and securely manage the configuration of your managed instances. A managed instance is any EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. SSM Run Command may work but it's not an elegant solution.</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</strong> - Lambda Functions can't SSH into EC2 instances, so this option is incorrect.</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</strong> - Adding the <code>latest</code> tag won't help because <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is same as <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app:latest</code>. The <code>latest</code> tag cannot provide visibility and identification to track where container images are deployed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/\">https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n"
        }
      },
      {
        "id": 138248239,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A government agency recently decided to modernize its network infrastructure using AWS. They are developing a solution to store confidential files containing Personally Identifiable Information (PII) and other sensitive financial records of its citizens. All data in the storage solution must be encrypted both at rest and in transit. In addition, all of its data must also be replicated in two locations that are at least 450 miles apart from each other. </p><p>As a DevOps Engineer, what solution should you implement to meet these requirements?</p>",
          "answers": [
            "<p>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3 SSE-C encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
            "<p>Set up primary and secondary S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
            "<p>Set up primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create an IAM role to enforce access to the buckets only through HTTPS. Set up a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</p>",
            "<p>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce AWS KMS encryption on all objects uploaded to the bucket. Enable Transfer Acceleration between the two buckets. Set up a KMS Key in the primary region for encrypting objects.</p>"
          ],
          "explanation": "<p><strong>Availability Zones</strong> give customers the ability to operate production applications and databases that are more highly available, fault-tolerant, and scalable than would be possible from a single data center. AWS maintains multiple AZs around the world and more zones are added at a fast pace. Each AZ can be multiple data centers (typically 3), and at full scale can be hundreds of thousands of servers. They are fully isolated partitions of the AWS Global Infrastructure. With their own power infrastructure, the AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles of each other).</p><p>All AZs are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. The network performance is sufficient to accomplish synchronous replication between AZs. AWS Availability Zones are also powerful tools for helping build highly available applications. AZs make partitioning applications about as easy as it can be. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more.</p><p><img src=\"https://media.tutorialsdojo.com/public/Amazon-S3.png\"></p><p>By default, Amazon S3 allows both HTTP and HTTPS requests. To comply with the <strong>s3-bucket-ssl-requests-only</strong> rule, confirm that your bucket policies explicitly deny access to HTTP requests. Bucket policies that allow HTTPS requests without explicitly denying HTTP requests might not comply with the rule.</p><p>To determine HTTP or HTTPS requests in a bucket policy, use a condition that checks for the key <strong>\"aws:SecureTransport\"</strong>. When this key is <strong>true</strong>, this means that the request is sent through HTTPS. To be sure to comply with the <strong>s3-bucket-ssl-requests-only</strong> rule, create a bucket policy that explicitly denies access when the request meets the condition <strong>\"aws:SecureTransport\": \"false\"</strong>. This policy explicitly denies access to HTTP requests.</p><p>In this scenario, you should use AWS Regions since AZs are physically separated by only 100 km (60 miles) from each other. Within each AWS Region, S3 operates in a minimum of three AZs, each separated by miles to protect against local events like fires, floods et cetera. Take note that you can't launch an AZ-based S3 bucket.</p><p>Hence, the correct answer is: <strong>Set up primary and secondary S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</strong></p><p>The option that says: <strong>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3 SSE-C encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets</strong> is incorrect. You can't create Amazon S3 buckets in two separate Availability Zones since this is a regional service.</p><p>The option that says: <strong>Set up primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create an IAM role to enforce access to the buckets only through HTTPS. Set up a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets</strong> is incorrect. You have to use the bucket policy to enforce access to the bucket using HTTPS only and not an IAM role.</p><p>The option that says: <strong>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce AWS KMS encryption on all objects uploaded to the bucket. Enable Transfer Acceleration between the two buckets. Set up a KMS Key in the primary region for encrypting objects</strong> is incorrect. You have to enable Cross-Region replication and not Transfer Acceleration. This feature simply enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket but not data replication.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html \">https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/ \">https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/</a></p><p><a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">https://aws.amazon.com/about-aws/global-infrastructure/regions_az/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p>"
        }
      },
      {
        "id": 82921352,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A Big Data analytics company has deployed a stream processing application using KCL to read records from Kinesis Data Streams configured with multiple shards. The application is running on one EC2 instance. It seems that the consuming application is lagging under a large load and therefore records are not processed in time and eventually dropped from the stream.</p>\n\n<p>As a DevOps Engineer, you have been tasked with improving the reliability of this application with minimal changes, what should you do? (Select two)</p>\n",
          "answers": [
            "<p>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></p>",
            "<p>Increase the stream data retention period</p>",
            "<p>Migrate the application to AWS Lambda</p>",
            "<p>Increase the number of shards in Kinesis to increase throughput</p>",
            "<p>Decrease the numbers of shards in Kinesis to decrease the load</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></strong></p>\n\n<p>In a typical Kinesis Data Streams architecture, you have producers that continually push data to Kinesis Data Streams, and the consumers process the data in real-time. Consumers (such as a custom application running on Amazon EC2 or an Amazon Kinesis Data Firehose delivery stream) can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>Key concepts for Kinesis Data Streams:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>The Kinesis Client Library (KCL) ensures that for every shard, there is a record processor running and processing that shard. The library also simplifies reading data from the stream. The Kinesis Client Library uses an Amazon DynamoDB table to store control data.</p>\n\n<p>For the given use-case, you need to run KCL on multiple EC2 instances behind an ASG. Running more KCL processes is the key here, and we need for that to have an Auto Scaling Group based on the metric <code>MillisBehindLatest</code>, which represents the time that the current iterator is behind from the latest record (tip) in the shard. The Kinesis Client Library (KCL) for Amazon Kinesis Data Streams publishes custom Amazon CloudWatch metrics on your behalf, using the name of your KCL application as the namespace.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n\n<p><strong>Increase the stream data retention period</strong></p>\n\n<p>The retention period is the length of time that data records are accessible after they are added to the stream. A stream\u2019s retention period is set to a default of 24 hours after creation. To avoid records being dropped, it's good to increase the stream retention time and allow ourselves a higher margin to process the records. The maximum retention you can set is 7 days.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to AWS Lambda</strong> - Migrating the application to AWS Lambda will not help with the processing time, as eventually, the same processing code would run under EC2 or Lambda.</p>\n\n<p><strong>Increase the number of shards in Kinesis to increase throughput</strong> - Increasing the number of shards in Kinesis can increase the total throughput of the stream, but this does not impact the processing performance of your processes (which is bound by what you do with the messages). Increasing the number of shards though would help you increase the number of processing processes in KCL if that was already an upper bound (but currently we only have one KCL process running so it's not running at capacity).</p>\n\n<p><strong>Decrease the numbers of shards in Kinesis to decrease the load</strong> - Decrease the number of shards would decrease the throughput but again would have no effect on processing applications regarding their performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n"
        }
      },
      {
        "id": 138248111,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company is planning to deploy a new version of their legacy application in AWS which is deployed to an Auto Scaling group of EC2 instances with an Application Load Balancer in front. To avoid any disruption of their services, they need to implement canary testing first before all of the traffic is shifted to the new application version.</p><p>Which of the following solutions can meet this requirement?</p>",
          "answers": [
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</p>",
            "<p>Do a Canary deployment using CodeDeploy with a <code>CodeDeployDefault.LambdaCanary10Percent30Minutes</code> deployment configuration.</p>",
            "<p>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers.</p>",
            "<p>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment.</p>"
          ],
          "explanation": "<p>The purpose of a canary deployment is to reduce the risk of deploying a new version that impacts the <a href=\"https://wa.aws.amazon.com/wat.concept.workload.en.html\" title=\"The set of components that together deliver business value.\">workload</a>. The method will incrementally deploy the new version, making it visible to new users in a slow fashion. As you gain confidence in the deployment, you will deploy it to replace the current version in its entirety.</p><p><img src=\"https://media.tutorialsdojo.com/public/Upgrades_Image1.jpeg\"></p><p>To properly implement the canary deployment, you should do the following steps:</p><p>- Use a router or load balancer that allows you to send a small percentage of users to the new version.</p><p>- Use a dimension on your KPIs to indicate which version is reporting the metrics.</p><p>- Use the metric to measure the success of the deployment; this indicates whether the deployment should continue or rollback.</p><p>- Increase the load on the new version until either all users are on the new version or you have fully rolled back.</p><p><br></p><p>Hence, the correct answer is: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Create weighted Alias A records in Route 53 for the two Application Load Balancers to adjust the traffic.</strong></p><p>The option that says: <strong>Do a Canary deployment using CodeDeploy with a </strong><code><strong>CodeDeployDefault.LambdaCanary10Percent30Minutes</strong></code><strong> deployment configuration</strong> is incorrect because this specific configuration type is only applicable for Lambda functions and for the applications hosted in an Auto Scaling group.</p><p>The option that says: <strong>Prepare another stack that consists of an Application Load Balancer and Auto Scaling group which contains the new application version for blue/green environments. Use an Amazon CloudFront web distribution to adjust the weight of the incoming traffic to the two Application Load Balancers</strong> is incorrect because you can't use CloudFront to adjust the weight of the incoming traffic to your application. You should use Route 53 instead.</p><p>The option that says: <strong>Set up an Amazon API Gateway private integration with an Application Load Balancer and prepare a separate stage for the new application version. Configure the API Gateway to do a canary release deployment</strong> is incorrect because you can only integrate a Network Load Balancer to your Amazon API Gateway. Moreover, this service is only applicable for APIs, not full-fledged web applications.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html\">https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/\">https://aws.amazon.com/blogs/devops/blue-green-deployments-with-application-load-balancer/</a></p>"
        }
      },
      {
        "id": 138248229,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A multinational investment bank is implementing regulatory compliance checks over its AWS accounts. All API calls made on each AWS resource across the company's accounts must be monitored and tracked for auditing purposes. AWS CloudTrail will be used to monitor all API activities and detect sensitive security issues in the company's AWS accounts. The DevOps Team was assigned to come up with a solution to prevent CloudTrail from being disabled on some AWS accounts automatically.</p><p>What solution should be applied to ensure CloudTrail log deliveries experience the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of <code>Configuration changes</code>. By default, this managed rule will automatically remediate the accounts that disabled its CloudTrail.</p>",
            "<p>Use the <code>cloudtrail-enabled</code> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a <code>StopLogging</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>StartLogging</code> API on the resource ARN.</p>",
            "<p>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications.</p>",
            "<p>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a <code>DeleteTrail</code> event is detected, the Lambda function will re-enable the logging for that trail by calling the <code>CreateTrail</code> API on the resource ARN.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides <em>AWS managed rules</em>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly assess whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p><strong><img src=\"https://media.tutorialsdojo.com/public/td-aws-config-diagram-13Jan2025.png\"></strong>You can customize the behavior of a managed rule to suit your needs. For example, you can define the rule's scope to constrain which resources trigger an evaluation for the rule, such as EC2 instances or volumes. You can customize the rule's parameters to define attributes that your resources must have to comply with the rule. For example, you can customize a parameter to specify that your security group should block incoming traffic to a specific port number.</p><p>After you activate a rule, AWS Config compares your resources to the rule's conditions. After this initial evaluation, AWS Config continues to run evaluations each time one is triggered. The evaluation triggers are defined as part of the rule, and they can include the following types:</p><p><strong>Configuration changes</strong> \u2013 AWS Config triggers the evaluation when any resource that matches the rule's scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p><p><strong>Periodic</strong> \u2013 AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p><p>The cloudtrail-enabled checks whether AWS CloudTrail is enabled in your AWS account. Optionally, you can specify which S3 bucket, SNS topic, and Amazon CloudWatch Logs ARN to use.</p><p><img src=\"https://media.tutorialsdojo.com/aws-config-cloudtrail-enabled.JPG\"></p><p>Hence, the correct answer is: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule with a periodic interval of 1 hour to evaluate whether your AWS account enabled the AWS CloudTrail. Set up an Amazon EventBridge rule for AWS Config rules compliance change. Launch a Lambda function that uses the AWS SDK and add the Amazon Resource Name (ARN) of the Lambda function as the target in the Amazon EventBridge rule. Once a </strong><code><strong>StopLogging</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>StartLogging</strong></code><strong> API on the resource ARN.</strong></p><p>The option that says: <strong>Use the </strong><code><strong>cloudtrail-enabled</strong></code><strong> AWS Config managed rule to evaluate whether the AWS account enabled AWS CloudTrail with a trigger type of </strong><code><strong>Configuration changes</strong></code><strong>. This managed rule will automatically remediate the accounts that disabled its CloudTrail </strong>is incorrect because, by default, AWS Config will not automatically remediate the accounts that disabled its CloudTrail. You must manually set this up using an Amazon EventBridge rule and a custom Lambda function that calls the StartLogging API to enable CloudTrail back again. Furthermore, the <code><strong>cloudtrail-enabled</strong></code> AWS Config managed rule is only available for the <code>periodic trigger</code> type and not <code>Configuration changes</code>.</p><p>The option that says: <strong>Use AWS CDK to evaluate the CloudTrail status. Configure CloudTrail to send information to an Amazon SNS topic. Subscribe to the Amazon SNS topic to receive notifications</strong> is incorrect. AWS Cloud Development Kit (AWS CDK) is only an open-source software development framework for building cloud applications and infrastructure using programming languages. It isn't used to check whether the CloudTrail is enabled in an AWS account.</p><p>The option that says: <strong>Integrate Amazon EventBridge and AWS Lambda to have an automated process that runs every minute to query the CloudTrail in the current account. Ensure that the Lambda function uses the AWS SDK. Once a </strong><code><strong>DeleteTrail</strong></code><strong> event is detected, the Lambda function will re-enable the logging for that trail by calling the </strong><code><strong>CreateTrail</strong></code><strong> API on the resource ARN</strong> is incorrect. Instead, you should detect the <code>StopLogging</code> event and call the StartLogging API to enable CloudTrail again. The <code>DeleteTrail</code> and <code>CreateTrail</code> events, as their name implies, are simply for deleting and creating the trails respectively.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/\">https://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/v2/guide/home.html\">https://docs.aws.amazon.com/cdk/v2/guide/home.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 99528227,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>An application is being deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The security team requires that the traffic is secured with SSL/TLS certificates. Protection against common web exploits must also be implemented. The solution should not have a performance impact on the EC2 instances.</p><p>What steps should be taken to secure the web application? (Select TWO.)</p>",
          "answers": [
            "<p>Create an AWS WAF web ACL and attach it to the ALB.</p>",
            "<p>Add an SSL/TLS certificate to a secure listener on the ALB.</p>",
            "<p>Install SSL/TLS certificates on the EC2 instances.</p>",
            "<p>Configure Server-Side Encryption with KMS managed keys.</p>",
            "<p>Enable EBS encryption for the EC2 volumes to encrypt all traffic.</p>"
          ],
          "explanation": "<p>To secure the traffic in transit an SSL/TLS certificate should be attached to a secure listener on the ALB. This will not affect the performance of the EC2 instances as the encryption takes place only between the client and the ALB. The certificate can be issued through AWS Certificate Manager (ACM).</p><p>The AWS Web Application Firewall (AWS WAF) protects against common web exploits. The company can create a web ACL with a rule and action and then attach it to the ALB. This will protect against web exploits.</p><p><strong>CORRECT: </strong>\"Add an SSL/TLS certificate to a secure listener on the ALB\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL and attach it to the ALB\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install SSL/TLS certificates on the EC2 instances\" is incorrect.</p><p>Encryption on the EC2 instances would impact the performance of those instances.</p><p><strong>INCORRECT:</strong> \"Configure Server-Side Encryption with KMS managed keys\" is incorrect.</p><p>This is not relevant to in transit encryption, this is used to encrypt data at rest on services such as Amazon S3.</p><p><strong>INCORRECT:</strong> \"Enable EBS encryption for the EC2 volumes to encrypt all traffic\" is incorrect.</p><p>EBS encryption is used for encrypting data at rest. The question requires encryption using SSL/TLS certificates which is encryption in transit.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\">https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\">https://aws.amazon.com/elasticloadbalancing/application-load-balancer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      }
    ],
    "answers": {
      "75949056": [
        "d"
      ],
      "75949118": [
        "a"
      ],
      "75949156": [
        "d"
      ],
      "75949174": [
        "c"
      ],
      "82921318": [
        "b"
      ],
      "82921352": [
        "a",
        "b"
      ],
      "82921464": [
        "a"
      ],
      "99528227": [
        "a",
        "b"
      ],
      "115961527": [
        "a"
      ],
      "134588393": [
        "d"
      ],
      "134588431": [
        "d"
      ],
      "138248111": [
        "b"
      ],
      "138248159": [
        "b"
      ],
      "138248203": [
        "a",
        "b"
      ],
      "138248207": [
        "a"
      ],
      "138248213": [
        "c"
      ],
      "138248217": [
        "d"
      ],
      "138248229": [
        "a"
      ],
      "138248239": [
        "b"
      ],
      "143860761": [
        "c",
        "d"
      ]
    }
  },
  {
    "id": "1769846401359",
    "date": "2026-01-31T08:00:01.359Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 7,
    "incorrect": 3,
    "unanswered": 0,
    "total": 10,
    "percent": 70,
    "duration": 5057695,
    "questions": [
      {
        "id": 134588405,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A business wants to leverage AWS CloudFormation to deploy its infrastructure. The business would like to restrict deployment to two particular regions and wants to implement a strict tagging requirement. Developers are expected to deploy various versions of the same application and want to guarantee that resources are deployed in compliance with the business policy while still enabling developers to deploy different versions of the application.</p><p>Which of the following is the MOST suitable solution?</p>",
          "answers": [
            "<p>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</p>",
            "<p>Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks.</p>",
            "<p>Utilize approved CloudFormation templates and launch CloudFormation StackSets.</p>",
            "<p>Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation.</p>"
          ],
          "explanation": "<p>With <strong>AWS Service Catalog</strong>, cloud resources can be centrally managed to achieve infrastructure as code (IaC) template governance at scale, whether written in CloudFormation or Terraform. Compliance requirements can be met while ensuring customers can efficiently deploy the necessary cloud resources.</p><p><img alt=\"Service Catalog\" height=\"540\" src=\"https://media.tutorialsdojo.com/public/dop-c02-service-catalog.png\" width=\"1000\"></p><p>Template constraints can be applied when limiting end-users' options during a product launch. This ensures that the organization's compliance requirements are not breached.</p><p>A product must be present within a Service Catalog portfolio to apply template constraints. A template constraint includes rules that narrow the allowable values for parameters in the underlying AWS CloudFormation template of the product. These parameters define the set of values available to users when creating a stack. For instance, an instance type parameter can be specified to limit the types of instances that users can choose from when launching a stack containing EC2 instances.</p><p>Hence, the correct answer is: <strong>Utilize AWS Service Catalog and create products with approved CloudFormation templates.</strong></p><p>The option that says:<strong> Utilize approved CloudFormation templates and launch CloudFormation StackSets </strong>is incorrect because StackSets manage deployments across accounts and regions but do not enforce tagging or region restrictions. They do not typically provide governance to prevent non-compliant implementations.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating AWS Trusted Advisor checks </strong>is incorrect because Trusted Advisor does not support checks for unauthorized StackSets or enforce CloudFormation template compliance.</p><p>The option that says:<strong> Detect and remediate unapproved CloudFormation Stacksets by creating a CloudFormation drift detection operation </strong>is incorrect because drift detection only identifies changes after deployment, but cannot prevent non-compliant resource creation or enforce policies before deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/servicecatalog/\">https://aws.amazon.com/servicecatalog/</a></p><p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/\">https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/</a></p><p><br></p><p><strong>Check out this AWS Service Catalog Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-service-catalog/?src=udemy\">https://tutorialsdojo.com/aws-service-catalog/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional-exam-guide-study-path-dop-c01-dop-c02/</a></p>"
        }
      },
      {
        "id": 115961529,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An online sales application is being migrated to AWS with the application layer hosted on Amazon EC2 instances and the database layer on a PostgreSQL database. It is mandated that the application must have minimal downtime as it receives traffic 24/7 and any downtime may reduce business revenue. The application must also be fault tolerant including the data layer.</p><p>Concerns have been raised around performance of the database layer during sales events and other peak periods. The application must also be continually scanned for vulnerabilities.</p><p>Which option will meet the above requirements?</p>",
          "answers": [
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use RDS PostgreSQL for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments.</p>",
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon Macie to perform automatic security assessments.</p>",
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon GuardDuty to perform automatic security assessments.</p>",
            "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments.</p>"
          ],
          "explanation": "<p>The above question clearly mandates three requirements:</p><p>1. Performance- Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group</p><p>2. Database performance- Amazon Aurora will perform better than PostgreSQL since it provides multi-master configuration and can be scaled better than RDS on a global scale.</p><p>3. Vulnerability assessment- Amazon Inspector is the right fit for the scanning. The difference between Amazon Inspector and Amazon GuardDuty is that the former \"checks what happens when you actually get an attack\" and the latter \"analyzes the actual logs to check if a threat exists\". The purpose of Amazon Inspector is to test whether you are addressing common security risks in the target AWS.</p><p>Database categorization and selection parameters:</p><p>\u00b7 If your scaling needs are for standard/ general purpose applications, RDS is the better option. You can auto-scale the database to max capacity with just a few clicks on the AWS console.</p><p>\u00b7 You also have the option of Aurora Serverless that can scale up or scale down well, you have to be aware of several <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations\">restrictions that apply in the Serverless mode</a>.</p><p>\u00b7 If you must handle a very high volume of read/write requests, DynamoDB is a better choice. It scales seamlessly with no impact on performance. You can run these database servers in on-demand or provisioned capacity mode.</p><p>If you have heavy write workloads and require more than five read replicas, Aurora is a better choice. Since Aurora uses shared storage for writer and readers, there is minimal replica lag. RDS allows only up to five replicas and the replication process is slower than Aurora.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-46-18-8cd4e56a381e9f1f934f96761a29cae7.jpg\"><p><strong>CORRECT: </strong>\"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use RDS PostgreSQL for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments\" is incorrect.</p><p>Amazon Aurora is a better fit for this use case as described above.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon Macie to perform automatic security assessments\" is incorrect.</p><p>Amazon Inspector should be used for performing the security assessments.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon GuardDuty to perform automatic security assessments\" is incorrect.</p><p>Amazon Inspector should be used for performing the security assessments.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p>https://aws.amazon.com/blogs/database/is-amazon-rds-for-postgresql-or-amazon-aurora-postgresql-a-better-choice-for-me/</p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>"
        }
      },
      {
        "id": 82921406,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A multi-national retail company has defined tagging guidelines and standard for all its resources in AWS and would like to create a dashboard to visualize the compliance of all the resources with the ability to find out the non-compliant resources. The company has hired you as an AWS Certified DevOps Engineer Professional to develop a solution for this requirement.</p>\n\n<p>Which of the following options would you suggest to address the use-case?</p>\n",
          "answers": [
            "<p>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</p>",
            "<p>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</p>",
            "<p>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</p>",
            "<p>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</strong></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p>Here, we can use AWS Config to track resource configuration, and we could create a rule to track the tagging of these resources. All the changes to resource configuration as well as tagging of resources are streamed to an SNS topic.</p>\n\n<p>A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags make it easier to manage, search for, and filter resources. You can use tags to categorize your AWS resources in different ways, for example, by purpose, owner, or environment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p>You can set up the Required-Tag managed rule for Config which requires up to 6 tags with optional values in a single rule. Previously, each rule accepted only a single tag/value combo. Additionally, the Required-Tag managed rule now accepts a comma-separated list of values for each checked tag. This allows for a rule to be compliant if any one of a supplied list of tags is present on the resource.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</strong> - AWS Service Catalog enabling AWS customers to create and deliver standardized services that provide the necessary control, while still empowering developers to choose the services that best fit their needs. You cannot use Service Catalog to get an inventory of all the resources in your account.</p>\n\n<p><strong>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</strong> - SSM inventory will only help with understanding what is installed on your managed instances. To view Systems Manager Inventory history and change tracking for all of your managed instances, you need to use AWS Config itself.</p>\n\n<p><strong>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</strong> - CloudTrail is used to track API calls, not resources. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n"
        }
      },
      {
        "id": 75949108,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>",
          "answers": [
            "<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"
          ],
          "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921460,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A gaming company would like to be able to receive near real-time notifications when the API call <code>DeleteTable</code> is invoked in DynamoDB.</p>\n\n<p>As a DevOps Engineer at the company, how would you implement this at a minimal cost?</p>\n",
          "answers": [
            "<p>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</p>",
            "<p>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</p>",
            "<p>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</p>",
            "<p>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. Rules in CloudWatch Events work only in the Region in which they are created. If you configure CloudTrail to track API calls in multiple Regions, and you want a rule-based on CloudTrail to trigger in each of those Regions, you must create a separate rule in each Region that you want to track.</p>\n\n<p>For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q40-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</strong> - A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. DynamoDB Streams do not capture <code>DeleteTable</code> API calls, they only capture item-level events.</p>\n\n<p><strong>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</strong> - Sending CloudTrail Logs to CloudWatch Logs and creating a filter on those will work but will be expensive, as we're streaming all the logs from CloudTrail just to extract a single event.</p>\n\n<p><strong>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</strong> - CloudTrail trails do not have event filters and cannot be directly sent to a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p>\n"
        }
      },
      {
        "id": 75949166,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An application sits behind a Network Load Balancer (NLB) that is configured with a TLS listener. The DevOps team must analyze traffic patterns and require information about the connections made by clients. The data that is captured must be stored securely with encryption at rest and should only be accessible to the DevOps team members.</p><p>Which actions should a DevOps engineer take?</p>",
          "answers": [
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019.</p>",
            "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing provides access logs that capture detailed information about the TLS requests sent to your Network Load Balancer. You can use these access logs to analyze traffic patterns and troubleshoot issues. The logs are sent to an Amazon S3 bucket you configure as the logging destination. This bucket can be encrypted using one of the available server-side encryption options.</p><p>When you enable access logging, you must specify an S3 bucket for the access logs. The policy must grant permission to the AWS service account \u2018delivery.logs.amazonaws.com\u2019. In this case, the DevOps team also require permissions to access the bucket, and this can be granted through an IAM policy attached to the team members, most likely via an IAM group.</p><p><strong>CORRECT: </strong>\"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account\" is incorrect.</p><p>This will not allow read access for the DevOps team as the only permission is write access.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 82921334,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.</p>\n\n<p>How can you do this most securely?</p>\n",
          "answers": [
            "<p>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</p>",
            "<p>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>",
            "<p>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</p>",
            "<p>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</strong></p>\n\n<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.</p>\n\n<p>You can use Secrets Manager to natively rotate credentials for Amazon Relational Database Service (RDS), Amazon DocumentDB, and Amazon Redshift. You can extend Secrets Manager to rotate other secrets, such as credentials for Oracle databases hosted on EC2 or OAuth refresh tokens, by modifying sample AWS Lambda functions available in the Secrets Manager documentation.</p>\n\n<p>To access PostgreSQL, you can use database credentials and they're best stored in Secrets Manager from a security best-practices perspective. Access to Secrets Manager itself is regulated using an IAM role with the requisite policy. You must write this IAM policy permitting your application on EC2 instances to access specific secrets. Then, in the application source code, you can replace secrets in plain text with code to retrieve these secrets programmatically using the Secrets Manager APIs. To access the DynamoDB table, you should also add the appropriate policy to this IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - As mentioned in the explanation above, Secrets Manager does NOT support DynamoDB, so this option is incorrect.</p>\n\n<p><strong>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</strong> - It's not recommended to store RDS credentials in a DynamoDB, as it can be accessed by everyone who has access to the underlying table. This constitutes a serious security threat.</p>\n\n<p><strong>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - Storing IAM user credentials in Secrets Manager is a distractor as IAM user credentials are not required to build a solution for this use-case. You can just use an IAM instance role for EC2 to access Secrets Manager.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/faqs/\">https://aws.amazon.com/secrets-manager/faqs/</a></p>\n"
        }
      },
      {
        "id": 75949116,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company has deployed a web service that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company has deployed the application in us-east-1. The web service uses Amazon Route 53 records for DNS requests for example.com. The records are configured with health checks that assess the availability of the web service.</p><p>A second environment has been deployed into eu-west-1. The company requires traffic to be routed to the environment that provides the lowest latency for user requests. In the event of a regional outage, traffic should be directed to the alternate Region.</p><p>Which configuration will achieve these requirements?</p>",
          "answers": [
            "<p>Create a subdomain named us.example.com with failover routing. Configure the US ALB as primary and the EU ALB as secondary. Create another subdomain named eu.example.com with failover routing. Configure the EU ALB as primary and the US ALB as secondary. Create latency-based routing records for example.com that are aliased to us.example.com and eu.example.com.</p>",
            "<p>Create a subdomain named us.example.com with weighted routing. Configure the US ALB with weight 2 and the EU ALB with weight 1. Create another subdomain named eu.example.com with weighted routing. Configure the EU ALB with weight 2 and the US ALB with weight 1. Create geolocation routing records for example.com with North America aliased to us.example.com and Europe aliased to eu.example.com.</p>",
            "<p>Create a subdomain named us.example.com with latency-based routing. Configure the US ALB as the first target and the EU ALB as the second target. Create another subdomain named eu.example.com with latency-based routing. Configure the EU ALB as the first target and the US ALB as the second target. Create failover routing records for example.com aliased to us.example.com as the first target and eu.example.com as the second target.</p>",
            "<p>Create a subdomain named us.example.com with multivalue answer routing. Configure the US ALB first and the EU ALB second. Create another subdomain named eu.example.com with multivalue answer routing. Configure the EU ALB first and the US ALB second. Create failover routing records for example.com that are aliased to us.example.com and eu.example.com.</p>"
          ],
          "explanation": "<p>There are two key requirements that inform the design. Firstly, the solution must route based on latency. Secondly, failover across Regions must be automatic. This is a more complex DNS routing configuration. To meet both requirements the company will need to use a combination of latency-based routing records and failover routing records.</p><p>The solution is to create subdomains for each Region that can be used for the failover records and pointing the secondary to the alternate Region. Then, on top of those records the solution includes a latency-based routing record for example.com.</p><p>With this solution example.com will resolve to the subdomain that represents the lowest latency from the user request location. If the environment in that Region is not available (has failed health checks) then the request will be failed over to the alternate Region.</p><p><strong>CORRECT: </strong>\"Create a subdomain named us.example.com with failover routing. Configure the US ALB as primary and the EU ALB as secondary. Create another subdomain named eu.example.com with failover routing. Configure the EU ALB as primary and the US ALB as secondary. Create latency-based routing records for example.com that are aliased to us.example.com and eu.example.com\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with weighted routing. Configure the US ALB with weight 2 and the EU ALB with weight 1. Create another subdomain named eu.example.com with weighted routing. Configure the EU ALB with weight 2 and the US ALB with weight 1. Create geolocation routing records for example.com with North America aliased to us.example.com and Europe aliased to eu.example.com\" is incorrect.</p><p>The solution should use failover routing and latency routing, not weighted routing and geolocation routing.</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with latency-based routing. Configure the US ALB as the first target and the EU ALB as the second target. Create another subdomain named eu.example.com with latency-based routing. Configure the EU ALB as the first target and the US ALB as the second target. Create failover routing records for example.com aliased to us.example.com as the first target and eu.example.com as the second target\" is incorrect.</p><p>This solution gets the failover and latency records the wrong way around. Failover routing should be used for the subdomain and latency routing for the apex domain (example.com).</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with multivalue answer routing. Configure the US ALB first and the EU ALB second. Create another subdomain named eu.example.com with multivalue answer routing. Configure the EU ALB first and the US ALB second. Create failover routing records for example.com that are aliased to us.example.com and eu.example.com\" is incorrect.</p><p>Multivalue routing is a form of DNS load balancing and will simply route records across all registered and available targets. This does not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>"
        }
      },
      {
        "id": 75949158,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A financial services company requires that DevOps engineers should not log directly into Amazon EC2 instances that process highly sensitive data except in exceptional circumstances. The security team requires a notification within 15 minutes if a DevOps engineer does log in to an instance.</p><p>Which solution will meet these requirements with the least operational overhead?</p>",
          "answers": [
            "<p>Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>",
            "<p>Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>"
          ],
          "explanation": "<p>The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. The agent includes the following components:</p><p>\u00b7 A plug-in to the AWS CLI that pushes log data to CloudWatch Logs.</p><p>\u00b7 A script (daemon) that initiates the process to push data to CloudWatch Logs.</p><p>\u00b7 A cron job that ensures that the daemon is always running.</p><p>You can create metric filters to match terms in your log events and convert log data into metrics. When a metric filter matches a term, it increments the metric's count. For example, you can create a metric filter that counts the number of times the word <strong><em>ERROR</em></strong> occurs in your log events.</p><p>In this case the metric filter can search for user login data and then if this information is found it can send an SNS notification to the security team.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>The Systems Manager agent will not gather this information from EC2 instances. The CloudWatch Logs agent must be installed.</p><p><strong>INCORRECT:</strong> \"Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>CloudTrail will only report on API activity, and this does not include login data from an Amazon EC2 instance.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>This is possible though it is not the best solution as it requires the script to be rerun on a regular basis and requires more operational overhead to create and maintain.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 75949120,
        "correct_response": [
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>An application running on an Amazon EC2 instance stores sensitive data on an attached Amazon EBS volume. The volume is not encrypted. A DevOps engineer must enable encryption at rest for the data.</p><p>Which actions should the engineer take? (Select TWO.)</p>",
          "answers": [
            "<p>Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume.</p>",
            "<p>Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data.</p>",
            "<p>Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted.</p>",
            "<p>Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume.</p>",
            "<p>Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume.</p>"
          ],
          "explanation": "<p>You cannot enable encryption for an existing EBS volume. You must enable encryption of the volume at creation time. There are a couple of ways to enable encryption of data stored on an unencrypted volume:</p><p>1) Create a snapshot of the volume. The snapshot will also be unencrypted, but you can then copy it and enable encryption. Then, you can create an encrypted volume from the snapshot and attach it to the instance.</p><p>2) Create and mount a new, encrypted EBS volume. The engineer would then need to move data onto the volume.</p><p>In both cases the engineer will need to update the application to use the new volume.</p><p><strong>CORRECT: </strong>\"Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume\" is incorrect.</p><p>This has not resulted in any change to the EBS volume, it is still unencrypted.</p><p><strong>INCORRECT:</strong> \"Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data\" is incorrect.</p><p>You cannot enable encryption for existing volumes through any AWS tools.</p><p><strong>INCORRECT:</strong> \"Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume\" is incorrect.</p><p>SSL/TLS certificates are used for enabling encryption in-transit, not encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>"
        }
      }
    ],
    "answers": {
      "75949108": [
        "c"
      ],
      "75949116": [
        "a"
      ],
      "75949120": [
        "c",
        "e"
      ],
      "75949158": [
        "a"
      ],
      "75949166": [
        "a"
      ],
      "82921334": [
        "a"
      ],
      "82921406": [
        "a"
      ],
      "82921460": [
        "d"
      ],
      "115961529": [
        "d"
      ],
      "134588405": [
        "a"
      ]
    }
  },
  {
    "id": "1769688092801",
    "date": "2026-01-29T12:01:32.801Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 12,
    "incorrect": 8,
    "unanswered": 0,
    "total": 20,
    "percent": 60,
    "duration": 5297548,
    "questions": [
      {
        "id": 82921412,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>An IT company is deploying a Python Flask based application and would like to ensure that it has a base AMI that contains the necessary Python runtime, as well as OS patches. That AMI must be used able to be referenced programmatically from across all regions in your account in a scalable way. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>Which of the following options would you recommend for this use-case? (Select two)</p>\n",
          "answers": [
            "<p>Create an SSM Automation document to create the AMI in a repeatable manner</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</p>",
            "<p>Use AWS Inspector to create a patched AMI using the latest working AMI</p>",
            "<p>Use AWS Lambda to create a patched AMI using the latest working AMI</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an SSM Automation document to create the AMI in a repeatable manner</strong></p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>You can use AWS Systems Manager to create a maintenance window, and then register an Automation task to automate the creation of the AMIs. This process is applicable for both Windows and Linux instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q46-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</strong></p>\n\n<p>The AMI ID is region-scoped, so the AMI must be copied across regions and therefore each SSM parameter store will have different AMI ID values. But you can still use the same SSM Parameter Store key across all regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</strong> - The AMI ID is region-scoped and the AMI must be copied across regions for the solution to work. This option only copies the value of the AMI ID across regions but the AMI itself stays in one region. So this option is incorrect.</p>\n\n<p><strong>Use AWS Inspector to create a patched AMI using the latest working AMI</strong> - AWS Inspector can be leveraged to analyze EC2 instance OS and network vulnerabilities. You cannot use Inspector to create a patched AMI.</p>\n\n<p><strong>Use AWS Lambda to create a patched AMI using the latest working AMI</strong> - AWS Lambda cannot be used to create AMIs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n"
        }
      },
      {
        "id": 138248241,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A commercial bank has a hybrid cloud architecture in AWS where its online banking platform is hosted. The CTO instructed its Lead DevOps Engineer to implement a suitable deployment set up that minimizes the impact on their production environment. The CI/CD process should be configured as follows: </p><p>- A new fleet of Amazon EC2 instances should be automatically launched first before the actual production deployment. The additional instances will serve traffic during the deployment. </p><p>- All available EC2 instances across various Availability Zones must be load-balanced and must automatically heal if it becomes impaired due to an underlying hardware failure in Amazon EC2. </p><p>- At least half of the incoming traffic must be rerouted to the new application version that is hosted to the new instances. </p><p>- The deployment should be considered successful if traffic is rerouted to at least half of the available EC2 instances. </p><p>- All temporary files must be deleted before routing traffic to the new fleet of instances. Ensure that any other files that were automatically generated during the deployment process are removed. </p><p>- To reduce costs, the EC2 instances that host the old version in the deployment group must be terminated immediately. </p><p>What should the Engineer do to satisfy these requirements?</p>",
          "answers": [
            "<p>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the <code>CodeDeployDefault.OneAtAtime</code> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the <code>AllowTraffic</code> hook within the <code>appspec.yml</code> configuration file to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the <code>BeforeAllowTraffic</code> Traffic hook within <code>appspec.yml</code> to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the <code>CodeDeployDefault.HalfAtAtime</code> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>",
            "<p>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the &lt;code&gt;CodeDeployDefault AllatOnce&lt;/code&gt; as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the <code>BlockTraffic</code> hook within <code>appspec.yml</code> to purge the temporary files.</p>"
          ],
          "explanation": "<p>The content in the <code>'hooks'</code> section of the AppSpec file varies, depending on the compute platform for your deployment. The <code>'hooks'</code> section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The <code>'hooks'</code> section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.</p><p>An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line. Here are descriptions of the hooks available for use in your AppSpec file.</p><p><code><strong>ApplicationStop</strong></code> \u2013 This deployment lifecycle event occurs even before the application revision is downloaded. You can specify scripts for this event to gracefully stop the application or remove currently installed packages in preparation for a deployment. The AppSpec file and scripts used for this deployment lifecycle event are from the previous successfully deployed application revision.</p><p><code><strong>DownloadBundle</strong></code> \u2013 During this deployment lifecycle event, the CodeDeploy agent copies the application revision files to a temporary location:</p><p><code>/opt/codedeploy-agent/deployment-root/<em>deployment-group-id</em>/<em>deployment-id</em>/deployment-archive</code> folder on Amazon Linux, Ubuntu Server, and RHEL Amazon EC2 instances.</p><p><code>C:\\ProgramData\\Amazon\\CodeDeploy\\<em>deployment-group-id</em>\\<em>deployment-id</em>\\deployment-archive</code> folder on Windows Server Amazon EC2 instances.</p><p>This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>BeforeInstall</strong></code> \u2013 You can use this deployment lifecycle event for preinstall tasks, such as decrypting files and creating a backup of the current version.</p><p><code><strong>Install</strong></code> \u2013 During this deployment lifecycle event, the CodeDeploy agent copies the revision files from the temporary location to the final destination folder. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterInstall</strong></code> \u2013 You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions.</p><p><code><strong>ApplicationStart</strong></code> \u2013 You typically use this deployment lifecycle event to restart services that were stopped during <code>ApplicationStop</code>.</p><p><code><strong>ValidateService</strong></code> \u2013 This is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.</p><p><code><strong>BeforeBlockTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances before they are deregistered from a load balancer.</p><p><code><strong>BlockTraffic</strong></code> \u2013 During this deployment lifecycle event, internet traffic is blocked from accessing instances that are currently serving traffic. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterBlockTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><code><strong>BeforeAllowTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances before they are registered with a load balancer.</p><p><code><strong>AllowTraffic</strong></code> \u2013 During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p><code><strong>AfterAllowTraffic</strong></code> \u2013 You can use this deployment lifecycle event to run tasks on instances after they are registered with a load balancer.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/2019-12-16_03-04-56-691d69e91f667f1c9bb8b0554c2da933.png\"></p><p>Hence, the correct answer is: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Create a custom deployment configuration for the deployment group in CodeDeploy with minimum healthy hosts defined as 50% and configure it to also terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BeforeAllowTraffic</strong></code><strong> Traffic hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files.</strong></p><p>The option that says: <strong>Launch an Application Load Balancer and use in-place deployment for releasing new application versions. Use the </strong><code><strong>CodeDeployDefault.OneAtAtime</strong></code><strong> as the deployment configuration and associate the Auto Scaling group with the deployment group. Configure AWS CodeDeploy to terminate all EC2 instances in the original Auto Scaling group and use the </strong><code><strong>AllowTraffic</strong></code><strong> hook within the </strong><code><strong>appspec.yml</strong></code><strong> configuration file to purge the temporary files</strong> is incorrect because you should use blue/green deployment instead of in-place. In addition, the <code><strong><em>AllowTraffic</em></strong></code> event just allows the incoming traffic to the instances after a deployment. This event is reserved for the CodeDeploy agent and cannot be used to run scripts.</p><p>The option that says: <strong>Launch an Application Load Balancer and use a blue/green deployment for releasing new application versions. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.HalfAtAtime</strong></code><strong> as the deployment configuration and configure it to terminate the original instances in the Auto Scaling group after deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appspec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because the <code>BlockTraffic</code> event is reserved for the CodeDeploy agent and cannot be used to run custom scripts such as deleting the temporary files.</p><p>The option that says: <strong>Launch an Application Load Balancer and use an in-place deployment for releasing new application versions. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. In CodeDeploy, use the </strong><code><strong>CodeDeployDefault.AllatOnce</strong></code><strong> as a deployment configuration and add a configuration to terminate the original instances in the Auto Scaling group after the deployment. Use the </strong><code><strong>BlockTraffic</strong></code><strong> hook within </strong><code><strong>appsec.yml</strong></code><strong> to purge the temporary files</strong> is incorrect because you should use a blue/green deployment instead of in-place. It is also incorrect to use the <code>CodeDeployDefault AllatOnce</code> deployment configuration as this attempts to deploy the application revision to as many instances as possible at once.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-lifecycle-event-failures</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-server</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p>"
        }
      },
      {
        "id": 82921416,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>The compliance department at a Wall Street trading firm has hired you as an AWS Certified DevOps Engineer Professional to help with several strategic DevOps initiatives. The department has asked you to regularly generate the list of all the software packages installed on the EC2 instances. The solution needs to be able to extend to future instances in the AWS account and send notifications if the instances are not set up correctly to track their software.</p>\n\n<p>Which of the following options are the best-fit solutions that require the least effort to meet the given requirements? (Select two)</p>\n",
          "answers": [
            "<p>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</p>",
            "<p>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</p>",
            "<p>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</p>",
            "<p>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</p>",
            "<p>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</strong></p>\n\n<p>SSM Agent is an Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources. The agent processes requests from the Systems Manager service in the AWS Cloud, and then run them as specified in the request. SSM Agent then sends status and execution information back to the Systems Manager service by using the Amazon Message Delivery Service (service prefix: ec2messages).</p>\n\n<p>SSM Inventory provides visibility into your Amazon EC2 and on-premises computing environment. You can use Inventory to collect metadata from your managed instances. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated.</p>\n\n<p>Sample Inventory Cards:\n<img src=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/images/inventory-cards.png\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><strong>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</strong></p>\n\n<p>Since SSM does not have any native capability to find out which instances are not currently tracked by it, so here we would need to create a custom Lambda function for this and send notifications if any new untracked instances are detected. We can trigger the Lambda function using CloudWatch Events.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</strong> - SSM does not have any native capability to find out which instances are not currently tracked by the SSM service.</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</strong> - You can use SSM Automation to build Automation workflows to configure and manage instances and AWS resources. You can also create custom workflows or use pre-defined workflows maintained by AWS. For the given requirement, SSM Automation could be used to get the list of packages but it would require a lot of manual work, so it is not the best fit for the given use-case.</p>\n\n<p><strong>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</strong> - Inspector is meant to find security vulnerabilities on EC2 instances, not to get a metadata list of your installed packages.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p>\n"
        }
      },
      {
        "id": 143860745,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company is using AWS Storage Gateway for a branch office location. The gateway is configured in file gateway mode in front of an Amazon S3 bucket that contains files that must be processed by workers in the branch office. Each night a batch process uploads many files to the S3 bucket. Users have reported that the new files are not visible in the morning though they do exist in the S3 bucket.</p><p>How can a DevOps engineer ensure that the files become visible?</p>",
          "answers": [
            "<p>Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration.</p>",
            "<p>Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded.</p>",
            "<p>Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway.</p>",
            "<p>Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command.</p>"
          ],
          "explanation": "<p>In file gateway mode the users and applications in the branch office can access the storage using either NFS or SMB protocols. The new objects in the bucket may sometimes not be visible. The RefreshCache operation refreshes the cached inventory of objects for the specified file share. This operation finds objects in the Amazon S3 bucket that were added, removed, or replaced since the gateway last listed the bucket's contents and cached the results.</p><p><strong>CORRECT: </strong>\"Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded\" is incorrect.</p><p>Volume Gateway mode uses block storage targets (iSCSI) rather than file storage targets (NFS/SMB) and would therefore be a major change to the architecture. Event notifications cannot be used to refresh the cache in a storage gateway.</p><p><strong>INCORRECT:</strong> \"Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway\" is incorrect. S3 replication cannot be used to replicate to storage gateway.</p><p><strong>INCORRECT:</strong> \"Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration\" is incorrect. Transfer acceleration is used to improve upload speeds and does not assist at all in this situation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html\">https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>"
        }
      },
      {
        "id": 82921384,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.</p>\n\n<p>What can be done to improve the performance of the setup?</p>\n",
          "answers": [
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</p>",
            "<p>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>",
            "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong></p>\n\n<p>A golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can also add the web application as part of the golden AMI. You can think of it as an input base AMI for creating a standardized application-specific golden AMI.</p>\n\n<p>Once you create a golden AMI for a product (a product can be a standardized OS-AMI that you want to distribute to accounts in your organization or an application-specific AMI you want to let your business unit(s) deploy in their environment), you can validate whether the AMI meets your expectations, and choose to approve or reject the AMI.</p>\n\n<p>About the golden AMI pipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q66-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</strong> - The local cache warmup can unfortunately not be improved, as caching is dynamic and data may change over time. So creating an AMI with a copy of the local data cache just serves as a distractor.</p>\n\n<p><strong>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong> - You cannot migrate from ElastiCache to DynamoDB for the given use-case, as it's primarily a NoSQL database and not a caching solution (You could use DAX as a caching solution with DynamoDB). Besides, the existing database is RDS MySQL which is a relational database, so DynamoDB does not really fit into this mix.</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</strong> - You cannot use Lambda to configure the instance local cache at boot time as caching is dynamic and data may change over time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n"
        }
      },
      {
        "id": 75949124,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps team manages an application that consists of four separate AWS Lambda functions. A DevOps Engineer on the team has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule that executes the pipeline after a change is made to the application source code. During testing, the engineer noticed that the pipeline takes a long time to complete.</p><p><br></p><p>What should the DevOps Engineer do to improve the speed of the pipeline?</p>",
          "answers": [
            "<p>Modify the CodeBuild projects within the pipeline to use a compute type with more available vCPUs.</p>",
            "<p>Configure parallel executions of the Lambda functions by specifying the same runOrder in the CodePipeline configuration for the stage.</p>",
            "<p>Modify CodeBuild execute the builds in batches using a build graph deployment and specify the dependency chain.</p>",
            "<p>Configure the CodeBuild projects to run within a VPC and use dedicated instances to increase the build throughput.</p>"
          ],
          "explanation": "<p>The best way to speed up the pipeline will be to run the builds in parallel. This can be achieved through the pipeline configuration by specifying the runOrder to be the same for the build of each function within the action structure.</p><p>To specify parallel actions, you use the same integer for each action you want to run in parallel.</p><p><strong>CORRECT: </strong>\"Configure parallel executions of the Lambda functions by specifying the same runOrder in the CodePipeline configuration for the stage\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the CodeBuild projects to run within a VPC and use dedicated instances to increase the build throughput\" is incorrect.</p><p>Connecting to a VPC does not help and using dedicated instances is not the best way to improve the speed of the pipeline. Without specifying other changes the builds will still run sequentially.</p><p><strong>INCORRECT:</strong> \"Modify the CodeBuild projects within the pipeline to use a compute type with more available vCPUs\" is incorrect.</p><p>This may offer some improvement in speed but not as much as running the builds in parallel.</p><p><strong>INCORRECT:</strong> \"Modify CodeBuild execute the builds in batches using a build graph deployment and specify the dependency chain\" is incorrect.</p><p>CodeBuild can be configured to run builds in batches but a build list or build matrix should be used for running the builds in parallel. The build graph deployment runs the builds sequentially with dependencies mapped out.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 82921450,
        "correct_response": [
          "a",
          "c",
          "e"
        ],
        "prompt": {
          "question": "<p>As the Lead DevOps Engineer at a retail company, you have a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. You must collect the logs before an instance is terminated to perform log analytics later on. It's also necessary to collect all the access logs. The analysis of these logs should be performed at a minimal cost, and only need to be run from time to time.</p>\n\n<p>Which of the following options would you suggest to implement the MOST cost-optimal solution for this requirement? (Select three)</p>\n",
          "answers": [
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</p>",
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</p>",
            "<p>Enable Access Logs at the Application Load Balancer level</p>",
            "<p>Enable Access Logs at the Target Group level</p>",
            "<p>Analyze the logs using AWS Athena</p>",
            "<p>Analyze the logs using an EMR cluster</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</strong></p>\n\n<p>Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. When a scale-in event occurs, the terminating instance is first deregistered from the load balancer and while the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q71-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p>For the given use-case, you can configure a lifecycle hook to invoke the CloudWatch Event rule to trigger a Lambda function that launches an SSM Run Command to extract the application logs and store them in S3.</p>\n\n<p><strong>Enable Access Logs at the Application Load Balancer level</strong></p>\n\n<p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p>\n\n<p><strong>Analyze the logs using AWS Athena</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don\u2019t even need to load your data into Athena, it works directly with data stored in S3. You can analyze the access logs stored in S3 via Athena.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</strong> - CloudWatch Logs agent can only be used for continuous log streaming, and NOT for a one-time log extract to S3.</p>\n\n<p><strong>Enable Access Logs at the Target Group level</strong> - Please note that access logs are enabled at the ALB level and NOT at the target group level.</p>\n\n<p><strong>Analyze the logs using an EMR cluster</strong> - Analyzing logs at a low cost and in a serverless fashion should be done using AWS Athena. EMR clusters are usually long-running and cost a lot of money, and don't have serverless scaling capabilities.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n"
        }
      },
      {
        "id": 75949148,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer needs to stream data from Amazon CloudWatch Logs to a VPC-based Amazon OpenSearch Service cluster. The DevOps engineer creates an OpenSearch subscription filter for the OpenSearch cluster. However, the DevOps engineer notices that the logs are not visible in Amazon OpenSearch.</p><p>What should the DevOps engineer do to solve this problem?</p>",
          "answers": [
            "<p>Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role.</p>",
            "<p>Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch.</p>",
            "<p>Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role.</p>",
            "<p>Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch.</p>"
          ],
          "explanation": "<p>The configuration requires that the Lambda IAM execution role that is specified in the CloudWatch Logs configuration must have the trust relationship configured to allow lambda.amazonaws.com to assume the role. The AWSLambdaVPCAccessExecutionRole policy must also be added to the function to allow access to the VPC-based OpenSearch cluster.</p><p><strong>CORRECT: </strong>\"Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role\" is incorrect.</p><p>The trust relationship is defined in the role attached to the CloudWatch Logs configuration.</p><p><strong>INCORRECT:</strong> \"Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch\" is incorrect.</p><p>This policy should be attached to the Lambda IAM execution role, not the OpenSearch cluster.</p><p><strong>INCORRECT:</strong> \"Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch\" is incorrect.</p><p>The subscription filter is used instead of using an export which is a better and more automated solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/\">https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-opensearch/\">https://digitalcloud.training/amazon-opensearch/</a></p>"
        }
      },
      {
        "id": 82921374,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.</p>\n\n<p>How can you improve the user experience with the least effort?</p>\n",
          "answers": [
            "<p>Enhance the health check so that the return status code corresponds to the connectivity to the database</p>",
            "<p>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</p>",
            "<p>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</p>",
            "<p>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enhance the health check so that the return status code corresponds to the connectivity to the database</strong></p>\n\n<p>Configuring health checks for the Application Load Balancer (ALB) is an important step to ensure that your AWS Cloud application runs smoothly. The ALB Health Check is configured with a protocol and port number to call on the target instances. A healthy EC2 instance is one that issues a response to a health check call with an HTTP 200 response code. Instances that return a status code that is other than the 2XX range or which time out are designated as being unhealthy and will not receive traffic from the ELB.</p>\n\n<p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q69-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>You could just add a simple health check endpoint to the ALB which accepts a request and immediately responds with an HTTP status of 200. This approach provides for a fast health check, but would not meet the requirement for the given use-case. You need to improve the quality of the health check and make sure it returns a proper status code. As the application depends on the database, you need to ensure that you include health checks for these components when determining the health of your service.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</strong> - Migrating to Beanstalk would require significant effort and even then it won't help gather detailed database-specific health checks.</p>\n\n<p><strong>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</strong> - Health Checks for the ALB are pretty basic and only work with the HTTP return status code, not the payload itself.</p>\n\n<p><strong>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</strong> - Route53 health checks can only be used to prevent DNS records from being returned from a DNS query, so it won't help for routing to specific instances behind an ALB (that's why we have health checks at the ALB level).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf\">https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf</a></p>\n"
        }
      },
      {
        "id": 138248103,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A fast-growing company has multiple AWS accounts which are consolidated using AWS Organizations, and the company expects to add new accounts soon. The DevOps engineer was instructed to design a centralized logging solution to deliver all Amazon VPC Flow Logs and Amazon CloudWatch Logs across all sub-accounts to its dedicated Audit account for compliance purposes. The logs should also be properly indexed in order to perform search, retrieval, and analysis.</p><p>Which of the following is the MOST suitable solution that the engineer should implement to meet the above requirements?</p>",
          "answers": [
            "<p>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account.</p>",
            "<p>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</p>",
            "<p>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account.</p>",
            "<p>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account.</p>"
          ],
          "explanation": "<p>You can load streaming data into your Amazon OpenSearch Service domain from many different sources in AWS. Some sources, like Amazon Data Firehose and Amazon CloudWatch Logs, have built-in support for Amazon OpenSearch Service (successor to Amazon ElasticSearch). Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers. The Lambda functions respond to new data by processing it and streaming it to your domain.</p><p><img src=\"https://media.tutorialsdojo.com/public/TD-Amazon-OpenSearch-Service-02-04-2025.png\"></p><p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, Amazon Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. To begin subscribing to log events, create the receiving source, such as a Kinesis stream, where the events will be delivered. A subscription filter defines the filter pattern to use for filtering which log events get delivered to your AWS resource, as well as information about where to send matching log events to.</p><p>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Amazon Kinesis stream to perform custom processing and analysis. Custom processing is especially useful when you collaborate and analyze data across many accounts. For example, a company's information security group might want to analyze data for real-time intrusion detection or anomalous behaviors so it could conduct an audit of accounts in all divisions in the company by collecting their federated production logs for central processing.</p><p>A real-time stream of event data across those accounts can be assembled and delivered to the information security groups, who can use Kinesis to attach the data to their existing security analytic systems. Kinesis streams are currently the only resource supported as a destination for cross-account subscriptions.</p><p>Hence, the correct solution is: <strong>In the Audit account, create a new stream in Amazon Kinesis Data Streams and an AWS Lambda function that acts as an event handler to send all of the logs to the Amazon OpenSearch cluster. Create a CloudWatch subscription filter and use Amazon Kinesis Data Streams to stream all of the VPC Flow Logs and CloudWatch Logs from the sub-accounts to the Amazon Kinesis data stream in the Audit account.</strong></p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function that will send all VPC Flow Logs and CloudWatch Logs to an Amazon OpenSearch cluster. Use an CloudWatch subscription filter in the sub-accounts to stream all of the logs to the AWS Lambda function in the Audit account</strong> is incorrect. While Lambda functions can be used to process logs, relying solely on a Lambda function to handle a large volume of logs might only lead to performance bottlenecks and scalability issues. Lambda has concurrency limits, and processing logs from multiple sub-accounts may exceed these limits, especially as the company grows and more accounts are added.</p><p>The option that says: <strong>In the Audit account, create an Amazon Simple Queue Service (Amazon SQS) FIFO queue that will push all logs to an Amazon OpenSearch cluster. Use a CloudWatch subscription filter to stream both VPC Flow Logs and CloudWatch Logs from their sub-accounts to the Amazon SQS queue in the Audit account </strong>is incorrect because the CloudWatch subscription filter doesn't directly support SQS. You should use a Kinesis Data Stream, Kinesis Firehose, or Lambda function.</p><p>The option that says: <strong>In the Audit account, launch a new AWS Lambda function, which will push all of the required logs to a self-hosted Amazon OpenSearch cluster in a large Amazon EC2 instance. Integrate the AWS Lambda function to a CloudWatch subscription filter to collect all of the logs from the sub-accounts and stream them to the AWS Lambda function deployed in the Audit account</strong> is incorrect. Although this approach typically works, using a self-hosted OpenSearch cluster on an EC2 instance introduces unnecessary complexity and overhead regarding management, scaling, and availability. AWS-managed OpenSearch Service would be a more efficient and cost-effective option. Additionally, relying on EC2 for hosting OpenSearch adds management and operational burdens, whereas using AWS-managed services like Kinesis Data Streams or Firehose with OpenSearch is more reliable and easier to scale.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//CrossAccountSubscriptions.html</a></p><p><a href=\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs//SubscriptionFilters.html#FirehoseExample</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 82921402,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>An IT company is creating an online booking system for hotels. The booking workflow that the company has implemented can take over 3 hours to complete as a manual verification step is required by a 3rd party provider to ensure big transactions are not fraudulent.</p>\n\n<p>As a DevOps Engineer, you need to expose this as a secure API for the end customers. The website must be able to sustain 5000 requests at the same time. How should you implement this in the simplest possible way?</p>\n",
          "answers": [
            "<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</p>",
            "<p>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</strong></p>\n\n<p>API Gateway APIs can directly invoke an AWS service and pass in a payload. It's a common way to provide a publicly available and secure API for your chosen AWS services.</p>\n\n<p>Amazon API Gateway integrates with AWS Step Functions, allowing you to call Step Functions with APIs that you create to simplify and customize interfaces to your applications. Step Functions makes it easy to coordinate the components of distributed applications and microservices as a series of steps in a visual workflow. You create state machines in the Step Functions Console or through the Step Functions API to specify and execute the steps of your application at scale. API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to implement the payment workflow using Step Functions. A key reason you need this integration is that AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</strong> - AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you. So there is no need to use Lambda as an intermediary for this workflow.</p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</strong></p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</strong></p>\n\n<p>Lambda functions cannot process the booking workflow as it may take 3 hours, which is more than the 15 minutes max timeout limit that Lambda supports. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html\">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/faqs/\">https://aws.amazon.com/step-functions/faqs/</a></p>\n"
        }
      },
      {
        "id": 99528237,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A development team is running a project that will involve deploying applications across several Amazon VPCs. The applications will require fully meshed network connectivity to enable transitive routing between VPCs. The development lead is concerned about security and has requested centralized control over network access controls.</p><p>Which deployment will satisfy the requirements with the most operational efficiency?</p>",
          "answers": [
            "<p>Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs.</p>",
            "<p>Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs.</p>",
            "<p>Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs.</p>",
            "<p>Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPCs.</p>"
          ],
          "explanation": "<p>AWS Transit Gateway can be configured to enable a fully meshed network topology which allows transitive routing between all the VPCs. With AWS Network Firewall, you can define firewall rules that provide fine-grained control over network traffic.</p><p>AWS Firewall Manager allows you to build policies based on Network Firewall rules and then centrally apply those policies across your virtual private clouds (VPCs) and accounts.</p><p><strong>CORRECT: </strong>\"Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs\" is incorrect.</p><p>VPC peering requires creating complex peering relationships and does not support transitive routing (though this can be achieved through a mesh of peering connections). VPC peering is less operationally efficient compared to using a Transit Gateway. AWS WAF is not the best solution for enforcing centralized network access controls, it is used for preventing web based attacks.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs\" is incorrect.</p><p>You cannot connect Amazon VPCs using AWS S2S VPNs. You can only use an AWS VPN to connect on-premises networks.</p><p><strong>INCORRECT:</strong> \"Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPC\" is incorrect.</p><p>PrivateLink is not used for creating this kind of network deployment. It is used for private access to AWS services using private IP addresses.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/network-firewall/\">https://aws.amazon.com/network-firewall/</a></p><p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>"
        }
      },
      {
        "id": 138248235,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company is developing a serverless application that uses AWS Lambda, AWS SAM, and Amazon API Gateway. There is a requirement to fully automate the backend Lambda deployment in such a way that the deployment will automatically run whenever a new commit is pushed to a GitHub repository. There should also be a separate environment pipeline for TEST and PROD environments. In addition, the TEST environment should be the only one that allows automatic deployment.</p><p>How can a DevOps Engineer satisfy these requirements?</p>",
          "answers": [
            "<p>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>",
            "<p>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</p>"
          ],
          "explanation": "<p>In <strong>AWS CodePipeline</strong>, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p>If the action is approved, the pipeline execution resumes. If the action is rejected\u2014or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping\u2014the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.</p><p>- You want someone to review new or updated text before it is published to a company website.</p><p><img src=\"https://media.tutorialsdojo.com/public/PipelineFlow.png\"></p><p>Hence, the correct answer is: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. Configure the PROD pipeline to have a manual approval step. Create a GitHub repository with a branch for each environment and configure the pipeline to retrieve the source code from GitHub according to its branch. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline.</strong></p><p>The option that says: <strong>Create a new pipeline using AWS CodePipeline and a new GitHub repository for each environment. Configure CodePipeline to retrieve the application source code from the appropriate repository. Deploy the Lambda functions with AWS CloudFormation by creating a deployment step in the pipeline</strong> is incorrect. You should add a manual approval step on the PROD pipeline as mentioned in the requirements of the scenario.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. In the PROD pipeline, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline</strong> is incorrect. You don't need to create separate GitHub repositories for the two environments. You just need to create two different branches from a single repository.</p><p>The option that says: <strong>Set up two pipelines using AWS CodePipeline for TEST and PROD environments. On both pipelines, set up a manual approval step for application deployment. Set up separate GitHub repositories for each environment and configure each pipeline to retrieve the source code from GitHub. Deploy the Lambda functions with AWS CloudFormation by setting up a deployment step in the pipeline </strong>is incorrect. You should add the manual approval step on the PROD pipeline only, excluding the TEST pipeline. Moreover, you don't need to create separate GitHub repositories for the two environments. You only need to create two different branches from a single repository.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html \">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html</a></p><p><br></p><p><strong>Check out this AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 134588393,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A large hospital has an online medical record system that is hosted in a fleet of Windows EC2 instances with several EBS volumes attached to it. The IT Security team mandated that the latest security patches should be installed to all of their Amazon EC2 instances using an automated patching system. They should also have to implement a functionality that checks all of their EC2 instances if they are using an approved Amazon Machine Image (AMI) in their AWS Cloud environment. The patching system should not impede developers from launching instances using an unapproved AMI, but nonetheless, they still have to be notified if there are non-compliant EC2 instances in their VPC. </p><p>As a DevOps Engineer, which of the following should you implement to protect and monitor all of your instances as required above?</p>",
          "answers": [
            "<p>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>",
            "<p>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
          ],
          "explanation": "<p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/ssm-patch-baselines.jpg\"></p><p><strong>Patch Manager</strong> uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p><strong>AWS Config</strong> provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements. Hence, the correct option is: <strong>Create a patch baseline in AWS Systems Manager Patch Manager that defines which patches are approved for installation on your instances. Set up the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</strong></p><p>The option that says: <strong>Set up an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, it is stated in the scenario that the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Use Amazon GuardDuty to continuously monitor your Amazon EC2 instances if the latest security patches are installed and also to check if there are any unapproved AMIs being used. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC </strong>is incorrect because Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise, however, it does not check if your EC2 instances are using an approved AMI or not.</p><p>The option that says: <strong>Automatically patch all of your Amazon EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs using AWS Shield Advanced. Create CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect because the AWS Shield Advanced service is more suitable in preventing DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 82921348,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>Your company has adopted CodeCommit and forces developers to create new branches and create pull requests before merging the code to master. The development team lead reviewing the pull request needs high confidence in the quality of the code and therefore would like the CICD system to automatically build a Pull Request to provide a testing badge with a pass/fail status.</p>\n\n<p>How can you implement the validation of Pull Requests by CodeBuild efficiently?</p>\n",
          "answers": [
            "<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>",
            "<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>",
            "<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>",
            "<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong></p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p>\n\n<p>CloudWatch Events Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>For the given use-case, we need to create two CloudWatch Event Rules. The first rule would trigger on CodeCommit Pull Request and have the target as CodeBuild.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i2.jpg\"></p>\n\n<p>The second rule would trigger on CodeBuild build success or failure event and have the target as a Lambda function that will update the pull request with the Build outcome</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong> - Using a scheduled rate of 5 minutes would work but would be inefficient. It is much better to configure a CloudWatch Event Rule that would trigger on CodeCommit Pull Request and carry out the rest of the solution workflow as outlined earlier.</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p>For both these options, invoking a Lambda function to start CodeBuild would work, but having the function wait on CodeBuild has two issues:\n1) The Lambda function may timeout and has a maximum timeout of 15 minutes. What if the test suite takes longer to run?\n2) You will be billed for the Lambda function wait time.</p>\n\n<p>Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n"
        }
      },
      {
        "id": 75949068,
        "correct_response": [
          "b",
          "c"
        ],
        "prompt": {
          "question": "<p>To increase security, a company plans to use AWS Systems Manager Session Manager to managed Amazon EC2 instances rather than SSH. The connectivity to Session Manager should also use a private network connection.</p><p>Which configuration actions should be taken to implement this? (Select TWO.)</p>",
          "answers": [
            "<p>Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range.</p>",
            "<p>Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager.</p>",
            "<p>Create a VPC endpoint for Systems Manager in the Region where the instances are running.</p>",
            "<p>Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables.</p>",
            "<p>Establish private Session Manager connectivity using the instance IDs of EC2 instances.</p>"
          ],
          "explanation": "<p>Systems Manager Session Manager enables secure remote access to EC2 instances without the need to open ports for SSH or create bastion hosts. You can connect to instances through Session Manager privately by establishing a VPC endpoint in your VPC. This ensures that all connectivity takes place using private addresses.</p><p>The EC2 instances must have the Systems Manager agent running and they must have permissions to be able to communicate with the Systems Manager service. An instance profile can be easily attached to provide these permissions to instances.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-59-28-dac1a7fafdbe3507d5ca4b1fe07ed213.jpg\"><p><strong>CORRECT: </strong>\"Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Systems Manager in the Region where the instances are running\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range\" is incorrect.</p><p>Port 22 is used by the SSH protocol and is not required by Systems Manager Session Manager.</p><p><strong>INCORRECT:</strong> \"Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables\" is incorrect.</p><p>You cannot connect to EC2 instances via a NAT gateway. These gateways are used for outbound (internet) connectivity from the EC2 instances in private subnets.</p><p><strong>INCORRECT:</strong> \"Establish private Session Manager connectivity using the instance IDs of EC2 instances\" is incorrect.</p><p>You cannot connect privately to instances simply by connecting via the instance ID. You must establish a VPC endpoint and attach the necessary permissions to instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 134588381,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company has a CI/CD pipeline built in AWS CodePipeline for deployment updates. Part of the deployment process is to perform database schema updates and is performed via AWS CodeBuild. A recent security audit has discovered that the AWS CodeBuild is downloading the database scripts via Amazon S3 in an unauthenticated manner. The security team requires a solution that will enhance the security of the company's CI/CD pipeline.</p><p>What action should the DevOps Engineer take to address the issue in the MOST secure way?</p>",
          "answers": [
            "<p>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>",
            "<p>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts.</p>",
            "<p>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</p>"
          ],
          "explanation": "<p><strong>Service role</strong>: A service role is an AWS Identity and Access Management (IAM) that grants permissions to an AWS service so that the service can access AWS resources. You need an AWS CodeBuild service role so that CodeBuild can interact with dependent AWS services on your behalf. You can create a CodeBuild service role by using the CodeBuild or AWS CodePipeline consoles.</p><p><img src=\"https://media.tutorialsdojo.com/public/dop-c02-codebuild-service-role.png\"></p><p>In this scenario, the S3 bucket will be safeguarded from unauthorized access by utilizing a bucket policy. Moreover, CodeBuild leverages the service role for executing S3 actions on your behalf.</p><p>Hence, the correct answer is: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Update the service role of the CodeBuild project to grant access to Amazon S3. Download the database scripts using the AWS CLI.</strong></p><p>The option that says: <strong>Secure the S3 bucket by removing unauthenticated access through a bucket policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Download the database scripts using the AWS CLI</strong> is incorrect. While the use of IAM access key and secret access key can provide S3 access to CodeBuild, it is not the most secure approach to address the issue.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket through an IAM Policy. Add an IAM access key and a secret access key in the CodeBuild as an environment variable to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because an IAM policy alone cannot secure an S3 bucket from unauthorized access. A bucket policy must be used instead. Furthermore, this option uses IAM access key and secret access key, which is not the most secure way.</p><p>The option that says: <strong>Deny unauthenticated access from the S3 bucket by using Amazon GuardDuty. Update the service role of the CodeBuild project to grant access to Amazon S3. Utilize the AWS CLI to fetch the database scripts</strong> is incorrect because Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. It is not used for removing unauthenticated access to the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/service-role.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role</a></p><p><a href=\"https://aws.amazon.com/codebuild/\">https://aws.amazon.com/codebuild/</a></p>"
        }
      },
      {
        "id": 82921330,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>As the Lead DevOps Engineer at an e-commerce company, you would like to upgrade the major version of your MySQL database, which is managed by CloudFormation with <code>AWS::RDS::DBInstance</code> and setup using Multi-AZ.</p>\n\n<p>You have a requirement to minimize the downtime as much as possible, what steps should you take to achieve this?</p>\n",
          "answers": [
            "<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>",
            "<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>",
            "<p>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</p>",
            "<p>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong></p>\n\n<p>You can minimize downtime on an upgrade by using a rolling upgrade using read replicas. Amazon RDS doesn\u2019t fully automate one-click rolling upgrades. However, you can still perform a rolling upgrade by creating a read replica, upgrading the replica by using the property <code>EngineVersion</code>, promoting the replica, and then routing traffic to the promoted replica. If you want to create a Read Replica DB instance, specify the ID of the source DB instance. The SourceDBInstanceIdentifier property determines whether a DB instance is a Read Replica.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p>You should also note that a Multi-AZ configuration does not prevent downtime during an upgrade. Multi-AZ is only recommended for a high availability use-case. However, in the case of a MySQL or MariaDB engine upgrade, Multi-AZ doesn\u2019t eliminate downtime. The slow shutdown and the physical changes made on the active server by the mysql_upgrade program require this downtime.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong> - You should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p><strong>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - If you update the <code>EngineVersion</code> property of an AWS::RDS::DBInstance resource type, AWS CloudFormation creates a new resource and replaces the current DB instance resource with the new one, so this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n\n<p><strong>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - Also, you should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n"
        }
      },
      {
        "id": 138248125,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company wants to implement a continuous delivery workflow that will facilitate the process of source code promotion in the development, staging, and production environments in AWS. In the event of system degradation or failure, the company should also have the ability to roll back the recent deployment of the application.</p><p>Which of the following CI/CD designs is the MOST suitable one to implement and will incur the LEAST amount of downtime?</p>",
          "answers": [
            "<p>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</p>",
            "<p>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production.</p>",
            "Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy."
          ],
          "explanation": "<p>A repository is the fundamental version control object in GitHub or GitLab. It\u2019s where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. In GitHub and GitLab, you can set up notifications to configure notifications so that repository users receive emails about events (for example, another user commenting on code). You can change default settings to customize the default settings for your repository. You can browse contents to easily navigate and browse the contents of your repository. You can create triggers to set up triggers so that code pushes or other events trigger actions, such as emails or code functions. You can also configure a repository on your local computer (a local repo) to push your changes to more than one repository.</p><p><img src=\"https://media.tutorialsdojo.com/public/setup-cicd-pipeline-100324.png\"></p><p>In designing your CI/CD process in AWS, you can use a single repository in GitHub (or GitLab) and create different branches for development, master, and release. You can use CodeBuild to build your application and run tests to verify that all of the core features of your application are working. For deployment, you can either select an in-place or blue/green deployment using CodeDeploy.</p><p>Hence, the correct answer is: <strong>Create a single repository in GitHub (or GitLab) and create a development branch to hold merged changes. Set up AWS CodeBuild to build and test the code stored in the development branch, which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy.</strong></p><p>The option that says: <strong>Create several repositories in GitHub (or GitLab) for each of their developers and then create a centralized development branch to hold merged changes from each of the developer\u2019s repository. Set up AWS CodeBuild to build and test the code stored in the development branch. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy </strong>is incorrect because creating a separate repository for each developer is absurd since they can simply clone the code instead. A single repository will suffice in this scenario which can have several branches for development and production deployment purposes.</p><p>The option that says: <strong>Create a repository in GitHub (or GitLab) for the development environment and another one for the production environment. Set up AWS CodeBuild to build and merge the two repositories. Do a blue/green deployment using AWS CodeDeploy to deploy the latest code in production</strong> is incorrect because you don't need to create two repositories for one application. Instead, you can just create at least two different branches to separate your development and production code.</p><p>The option that says: <strong>Create an Amazon ECR repository and then create a development branch to hold merged changes made by the developers. Set up AWS CodeBuild to build and test the code stored in the development branch which is triggered to run on every new commit. Merge to the master branch using pull requests that will be approved by senior developers. To deploy the latest code to the production environment, set up a blue/green deployment using AWS CodeDeploy</strong> is incorrect because Amazon ECR is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. This is not a suitable service to be used to store your application code.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/devops/continuous-integration/\">https://aws.amazon.com/devops/continuous-integration/</a></p><p><a href=\"https://aws.amazon.com/devops/continuous-delivery/\">https://aws.amazon.com/devops/continuous-delivery/</a></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html</a></p><p><br></p><p><strong>Check out this AWS CodePipelin Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 134588407,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A multinational company is using multiple AWS accounts for its global cloud architecture. The AWS resources in their production account are shared among various business units of the company. A single business unit may have one or more AWS accounts that have resources in the production account. Recently, there were a lot of incidents in which the developers from a specific business unit accidentally terminated the Amazon EC2 instances owned by another business unit. A DevOps Engineer was tasked to come up with a solution to only allow a specific business unit who owns the EC2 instances and other AWS resources to terminate their own resources. </p><p>How should the Engineer implement a multi-account strategy to satisfy this requirement?</p>",
          "answers": [
            "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access.</p>",
            "<p>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources.</p>",
            "<p>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</p>",
            "<p>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts.</p>"
          ],
          "explanation": "<p><strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an <em>organization</em> that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.</p><p>You can use organizational units (OUs) to group accounts together to administer as a single unit. This greatly simplifies the management of your accounts. For example, you can attach a policy-based control to an OU, and all accounts within the OU automatically inherit the policy. You can create multiple OUs within a single organization, and you can create OUs within other OUs. Each OU can contain multiple accounts, and you can move accounts from one OU to another. However, OU names must be unique within a parent OU or root.</p><p><img src=\"https://media.tutorialsdojo.com/aws-organizations.jpg\"></p><p>Resource-level permissions refer to the ability to specify which resources users are allowed to perform actions on. Amazon EC2 has partial support for resource-level permissions. This means that for certain Amazon EC2 actions, you can control when users are allowed to use those actions based on conditions that have to be fulfilled or specific resources that users are allowed to use. For example, you can grant users permissions to launch instances, but only of a specific type and only using a specific AMI.</p><p>Hence, the correct answer is: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Set up an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the IAM policy to every member accounts of the OU.</strong></p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Organizations. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up an IAM Role in the production account for each business unit which has a policy that allows access to the Amazon EC2 instances including resource-level permission to terminate the instances that it owns. Use an AWSServiceRoleForOrganizations service-linked role to the individual member accounts of the OU to enable trusted access</strong> is incorrect. The AWSServiceRoleForOrganizations service-linked role is primarily used to only allow AWS Organizations to create service-linked roles for other AWS services. This service-linked role is present in all organizations and not just in a specific OU.</p><p>The option that says: <strong>Centrally manage all of your accounts using a multi-account aggregator in AWS Config and AWS Control Tower. Configure AWS Config to allow access to certain Amazon EC2 instances in production per business unit. Launch the Customizations for AWS Control Tower (CfCT) in the different account where your AWS Control Tower landing zone is deployed. Configure the CfCT to only allow a specific business unit that owns the EC2 instances and other AWS resources to terminate their own resources<em> </em></strong>is incorrect. Although the use of the AWS Control Tower is right, the aggregator feature is simply an AWS Config resource type that collects AWS Config configuration and compliance data from the following various AWS accounts. In addition, you have to launch the Customizations for AWS Control Tower (CfCT) on the same AWS region where your AWS Control Tower landing zone is deployed, and not on a different account, to put it in effect.</p><p>The option that says: <strong>Centrally manage all of your accounts using AWS Service Catalog. Group your accounts, which belong to a specific business unit, to the individual Organization Unit (OU). Set up a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Associate the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts</strong> is incorrect. AWS Service Catalog simply allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A more suitable service to use here is AWS Organizations.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p>"
        }
      }
    ],
    "answers": {
      "75949068": [
        "b",
        "e"
      ],
      "75949124": [
        "b"
      ],
      "75949148": [
        "b"
      ],
      "82921330": [
        "a"
      ],
      "82921348": [
        "a"
      ],
      "82921374": [
        "a"
      ],
      "82921384": [
        "a"
      ],
      "82921402": [
        "b"
      ],
      "82921412": [
        "a",
        "b"
      ],
      "82921416": [
        "a",
        "d"
      ],
      "82921450": [
        "b",
        "c",
        "e"
      ],
      "99528237": [
        "a"
      ],
      "134588381": [
        "d"
      ],
      "134588393": [
        "d"
      ],
      "134588407": [
        "b"
      ],
      "138248103": [
        "a"
      ],
      "138248125": [
        "a"
      ],
      "138248235": [
        "c"
      ],
      "138248241": [
        "c"
      ],
      "143860745": [
        "d"
      ]
    }
  },
  {
    "id": "1769619233089",
    "date": "2026-01-28T16:53:53.089Z",
    "course": "DOP-C02",
    "mode": "exam",
    "score": 15,
    "incorrect": 5,
    "unanswered": 0,
    "total": 20,
    "percent": 75,
    "duration": 2519968,
    "questions": [
      {
        "id": 75949046,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company manages a continuous integration and continuous delivery (CI/CD) pipeline that includes a Jenkins implementation that runs on Amazon EC2. The security team has requested that all build artifacts are encrypted as they contain company sensitive data.</p><p>Which changes should a DevOps engineer make to meet these requirements whilst reducing operational overhead for ongoing management?</p>",
          "answers": [
            "<p>Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes.</p>",
            "<p>Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption.</p>",
            "<p>Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group.</p>",
            "<p>Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM).</p>"
          ],
          "explanation": "<p>The existing Jenkins implementation runs on an Amazon EC2 instance, and this requires more operational management compared to using AWS CodeBuild which is a managed service. The DevOps engineer should replace Jenkins with AWS CodeBuild which is a fully managed build service.</p><p>CodeBuild compiles source code, runs unit tests, and produces artifacts that are ready to deploy. Encryption for build artifacts such as a cache, logs, exported raw test report data files, and build results, is enabled by default, and uses AWS managed keys (AWS KMS).</p><p><strong>CORRECT: </strong>\"Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes\" is incorrect.</p><p>This solution may be secure, but it does require more operational management compared to using AWS CodeBuild and is therefore not the best option.</p><p><strong>INCORRECT:</strong> \"Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group\" is incorrect.</p><p>Moving Jenkins to an Auto Scaling group does not reduce the operational management. This is also an option that requires more overhead to manage compared to using a managed service such as AWS CodeBuild.</p><p><strong>INCORRECT:</strong> \"Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM)\" is incorrect.</p><p>Artifacts cannot be encrypted using ACM as that is a service that issues SSL/TLS certificates which are used for encryption in-transit rather than encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 67357100,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.</p>\n\n<p>As a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?</p>\n",
          "answers": [
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>",
            "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>AWS::EC2::Volume</code> that returns a compliance failure if the custom tag is not applied on the EBS volume. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong></p>\n\n<p>Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type.</p>\n\n<p>AWS Config managed rule that checks required-tags:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p>This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes.</p>\n\n<p>AWS Config rules remediation with automation runbooks architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it's possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n"
        }
      },
      {
        "id": 75949146,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>",
          "answers": [
            "<p>Cross-zone load balancing has not been enabled for the ALB.</p>",
            "<p>The new AZ has not been enabled for the ALB.</p>",
            "<p>The AMI has not been updated to include the new AZ.</p>",
            "<p>There are no subnets associated with the new AZ.</p>"
          ],
          "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>"
        }
      },
      {
        "id": 99528239,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A company runs a Java application in an on-premises data center. The application is not currently containerized but the company plans to migrate it to the AWS Cloud and modernize the application into a containerized deployment. A CI/CD pipeline should also be created to automate application updates.</p><p>Which solution can a DevOps engineer use to meet all these requirements?</p>",
          "answers": [
            "<p>Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates.</p>",
            "<p>Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy.</p>",
            "<p>Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository.</p>",
            "<p>Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy.</p>"
          ],
          "explanation": "<p>App2Container (A2C) is a command-line tool for modernizing .NET and Java applications into containerized applications. A2C analyzes and builds an inventory of all applications running in virtual machines on-premises or in the cloud.</p><p>With A2C you can simply select the application you want to containerize, and A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the ECS task and Kubernetes pod definitions.</p><p>App2Container creates CI/CD pipelines for Amazon DevOps services such as CodeBuild and CodeDeploy to build and deploy containers. If you have existing CI/CD tooling (for example, Azure DevOps and Jenkins), then you can integrate A2C provided artifacts \u2013 dockerfile, ECS task definition, EKS deployment YAML - into your existing CI/CD workflows.</p><p><strong>CORRECT: </strong>\"Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates\" is incorrect.</p><p>AWS Copilot is a command line interface (CLI) that you can use to quickly launch and manage containerized applications on AWS. It simplifies running applications on Amazon Elastic Container Service (ECS), AWS Fargate, and AWS App Runner. It is not able to inventory and analyze existing applications or migrate them to containers.</p><p><strong>INCORRECT:</strong> \"Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository\" is incorrect.</p><p>AWS Proton is used to create application stack templates and build infrastructure for applications using the CLI, API, or UI. It is not used for refactoring applications or migrating them to containerized deployments.</p><p><strong>INCORRECT:</strong> \"Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy\" is incorrect.</p><p>Amazon AppFlow is used to automate bi-directional data flows between SaaS applications and AWS services. It is not used to migrate applications between on-premises deployments and Amazon ECS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/app2container/features/\">https://aws.amazon.com/app2container/features/</a></p>"
        }
      },
      {
        "id": 75949108,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>",
          "answers": [
            "<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>",
            "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"
          ],
          "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949138,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "prompt": {
          "question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>",
            "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>",
            "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>",
            "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>",
            "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>",
            "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"
          ],
          "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>"
        }
      },
      {
        "id": 75949100,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A company is deploying a new serverless application that uses AWS Lambda functions. A DevOps engineer must create a continuous deployment pipeline for the application. The deployment preferences must be configured to minimize the impact of failed deployments.</p><p>Which deployment configuration will meet these requirements?</p>",
          "answers": [
            "<p>Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type.</p>",
            "<p>Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch.</p>",
            "<p>Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version.</p>",
            "<p>Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version.</p>"
          ],
          "explanation": "<p>The benefits of using AWS SAM to create the serverless application include that it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM can perform the following actions:</p><ul><li><p>Deploys new versions of the Lambda function, and automatically creates aliases that point to the new version.</p></li><li><p>Gradually shifts customer traffic to the new version until you\u2019re satisfied that it's working as expected, or you roll back the update.</p></li><li><p>Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly, and your application operates as expected.</p></li><li><p>Rolls back the deployment if CloudWatch alarms are triggered.</p></li></ul><p>The DevOps engineer can choose the Deployment Preference Type. The following options are available:</p><ul><li><p><strong>Canary:</strong> Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p></li><li><p><strong>Linear:</strong> Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment.</p></li><li><p><strong>All-at-once:</strong> All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</p></li></ul><p>The best option to minimize the impact of failed deployments is to use the canary deployment type. This will ensure that only a small amount of traffic reaches the new Lambda function in the first shift and if any issues occur the deployment can be stopped.</p><p><strong>CORRECT: </strong>\"Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch\" is incorrect.</p><p>The all-at-once deployment preference type would shift all traffic across to the new functions which would increase the impact of failed deployments. Also, CloudFormation is not a continuous deployment tool and is not a suitable substitute for CodePipeline and CodeDeploy.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version\" is incorrect.</p><p>As above, CloudFormation is not designed for continuous delivery and should not be used in this scenario. Creating a CodePipeline that leverages CodeDeploy and a source control service such as CodeCommit would be a better solution.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version\" is incorrect.</p><p>This option requires more operational overhead and testing on behalf of the DevOps team. The better solution is an automated pipeline that shifts some traffic across to the new functions so actual end users are attempting to use the application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949142,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.</p><p>How can the company meet these requirements with the LEAST amount of effort?</p>",
          "answers": [
            "<p>Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>",
            "<p>Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns.</p>",
            "<p>Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket.</p>"
          ],
          "explanation": "<p>The simplest solution for this request is to enable server access logging on the bucket and then analyze the data that is logged using Amazon Athena. Server access logging provides detailed records for the requests that are made to a bucket. This includes the information requested by the company.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p><strong>CORRECT: </strong>\"Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket\" is incorrect $</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/\">https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>"
        }
      },
      {
        "id": 75949172,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A service provider has created business relationships with several companies. The service provider plans to deploy an application to multiple AWS accounts managed by these partner companies using AWS CloudFormation. Each partner company has granted the permissions to create IAM roles with permissions for the deployment in their respective accounts. The organization must minimize operational overhead and stack management.</p><p>Which actions should be taken to deploy the application across these accounts?</p>",
          "answers": [
            "<p>Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template.</p>",
            "<p>Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>",
            "<p>Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application.</p>"
          ],
          "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf</p><p>In this case the best solution is to use self-managed permissions as the partner companies have only granted the ability to create IAM roles. They would certainly not want their entire account to be controlled by the service provider through AWS Organizations just so they can deploy a single application.</p><p><strong>CORRECT: </strong>\"Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>You cannot have a role in one account that has permissions in another account. The roles must be created in each AWS account and self-managed permissions must be used with AWS CloudFormation stack sets.</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>The partner companies would not want the service provider to control their accounts with AWS Organizations just for a single application. They have granted the ability to create roles which is enough for the service provider to deploy the application using stack sets with self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template\" is incorrect.</p><p>The partner companies have granted the ability to create roles, not user accounts. Also, this would be inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>"
        }
      },
      {
        "id": 75949092,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer is developing an application that calls AWS Lambda functions. The functions must connect to a database and credentials must not be stored in source code. The credentials for connection to the database must be regularly rotated to meet security policy requirements.</p><p>What should a DevOps engineer do to meet these requirements?</p>",
          "answers": [
            "<p>Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID.</p>",
            "<p>Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID.</p>",
            "<p>Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution.</p>",
            "<p>Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID.</p>"
          ],
          "explanation": "<p>AWS Secrets Manager is ideal for this scenario as it can be used to securely store the secrets. Automatic rotation can be enabled for several AWS databases and can be configured through custom Lambda functions for other databases.</p><p><strong>CORRECT: </strong>\"Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID\" is incorrect.</p><p>CloudHSM is used for storing encryption keys, not connection credentials.</p><p><strong>INCORRECT:</strong> \"Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution\" is incorrect.</p><p>This is not a secure method of storing the credentials and Secrets Manager is more secure.</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID\" is incorrect.</p><p>KMS is used for storing encryption keys, not connection credentials.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>"
        }
      },
      {
        "id": 115961513,
        "correct_response": [
          "b"
        ],
        "prompt": {
          "question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>",
          "answers": [
            "<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>",
            "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>",
            "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>",
            "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"
          ],
          "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 99528229,
        "correct_response": [
          "a"
        ],
        "prompt": {
          "question": "<p>A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.</p><p>Which actions should the engineer take?</p>",
          "answers": [
            "<p>Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL.</p>",
            "<p>Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield.</p>",
            "<p>Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL.</p>",
            "<p>Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty.</p>"
          ],
          "explanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website.</p><p>AWS Web Application Firewall (WAF) can be used to protect the static website from web exploits. To do this, the engineer should create an AWS WAF web ACL.</p><p><strong>CORRECT: </strong>\"Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield\" is incorrect.</p><p>Both EC2 and ElastiCache use EC2 instances so they are not serverless services. AWS Shield protects against DDoS attacks but AWS WAF should be used for web exploits.</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL\" is incorrect.</p><p>ECS with Fargate is serverless but you cannot use a Redis cluster to cache content served by a task running on Amazon ECS. ElastiCache is used for caching database content.</p><p><strong>INCORRECT:</strong> \"Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty\" is incorrect.</p><p>Lambda with a REST API as the frontend is definitely possible, however the static website on S3 would be easier to deploy and manage. AWS GuardDuty does not protect against web exploits.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>"
        }
      },
      {
        "id": 75949128,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>The DevOps team at an e-commerce company introduced multiple stages of security to the code release process. As an additional measure, they want to add additional SAST &amp; DAST tools into an automated pipeline. These tools should be invoked for every code push in an AWS CodeCommit repository. The code must be sent via an external API.</p><p>Which actions should a DevOps engineer take to achieve these requirements MOST efficiently?</p>",
          "answers": [
            "<p>Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API.</p>",
            "<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected.</p>",
            "<p>Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API.</p>",
            "<p>Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API.</p>"
          ],
          "explanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that can be set up, you can match events and route them to one or more target functions or streams. Custom application-level events can be generated and published to CloudWatch Events. Scheduled events can be set up that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event Rules cannot have S3 buckets as a target. It is more efficient to directly invoke the Lambda function from the CloudWatch Event rule than use S3 as a temporary store and then trigger a function execution.</p><p><strong>INCORRECT:</strong> \"Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected\" is incorrect.</p><p>You cannot create a CodeCommit hook on an EC2 instance, so this is not possible</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So, this option is ruled out.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/\">https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 75949174,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>",
          "answers": [
            "<p>Pause the deployment stage to perform a final validation on the deployment.</p>",
            "<p>Add test stages before and after the deploy action.</p>",
            "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>",
            "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"
          ],
          "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528205,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team has prohibited unauthenticated requests to S3 buckets for this project.</p><p>How can this issue be resolved in the <strong>MOST secure</strong> manner?</p>",
          "answers": [
            "<p>Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.</p>",
            "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script.</p>",
            "<p>Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</p>"
          ],
          "explanation": "<p>Removing unauthenticated access from the bucket through a bucket policy ensures that the S3 bucket is protected from unauthorized access. Granting the CodeBuild project's service role explicit permissions to access the S3 bucket aligns with the principle of least privilege. Using the AWS CLI within the build spec ensures secure and seamless access to the bucket.<br><br></p><p><strong>CORRECT: </strong>\"Remove unauthenticated access from the S3 bucket using a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access permissions. Use the AWS CLI to download the database population script\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script\" is incorrect.</p><p>Adding the bucket name to the AllowedBuckets section does not address the security issue. The CodeBuild project needs proper IAM permissions to access the S3 bucket securely. Relying on unauthenticated access remains insecure and against the security team's requirements.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script\" is incorrect.</p><p>Enabling HTTPS basic authentication for the S3 bucket is not supported by AWS. Amazon S3 uses IAM policies and bucket policies for access control, not basic authentication. Additionally, using cURL to pass tokens does not align with AWS's recommended best practices.</p><p><strong>INCORRECT:</strong> \"Remove unauthenticated access from the S3 bucket using a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key\" is incorrect.</p><p>Using IAM access keys and secret access keys directly is less secure than granting a CodeBuild service role the necessary permissions. Hardcoding access keys in the build spec or environment can lead to security vulnerabilities, such as accidental exposure of credentials.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up-service-role.html</a></p>"
        }
      },
      {
        "id": 75949134,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>An automotive organization is planning to migrate their website into AWS across multiple accounts. The current infrastructure uses an on-premises Microsoft IIS web server and Microsoft SQL server for the data persistence layer.</p><p>They want to be able to scale their infrastructure based on demand. Along with the current website, they also want to collect user interest data from ad clicks that occur on the website. Amazon RedShift has been chosen for the consumption and aggregation of data.</p><p>Which of the below architectures best suits their needs?</p>",
          "answers": [
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>",
            "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.</p>"
          ],
          "explanation": "<p>Amazon Kinesis Data Firehose is used to reliably load streaming data into data lakes, data stores, and analytics tools like Amazon Redshift. Process the incoming data from Firehose with Kinesis Data Analytics to provide real-time dashboarding of website activity.</p><p><strong>CORRECT: </strong>\"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>To load data into S3, Kinesis Firehose is a more suitable tool.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>As per the option above, Kinesis Data Firehose is a better service for this use case.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>Athena is more suited to data analysis within S3 buckets and cannot be used for loading data into S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/\">https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>"
        }
      },
      {
        "id": 75949074,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A retail company has hired a DevOps engineer to provide consultancy services. The company runs Oracle and PostgreSQL services on Amazon RDS for storing large quantities of data generated by manufacturing equipment.</p><p>The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team.</p><p>To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p><p>Which solution should the DevOps engineer recommend to achieve these requirements?</p>",
          "answers": [
            "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift.</p>",
            "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift.</p>"
          ],
          "explanation": "<p>The AWS Database Migration Service (DMS) helps migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS DMS the data can be continuously replicated with high availability and the multiple databases can be consolidated into a petabyte-scale data warehouse.</p><p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance. During a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-02-48-34cbd839e896555130df72dfa20efc35.jpg\"><p><strong>CORRECT: </strong>\u201cUse AWS Database Migration Service to replicate the data from the databases into Amazon Redshift\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>AWS Glue is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.</p><p><strong>INCORRECT:</strong> \"Use Amazon EMR to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon EMR (previously called Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon Kinesis Data Streams is a fully managed, serverless data streaming service that stores and ingests various streaming data in real time at any scale. It is not suitable for migrating data between databases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>"
        }
      },
      {
        "id": 99528223,
        "correct_response": [
          "a",
          "b"
        ],
        "prompt": {
          "question": "<p>A data intelligence and analytics company has implemented a CI/CD pipeline using AWS CodePipeline which takes code from an AWS CodeCommit repository and then builds it using AWS CodeBuild. During the deploy stage, the application is deployed onto an Amazon ECS cluster. During deployment, the application is only partly updated on some ECS tasks which are running an older version of the image.</p><p>A DevOps engineer investigated and found that terminating the task or clearing the local Docker cache fixes the issue, but a more robust solution is required that provides visibility and identification to track where container images are deployed. Also, the start-up time of the containers needs to be optimized.</p><p>Which actions should the DevOps engineer take to achieve these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Move all the dependencies into a single image and pull them from a single container registry.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time.</p>",
            "<p>After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task.</p>",
            "<p>Move secondary dependencies to be downloaded at application startup rather than including them within a static container image.</p>"
          ],
          "explanation": "<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. The IMAGEID property is the SHA digest for the Docker image used to start the container.</p><p>Ideally, a container image is intended to be a complete snapshot of everything that the application requires to function. With a complete container image, the application could be run by downloading one container image from one place.</p><p>There is no need to download several separate pieces from different locations. Therefore, as a best practice, store all application dependencies as static files inside the container image. This will improve performance and start up time.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-00-17-d1b2e063cff66b89f7caee81435b0971.jpg\"><p><strong>CORRECT: </strong>\"Move all the dependencies into a single image and pull them from a single container registry\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time\" is incorrect.</p><p>The exact image tag needs to be pulled because the latest tag will only pull the image that was pushed last.</p><p><strong>INCORRECT:</strong> \"After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task\" is incorrect.</p><p>Using Lambda to SSH onto instances is not a workable solution. SSM Run Command may be better but it\u2019s also not ideal.</p><p><strong>INCORRECT:</strong> \"Move secondary dependencies to be downloaded at application startup rather than including them within a static container image\" is incorrect.</p><p>This would delay the image spin up time rather than improving it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html\">https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>"
        }
      },
      {
        "id": 99528211,
        "correct_response": [
          "c"
        ],
        "prompt": {
          "question": "<p>A legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. It should not be possible to create IAM users unless the user account making the requires is specific on an exception list. A DevOps engineer must apply these restrictions.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>",
            "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list.</p>",
            "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>"
          ],
          "explanation": "<p>With AWS Organizations Service Control Policies (SCPs) you can restrict access to specific resources or define conditions for when SCPs are in effect. The condition element is supported when the Effect element has a value of Deny.</p><p>The StringNotLike condition will check the exception list for the aws:username that made the request. If the username is not present the request will be denied.</p><p><strong>CORRECT: </strong>\"Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list\" is incorrect.</p><p>The API action iam:CreateLoginProfile creates a password for the specified IAM user. This is not used to create the user account.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The API action iam:GetUser retrieves information about the specified IAM user, including the user's creation date, path, unique ID, and ARN.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The function should be used to validate that the iam:CreateUser request is being initiated by a user on the exception list. It should not delete the requesting user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>"
        }
      },
      {
        "id": 75949048,
        "correct_response": [
          "d"
        ],
        "prompt": {
          "question": "<p>A company has several AWS accounts and an on-premises data center. Several microservices applications run across the accounts and data center. The distributed architecture results in challenges with investigating application issues as the logs are saved in a variety of locations. A DevOps engineer must configure a solution that centralizes and aggregates the logs for analytics.</p><p>What is the MOST efficient and cost-effective solution?</p>",
          "answers": [
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account.</p>",
            "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account.</p>"
          ],
          "explanation": "<p>The most efficient and cost-effective solution is to use the CloudWatch agent to collect log files from both AWS resources and on-premises servers and save that data to a centralized Amazon S3 bucket. S3 event notifications can be used to trigger an AWS Lambda function that analyzes the data looking for anomalies.</p><p>Amazon Athena is ideal for running ad-hoc SQL queries on data stored in S3. This can be used by the company when they have specific queries they need to run against the data.</p><p><strong>CORRECT: </strong>\"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center\" is incorrect.</p><p>The best solution is not to store the data on-premises, it should be in the AWS Cloud. You would then be able to use the ELK stack to analyze the data.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket\" is incorrect.</p><p>You cannot use S3 APIs to export log files from on-premises servers. You also cannot use KDA to analyze data in S3, it is used for analyzing data in other Kinesis services.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account\" is incorrect.</p><p>As above, you cannot use the S3 API with on-premises resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p><p><a href=\"https://aws.amazon.com/solutions/implementations/centralized-logging/\">https://aws.amazon.com/solutions/implementations/centralized-logging/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      }
    ],
    "answers": {
      "67357100": [
        "d"
      ],
      "75949046": [
        "b"
      ],
      "75949048": [
        "d"
      ],
      "75949074": [
        "c"
      ],
      "75949092": [
        "b"
      ],
      "75949100": [
        "a"
      ],
      "75949108": [
        "a"
      ],
      "75949128": [
        "c"
      ],
      "75949134": [
        "d"
      ],
      "75949138": [
        "b",
        "c",
        "d"
      ],
      "75949142": [
        "c"
      ],
      "75949146": [
        "a"
      ],
      "75949172": [
        "b"
      ],
      "75949174": [
        "b"
      ],
      "99528205": [
        "c"
      ],
      "99528211": [
        "c"
      ],
      "99528223": [
        "b",
        "d"
      ],
      "99528229": [
        "a"
      ],
      "99528239": [
        "b"
      ],
      "115961513": [
        "b"
      ]
    }
  }
]