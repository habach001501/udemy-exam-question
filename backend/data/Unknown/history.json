[
  {
    "id": "1772183158972",
    "date": "2026-02-27T09:05:58.972Z",
    "course": "Unknown",
    "mode": "exam",
    "sourceLabel": null,
    "score": 0,
    "incorrect": 8,
    "unanswered": 9,
    "total": 17,
    "percent": 0,
    "duration": 14202,
    "questions": [
      {
        "id": 67357122,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company uses multiple AWS accounts to help isolate and manage business applications. This multi-account environment consists of an AWS Transit Gateway to route all outbound traffic through a common network account. A firewall appliance inspects all traffic before it is forwarded to an internet gateway. The firewall appliance is configured to send logs to Amazon CloudWatch Logs for all events generated.</p>\n\n<p>Recently, the security team has advised about probable illegal access of resources. As DevOps Engineer, you have been advised to configure an alert to the security team if the firewall appliance generates an event of Critical severity.</p>\n\n<p>How should a DevOps engineer configure this requirement?</p>\n",
          "answers": [
            "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>",
            "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send notification to an Amazon Simple Notification Service (Amazon SNS) topic if Critical event is detected. Subscribe the email address of the security team to the SNS topic</p>",
            "<p>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>",
            "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong></p>\n\n<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p>\n\n<p>A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager.</p>\n\n<p>Amazon CloudWatch metrics and alarms:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send a notification to an Amazon Simple Notification Service (Amazon SNS) topic if a Critical event is detected. Subscribe the email address of the security team to the SNS topic</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations. Kinesis Data Firehose is not supported as a destination for the metric streams.</p>\n\n<p><strong>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - Transit Gateway Flow Logs is a feature that enables you to capture information about the IP traffic going to and from your transit gateways. This does not meet our objective of analyzing firewall log data.</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications running on AWS Lambda. The solution collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network. This option acts as a distractor for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html</a></p>\n"
        }
      },
      {
        "id": 67357128,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An Aurora cluster is configured with a single DB instance for a web application. The application uses the instance endpoint to read/write data to the database. The operations team has scheduled an update on the cluster during the upcoming maintenance window. The application support team has requested help to ensure uninterrupted access to the application during the maintenance window.</p>\n\n<p>Which step should a DevOps Engineer take so that the users experience the least possible interruption during the maintenance window?</p>\n",
          "answers": [
            "<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</p>",
            "<p>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a  Multi-AZ cluster configuration</p>",
            "<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</p>",
            "<p>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</strong></p>\n\n<p>Aurora Replicas also referred to as reader instances have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability.</p>\n\n<p>By adding a reader instance to the Aurora cluster, the read-only traffic requests can be served from the reader instance, greatly reducing the traffic to the primary DB and avoiding interruption.</p>\n\n<p>An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its unique instance endpoint. A cluster endpoint (or writer endpoint) for an Aurora DB cluster connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements.</p>\n\n<p>The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service. This is the reason we need to change the application configuration to point to cluster endpoint and not to instance endpoint, in the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</strong></p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a Multi-AZ cluster configuration</strong></p>\n\n<p>Once you have created an Aurora cluster, you cannot change its configuration to Multi-AZ, so both of these options are incorrect.</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</strong> - The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read/write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. Leveraging a custom endpoint for this use case is overkill, so this option is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html</a></p>\n"
        }
      },
      {
        "id": 67357160,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company wants to enforce regulations to prevent frequent logins by DevOps engineers to the Amazon EC2 instances, with the added condition that immediate notification must be sent to the security team if any login occurs.</p>\n\n<p>What solution would you suggest to meet these requirements?</p>\n",
          "answers": [
            "<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>",
            "<p>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</p>",
            "<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>",
            "<p>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong></p>\n\n<p>The Amazon CloudWatch agent can be installed on EC2 instances to collect and send log data to CloudWatch Logs. By setting up a metric filter within CloudWatch Logs, it is possible to search for specific patterns, such as user login events. If a user login is found in the log data, Amazon SNS is used to send an immediate notification to the security team.</p>\n\n<p>You can use CloudWatch Logs to monitor applications and systems using log data in near real-time. For example, CloudWatch Logs can track the number of errors that occur in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify. The CloudWatch Logs Agent will send log data every five seconds by default and is configurable by the user. You can monitor log events as they are sent to CloudWatch Logs by creating Metric Filters. Metric Filters turn log data into Amazon CloudWatch Metrics for graphing or alarming.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q33-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. A subscription filter defines the filter pattern to use for filtering which logs events get delivered to your AWS resource, as well as information about where to send matching log events to. You need to use metric Filters (not subscription filters) to turn log data into Amazon CloudWatch Metrics for graphing or alarming. Therefore, this option is incorrect.</p>\n\n<p><strong>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</strong> - AWS CloudTrail enables the logging of AWS API calls, including login events, which are then sent to CloudWatch Logs. By subscribing CloudWatch Logs to Amazon Kinesis, the logs can be processed in real-time. An AWS Lambda function attached to Kinesis can parse the logs and identify user logins. If a login is detected, Amazon SNS is used to immediately notify the security team. While AWS CloudTrail captures login events, it typically delivers logs within an average of about 5 minutes of an API call. However, this time is not guaranteed. Therefore, this option is incorrect.</p>\n\n<p><strong>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - Amazon Inspector is a vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions for known software vulnerabilities and unintended network exposure. You cannot use Amazon Inspector to detect user login events in real time, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs\">https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p>\n"
        }
      },
      {
        "id": 67357168,
        "correct_response": [
          "c",
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer is currently involved in a data archival project where the task is to migrate on-premises data to an Amazon S3 bucket. The engineer has created a script that handles the incremental archiving of on-premises data, specifically transferring data older than 6 months to Amazon S3. As part of the process, the data is removed from the on-premises location after being successfully transferred using the S3 PutObject operation.</p>\n\n<p>During a thorough code review, the DevOps engineer identified a crucial issue in the script. The script does not include any validation to confirm whether the data is copied to Amazon S3 without any corruption. To ensure data integrity throughout the transmission, the DevOps engineer needs to update the script accordingly. The new solution must use MD5 checksums to verify the data integrity before allowing the deletion of the on-premises data.</p>\n\n<p>Considering these requirements, what modifications or solutions should the DevOps engineer implement in the script to ensure successful data transfer and integrity validation? (Select two)</p>\n",
          "answers": [
            "<p>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</p>",
            "<p>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</p>",
            "<p>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</p>",
            "<p>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</p>",
            "<p>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</strong></p>\n\n<p>Amazon S3 uses checksum values to verify the integrity of data that you upload to or download from Amazon S3. In addition, you can request that another checksum value be calculated for any object that you store in Amazon S3. You can select from one of several checksum algorithms to use when uploading or copying your data. Amazon S3 uses this algorithm to compute an additional checksum value and store it as part of the object metadata.</p>\n\n<p>One way to verify the integrity of your object after uploading is to provide an MD5 digest of the object when you upload it. If you calculate the MD5 digest for your object, you can provide the digest with the PUT command by using the Content-MD5 header. After uploading the object, Amazon S3 calculates the MD5 digest of the object and compares it to the value that you provided. The request succeeds only if the two digests match.</p>\n\n<p><strong>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</strong></p>\n\n<p>The entity tag (ETag) for an object represents a specific version of that object. Keep in mind that the ETag reflects changes only to the content of an object, not to its metadata. If only the metadata of an object changes, the ETag remains the same. For objects where the ETag is the Content-MD5 digest of the object, you can compare the ETag value of the object with a calculated or previously stored Content-MD5 digest.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</strong> - When uploading objects to Amazon S3, you can either provide a precalculated checksum for the object or use an AWS SDK to automatically create trailing checksums on your behalf. You cannot provide a trailing checksum as it is calculated by AWS, so this option is incorrect.</p>\n\n<p><strong>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</strong> - You need to provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command, and NOT as a custom name-value pair in the metadata of the object.</p>\n\n<p><strong>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</strong> - Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. When you enable versioning in a bucket, all new objects are versioned and given a unique version ID. You cannot use version ID to compare with the Content-MD5 digest.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html\">https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html</a></p>\n"
        }
      },
      {
        "id": 75949076,
        "correct_response": [
          "a",
          "b",
          "d"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company uses a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. Whenever traffic spikes occur, there have been inconsistent errors when the application auto scales. The following error message was generated:</p><p><em>\u201cInstance failed to complete user's Lifecycle Action: Lifecycle Action with token&lt;token-Id&gt; was abandoned: Heartbeat Timeout\u201d.</em></p><p>Which actions should a DevOps engineer take to collect logs for all affected instances and store them for later analysis? (Select THREE.)</p>",
          "answers": [
            "<p>Update the deployment group as the AWS CodeDeploy limits have been reached.</p>",
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create an Amazon EventBridge rule and invoke a Lambda function that uses SSM Run Command to extract the application logs and store them in S3.</p>",
            "<p>Enable Access Logs at the Target Group level and configure an Amazon S3 bucket as the destination.</p>",
            "<p>Analyze the logs directly in the Amazon S3 bucket using Amazon Athena.</p>",
            "<p>Analyze the logs by loading them into an Amazon EMR cluster.</p>",
            "<p>Update the health checks on the Auto Scaling group to use the correct port and protocol for the application.</p>"
          ],
          "explanation": "<p>The lifecycle-action-token is provided by Auto Scaling in the message sent as part of processing the lifecycle hook. You need to get the token from the original message.</p><p>The error message reported usually indicates one of the following:</p><ul><li><p>The maximum number of concurrent AWS CodeDeploy deployments associated with an AWS account was reached.</p></li><li><p>The Auto Scaling group tried to launch too many EC2 instances too quickly. The API calls to RecordLifecycleActionHeartbeat or CompleteLifecycleAction for each new instance were throttled.</p></li><li><p>An application in CodeDeploy was deleted before its associated deployment groups were updated or deleted.</p></li></ul><p>Therefore, the engineer can update the deployment group is limits have been reached and create a solution for extracting the application logs for later analysis. This solution can use Amazon EventBridge, Lambda and SSM Run Command with S3 as the destination.</p><p>Amazon Athena allows for running SQL queries against data in an Amazon S3 bucket. The engineer can then perform analysis to identify there are any application issues that must be fixed.</p><p><strong>CORRECT: \"</strong>Update the deployment group as the AWS CodeDeploy limits have been reached<strong>\" is a correct answer (as explained above.)</strong></p><p><strong>CORRECT: \"</strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create an Amazon EventBridge rule and invoke a Lambda function that uses SSM Run Command to extract the application logs and store them in S3<strong>\" is also a correct answer (as explained above.)</strong></p><p><strong>CORRECT: \"</strong>Analyze the logs directly in the Amazon S3 bucket using Amazon Athena<strong>\" is also a correct answer (as explained above.)</strong></p><p><strong>INCORRECT: \"</strong>Enable Access Logs at the Target Group level and configure an Amazon S3 bucket as the destination<strong>\" is incorrect.</strong></p><p><strong>The access logs will not provide the necessary information to troubleshoot and analyze the issues that are occurring. Access logs record information about the requests from clients.</strong></p><p><strong>INCORRECT: \"</strong>Analyze the logs by loading them into an Amazon EMR cluster<strong>\" is incorrect.</strong></p><p><strong>This won\u2019t be needed as this is not a map reduce use case and data can be analyzed by EMR in Amazon S3.</strong></p><p><strong>INCORRECT: \"</strong>Update the health checks on the Auto Scaling group to use the correct port and protocol for the application<strong>\" is incorrect.</strong></p><p><strong>There is no evidence that health checks are misconfigured from the errors that were generated. Auto scaling must be using the health checks as it is managing to auto scale and bring instances into service.</strong></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-reboot\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-reboot</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 82921380,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a presentation software company is deploying their flagship application using Elastic Beanstalk. The application is deployed using a Deploy stage in a CodePipeline pipeline. The technical requirements mandate changing the configuration of the Application Load Balancer tied to Elastic Beanstalk by adding an HTTP to HTTPS redirection rule.</p>\n\n<p>As a DevOps Engineer, you don't have the permissions to directly edit the Elastic Beanstalk environment, how can you proceed?</p>\n",
          "answers": [
            "<p>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add an <code>option_settings</code> block for which you will specify the Rules for the key <code>aws:elbv2:listener:default</code>. Push your code and let the CodePipeline run</p>",
            "<p>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Configure CodePipeline to deploy to Elastic Beanstalk using the EB CLI and push the code</p>",
            "<p>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Run a deploy using the EB CLI from your computer onto the Elastic Beanstalk Environment</p>",
            "<p>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add a <code>container_commands</code> block for which you will specify a container command that will run in <code>leader_only</code> mode. The EC2 instance will issue an API call to the Load Balancer to add the redirection rule</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add an <code>option_settings</code> block for which you will specify the Rules for the key <code>aws:elbv2:listener:default</code>. Push your code and let the CodePipeline run</strong></p>\n\n<p>You can use Elastic Beanstalk configuration files (.ebextensions) with your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the option_settings key to modify the environment configuration. You can choose from general options for all environments and platform-specific options.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p>Note: Recommended values are applied when you create or update an environment on the Elastic Beanstalk API by a client. For example, the client could be the AWS Management Console, Elastic Beanstalk Command Line Interface (EB CLI), AWS Command Line Interface (AWS CLI), or SDKs. Recommended values are directly set at the API level and have the highest precedence. The configuration setting applied at the API level can't be changed using option_settings, as the API has the highest precedence.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q22-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html</a></p>\n\n<p>Configuration changes made to your Elastic Beanstalk environment won't persist if you use the following configuration methods:</p>\n\n<p>Configuring an Elastic Beanstalk resource directly from the console of a specific AWS service.</p>\n\n<p>Installing a package, creating a file, or running a command directly from your Amazon EC2 instance.</p>\n\n<p>For the given use-case, using a <code>.ebextensions</code> file and configuring the rules in the <code>option_settings</code> block is the right option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Configure CodePipeline to deploy to Elastic Beanstalk using the EB CLI and push the code</strong> - This option has been added as a distractor as you cannot configure CodePipeline to deploy using the EB CLI.</p>\n\n<p><strong>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Run a deploy using the EB CLI from your computer onto the Elastic Beanstalk Environment</strong> - Using the EB CLI on your computer would normally work, but here the question specifies that we don't have the necessary permissions to make direct changes against the Beanstalk environment. We, therefore, have to use CodePipeline.</p>\n\n<p><strong>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add a <code>container_commands</code> block for which you will specify a container command that will run in <code>leader_only</code> mode. The EC2 instance will issue an API call to the Load Balancer to add the redirection rule</strong> - Using a <code>container_command</code> may work, but it wouldn't be best practice as the EC2 would issue a command to the ALB and therefore the configuration of it would be different from the one specified by Beanstalk itself, and the EC2 instance may not have enough permissions through IAM role to issue that command in the first place. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html</a></p>\n"
        }
      },
      {
        "id": 82921432,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A health-care services company has strong regulatory requirements and it has come to light recently that some of the EBS volumes have not been encrypted. It is necessary for the company to monitor and audit compliance over time and alert the corresponding teams if unencrypted EBS volumes are detected.</p>\n\n<p>How should a DevOps Engineer implement an alert for the unencrypted EBS volumes with the least administrative overhead?</p>\n",
          "answers": [
            "<p>Create an AWS Config managed rule checking for EBS volume encryption. Connect the rule to an SNS topic to provide alerting</p>",
            "<p>Create an AWS Config managed rule checking for EBS volume encryption. Use a CloudWatch Event rule to provide alerting</p>",
            "<p>Create an AWS Config custom rule checking for the EC2 instances, and their EBS attachments. Connect the rule to an SNS topic to provide alerting</p>",
            "<p>Create an AWS Lambda Function that is triggered by a CloudWatch Event rule. The rule is monitoring for new EBS volumes being created. The Lambda function should send a notification to SNS in case of a compliance check</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Config managed rule checking for EBS volume encryption. Use a CloudWatch Event rule to provide alerting</strong></p>\n\n<p>AWS Config provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your EBS volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules.</p>\n\n<p>AWS Config uses Amazon SNS to deliver notifications to subscription endpoints. These notifications provide the delivery status for configuration snapshots and configuration histories, and they provide each configuration item that AWS Config creates when the configurations of recorded AWS resources change. AWS Config also sends notifications that show whether your resources are compliant with your rules. SNS topics when directly integrated with Config can only be used to stream all the notifications and configuration changes and NOT selectively for a given rule.</p>\n\n<p>AWS Config has a managed rule to check for EBS volume encryption. For the given use-case, you need to isolate alerts for this managed rule, so you have to use CloudWatch Events which can then have a specific SNS topic as a target for alerting.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html\">https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q47-i2.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Config managed rule checking for EBS volume encryption. Connect the rule to an SNS topic to provide alerting</strong></p>\n\n<p><strong>Create an AWS Config custom rule checking for the EC2 instances, and their EBS attachments. Connect the rule to an SNS topic to provide alerting</strong></p>\n\n<p>As mentioned in the explanation above, SNS topics in Config can only be used to stream all the notifications and configuration changes. To isolate alerts for a single rule, you have to use CloudWatch Events. Therefore both these options are incorrect.</p>\n\n<p><strong>Create an AWS Lambda Function that is triggered by a CloudWatch Event rule. The rule is monitoring for new EBS volumes being created. The Lambda function should send a notification to SNS in case of a compliance check</strong> - Using AWS Lambda may work, but it will not provide you the auditing capability that AWS Config provides (a timeline dashboard with compliance over time).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html\">https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/\">https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/</a></p>\n"
        }
      },
      {
        "id": 82921438,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>Your company has adopted a git repository technology to store and have version control on the application code. Your company would like to make sure the production branch of the code is deployed to the production environment, but also would like to enable other versions of the code to be deployed to the development and staging environments for performing various kinds of user acceptance testing.</p>\n\n<p>As a DevOps Engineer, which solution would you implement for the given requirement?</p>\n",
          "answers": [
            "<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</p>",
            "<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</p>",
            "<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</p>",
            "<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Projects/CICD%20Pipeline/setup-cicd-pipeline2.5cefde1406fa6787d9d3c38ae6ba3a53e8df3be8.png\">\nvia - <a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p>Here you only need one git repository and create a production branch for deploys to production. The other key requirement of the given use-case is that two versions of the code need to be deployed to different environments. As such, you will need two CodePipelines. If you had one with a manual approval step at the end, then the code deployed to production would be coming from the master branch instead of the production branch. Here, we specifically need code in the production branch to be deployed to production, so, therefore, we need a second CodePipeline and to merge code from master to production through Pull Requests.</p>\n\n<p>Code Pipeline Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</strong> - As mentioned in the explanation above, a key requirement is that two versions of the code need to be deployed to different environments. If you use a manual approval step after the deployment to staging then the same version of the code from the master branch would also be deployed to the production environment. Instead, you need to maintain a production branch of the code that can be deployed to the production environment.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</strong> - It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to merging through a pull request has been added as a distractor.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</strong> - It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to the manual approval step has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/faqs/\">https://aws.amazon.com/codecommit/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n"
        }
      },
      {
        "id": 82921454,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>As a DevOps Engineer at an IT company, you are looking to create a daily EBS backup workflow. That workflow must take an EBS volume, and create a snapshot from it. When the snapshot is created, it must be copied to another region. In case the other region is unavailable because of a disaster, then that backup should be copied to a third region. An email address must be notified of the final result. There's a requirement to keep an audit trail of all executions as well.</p>\n\n<p>How can you implement this efficiently and in a fail-safe way?</p>\n",
          "answers": [
            "<p>Create an AWS Step Function. Implement each step as a Lambda function and add failure logic between the steps to deal with conditional cases</p>",
            "<p>Create an EC2 instance in the region where the EBS volume is. Create a CRON script that will invoke a Python script that performs all the steps and logic outlined above. For each step completion, write metadata to a DynamoDB table</p>",
            "<p>Create a CloudWatch Event rule that gets triggered every day. It triggers a Lambda function written in Python that performs all the steps and logic outlined above. Analyze the history of execution using AWS Config</p>",
            "<p>Create an SSM Automation that will perform each action. Add failure logic between steps to deal with conditional cases</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Step Function. Implement each step as a Lambda function and add failure logic between the steps to deal with conditional cases</strong></p>\n\n<p>Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to combine Step Functions, Lambda and CloudWatch Events into a single coherent solution. You can use the Step Functions to coordinate the business logic to automate the snapshot management flow with error handling, retry logic, and workflow logic all baked into the Step Functions definition. CloudWatch Events integrates with Step Functions and Lambda to let you execute your custom code when relevant events occur.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q62-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>For a deep-dive on this solution, highly recommend the following reference material:\n<a href=\"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an EC2 instance in the region where the EBS volume is. Create a CRON script that will invoke a Python script that performs all the steps and logic outlined above. For each step completion, write metadata to a DynamoDB table</strong> - Creating an EC2 instance may work, but if it gets terminated we have to re-create a new one. Failure scenarios may be tough to analyze and having the audit trail in DynamoDB probably won't be easy to use.</p>\n\n<p><strong>Create a CloudWatch Event rule that gets triggered every day. It triggers a Lambda function written in Python that performs all the steps and logic outlined above. Analyze the history of execution using AWS Config</strong> - Creating a CW event rule + Lambda function may work, but the Lambda function may have a timeout issue if the backup is taking longer than 15 minutes, and AWS Config cannot store the history of the execution. AWS Config only provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.</p>\n\n<p><strong>Create an SSM Automation that will perform each action. Add failure logic between steps to deal with conditional cases</strong> - An SSM automation cannot contain complex logic to handle failures, although it would provide an execution history.</p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n"
        }
      },
      {
        "id": 99528197,
        "correct_response": [
          "a",
          "c",
          "e"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has deployed AWS Single Sign-On (AWS SSO) and needs to ensure that user accounts are not created within AWS Identity and Access Management (AWS IAM). A DevOps engineer must create an automated solution for immediately disabling credentials of any new IAM user that is created. The security team must be notified when user creation events take place.</p><p>Which combination of steps should the DevOps engineer take to meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create an Amazon EventBridge rule that is triggered by IAM CreateUser API calls in AWS CloudTrail.</p>",
            "<p>Create an Amazon EventBridge rule that is triggered by IAM GetLoginProfile API calls in AWS CloudTrail.</p>",
            "<p>Create an AWS Lambda function that disables the access keys and deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule.</p>",
            "<p>Create an AWS Lambda function that deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule.</p>",
            "<p>Create an Amazon SNS topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic.</p>",
            "<p>Create an AWS Config rule that sends a notification to the security team using Amazon SNS when user accounts are modified.</p>"
          ],
          "explanation": "<p>The company is using AWS SSO and we can presume have an identity source that is outside of AWS IAM. They therefore want to control creation of IAM users. The solution uses an EventBridge rule that monitors for CreateUser API calls in AWS CloudTrail. This will pick up all user creation events.</p><p>Then, an AWS Lambda function will disable both the access keys (if created) and login profile (if created) that are associated with the newly created user account. Then, an SNS notification will be sent to the security team.</p><p>This solution meets all the stated requirements.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule that is triggered by IAM CreateUser API calls in AWS CloudTrail\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS Lambda function that disables the access keys and deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an Amazon SNS topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule that is triggered by IAM GetLoginProfile API calls in AWS CloudTrail\" is incorrect.</p><p>This would only be triggered when user accounts with console (password) access are created. A user with programmatic access does not have a login profile unless you create a password for the user to access the AWS Management Console. Therefore, this would miss users that are created with programmatic-only access.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule\" is incorrect.</p><p>As above, login profiles are associated with console-based password access only, they do not apply to users with programmatic-only access.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config rule that sends a notification to the security team using Amazon SNS when user accounts are modified\" is incorrect.</p><p>The DevOps engineer should directly configure Amazon SNS to be triggered by EventBridge. There is no need to send notifications related to user modifications, only creation events.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetLoginProfile.html\">https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetLoginProfile.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>"
        }
      },
      {
        "id": 115961523,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A critical application runs on Amazon EC2 instances in an Auto Scaling group. A script runs on the instances every 10 seconds to check application availability. A DevOps engineer must use this information returned by the script to monitor the application and trigger an alarm if there is an issue. The data should be collected every 1-minute and the solution must be cost-effective.</p><p>Which action should the engineer take?</p>",
          "answers": [
            "<p>Use a custom Amazon CloudWatch metric and configure a statistic set that aggregates data points and publishes the data every 1-minute.</p>",
            "<p>Use a default CloudWatch metric with a high resolution, aggregate multiple data points, and publish the data every 1-minute.</p>",
            "<p>Use a custom Amazon CloudWatch metric with a high resolution and publish the data every 10 seconds.</p>",
            "<p>Use a default CloudWatch metric with a standard resolution, use a dimension to publish data sets every 1-minute.</p>"
          ],
          "explanation": "<p>You can create custom metrics to send to Amazon CloudWatch. With custom metrics you can choose standard or high resolution and you can aggregate multiple data points and publish data as a statistic set to reduce cost and increase efficiency.</p><p>Each metric is one of the following:</p><ul><li><p>Standard resolution, with data having a one-minute granularity.</p></li><li><p>High resolution, with data at a granularity of one second.</p></li></ul><p>Metrics produced by AWS services are standard resolution by default. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p><p>You can aggregate your data before you publish to CloudWatch. When you have multiple data points per minute, aggregating data minimizes the number of calls to <strong>put-metric-data</strong>.</p><p>Therefore, the engineer can use statistic sets to aggregate and publish the data every one minute. This is the most cost-effective solution that meets the requirements.</p><p><strong>CORRECT: </strong>\"Use a custom Amazon CloudWatch metric and configure a statistic set that aggregates data points and publishes the data every 1-minute\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use a default CloudWatch metric with a high resolution, aggregate multiple data points, and publish the data every 1-minute\" is incorrect.</p><p>You cannot use high resolution with a default CloudWatch metric and a default CloudWatch metric would not be available for the application availability data.</p><p><strong>INCORRECT:</strong> \"Use a custom Amazon CloudWatch metric with a high resolution and publish the data every 10 seconds\" is incorrect.</p><p>This would be less efficient and more costly as the put-metric-data API action would be run every 10 seconds. Fewer API calls means lower cost so aggregating into a statistic set is better and publishing every 1-minute.</p><p><strong>INCORRECT:</strong> \"Use a default CloudWatch metric with a standard resolution, use a dimension to publish data sets every 1-minute\" is incorrect.</p><p>There would not be a default metric available that uses the data returned from the application availability script. A dimension is used for organizing and clarifying what the metric data is and what it stores.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 138248147,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multinational investment bank is using AWS Organizations to handle its multiple AWS accounts across various AWS regions around the world. To comply with the strict financial IT regulations, the bank must ensure that all of its Amazon EBS volumes in its AWS accounts are encrypted. A DevOps engineer has been requested to set up an automated solution that will provide a detailed report of all unencrypted EBS volumes of the company as well as to notify if there is a newly launched Amazon EC2 instance which uses an unencrypted volume.</p><p>Which of the following should the DevOps engineer implement to meet this requirement with the LEAST amount of operational overhead?</p>",
          "answers": [
            "<p>Set up an AWS Config rule with a corresponding AWS Lambda function on all the target accounts of the company. Collect data from multiple accounts and AWS Regions using Config\u2019s aggregators. Export the aggregated report to an Amazon S3 bucket then deliver the notifications using Amazon SNS.</p>",
            "<p>Configure AWS CloudTrail to deliver all events to an S3 bucket in a centralized AWS account. Run a Lambda function to parse CloudTrail logs whenever logs are delivered to the S3 bucket using the S3 event notification. Use the same Lambda function to publish the results to SNS.</p>",
            "<p>Prepare an AWS CloudFormation template which contains a Config managed rule for EBS encryption of your EBS volumes. Deploy the template across all accounts and regions of the company using the CloudFormation stack set. Store consolidated results of the Config rules evaluation in an S3 bucket. When non-compliant EBS resources are detected, send a notification to the Operations team using SNS.</p>",
            "<p>Use the AWS Systems Manager Configuration Compliance to monitor all EBS volumes across all the accounts and AWS Regions of the company. Export and store the detailed compliance report to an S3 bucket and then deliver the notifications using SNS.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.</p><p><img src=\"https://media.tutorialsdojo.com/public/Aggregate_Data_Landing_Page_Diagram.png\">An AWS <em>resource</em> is an entity you can work with in AWS, such as an Amazon Elastic Compute Cloud (EC2) instance, an Amazon Elastic Block Store (EBS) volume, a security group, or an Amazon Virtual Private Cloud (VPC).</p><p>With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p><p>- Multiple accounts and multiple regions.</p><p>- Single account and multiple regions.</p><p>- An organization in AWS Organizations and all the accounts in that organization.</p><p>You can use an aggregator to view the resource configuration and compliance data recorded in AWS Config.</p><p>Hence, the correct answer is: <strong>Set up an AWS Config rule with a corresponding AWS Lambda function on all the target accounts of the company. Collect data from multiple accounts and AWS Regions using Config\u2019s aggregators. Export the aggregated report to an Amazon S3 bucket then deliver the notifications using Amazon SNS.</strong></p><p>The option that says: <strong>Configure AWS CloudTrail to deliver all events to an S3 bucket in a centralized AWS account. Run a Lambda function to parse CloudTrail logs whenever logs are delivered to the S3 bucket using the S3 event notification. Use the same Lambda function to publish the results to SNS </strong>is incorrect. Although this solution may work, it certainly entails a lot of operational overhead to execute and implement. Parsing thousands of API actions from all of your accounts in CloudTrail just to ensure that the EBS encryption was enabled on all volumes could take a significant amount of time compared with just using AWS Config.</p><p>The option that says: <strong>Prepare an AWS CloudFormation template which contains a Config managed rule for EBS encryption of your EBS volumes. Deploy the template across all accounts and regions of the company using the CloudFormation stack set. Store consolidated results of the Config rules evaluation in an S3 bucket. When non-compliant EBS resources are detected, send a notification to the Operations team using SNS </strong>is incorrect. Although it is right to use AWS Config here, this solution still entails a lot of management overhead to maintain all of the CloudFormation templates. A better solution is to use AWS Config aggregators instead.</p><p>The option that says: <strong>Use the AWS Systems Manager Configuration Compliance to monitor all EBS volumes across all the accounts and AWS Regions of the company. Export and store the detailed compliance report to an S3 bucket and then deliver the notifications using SNS</strong> is incorrect. Although you can collect and aggregate data from multiple AWS accounts and Regions using the AWS Systems Manager Configuration Compliance service, this solution has a lot of prerequisites and configuration needed. You have to install SSM agent to all of your EC2 instances, create Resource Data Syncs, set up a custom compliance type to check the EBS encryption and many others. Moreover, the AWS Systems Manager Configuration Compliance service is more suitable only for verifying the patch compliance of all your resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\">https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 138248153,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A government agency has a VMware-based automated server build system on its on-premises network that uses virtualization software that allows the creation of server images of the application. The DevOps Engineer was tasked to set up a system that will allow to test its server images using its on-premises server pipeline to resemble the build and behavior on Amazon EC2. In this way, the agency can verify the functionality of the application, detect incompatibility issues, and determine any prerequisites on the new Amazon Linux 2 operating system that will be used in AWS.</p><p>Which of the following solutions should the DevOps Engineer implement to accomplish this task?</p>",
          "answers": [
            "<p>Download the latest AmazonLinux2.iso of the Amazon Linux 2 operating system and import it to your on-premises network. Directly launch a new on-premises server based on the imported ISO, without any virtual platform. Deploy the application, and commence testing.</p>",
            "<p>Configure a hybrid cloud environment using AWS Outposts, install the Linux 2 operating system on the AWS Outposts servers, and connect them to your on-premises network. Deploy the application on the Linux 2 servers in the Outposts environment for testing.</p>",
            "<p>Launch an EC2 instance with the latest Linux OS in AWS. Use the AWS VM Import/Export service to import the EC2 image, export it to a VMware ISO in an Amazon S3 bucket, and then import the ISO to an on-premises server. Once done, commence the testing activity to verify the application's functionalities.</p>",
            "Launch a new on-premises server with any distribution of Linux operating system such as CentOS, Ubuntu or Fedora since these are technically the same. Deploy the application to the server for testing."
          ],
          "explanation": "<p>The <strong>VM Import/Export</strong> enables you to easily import virtual machine images from your existing environment to Amazon EC2 instances and export them back to your on-premises environment. This offering allows you to leverage your existing investments in the virtual machines that you have built to meet your IT security, configuration management, and compliance requirements by bringing those virtual machines into Amazon EC2 as ready-to-use instances. You can also export imported instances back to your on-premises virtualization infrastructure, allowing you to deploy workloads across your IT infrastructure.</p><p>To import your images, use the AWS CLI or other developer tools to import a virtual machine (VM) image from your VMware environment. If you use the VMware vSphere virtualization platform, you can also use the AWS Management Portal for vCenter to import your VM. As part of the import process, VM Import will convert your VM into an Amazon EC2 AMI, which you can use to run Amazon EC2 instances. Once your VM has been imported, you can take advantage of Amazon\u2019s elasticity, scalability, and monitoring via offerings like Auto Scaling, Elastic Load Balancing, and CloudWatch to support your imported images.</p><p><img src=\"https://media.tutorialsdojo.com/public/vmimport-export-architecture-ami-copy.png\"></p><p>You can export previously imported EC2 instances using the Amazon EC2 API tools. You simply specify the target instance, virtual machine file format and a destination S3 bucket, and VM Import/Export will automatically export the instance to the S3 bucket. You can then download and launch the exported VM within your on-premises virtualization infrastructure.</p><p>You can import Windows and Linux VMs that use VMware ESX or Workstation, Microsoft Hyper-V, and Citrix Xen virtualization formats. And you can export previously imported EC2 instances to VMware ESX, Microsoft Hyper-V or Citrix Xen formats.</p><p>Hence, the correct answer is: <strong>Launch an EC2 instance with the latest Linux OS in AWS. Use the AWS VM Import/Export service to import the EC2 image, export it to a VMware ISO in an Amazon S3 bucket, and then import the ISO to an on-premises server. Once done, commence the testing activity to verify the application's functionalities.</strong></p><p>The option that says: <strong>Download the latest </strong><code><strong>AmazonLinux2.iso</strong></code><strong> of the Amazon Linux 2 operating system and import it to your on-premises network. Directly launch a new on-premises server based on the imported ISO, without any virtual platform. Deploy the application, and commence testing<em> </em></strong>is incorrect because there is no way to directly download the <code>AmazonLinux2.iso</code> for Amazon Linux 2. You just have to use VM Import/Export service instead or, alternatively, run the Amazon Linux 2 as a virtual machine in your on-premises data center. Again, you won't be able to directly download the ISO image, but you can get the Amazon Linux 2 image for the specific virtualization platform of your choice. If you are using VMware, you can download the ESX image *.ova, and for VirtualBox, you'll get the *.vdi image file. What you should do first is to prepare the seed.iso boot image and then connect it to the VM of your choice on the first boot.</p><p>The option that says: <strong>Configure a hybrid cloud environment using AWS Outposts, install the Linux 2 operating system on the AWS Outposts servers, and connect them to your on-premises network. Deploy the application on the Linux 2 servers in the Outposts environment for testing</strong> is incorrect. While AWS Outposts would indeed provide access to Amazon Linux 2, it introduces an unnecessary layer of complexity and cost to what should be a straightforward testing process. The agency's goal is to simulate EC2 environments using their current on-premises VMware-based system, not to extend their AWS infrastructure to on-premises environments.</p><p>The option that says: <strong>Launch a new on-premises server with any distribution of Linux operating system such as CentOS, Ubuntu or Fedora since these are technically the same. Deploy the application to the server for testing </strong>is incorrect because these Linux distributions are actually different from one another. There could be some incompatibility issues between the different Linux operating systems, which is why you need to test your application on a specific Amazon Linux 2 type only.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html\">https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html</a></p><p><a href=\"https://aws.amazon.com/ec2/vm-import/\">https://aws.amazon.com/ec2/vm-import/</a></p><p><a href=\"https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html\">https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-linux-2-virtual-machine.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-linux-2-virtual-machine.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>"
        }
      },
      {
        "id": 138248167,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading telecommunications company is using CloudFormation templates to deploy enterprise applications to their production, staging, and development environments in AWS. Their current process involves manual&nbsp;changes to their CloudFormation templates in order to specify the configuration variables and static attributes for each environment. The DevOps Engineer was tasked to set up automated deployments using AWS CodePipeline and ensure that the CloudFormation template is reusable across multiple pipelines. </p><p>How should the DevOps Engineer satisfy this requirement?</p>",
          "answers": [
            "<p>Set up a Lambda-backed custom resource in the CloudFormation templates. Configure the custom resource to monitor the status of the pipeline in AWS CodePipeline in order to detect which environment was launched. Use the cfn-init helper script to modify the launch template of each application stack based on its environment.</p>",
            "<p>Launch a new pipeline using&nbsp;AWS CodePipeline that has multiple stages for each environment and configure it to use input parameters. Switch the associated <code>UserData</code> of the EC2 instances to match the environment where the application stack is being launched using CloudFormation mappings. Specify&nbsp;parameter overrides&nbsp;for AWS CloudFormation actions.&nbsp;</p>",
            "<p>Launch a new pipeline using&nbsp;AWS CodePipeline for each environment with multiple stages for each application. Trigger the CloudFormation deployments using a Lambda function to dynamically modify the <code>UserData</code> of the EC2 instances that were launched in each environment.</p>",
            "<p>Manually configure the CloudFormation templates to use input parameters. Add a configuration that whenever the CloudFormation stack is updated, it will dynamically modify the <code>LaunchConfiguration</code> and <code>UserData</code> sections of the EC2 instances.</p>"
          ],
          "explanation": "<p>Continuous delivery is a release practice in which code changes are automatically built, tested, and prepared for release to production. With <strong>AWS CloudFormation</strong> and <strong>CodePipeline</strong>, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. This release process lets you rapidly and reliably make changes to your AWS infrastructure.</p><p>For example, you can create a workflow that automatically builds a test stack when you submit an updated template to a code repository. After AWS CloudFormation builds the test stack, you can test it and then decide whether to push the changes to a production stack.</p><p>You can use CodePipeline to build a continuous delivery workflow by building a pipeline for AWS CloudFormation stacks. CodePipeline has built-in integration with AWS CloudFormation, so you can specify AWS CloudFormation-specific actions, such as creating, updating, or deleting a stack within a pipeline.</p><p><img src=\"https://media.tutorialsdojo.com/public/product-page-diagram_CodePipeLine.7b8dd19eb6478b7f6f747d936c2f0b0b66757bbf.png\"></p><p>In a CodePipeline stage, you can specify parameter overrides for AWS CloudFormation actions. Parameter overrides let you specify template parameter values that override values in a template configuration file. AWS CloudFormation provides functions to help you specify dynamic values (values that are unknown until the pipeline runs).</p><p>You can set the <code>Fn::GetArtifactAtt</code> function which retrieves the value of an attribute from an input artifact, such as the S3 bucket name where the artifact is stored. You can use this function to specify attributes of an artifact, such as its filename or S3 bucket name, that can be used in the pipeline.</p><p>Hence, the correct answer is: <strong>Launch a new pipeline using AWS CodePipeline that has multiple stages for each environment and configure it to use input parameters. Switch the associated </strong><code><strong>UserData</strong></code><strong> of the EC2 instances to match the environment where the application stack is being launched using CloudFormation mappings. Specify parameter overrides for AWS CloudFormation actions.</strong></p><p>The option that says: <strong>Set up a Lambda-backed custom resource in the CloudFormation templates. Configure the custom resource to monitor the status of the pipeline in AWS CodePipeline in order to detect which environment was launched. Use the cfn-init helper script to modify the launch template of each application stack based on its environment</strong> is incorrect because monitoring the pipeline using a custom resource in CloudFormation entails a lot of administrative overhead. A better solution would be to use input parameters or parameter overrides for AWS CloudFormation actions.</p><p>The option that says: <strong>Launch a new pipeline using AWS CodePipeline for each environment with multiple stages for each application. Trigger the CloudFormation deployments using a Lambda function to dynamically modify the </strong><code><strong>UserData</strong></code><strong> of the EC2 instances that were launched in each environment</strong> is incorrect because using a Lambda function to modify the <code><strong>UserData</strong></code> of the already running EC2 instances is not a suitable solution. The parameters should have been dynamically populated and set before the resources were launched by using parameter overrides.</p><p>The option that says: <strong>Manually configure the CloudFormation templates to use input parameters. Add a configuration that whenever the CloudFormation stack is updated, it will dynamically modify the </strong><code><strong>LaunchConfiguration</strong></code><strong> and </strong><code><strong>UserData</strong></code><strong> sections of the EC2 instances</strong> is incorrect. Although using input parameters is helpful in this scenario, you should still integrate CloudFormation and CodePipeline in order to properly map the configuration files for each environment.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-parameter-override-functions.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-parameter-override-functions.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p><p><a href=\"https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/?src=udemy\">https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</a></p>"
        }
      },
      {
        "id": 138248221,
        "correct_response": [
          "b",
          "e"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is re-architecting its monolithic system to a serverless application in AWS to save on cost. The deployment of the succeeding new version of the application must be initially rolled out to a small number of users first for testing before the full release. If the post-hook tests fail, there should be an easy way to roll back the deployment. The DevOps Engineer was assigned to design an efficient deployment setup that mitigates any unnecessary outage that impacts their production environment.</p><p>As a DevOps Engineer, how should you satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Launch an Application Load Balancer with an Amazon API Gateway private integration. Attach a single target group to the load balancer and select the \"Canary\" routing option which will automatically route incoming traffic to the new version.</p>",
            "<p>Set up one AWS Lambda Function Alias that points to both the current and new versions. Route 20% of incoming traffic to the new version and once it is considered stable, update the alias to route all traffic to the new version.</p>",
            "<p>Create a new record in Route 53 with a Failover routing policy. Configure the primary record to route 20% of incoming traffic to the new version and set the secondary record to route the rest of the traffic to the current version. Once the new version stabilizes, update the primary record to route all traffic to the new version.</p>",
            "<p>Launch a Network Load Balancer with an Amazon API Gateway private integration. Attach two target groups to the load balancer. Configure the first target group with the current version and the second target group with the new version. Configure the load balancer to route 20% of the incoming traffic to the new version and once it becomes stable, detach the first target group from the load balancer.</p>",
            "<p>Set up a canary deployment in Amazon API Gateway that routes 20% of the incoming traffic to the canary release. Promote the canary release to production once the initial tests have passed.</p>"
          ],
          "explanation": "<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Each alias has a unique ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. Event sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. In a resource policy, you can grant permissions for event sources to use your Lambda function. If you specify an alias ARN in the policy, you don't need to update the policy when the function version changes.</p><p>Use routing configuration on an alias to send a portion of traffic to a second function version. For example, you can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version. You can point an alias to a maximum of two Lambda function versions.</p><p><img src=\"https://media.tutorialsdojo.com/public/API_gateway.png\"></p><p>In API Gateway, you create a canary release deployment when deploying the API with <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/link-relation/deployment-create/#canarySettings\">canary settings</a> as an additional input to the <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/link-relation/deployment-create/\">deployment creation</a> operation.</p><p>You can also create a canary release deployment from an existing non-canary deployment by making a <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/link-relation/stage-update/\"><code>stage:update</code></a> request to add the canary settings on the stage.</p><p>When creating a non-canary release deployment, you can specify a non-existing stage name. API Gateway creates one if the specified stage does not exist. However, you cannot specify any non-existing stage name when creating a canary release deployment. You will get an error and API Gateway will not create any canary release deployment.</p><p>Hence, the correct answers are:</p><p><strong>- Set up one AWS Lambda Function Alias that points to both the current and new versions. Route 20% of incoming traffic to the new version and once it is considered stable, update the alias to route all traffic to the new version.</strong></p><p>- <strong>Set up a canary deployment in Amazon API Gateway that routes 20% of the incoming traffic to the canary release. Promote the canary release to production once the initial tests have passed.</strong></p><p>The option that says:<strong><em> </em>Launch an Application Load Balancer with an Amazon API Gateway private integration. Attach a single target group to the load balancer and select the \"Canary\" routing option which will automatically route incoming traffic to the new version</strong> is incorrect because there is no Canary routing option in an Application Load Balancer.</p><p>The option that says: <strong>Launch a Network Load Balancer with an Amazon API Gateway private integration. Attach two target groups to the load balancer. Configure the first target group with the current version and the second target group with the new version. Configure the load balancer to route 20% of the incoming traffic to the new version and once it becomes stable, detach the first target group from the load balancer </strong>is incorrect because the Network Load Balancer does not support weighted target groups, unlike the Application Load Balancer.</p><p>The option that says: <strong>Create a new record in Route 53 with a Failover routing policy. Configure the primary record to route 20% of incoming traffic to the new version and set the secondary record to route the rest of the traffic to the current version. Once the new version stabilizes, update the primary record to route all traffic to the new version </strong>is incorrect because the failover routing policy simply lets you route traffic to a resource when the resource is healthy, or to a different resource when the first resource is unhealthy. This type of routing is not an appropriate setup. A better solution is to use Canary deployment release in API Gateway to deploy the serverless application.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p><p><br></p><p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 143860757,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps Engineer manages an application running across accounts which is deployed via AWS CloudFormation.</p><p>The application stack has an Amazon ECS Fargate cluster which spins up multiple tasks for the application layer and utilizes an Amazon ElastiCache Redis cache to store frequently accessed data.</p><p>The accounts are labelled \u201csandbox\u201d and \u201cstaging\u201d. While the stack spins up fine, the application is unable to connect to Redis with the below error logged in Amazon CloudWatch Logs.</p><p>\u201cStopped reason ResourceInitializationError: unable to pull secrets or registry auth: pull command failed :: signal: killed\u201c.</p><p>What is the possible fix for the above error? (Select TWO.)</p>",
          "answers": [
            "<p>Ensure inbound connectivity is allowed in the application security group on port 6379 from the Redis cluster subnet.</p>",
            "<p>Ensure an ENI is configured for the ECS tasks in the AWS Fargate cluster and there is an api.ecr endpoint.</p>",
            "<p>Ensure the ECS tasks are launched in a public subnet and public IP addresses are assigned to them.</p>",
            "<p>Ensure that the IAM role provides the required permissions.</p>",
            "<p>Ensure that the application security group has a rule that accepts connections from 0.0.0.0/0.</p>"
          ],
          "explanation": "<p>AWS Fargate clusters with the 1.4 version upgrade provides ENI at the task level. Below is AWS documentation for the same:</p><p>Fargate tasks run on a fleet of virtual machines that AWS manages on behalf of the customer. These VMs are connected to AWS owned VPCs via so called \u201cFargate ENIs\u201d. When a user launches a task on Fargate, the task is assigned an ENI and this ENI is connected to the customer owned VPC</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-05-26-622074b8a2ac8ab2c36144cac8b52c3e.jpg\"><p><strong>CORRECT: </strong>\"Ensure inbound connectivity is allowed in the application security group on port 6379 from the Redis cluster subnet\" is a correct answer (as explained above).</p><p><strong>CORRECT: </strong>\"Ensure an ENI is configured for the ECS tasks in the AWS Fargate cluster and there is an api.ecr endpoint\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Ensure the ECS tasks are launched in a public subnet and public IP addresses are assigned to them\" is incorrect since your tasks can sit in a private subnet as well.</p><p><strong>INCORRECT:</strong> \"Ensure that the IAM role provides the required permissions\" is incorrect since question is more around connectivity and stack worked fine in one of the environments.</p><p><strong>INCORRECT:</strong> \"Ensure that the application security group has a rule that accepts connections from 0.0.0.0/0\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/containers/aws-fargate-launches-platform-version-1-4/\">AWS Fargate launches platform version 1.4.0 | Containers (amazon.com)</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>"
        }
      },
      {
        "id": 143860771,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The application is used by users around the world who access the application using a custom DNS domain name. The application must support encryption in transit, be protected from DDoS attacks and web exploits, should be optimized for performance.</p><p>Which actions should a DevOps engineer take to meet these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Create an Amazon CloudFront distribution with the Auto Scaling group as an origin. Configure the custom domain name and attach an SSL/TLS certificate.</p>",
            "<p>Create an Amazon CloudFront distribution with the ALB as an origin. Configure the custom domain name and attach an SSL/TLS certificate.</p>",
            "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the CloudFront distribution that WAF should inspect.</p>",
            "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the ALB that WAF should inspect.</p>",
            "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the Auto Scaling group that WAF should inspect.</p>"
          ],
          "explanation": "<p>To improve performance for global users the solution should use Amazon CloudFront. The distribution should specify the ALB as the origin and use a custom domain name and SSL/TLS certificate. This will enable caching of content in Edge Locations around the world and CloudFront offers DDoS protection.</p><p>To protect against web exploits AWS WAF should be used. A Web ACL must be created with an action and rule specified to deal with threats. The web ACL can be specified in the CloudFront distribution.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution with the ALB as an origin. Configure the custom domain name and attach an SSL/TLS certificate\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL, configure a default action and rule, and specify the CloudFront distribution that WAF should inspect\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution with the Auto Scaling group as an origin. Configure the custom domain name and attach an SSL/TLS certificate\" is incorrect.</p><p>You can configure an ELB as an origin for the distribution, but you cannot specify an ASG.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL, configure a default action and rule, and specify the ALB that WAF should inspect\" is incorrect.</p><p>The web ACL should be attached to the CloudFront distribution in this case as it sits in front of the ALB. It is always better to protect as close to the edge as possible.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL, configure a default action and rule, and specify the Auto Scaling group that WAF should inspect\" is incorrect.</p><p>The web ACL must be attached to the CloudFront distribution, not the ASG.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>"
        }
      }
    ],
    "answers": {
      "67357122": [
        "c"
      ],
      "67357128": [
        "d"
      ],
      "67357160": [
        "d"
      ],
      "67357168": [
        "b"
      ],
      "75949076": [
        "b"
      ],
      "82921432": [
        "a"
      ],
      "82921438": [
        "d"
      ],
      "82921454": [
        "d"
      ]
    }
  },
  {
    "id": "1772183132304",
    "date": "2026-02-27T09:05:32.304Z",
    "course": "Unknown",
    "mode": "exam",
    "sourceLabel": null,
    "score": 0,
    "incorrect": 0,
    "unanswered": 17,
    "total": 17,
    "percent": 0,
    "duration": 3031,
    "questions": [
      {
        "id": 67357122,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company uses multiple AWS accounts to help isolate and manage business applications. This multi-account environment consists of an AWS Transit Gateway to route all outbound traffic through a common network account. A firewall appliance inspects all traffic before it is forwarded to an internet gateway. The firewall appliance is configured to send logs to Amazon CloudWatch Logs for all events generated.</p>\n\n<p>Recently, the security team has advised about probable illegal access of resources. As DevOps Engineer, you have been advised to configure an alert to the security team if the firewall appliance generates an event of Critical severity.</p>\n\n<p>How should a DevOps engineer configure this requirement?</p>\n",
          "answers": [
            "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>",
            "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send notification to an Amazon Simple Notification Service (Amazon SNS) topic if Critical event is detected. Subscribe the email address of the security team to the SNS topic</p>",
            "<p>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>",
            "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong></p>\n\n<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p>\n\n<p>A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager.</p>\n\n<p>Amazon CloudWatch metrics and alarms:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send a notification to an Amazon Simple Notification Service (Amazon SNS) topic if a Critical event is detected. Subscribe the email address of the security team to the SNS topic</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations. Kinesis Data Firehose is not supported as a destination for the metric streams.</p>\n\n<p><strong>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - Transit Gateway Flow Logs is a feature that enables you to capture information about the IP traffic going to and from your transit gateways. This does not meet our objective of analyzing firewall log data.</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications running on AWS Lambda. The solution collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network. This option acts as a distractor for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html</a></p>\n"
        }
      },
      {
        "id": 67357128,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An Aurora cluster is configured with a single DB instance for a web application. The application uses the instance endpoint to read/write data to the database. The operations team has scheduled an update on the cluster during the upcoming maintenance window. The application support team has requested help to ensure uninterrupted access to the application during the maintenance window.</p>\n\n<p>Which step should a DevOps Engineer take so that the users experience the least possible interruption during the maintenance window?</p>\n",
          "answers": [
            "<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</p>",
            "<p>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a  Multi-AZ cluster configuration</p>",
            "<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</p>",
            "<p>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</strong></p>\n\n<p>Aurora Replicas also referred to as reader instances have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability.</p>\n\n<p>By adding a reader instance to the Aurora cluster, the read-only traffic requests can be served from the reader instance, greatly reducing the traffic to the primary DB and avoiding interruption.</p>\n\n<p>An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its unique instance endpoint. A cluster endpoint (or writer endpoint) for an Aurora DB cluster connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements.</p>\n\n<p>The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service. This is the reason we need to change the application configuration to point to cluster endpoint and not to instance endpoint, in the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</strong></p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a Multi-AZ cluster configuration</strong></p>\n\n<p>Once you have created an Aurora cluster, you cannot change its configuration to Multi-AZ, so both of these options are incorrect.</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</strong> - The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read/write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. Leveraging a custom endpoint for this use case is overkill, so this option is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html</a></p>\n"
        }
      },
      {
        "id": 67357160,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company wants to enforce regulations to prevent frequent logins by DevOps engineers to the Amazon EC2 instances, with the added condition that immediate notification must be sent to the security team if any login occurs.</p>\n\n<p>What solution would you suggest to meet these requirements?</p>\n",
          "answers": [
            "<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>",
            "<p>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</p>",
            "<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>",
            "<p>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong></p>\n\n<p>The Amazon CloudWatch agent can be installed on EC2 instances to collect and send log data to CloudWatch Logs. By setting up a metric filter within CloudWatch Logs, it is possible to search for specific patterns, such as user login events. If a user login is found in the log data, Amazon SNS is used to send an immediate notification to the security team.</p>\n\n<p>You can use CloudWatch Logs to monitor applications and systems using log data in near real-time. For example, CloudWatch Logs can track the number of errors that occur in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify. The CloudWatch Logs Agent will send log data every five seconds by default and is configurable by the user. You can monitor log events as they are sent to CloudWatch Logs by creating Metric Filters. Metric Filters turn log data into Amazon CloudWatch Metrics for graphing or alarming.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q33-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. A subscription filter defines the filter pattern to use for filtering which logs events get delivered to your AWS resource, as well as information about where to send matching log events to. You need to use metric Filters (not subscription filters) to turn log data into Amazon CloudWatch Metrics for graphing or alarming. Therefore, this option is incorrect.</p>\n\n<p><strong>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</strong> - AWS CloudTrail enables the logging of AWS API calls, including login events, which are then sent to CloudWatch Logs. By subscribing CloudWatch Logs to Amazon Kinesis, the logs can be processed in real-time. An AWS Lambda function attached to Kinesis can parse the logs and identify user logins. If a login is detected, Amazon SNS is used to immediately notify the security team. While AWS CloudTrail captures login events, it typically delivers logs within an average of about 5 minutes of an API call. However, this time is not guaranteed. Therefore, this option is incorrect.</p>\n\n<p><strong>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - Amazon Inspector is a vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions for known software vulnerabilities and unintended network exposure. You cannot use Amazon Inspector to detect user login events in real time, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs\">https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p>\n"
        }
      },
      {
        "id": 67357168,
        "correct_response": [
          "c",
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer is currently involved in a data archival project where the task is to migrate on-premises data to an Amazon S3 bucket. The engineer has created a script that handles the incremental archiving of on-premises data, specifically transferring data older than 6 months to Amazon S3. As part of the process, the data is removed from the on-premises location after being successfully transferred using the S3 PutObject operation.</p>\n\n<p>During a thorough code review, the DevOps engineer identified a crucial issue in the script. The script does not include any validation to confirm whether the data is copied to Amazon S3 without any corruption. To ensure data integrity throughout the transmission, the DevOps engineer needs to update the script accordingly. The new solution must use MD5 checksums to verify the data integrity before allowing the deletion of the on-premises data.</p>\n\n<p>Considering these requirements, what modifications or solutions should the DevOps engineer implement in the script to ensure successful data transfer and integrity validation? (Select two)</p>\n",
          "answers": [
            "<p>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</p>",
            "<p>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</p>",
            "<p>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</p>",
            "<p>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</p>",
            "<p>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</strong></p>\n\n<p>Amazon S3 uses checksum values to verify the integrity of data that you upload to or download from Amazon S3. In addition, you can request that another checksum value be calculated for any object that you store in Amazon S3. You can select from one of several checksum algorithms to use when uploading or copying your data. Amazon S3 uses this algorithm to compute an additional checksum value and store it as part of the object metadata.</p>\n\n<p>One way to verify the integrity of your object after uploading is to provide an MD5 digest of the object when you upload it. If you calculate the MD5 digest for your object, you can provide the digest with the PUT command by using the Content-MD5 header. After uploading the object, Amazon S3 calculates the MD5 digest of the object and compares it to the value that you provided. The request succeeds only if the two digests match.</p>\n\n<p><strong>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</strong></p>\n\n<p>The entity tag (ETag) for an object represents a specific version of that object. Keep in mind that the ETag reflects changes only to the content of an object, not to its metadata. If only the metadata of an object changes, the ETag remains the same. For objects where the ETag is the Content-MD5 digest of the object, you can compare the ETag value of the object with a calculated or previously stored Content-MD5 digest.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</strong> - When uploading objects to Amazon S3, you can either provide a precalculated checksum for the object or use an AWS SDK to automatically create trailing checksums on your behalf. You cannot provide a trailing checksum as it is calculated by AWS, so this option is incorrect.</p>\n\n<p><strong>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</strong> - You need to provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command, and NOT as a custom name-value pair in the metadata of the object.</p>\n\n<p><strong>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</strong> - Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. When you enable versioning in a bucket, all new objects are versioned and given a unique version ID. You cannot use version ID to compare with the Content-MD5 digest.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html\">https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html</a></p>\n"
        }
      },
      {
        "id": 75949076,
        "correct_response": [
          "a",
          "b",
          "d"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company uses a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. Whenever traffic spikes occur, there have been inconsistent errors when the application auto scales. The following error message was generated:</p><p><em>\u201cInstance failed to complete user's Lifecycle Action: Lifecycle Action with token&lt;token-Id&gt; was abandoned: Heartbeat Timeout\u201d.</em></p><p>Which actions should a DevOps engineer take to collect logs for all affected instances and store them for later analysis? (Select THREE.)</p>",
          "answers": [
            "<p>Update the deployment group as the AWS CodeDeploy limits have been reached.</p>",
            "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create an Amazon EventBridge rule and invoke a Lambda function that uses SSM Run Command to extract the application logs and store them in S3.</p>",
            "<p>Enable Access Logs at the Target Group level and configure an Amazon S3 bucket as the destination.</p>",
            "<p>Analyze the logs directly in the Amazon S3 bucket using Amazon Athena.</p>",
            "<p>Analyze the logs by loading them into an Amazon EMR cluster.</p>",
            "<p>Update the health checks on the Auto Scaling group to use the correct port and protocol for the application.</p>"
          ],
          "explanation": "<p>The lifecycle-action-token is provided by Auto Scaling in the message sent as part of processing the lifecycle hook. You need to get the token from the original message.</p><p>The error message reported usually indicates one of the following:</p><ul><li><p>The maximum number of concurrent AWS CodeDeploy deployments associated with an AWS account was reached.</p></li><li><p>The Auto Scaling group tried to launch too many EC2 instances too quickly. The API calls to RecordLifecycleActionHeartbeat or CompleteLifecycleAction for each new instance were throttled.</p></li><li><p>An application in CodeDeploy was deleted before its associated deployment groups were updated or deleted.</p></li></ul><p>Therefore, the engineer can update the deployment group is limits have been reached and create a solution for extracting the application logs for later analysis. This solution can use Amazon EventBridge, Lambda and SSM Run Command with S3 as the destination.</p><p>Amazon Athena allows for running SQL queries against data in an Amazon S3 bucket. The engineer can then perform analysis to identify there are any application issues that must be fixed.</p><p><strong>CORRECT: \"</strong>Update the deployment group as the AWS CodeDeploy limits have been reached<strong>\" is a correct answer (as explained above.)</strong></p><p><strong>CORRECT: \"</strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create an Amazon EventBridge rule and invoke a Lambda function that uses SSM Run Command to extract the application logs and store them in S3<strong>\" is also a correct answer (as explained above.)</strong></p><p><strong>CORRECT: \"</strong>Analyze the logs directly in the Amazon S3 bucket using Amazon Athena<strong>\" is also a correct answer (as explained above.)</strong></p><p><strong>INCORRECT: \"</strong>Enable Access Logs at the Target Group level and configure an Amazon S3 bucket as the destination<strong>\" is incorrect.</strong></p><p><strong>The access logs will not provide the necessary information to troubleshoot and analyze the issues that are occurring. Access logs record information about the requests from clients.</strong></p><p><strong>INCORRECT: \"</strong>Analyze the logs by loading them into an Amazon EMR cluster<strong>\" is incorrect.</strong></p><p><strong>This won\u2019t be needed as this is not a map reduce use case and data can be analyzed by EMR in Amazon S3.</strong></p><p><strong>INCORRECT: \"</strong>Update the health checks on the Auto Scaling group to use the correct port and protocol for the application<strong>\" is incorrect.</strong></p><p><strong>There is no evidence that health checks are misconfigured from the errors that were generated. Auto scaling must be using the health checks as it is managing to auto scale and bring instances into service.</strong></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-reboot\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-reboot</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>"
        }
      },
      {
        "id": 82921380,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The DevOps team at a presentation software company is deploying their flagship application using Elastic Beanstalk. The application is deployed using a Deploy stage in a CodePipeline pipeline. The technical requirements mandate changing the configuration of the Application Load Balancer tied to Elastic Beanstalk by adding an HTTP to HTTPS redirection rule.</p>\n\n<p>As a DevOps Engineer, you don't have the permissions to directly edit the Elastic Beanstalk environment, how can you proceed?</p>\n",
          "answers": [
            "<p>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add an <code>option_settings</code> block for which you will specify the Rules for the key <code>aws:elbv2:listener:default</code>. Push your code and let the CodePipeline run</p>",
            "<p>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Configure CodePipeline to deploy to Elastic Beanstalk using the EB CLI and push the code</p>",
            "<p>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Run a deploy using the EB CLI from your computer onto the Elastic Beanstalk Environment</p>",
            "<p>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add a <code>container_commands</code> block for which you will specify a container command that will run in <code>leader_only</code> mode. The EC2 instance will issue an API call to the Load Balancer to add the redirection rule</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add an <code>option_settings</code> block for which you will specify the Rules for the key <code>aws:elbv2:listener:default</code>. Push your code and let the CodePipeline run</strong></p>\n\n<p>You can use Elastic Beanstalk configuration files (.ebextensions) with your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the option_settings key to modify the environment configuration. You can choose from general options for all environments and platform-specific options.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p>Note: Recommended values are applied when you create or update an environment on the Elastic Beanstalk API by a client. For example, the client could be the AWS Management Console, Elastic Beanstalk Command Line Interface (EB CLI), AWS Command Line Interface (AWS CLI), or SDKs. Recommended values are directly set at the API level and have the highest precedence. The configuration setting applied at the API level can't be changed using option_settings, as the API has the highest precedence.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q22-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html</a></p>\n\n<p>Configuration changes made to your Elastic Beanstalk environment won't persist if you use the following configuration methods:</p>\n\n<p>Configuring an Elastic Beanstalk resource directly from the console of a specific AWS service.</p>\n\n<p>Installing a package, creating a file, or running a command directly from your Amazon EC2 instance.</p>\n\n<p>For the given use-case, using a <code>.ebextensions</code> file and configuring the rules in the <code>option_settings</code> block is the right option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Configure CodePipeline to deploy to Elastic Beanstalk using the EB CLI and push the code</strong> - This option has been added as a distractor as you cannot configure CodePipeline to deploy using the EB CLI.</p>\n\n<p><strong>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Run a deploy using the EB CLI from your computer onto the Elastic Beanstalk Environment</strong> - Using the EB CLI on your computer would normally work, but here the question specifies that we don't have the necessary permissions to make direct changes against the Beanstalk environment. We, therefore, have to use CodePipeline.</p>\n\n<p><strong>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add a <code>container_commands</code> block for which you will specify a container command that will run in <code>leader_only</code> mode. The EC2 instance will issue an API call to the Load Balancer to add the redirection rule</strong> - Using a <code>container_command</code> may work, but it wouldn't be best practice as the EC2 would issue a command to the ALB and therefore the configuration of it would be different from the one specified by Beanstalk itself, and the EC2 instance may not have enough permissions through IAM role to issue that command in the first place. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html</a></p>\n"
        }
      },
      {
        "id": 82921432,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A health-care services company has strong regulatory requirements and it has come to light recently that some of the EBS volumes have not been encrypted. It is necessary for the company to monitor and audit compliance over time and alert the corresponding teams if unencrypted EBS volumes are detected.</p>\n\n<p>How should a DevOps Engineer implement an alert for the unencrypted EBS volumes with the least administrative overhead?</p>\n",
          "answers": [
            "<p>Create an AWS Config managed rule checking for EBS volume encryption. Connect the rule to an SNS topic to provide alerting</p>",
            "<p>Create an AWS Config managed rule checking for EBS volume encryption. Use a CloudWatch Event rule to provide alerting</p>",
            "<p>Create an AWS Config custom rule checking for the EC2 instances, and their EBS attachments. Connect the rule to an SNS topic to provide alerting</p>",
            "<p>Create an AWS Lambda Function that is triggered by a CloudWatch Event rule. The rule is monitoring for new EBS volumes being created. The Lambda function should send a notification to SNS in case of a compliance check</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Config managed rule checking for EBS volume encryption. Use a CloudWatch Event rule to provide alerting</strong></p>\n\n<p>AWS Config provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your EBS volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules.</p>\n\n<p>AWS Config uses Amazon SNS to deliver notifications to subscription endpoints. These notifications provide the delivery status for configuration snapshots and configuration histories, and they provide each configuration item that AWS Config creates when the configurations of recorded AWS resources change. AWS Config also sends notifications that show whether your resources are compliant with your rules. SNS topics when directly integrated with Config can only be used to stream all the notifications and configuration changes and NOT selectively for a given rule.</p>\n\n<p>AWS Config has a managed rule to check for EBS volume encryption. For the given use-case, you need to isolate alerts for this managed rule, so you have to use CloudWatch Events which can then have a specific SNS topic as a target for alerting.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html\">https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q47-i2.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Config managed rule checking for EBS volume encryption. Connect the rule to an SNS topic to provide alerting</strong></p>\n\n<p><strong>Create an AWS Config custom rule checking for the EC2 instances, and their EBS attachments. Connect the rule to an SNS topic to provide alerting</strong></p>\n\n<p>As mentioned in the explanation above, SNS topics in Config can only be used to stream all the notifications and configuration changes. To isolate alerts for a single rule, you have to use CloudWatch Events. Therefore both these options are incorrect.</p>\n\n<p><strong>Create an AWS Lambda Function that is triggered by a CloudWatch Event rule. The rule is monitoring for new EBS volumes being created. The Lambda function should send a notification to SNS in case of a compliance check</strong> - Using AWS Lambda may work, but it will not provide you the auditing capability that AWS Config provides (a timeline dashboard with compliance over time).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html\">https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/\">https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/</a></p>\n"
        }
      },
      {
        "id": 82921438,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>Your company has adopted a git repository technology to store and have version control on the application code. Your company would like to make sure the production branch of the code is deployed to the production environment, but also would like to enable other versions of the code to be deployed to the development and staging environments for performing various kinds of user acceptance testing.</p>\n\n<p>As a DevOps Engineer, which solution would you implement for the given requirement?</p>\n",
          "answers": [
            "<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</p>",
            "<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</p>",
            "<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</p>",
            "<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Projects/CICD%20Pipeline/setup-cicd-pipeline2.5cefde1406fa6787d9d3c38ae6ba3a53e8df3be8.png\">\nvia - <a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p>Here you only need one git repository and create a production branch for deploys to production. The other key requirement of the given use-case is that two versions of the code need to be deployed to different environments. As such, you will need two CodePipelines. If you had one with a manual approval step at the end, then the code deployed to production would be coming from the master branch instead of the production branch. Here, we specifically need code in the production branch to be deployed to production, so, therefore, we need a second CodePipeline and to merge code from master to production through Pull Requests.</p>\n\n<p>Code Pipeline Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</strong> - As mentioned in the explanation above, a key requirement is that two versions of the code need to be deployed to different environments. If you use a manual approval step after the deployment to staging then the same version of the code from the master branch would also be deployed to the production environment. Instead, you need to maintain a production branch of the code that can be deployed to the production environment.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</strong> - It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to merging through a pull request has been added as a distractor.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</strong> - It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to the manual approval step has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/faqs/\">https://aws.amazon.com/codecommit/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n"
        }
      },
      {
        "id": 82921454,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>As a DevOps Engineer at an IT company, you are looking to create a daily EBS backup workflow. That workflow must take an EBS volume, and create a snapshot from it. When the snapshot is created, it must be copied to another region. In case the other region is unavailable because of a disaster, then that backup should be copied to a third region. An email address must be notified of the final result. There's a requirement to keep an audit trail of all executions as well.</p>\n\n<p>How can you implement this efficiently and in a fail-safe way?</p>\n",
          "answers": [
            "<p>Create an AWS Step Function. Implement each step as a Lambda function and add failure logic between the steps to deal with conditional cases</p>",
            "<p>Create an EC2 instance in the region where the EBS volume is. Create a CRON script that will invoke a Python script that performs all the steps and logic outlined above. For each step completion, write metadata to a DynamoDB table</p>",
            "<p>Create a CloudWatch Event rule that gets triggered every day. It triggers a Lambda function written in Python that performs all the steps and logic outlined above. Analyze the history of execution using AWS Config</p>",
            "<p>Create an SSM Automation that will perform each action. Add failure logic between steps to deal with conditional cases</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Step Function. Implement each step as a Lambda function and add failure logic between the steps to deal with conditional cases</strong></p>\n\n<p>Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to combine Step Functions, Lambda and CloudWatch Events into a single coherent solution. You can use the Step Functions to coordinate the business logic to automate the snapshot management flow with error handling, retry logic, and workflow logic all baked into the Step Functions definition. CloudWatch Events integrates with Step Functions and Lambda to let you execute your custom code when relevant events occur.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q62-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>For a deep-dive on this solution, highly recommend the following reference material:\n<a href=\"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an EC2 instance in the region where the EBS volume is. Create a CRON script that will invoke a Python script that performs all the steps and logic outlined above. For each step completion, write metadata to a DynamoDB table</strong> - Creating an EC2 instance may work, but if it gets terminated we have to re-create a new one. Failure scenarios may be tough to analyze and having the audit trail in DynamoDB probably won't be easy to use.</p>\n\n<p><strong>Create a CloudWatch Event rule that gets triggered every day. It triggers a Lambda function written in Python that performs all the steps and logic outlined above. Analyze the history of execution using AWS Config</strong> - Creating a CW event rule + Lambda function may work, but the Lambda function may have a timeout issue if the backup is taking longer than 15 minutes, and AWS Config cannot store the history of the execution. AWS Config only provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.</p>\n\n<p><strong>Create an SSM Automation that will perform each action. Add failure logic between steps to deal with conditional cases</strong> - An SSM automation cannot contain complex logic to handle failures, although it would provide an execution history.</p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n"
        }
      },
      {
        "id": 99528197,
        "correct_response": [
          "a",
          "c",
          "e"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has deployed AWS Single Sign-On (AWS SSO) and needs to ensure that user accounts are not created within AWS Identity and Access Management (AWS IAM). A DevOps engineer must create an automated solution for immediately disabling credentials of any new IAM user that is created. The security team must be notified when user creation events take place.</p><p>Which combination of steps should the DevOps engineer take to meet these requirements? (Select THREE.)</p>",
          "answers": [
            "<p>Create an Amazon EventBridge rule that is triggered by IAM CreateUser API calls in AWS CloudTrail.</p>",
            "<p>Create an Amazon EventBridge rule that is triggered by IAM GetLoginProfile API calls in AWS CloudTrail.</p>",
            "<p>Create an AWS Lambda function that disables the access keys and deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule.</p>",
            "<p>Create an AWS Lambda function that deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule.</p>",
            "<p>Create an Amazon SNS topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic.</p>",
            "<p>Create an AWS Config rule that sends a notification to the security team using Amazon SNS when user accounts are modified.</p>"
          ],
          "explanation": "<p>The company is using AWS SSO and we can presume have an identity source that is outside of AWS IAM. They therefore want to control creation of IAM users. The solution uses an EventBridge rule that monitors for CreateUser API calls in AWS CloudTrail. This will pick up all user creation events.</p><p>Then, an AWS Lambda function will disable both the access keys (if created) and login profile (if created) that are associated with the newly created user account. Then, an SNS notification will be sent to the security team.</p><p>This solution meets all the stated requirements.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule that is triggered by IAM CreateUser API calls in AWS CloudTrail\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS Lambda function that disables the access keys and deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an Amazon SNS topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule that is triggered by IAM GetLoginProfile API calls in AWS CloudTrail\" is incorrect.</p><p>This would only be triggered when user accounts with console (password) access are created. A user with programmatic access does not have a login profile unless you create a password for the user to access the AWS Management Console. Therefore, this would miss users that are created with programmatic-only access.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule\" is incorrect.</p><p>As above, login profiles are associated with console-based password access only, they do not apply to users with programmatic-only access.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config rule that sends a notification to the security team using Amazon SNS when user accounts are modified\" is incorrect.</p><p>The DevOps engineer should directly configure Amazon SNS to be triggered by EventBridge. There is no need to send notifications related to user modifications, only creation events.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetLoginProfile.html\">https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetLoginProfile.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>"
        }
      },
      {
        "id": 115961523,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A critical application runs on Amazon EC2 instances in an Auto Scaling group. A script runs on the instances every 10 seconds to check application availability. A DevOps engineer must use this information returned by the script to monitor the application and trigger an alarm if there is an issue. The data should be collected every 1-minute and the solution must be cost-effective.</p><p>Which action should the engineer take?</p>",
          "answers": [
            "<p>Use a custom Amazon CloudWatch metric and configure a statistic set that aggregates data points and publishes the data every 1-minute.</p>",
            "<p>Use a default CloudWatch metric with a high resolution, aggregate multiple data points, and publish the data every 1-minute.</p>",
            "<p>Use a custom Amazon CloudWatch metric with a high resolution and publish the data every 10 seconds.</p>",
            "<p>Use a default CloudWatch metric with a standard resolution, use a dimension to publish data sets every 1-minute.</p>"
          ],
          "explanation": "<p>You can create custom metrics to send to Amazon CloudWatch. With custom metrics you can choose standard or high resolution and you can aggregate multiple data points and publish data as a statistic set to reduce cost and increase efficiency.</p><p>Each metric is one of the following:</p><ul><li><p>Standard resolution, with data having a one-minute granularity.</p></li><li><p>High resolution, with data at a granularity of one second.</p></li></ul><p>Metrics produced by AWS services are standard resolution by default. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p><p>You can aggregate your data before you publish to CloudWatch. When you have multiple data points per minute, aggregating data minimizes the number of calls to <strong>put-metric-data</strong>.</p><p>Therefore, the engineer can use statistic sets to aggregate and publish the data every one minute. This is the most cost-effective solution that meets the requirements.</p><p><strong>CORRECT: </strong>\"Use a custom Amazon CloudWatch metric and configure a statistic set that aggregates data points and publishes the data every 1-minute\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use a default CloudWatch metric with a high resolution, aggregate multiple data points, and publish the data every 1-minute\" is incorrect.</p><p>You cannot use high resolution with a default CloudWatch metric and a default CloudWatch metric would not be available for the application availability data.</p><p><strong>INCORRECT:</strong> \"Use a custom Amazon CloudWatch metric with a high resolution and publish the data every 10 seconds\" is incorrect.</p><p>This would be less efficient and more costly as the put-metric-data API action would be run every 10 seconds. Fewer API calls means lower cost so aggregating into a statistic set is better and publishing every 1-minute.</p><p><strong>INCORRECT:</strong> \"Use a default CloudWatch metric with a standard resolution, use a dimension to publish data sets every 1-minute\" is incorrect.</p><p>There would not be a default metric available that uses the data returned from the application availability script. A dimension is used for organizing and clarifying what the metric data is and what it stores.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>"
        }
      },
      {
        "id": 138248147,
        "correct_response": [
          "a"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multinational investment bank is using AWS Organizations to handle its multiple AWS accounts across various AWS regions around the world. To comply with the strict financial IT regulations, the bank must ensure that all of its Amazon EBS volumes in its AWS accounts are encrypted. A DevOps engineer has been requested to set up an automated solution that will provide a detailed report of all unencrypted EBS volumes of the company as well as to notify if there is a newly launched Amazon EC2 instance which uses an unencrypted volume.</p><p>Which of the following should the DevOps engineer implement to meet this requirement with the LEAST amount of operational overhead?</p>",
          "answers": [
            "<p>Set up an AWS Config rule with a corresponding AWS Lambda function on all the target accounts of the company. Collect data from multiple accounts and AWS Regions using Config\u2019s aggregators. Export the aggregated report to an Amazon S3 bucket then deliver the notifications using Amazon SNS.</p>",
            "<p>Configure AWS CloudTrail to deliver all events to an S3 bucket in a centralized AWS account. Run a Lambda function to parse CloudTrail logs whenever logs are delivered to the S3 bucket using the S3 event notification. Use the same Lambda function to publish the results to SNS.</p>",
            "<p>Prepare an AWS CloudFormation template which contains a Config managed rule for EBS encryption of your EBS volumes. Deploy the template across all accounts and regions of the company using the CloudFormation stack set. Store consolidated results of the Config rules evaluation in an S3 bucket. When non-compliant EBS resources are detected, send a notification to the Operations team using SNS.</p>",
            "<p>Use the AWS Systems Manager Configuration Compliance to monitor all EBS volumes across all the accounts and AWS Regions of the company. Export and store the detailed compliance report to an S3 bucket and then deliver the notifications using SNS.</p>"
          ],
          "explanation": "<p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.</p><p><img src=\"https://media.tutorialsdojo.com/public/Aggregate_Data_Landing_Page_Diagram.png\">An AWS <em>resource</em> is an entity you can work with in AWS, such as an Amazon Elastic Compute Cloud (EC2) instance, an Amazon Elastic Block Store (EBS) volume, a security group, or an Amazon Virtual Private Cloud (VPC).</p><p>With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p><p>- Multiple accounts and multiple regions.</p><p>- Single account and multiple regions.</p><p>- An organization in AWS Organizations and all the accounts in that organization.</p><p>You can use an aggregator to view the resource configuration and compliance data recorded in AWS Config.</p><p>Hence, the correct answer is: <strong>Set up an AWS Config rule with a corresponding AWS Lambda function on all the target accounts of the company. Collect data from multiple accounts and AWS Regions using Config\u2019s aggregators. Export the aggregated report to an Amazon S3 bucket then deliver the notifications using Amazon SNS.</strong></p><p>The option that says: <strong>Configure AWS CloudTrail to deliver all events to an S3 bucket in a centralized AWS account. Run a Lambda function to parse CloudTrail logs whenever logs are delivered to the S3 bucket using the S3 event notification. Use the same Lambda function to publish the results to SNS </strong>is incorrect. Although this solution may work, it certainly entails a lot of operational overhead to execute and implement. Parsing thousands of API actions from all of your accounts in CloudTrail just to ensure that the EBS encryption was enabled on all volumes could take a significant amount of time compared with just using AWS Config.</p><p>The option that says: <strong>Prepare an AWS CloudFormation template which contains a Config managed rule for EBS encryption of your EBS volumes. Deploy the template across all accounts and regions of the company using the CloudFormation stack set. Store consolidated results of the Config rules evaluation in an S3 bucket. When non-compliant EBS resources are detected, send a notification to the Operations team using SNS </strong>is incorrect. Although it is right to use AWS Config here, this solution still entails a lot of management overhead to maintain all of the CloudFormation templates. A better solution is to use AWS Config aggregators instead.</p><p>The option that says: <strong>Use the AWS Systems Manager Configuration Compliance to monitor all EBS volumes across all the accounts and AWS Regions of the company. Export and store the detailed compliance report to an S3 bucket and then deliver the notifications using SNS</strong> is incorrect. Although you can collect and aggregate data from multiple AWS accounts and Regions using the AWS Systems Manager Configuration Compliance service, this solution has a lot of prerequisites and configuration needed. You have to install SSM agent to all of your EC2 instances, create Resource Data Syncs, set up a custom compliance type to check the EBS encryption and many others. Moreover, the AWS Systems Manager Configuration Compliance service is more suitable only for verifying the patch compliance of all your resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\">https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p>"
        }
      },
      {
        "id": 138248153,
        "correct_response": [
          "c"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A government agency has a VMware-based automated server build system on its on-premises network that uses virtualization software that allows the creation of server images of the application. The DevOps Engineer was tasked to set up a system that will allow to test its server images using its on-premises server pipeline to resemble the build and behavior on Amazon EC2. In this way, the agency can verify the functionality of the application, detect incompatibility issues, and determine any prerequisites on the new Amazon Linux 2 operating system that will be used in AWS.</p><p>Which of the following solutions should the DevOps Engineer implement to accomplish this task?</p>",
          "answers": [
            "<p>Download the latest AmazonLinux2.iso of the Amazon Linux 2 operating system and import it to your on-premises network. Directly launch a new on-premises server based on the imported ISO, without any virtual platform. Deploy the application, and commence testing.</p>",
            "<p>Configure a hybrid cloud environment using AWS Outposts, install the Linux 2 operating system on the AWS Outposts servers, and connect them to your on-premises network. Deploy the application on the Linux 2 servers in the Outposts environment for testing.</p>",
            "<p>Launch an EC2 instance with the latest Linux OS in AWS. Use the AWS VM Import/Export service to import the EC2 image, export it to a VMware ISO in an Amazon S3 bucket, and then import the ISO to an on-premises server. Once done, commence the testing activity to verify the application's functionalities.</p>",
            "Launch a new on-premises server with any distribution of Linux operating system such as CentOS, Ubuntu or Fedora since these are technically the same. Deploy the application to the server for testing."
          ],
          "explanation": "<p>The <strong>VM Import/Export</strong> enables you to easily import virtual machine images from your existing environment to Amazon EC2 instances and export them back to your on-premises environment. This offering allows you to leverage your existing investments in the virtual machines that you have built to meet your IT security, configuration management, and compliance requirements by bringing those virtual machines into Amazon EC2 as ready-to-use instances. You can also export imported instances back to your on-premises virtualization infrastructure, allowing you to deploy workloads across your IT infrastructure.</p><p>To import your images, use the AWS CLI or other developer tools to import a virtual machine (VM) image from your VMware environment. If you use the VMware vSphere virtualization platform, you can also use the AWS Management Portal for vCenter to import your VM. As part of the import process, VM Import will convert your VM into an Amazon EC2 AMI, which you can use to run Amazon EC2 instances. Once your VM has been imported, you can take advantage of Amazon\u2019s elasticity, scalability, and monitoring via offerings like Auto Scaling, Elastic Load Balancing, and CloudWatch to support your imported images.</p><p><img src=\"https://media.tutorialsdojo.com/public/vmimport-export-architecture-ami-copy.png\"></p><p>You can export previously imported EC2 instances using the Amazon EC2 API tools. You simply specify the target instance, virtual machine file format and a destination S3 bucket, and VM Import/Export will automatically export the instance to the S3 bucket. You can then download and launch the exported VM within your on-premises virtualization infrastructure.</p><p>You can import Windows and Linux VMs that use VMware ESX or Workstation, Microsoft Hyper-V, and Citrix Xen virtualization formats. And you can export previously imported EC2 instances to VMware ESX, Microsoft Hyper-V or Citrix Xen formats.</p><p>Hence, the correct answer is: <strong>Launch an EC2 instance with the latest Linux OS in AWS. Use the AWS VM Import/Export service to import the EC2 image, export it to a VMware ISO in an Amazon S3 bucket, and then import the ISO to an on-premises server. Once done, commence the testing activity to verify the application's functionalities.</strong></p><p>The option that says: <strong>Download the latest </strong><code><strong>AmazonLinux2.iso</strong></code><strong> of the Amazon Linux 2 operating system and import it to your on-premises network. Directly launch a new on-premises server based on the imported ISO, without any virtual platform. Deploy the application, and commence testing<em> </em></strong>is incorrect because there is no way to directly download the <code>AmazonLinux2.iso</code> for Amazon Linux 2. You just have to use VM Import/Export service instead or, alternatively, run the Amazon Linux 2 as a virtual machine in your on-premises data center. Again, you won't be able to directly download the ISO image, but you can get the Amazon Linux 2 image for the specific virtualization platform of your choice. If you are using VMware, you can download the ESX image *.ova, and for VirtualBox, you'll get the *.vdi image file. What you should do first is to prepare the seed.iso boot image and then connect it to the VM of your choice on the first boot.</p><p>The option that says: <strong>Configure a hybrid cloud environment using AWS Outposts, install the Linux 2 operating system on the AWS Outposts servers, and connect them to your on-premises network. Deploy the application on the Linux 2 servers in the Outposts environment for testing</strong> is incorrect. While AWS Outposts would indeed provide access to Amazon Linux 2, it introduces an unnecessary layer of complexity and cost to what should be a straightforward testing process. The agency's goal is to simulate EC2 environments using their current on-premises VMware-based system, not to extend their AWS infrastructure to on-premises environments.</p><p>The option that says: <strong>Launch a new on-premises server with any distribution of Linux operating system such as CentOS, Ubuntu or Fedora since these are technically the same. Deploy the application to the server for testing </strong>is incorrect because these Linux distributions are actually different from one another. There could be some incompatibility issues between the different Linux operating systems, which is why you need to test your application on a specific Amazon Linux 2 type only.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html\">https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html</a></p><p><a href=\"https://aws.amazon.com/ec2/vm-import/\">https://aws.amazon.com/ec2/vm-import/</a></p><p><a href=\"https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html\">https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-linux-2-virtual-machine.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-linux-2-virtual-machine.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p>"
        }
      },
      {
        "id": 138248167,
        "correct_response": [
          "b"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A leading telecommunications company is using CloudFormation templates to deploy enterprise applications to their production, staging, and development environments in AWS. Their current process involves manual&nbsp;changes to their CloudFormation templates in order to specify the configuration variables and static attributes for each environment. The DevOps Engineer was tasked to set up automated deployments using AWS CodePipeline and ensure that the CloudFormation template is reusable across multiple pipelines. </p><p>How should the DevOps Engineer satisfy this requirement?</p>",
          "answers": [
            "<p>Set up a Lambda-backed custom resource in the CloudFormation templates. Configure the custom resource to monitor the status of the pipeline in AWS CodePipeline in order to detect which environment was launched. Use the cfn-init helper script to modify the launch template of each application stack based on its environment.</p>",
            "<p>Launch a new pipeline using&nbsp;AWS CodePipeline that has multiple stages for each environment and configure it to use input parameters. Switch the associated <code>UserData</code> of the EC2 instances to match the environment where the application stack is being launched using CloudFormation mappings. Specify&nbsp;parameter overrides&nbsp;for AWS CloudFormation actions.&nbsp;</p>",
            "<p>Launch a new pipeline using&nbsp;AWS CodePipeline for each environment with multiple stages for each application. Trigger the CloudFormation deployments using a Lambda function to dynamically modify the <code>UserData</code> of the EC2 instances that were launched in each environment.</p>",
            "<p>Manually configure the CloudFormation templates to use input parameters. Add a configuration that whenever the CloudFormation stack is updated, it will dynamically modify the <code>LaunchConfiguration</code> and <code>UserData</code> sections of the EC2 instances.</p>"
          ],
          "explanation": "<p>Continuous delivery is a release practice in which code changes are automatically built, tested, and prepared for release to production. With <strong>AWS CloudFormation</strong> and <strong>CodePipeline</strong>, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. This release process lets you rapidly and reliably make changes to your AWS infrastructure.</p><p>For example, you can create a workflow that automatically builds a test stack when you submit an updated template to a code repository. After AWS CloudFormation builds the test stack, you can test it and then decide whether to push the changes to a production stack.</p><p>You can use CodePipeline to build a continuous delivery workflow by building a pipeline for AWS CloudFormation stacks. CodePipeline has built-in integration with AWS CloudFormation, so you can specify AWS CloudFormation-specific actions, such as creating, updating, or deleting a stack within a pipeline.</p><p><img src=\"https://media.tutorialsdojo.com/public/product-page-diagram_CodePipeLine.7b8dd19eb6478b7f6f747d936c2f0b0b66757bbf.png\"></p><p>In a CodePipeline stage, you can specify parameter overrides for AWS CloudFormation actions. Parameter overrides let you specify template parameter values that override values in a template configuration file. AWS CloudFormation provides functions to help you specify dynamic values (values that are unknown until the pipeline runs).</p><p>You can set the <code>Fn::GetArtifactAtt</code> function which retrieves the value of an attribute from an input artifact, such as the S3 bucket name where the artifact is stored. You can use this function to specify attributes of an artifact, such as its filename or S3 bucket name, that can be used in the pipeline.</p><p>Hence, the correct answer is: <strong>Launch a new pipeline using AWS CodePipeline that has multiple stages for each environment and configure it to use input parameters. Switch the associated </strong><code><strong>UserData</strong></code><strong> of the EC2 instances to match the environment where the application stack is being launched using CloudFormation mappings. Specify parameter overrides for AWS CloudFormation actions.</strong></p><p>The option that says: <strong>Set up a Lambda-backed custom resource in the CloudFormation templates. Configure the custom resource to monitor the status of the pipeline in AWS CodePipeline in order to detect which environment was launched. Use the cfn-init helper script to modify the launch template of each application stack based on its environment</strong> is incorrect because monitoring the pipeline using a custom resource in CloudFormation entails a lot of administrative overhead. A better solution would be to use input parameters or parameter overrides for AWS CloudFormation actions.</p><p>The option that says: <strong>Launch a new pipeline using AWS CodePipeline for each environment with multiple stages for each application. Trigger the CloudFormation deployments using a Lambda function to dynamically modify the </strong><code><strong>UserData</strong></code><strong> of the EC2 instances that were launched in each environment</strong> is incorrect because using a Lambda function to modify the <code><strong>UserData</strong></code> of the already running EC2 instances is not a suitable solution. The parameters should have been dynamically populated and set before the resources were launched by using parameter overrides.</p><p>The option that says: <strong>Manually configure the CloudFormation templates to use input parameters. Add a configuration that whenever the CloudFormation stack is updated, it will dynamically modify the </strong><code><strong>LaunchConfiguration</strong></code><strong> and </strong><code><strong>UserData</strong></code><strong> sections of the EC2 instances</strong> is incorrect. Although using input parameters is helpful in this scenario, you should still integrate CloudFormation and CodePipeline in order to properly map the configuration files for each environment.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-parameter-override-functions.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-parameter-override-functions.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p><p><a href=\"https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/?src=udemy\">https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</a></p>"
        }
      },
      {
        "id": 138248221,
        "correct_response": [
          "b",
          "e"
        ],
        "source": "Dojo Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is re-architecting its monolithic system to a serverless application in AWS to save on cost. The deployment of the succeeding new version of the application must be initially rolled out to a small number of users first for testing before the full release. If the post-hook tests fail, there should be an easy way to roll back the deployment. The DevOps Engineer was assigned to design an efficient deployment setup that mitigates any unnecessary outage that impacts their production environment.</p><p>As a DevOps Engineer, how should you satisfy this requirement? (Select TWO.)</p>",
          "answers": [
            "<p>Launch an Application Load Balancer with an Amazon API Gateway private integration. Attach a single target group to the load balancer and select the \"Canary\" routing option which will automatically route incoming traffic to the new version.</p>",
            "<p>Set up one AWS Lambda Function Alias that points to both the current and new versions. Route 20% of incoming traffic to the new version and once it is considered stable, update the alias to route all traffic to the new version.</p>",
            "<p>Create a new record in Route 53 with a Failover routing policy. Configure the primary record to route 20% of incoming traffic to the new version and set the secondary record to route the rest of the traffic to the current version. Once the new version stabilizes, update the primary record to route all traffic to the new version.</p>",
            "<p>Launch a Network Load Balancer with an Amazon API Gateway private integration. Attach two target groups to the load balancer. Configure the first target group with the current version and the second target group with the new version. Configure the load balancer to route 20% of the incoming traffic to the new version and once it becomes stable, detach the first target group from the load balancer.</p>",
            "<p>Set up a canary deployment in Amazon API Gateway that routes 20% of the incoming traffic to the canary release. Promote the canary release to production once the initial tests have passed.</p>"
          ],
          "explanation": "<p>You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Each alias has a unique ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. Event sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. In a resource policy, you can grant permissions for event sources to use your Lambda function. If you specify an alias ARN in the policy, you don't need to update the policy when the function version changes.</p><p>Use routing configuration on an alias to send a portion of traffic to a second function version. For example, you can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version. You can point an alias to a maximum of two Lambda function versions.</p><p><img src=\"https://media.tutorialsdojo.com/public/API_gateway.png\"></p><p>In API Gateway, you create a canary release deployment when deploying the API with <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/link-relation/deployment-create/#canarySettings\">canary settings</a> as an additional input to the <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/link-relation/deployment-create/\">deployment creation</a> operation.</p><p>You can also create a canary release deployment from an existing non-canary deployment by making a <a href=\"https://docs.aws.amazon.com/apigateway/api-reference/link-relation/stage-update/\"><code>stage:update</code></a> request to add the canary settings on the stage.</p><p>When creating a non-canary release deployment, you can specify a non-existing stage name. API Gateway creates one if the specified stage does not exist. However, you cannot specify any non-existing stage name when creating a canary release deployment. You will get an error and API Gateway will not create any canary release deployment.</p><p>Hence, the correct answers are:</p><p><strong>- Set up one AWS Lambda Function Alias that points to both the current and new versions. Route 20% of incoming traffic to the new version and once it is considered stable, update the alias to route all traffic to the new version.</strong></p><p>- <strong>Set up a canary deployment in Amazon API Gateway that routes 20% of the incoming traffic to the canary release. Promote the canary release to production once the initial tests have passed.</strong></p><p>The option that says:<strong><em> </em>Launch an Application Load Balancer with an Amazon API Gateway private integration. Attach a single target group to the load balancer and select the \"Canary\" routing option which will automatically route incoming traffic to the new version</strong> is incorrect because there is no Canary routing option in an Application Load Balancer.</p><p>The option that says: <strong>Launch a Network Load Balancer with an Amazon API Gateway private integration. Attach two target groups to the load balancer. Configure the first target group with the current version and the second target group with the new version. Configure the load balancer to route 20% of the incoming traffic to the new version and once it becomes stable, detach the first target group from the load balancer </strong>is incorrect because the Network Load Balancer does not support weighted target groups, unlike the Application Load Balancer.</p><p>The option that says: <strong>Create a new record in Route 53 with a Failover routing policy. Configure the primary record to route 20% of incoming traffic to the new version and set the secondary record to route the rest of the traffic to the current version. Once the new version stabilizes, update the primary record to route all traffic to the new version </strong>is incorrect because the failover routing policy simply lets you route traffic to a resource when the resource is healthy, or to a different resource when the first resource is unhealthy. This type of routing is not an appropriate setup. A better solution is to use Canary deployment release in API Gateway to deploy the serverless application.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p><p><br></p><p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified DevOps Engineer Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-devops-engineer-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-devops-engineer-professional/</a></p>"
        }
      },
      {
        "id": 143860757,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps Engineer manages an application running across accounts which is deployed via AWS CloudFormation.</p><p>The application stack has an Amazon ECS Fargate cluster which spins up multiple tasks for the application layer and utilizes an Amazon ElastiCache Redis cache to store frequently accessed data.</p><p>The accounts are labelled \u201csandbox\u201d and \u201cstaging\u201d. While the stack spins up fine, the application is unable to connect to Redis with the below error logged in Amazon CloudWatch Logs.</p><p>\u201cStopped reason ResourceInitializationError: unable to pull secrets or registry auth: pull command failed :: signal: killed\u201c.</p><p>What is the possible fix for the above error? (Select TWO.)</p>",
          "answers": [
            "<p>Ensure inbound connectivity is allowed in the application security group on port 6379 from the Redis cluster subnet.</p>",
            "<p>Ensure an ENI is configured for the ECS tasks in the AWS Fargate cluster and there is an api.ecr endpoint.</p>",
            "<p>Ensure the ECS tasks are launched in a public subnet and public IP addresses are assigned to them.</p>",
            "<p>Ensure that the IAM role provides the required permissions.</p>",
            "<p>Ensure that the application security group has a rule that accepts connections from 0.0.0.0/0.</p>"
          ],
          "explanation": "<p>AWS Fargate clusters with the 1.4 version upgrade provides ENI at the task level. Below is AWS documentation for the same:</p><p>Fargate tasks run on a fleet of virtual machines that AWS manages on behalf of the customer. These VMs are connected to AWS owned VPCs via so called \u201cFargate ENIs\u201d. When a user launches a task on Fargate, the task is assigned an ENI and this ENI is connected to the customer owned VPC</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-05-26-622074b8a2ac8ab2c36144cac8b52c3e.jpg\"><p><strong>CORRECT: </strong>\"Ensure inbound connectivity is allowed in the application security group on port 6379 from the Redis cluster subnet\" is a correct answer (as explained above).</p><p><strong>CORRECT: </strong>\"Ensure an ENI is configured for the ECS tasks in the AWS Fargate cluster and there is an api.ecr endpoint\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Ensure the ECS tasks are launched in a public subnet and public IP addresses are assigned to them\" is incorrect since your tasks can sit in a private subnet as well.</p><p><strong>INCORRECT:</strong> \"Ensure that the IAM role provides the required permissions\" is incorrect since question is more around connectivity and stack worked fine in one of the environments.</p><p><strong>INCORRECT:</strong> \"Ensure that the application security group has a rule that accepts connections from 0.0.0.0/0\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/containers/aws-fargate-launches-platform-version-1-4/\">AWS Fargate launches platform version 1.4.0 | Containers (amazon.com)</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>"
        }
      },
      {
        "id": 143860771,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The application is used by users around the world who access the application using a custom DNS domain name. The application must support encryption in transit, be protected from DDoS attacks and web exploits, should be optimized for performance.</p><p>Which actions should a DevOps engineer take to meet these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Create an Amazon CloudFront distribution with the Auto Scaling group as an origin. Configure the custom domain name and attach an SSL/TLS certificate.</p>",
            "<p>Create an Amazon CloudFront distribution with the ALB as an origin. Configure the custom domain name and attach an SSL/TLS certificate.</p>",
            "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the CloudFront distribution that WAF should inspect.</p>",
            "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the ALB that WAF should inspect.</p>",
            "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the Auto Scaling group that WAF should inspect.</p>"
          ],
          "explanation": "<p>To improve performance for global users the solution should use Amazon CloudFront. The distribution should specify the ALB as the origin and use a custom domain name and SSL/TLS certificate. This will enable caching of content in Edge Locations around the world and CloudFront offers DDoS protection.</p><p>To protect against web exploits AWS WAF should be used. A Web ACL must be created with an action and rule specified to deal with threats. The web ACL can be specified in the CloudFront distribution.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution with the ALB as an origin. Configure the custom domain name and attach an SSL/TLS certificate\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL, configure a default action and rule, and specify the CloudFront distribution that WAF should inspect\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution with the Auto Scaling group as an origin. Configure the custom domain name and attach an SSL/TLS certificate\" is incorrect.</p><p>You can configure an ELB as an origin for the distribution, but you cannot specify an ASG.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL, configure a default action and rule, and specify the ALB that WAF should inspect\" is incorrect.</p><p>The web ACL should be attached to the CloudFront distribution in this case as it sits in front of the ALB. It is always better to protect as close to the edge as possible.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL, configure a default action and rule, and specify the Auto Scaling group that WAF should inspect\" is incorrect.</p><p>The web ACL must be attached to the CloudFront distribution, not the ASG.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>"
        }
      }
    ],
    "answers": {}
  }
]