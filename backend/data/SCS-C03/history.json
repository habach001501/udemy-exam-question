[
  {
    "id": "1770878393486",
    "date": "2026-02-12T06:39:53.486Z",
    "course": "SCS-C03",
    "mode": "exam",
    "score": 0,
    "incorrect": 0,
    "unanswered": 75,
    "total": 75,
    "percent": 0,
    "duration": 2241,
    "questions": [
      {
        "id": 73686828,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application runs on a fleet of Amazon EC2 instances in a private subnet. The EC2 instances read and write data to an Amazon S3 bucket. The data is highly confidential and a private and secure connection is required between the EC2 instances and the S3 bucket.</p><p>Which solution meets these requirements?</p>",
          "answers": [
            "<p>Configure encryption for the S3 bucket using an AWS KMS key.</p>",
            "<p>Configure a custom SSL/TLS certificate on the S3 bucket.</p>",
            "<p>Set up S3 bucket policies to allow access from a VPC endpoint.</p>",
            "<p>Set up an IAM policy to grant read-write access to the S3 bucket.</p>"
          ],
          "explanation": "<p>A VPC gateway endpoint can be used to access an Amazon S3 bucket using private IP addresses. To further secure the solution an S3 bucket policy can be created that restricts access to the VPC endpoint so connections cannot be made to the bucket from other sources.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_04-06-32-015823a2883db0e154d0f381098561cb.jpg\"><p><strong>CORRECT: </strong>\"Set up S3 bucket policies to allow access from a VPC endpoint\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure encryption for the S3 bucket using an AWS KMS key\" is incorrect.</p><p>This will encrypt data at rest but does not secure the connection to the bucket or ensure private connections must be made.</p><p><strong>INCORRECT:</strong> \"Configure a custom SSL/TLS certificate on the S3 bucket\" is incorrect.</p><p>You cannot add a custom SSL/TLS certificate to Amazon S3.</p><p><strong>INCORRECT:</strong> \"Set up an IAM policy to grant read-write access to the S3 bucket\" is incorrect.</p><p>This does not enable private access from EC2. A gateway VPC endpoint is required.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html</a></p>"
        }
      },
      {
        "id": 99528323,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A multinational enterprise uses AWS Organizations to manage several AWS accounts spread across different regions. The company's IT department centrally manages the creation of IAM roles. Recently, the company decided to delegate the IAM role creation to various regional teams to speed up the process and reduce the IT department's workload. However, it is critical to prevent privilege escalation and ensure the scope of IAM roles remains within the defined limits.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
          "answers": [
            "<p>Assign an IAM group for each regional team, attach relevant policies to these groups, and then create IAM users for each regional team member. Add the respective IAM users to their relevant IAM group using Role-Based Access Control (RBAC).</p>",
            "<p>Empower regional team leaders to create IAM roles for their teams and conduct bi-annual audits of the IAM roles they have created. Provide necessary training to these leaders about the rules and best practices of IAM role creation.</p>",
            "<p>Organize each AWS account into an Organizational Unit (OU) based on the regions. Attach Service Control Policies (SCP) to each OU, granting access only to the AWS services required by the regional teams.</p>",
            "<p>Establish an SCP and a permissions boundary for IAM roles. Apply the SCP to the root OU so that only roles with the attached permissions boundary can create any new IAM roles.</p>"
          ],
          "explanation": "<p>In AWS Organizations, Service Control Policies (SCPs) are one type of policy that you can use to manage permissions. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help to ensure your accounts stay within your organization\u2019s access control guidelines.</p><p>When you attach an SCP to the root, it affects all the OUs, accounts, and users under the root. Any permissions that are not explicitly granted by the SCP are implicitly denied.</p><p>A permissions boundary is an advanced feature in which you set the maximum permissions that an entity (user or role) can have. By setting a permissions boundary for IAM roles, the security team can delegate the role creation to application teams but still control the maximum permissions any IAM role can have.</p><p>Using a combination of SCPs and permissions boundaries provides a mechanism to delegate IAM role creation to the application teams while limiting the permissions those roles can have, thus preventing privilege escalation and controlling the scope of IAM roles.</p><p>By attaching the SCP to the root OU, you ensure that these restrictions apply to all accounts in your organization, not just the ones you specifically identify. And by using a permissions boundary, you can provide a 'safety net' that limits the permissions of any roles that are created, regardless of who creates them or what permissions they attempt to assign to the role. This mechanism significantly reduces the operational overhead and ensures security compliance across all teams and roles.</p><p><strong>CORRECT: </strong>\"Establish an SCP and a permissions boundary for IAM roles. Apply the SCP to the root OU so that only roles with the attached permissions boundary can create any new IAM roles\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Assign an IAM group for each regional team, attach relevant policies to these groups, and then create IAM users for each regional team member. Add the respective IAM users to their relevant IAM group using Role-Based Access Control (RBAC)\" is incorrect.</p><p>Although IAM groups and RBAC can help in managing permissions, this solution does not inherently prevent privilege escalation or limit the scope of IAM roles.</p><p><strong>INCORRECT:</strong> \"Empower regional team leaders to create IAM roles for their teams and conduct bi-annual audits of the IAM roles they have created. Provide necessary training to these leaders about the rules and best practices of IAM role creation\" is incorrect.</p><p>This approach places a lot of responsibility and potential risk with the team leaders, and bi-annual audits could allow for extended periods of misconfigured access. It also adds operational overhead due to training needs.</p><p><strong>INCORRECT:</strong> \"Organize each AWS account into an Organizational Unit (OU) based on the regions. Attach Service Control Policies (SCP) to each OU, granting access only to the AWS services required by the regional teams\" is incorrect.</p><p>While this can limit access to certain services, it does not necessarily limit the scope of IAM roles or prevent privilege escalation within the allowed services.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>"
        }
      },
      {
        "id": 83960130,
        "correct_response": [
          "a",
          "c",
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The security team at a company is working to create VPC endpoints so that the AWS Systems Manager can be used to manage private EC2 instances without internet access.</p>\n\n<p>As an AWS Certified Security Specialist, which options will you combine to build a solution to meet the given requirements? (Select three)</p>\n",
          "answers": [
            "<p>Verify that SSM Agent is installed on the instance</p>",
            "<p>Allow outbound internet access on your managed instances. The managed instances must be configured to also allow HTTPS (port 443) outbound traffic to the following endpoints: <code>ssm.[region].amazonaws.com</code>, <code>ssmmessages.[region].amazonaws.com</code>, <code>ec2messages.[region].amazonaws.com</code></p>",
            "<p>Create an AWS Identity and Access Management (IAM) instance profile for Systems Manager. Attach the IAM role to your private EC2 instance</p>",
            "<p>Create three virtual private cloud (VPC) endpoints for Systems Manager with service names: <code>com.amazonaws.[region].ssm</code>, <code>com.amazonaws.[region].ec2messages</code>, <code>com.amazonaws.[region].ssmmessages</code></p>",
            "<p>Create an AWS Identity and Access Management (IAM) instance profile for Systems Manager. Attach the IAM role to the VPC endpoint policy</p>",
            "<p>Create a virtual private cloud (VPC) endpoint for Systems Manager with service name <code>com.amazonaws.[region].ssm</code></p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Verify that SSM Agent is installed on the instance</strong></p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) instance profile for the Systems Manager. Attach the IAM role to your private EC2 instance</strong></p>\n\n<p><strong>Create three virtual private cloud (VPC) endpoints for the Systems Manager with service names: <code>com.amazonaws.[region].ssm</code>, <code>com.amazonaws.[region].ec2messages</code>, <code>com.amazonaws.[region].ssmmessages</code></strong></p>\n\n<p>You need to create a virtual private cloud (VPC) endpoint for Systems Manager for three services: <code>com.amazonaws.[region].ssm</code>, <code>com.amazonaws.[region].ec2messages</code> and <code>com.amazonaws.[region].ssmmessages</code>. After the three endpoints are created, your instance appears in Managed Instances and can be managed using Systems Manager.</p>\n\n<p>Complete steps to be followed for above expected configuration:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q15-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) instance profile for the Systems Manager. Attach the IAM role to the VPC endpoint policy</strong></p>\n\n<p><strong>Create a virtual private cloud (VPC) endpoint for Systems Manager with service name <code>com.amazonaws.[region].ssm</code></strong></p>\n\n<p>These two statements are incorrect and given only as distractors.</p>\n\n<p><strong>Allow outbound internet access on your managed instances. The managed instances must be configured to also allow HTTPS (port 443) outbound traffic to the following endpoints: <code>ssm.[region].amazonaws.com</code></strong> - The question mentions that the instance is in a private subnet with no internet access, hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p>\n"
        }
      },
      {
        "id": 83960192,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A data analytics company uses Amazon GuardDuty to identify unexpected, potentially unauthorized, and malicious activity within its AWS environment. The security team at the company wants all Medium/High Severity findings to automatically generate a ticket in a third-party ticketing system through email integration.</p>\n\n<p>As an AWS Certified Security Specialist, what would you suggest as the most optimal solution?</p>\n",
          "answers": [
            "<p>Leverage the GuardDuty CreateFilter API operation to set up a filter in GuardDuty to monitor for Medium/High severity findings. Set up an SES endpoint as the target for the GuardDuty CreateFilter API so that SES can send out an email to the third-party ticketing email system</p>",
            "<p>Create an Amazon EventBridge rule that includes an event pattern that matches Medium/High severity GuardDuty findings. Set up an SES endpoint as the target for the EventBridge rule so that SES can send out an email to the third-party ticketing email system</p>",
            "<p>Create an Amazon EventBridge rule that includes an event pattern that matches Medium/High severity GuardDuty findings. Set up an Amazon Simple Notification Service (Amazon SNS) topic. Configure the third-party ticketing email system as a subscriber to the SNS topic. Set the SNS topic as the target for the EventBridge rule</p>",
            "<p>Leverage the GuardDuty CreateFilter API operation to set up a filter in GuardDuty to monitor for Medium/High severity findings. Set up an Amazon Simple Notification Service (Amazon SNS) topic. Configure the third-party ticketing email system as a subscriber to the SNS topic. Set the SNS topic as the target for the GuardDuty CreateFilter API</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an Amazon EventBridge rule that includes an event pattern that matches Medium/High severity GuardDuty findings. Set up an Amazon Simple Notification Service (Amazon SNS) topic. Configure the third-party ticketing email system as a subscriber to the SNS topic. Set the SNS topic as the target for the EventBridge rule</strong></p>\n\n<p>Amazon GuardDuty is a security monitoring service that analyzes and processes Foundational data sources, such as AWS CloudTrail management events, AWS CloudTrail event logs, VPC flow logs (from Amazon EC2 instances), and DNS logs.</p>\n\n<p>GuardDuty informs you of the status of your AWS environment by producing security findings that you can view in the GuardDuty console or through Amazon EventBridge.</p>\n\n<p>EventBridge is a serverless service that uses events to connect application components together, making it easier for you to build scalable event-driven applications. Event-driven architecture is a style of building loosely coupled software systems that work together by emitting and responding to events. Event-driven architecture can help you boost agility and build reliable, scalable applications. An Eventbridge target is a resource or endpoint that EventBridge sends an event to when the event matches the event pattern defined for a rule. The rule processes the event data and sends the pertinent information to the target. To deliver event data to a target, EventBridge needs permission to access the target resource. You can define up to five targets for each rule.</p>\n\n<p>For the given use case, you can use a custom event pattern with the EventBridge rule to match Medium/High severity GuardDuty findings. Then, route the response to an Amazon Simple Notification Service (Amazon SNS) topic. You also need to set the third-party ticketing email system as a subscriber to the given SNS topic.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage the GuardDuty CreateFilter API operation to set up a filter in GuardDuty to monitor for Medium/High severity findings. Set up an SES endpoint as the target for the GuardDuty CreateFilter API so that SES can send out an email to the third-party ticketing email system</strong> -</p>\n\n<p><strong>Leverage the GuardDuty CreateFilter API operation to set up a filter in GuardDuty to monitor for Medium/High severity findings. Set up an Amazon Simple Notification Service (Amazon SNS) topic. Configure the third-party ticketing email system as a subscriber to the SNS topic. Set the SNS topic as the target for the GuardDuty CreateFilter API</strong></p>\n\n<p>GuardDuty CreateFilter API creates a filter using the specified finding criteria. If the action is successful, the service sends back an HTTP 200 response along with the name of the successfully created filter. There is no such thing as setting a target for the GuardDuty CreateFilter API. So both these options are incorrect.</p>\n\n<p><strong>Create an Amazon EventBridge rule that includes an event pattern that matches Medium/High severity GuardDuty findings. Set up an SES endpoint as the target for the EventBridge rule so that SES can send out an email to the third-party ticketing email system</strong> - Eventbridge does not support Amazon SES endpoint as a target, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html\">https://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings_cloudwatch.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_findings_cloudwatch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/APIReference/API_CreateFilter.html\">https://docs.aws.amazon.com/guardduty/latest/APIReference/API_CreateFilter.html</a></p>\n\n<p><a href=\"https://repost.aws/knowledge-center/guardduty-eventbridge-sns-rule\">https://repost.aws/knowledge-center/guardduty-eventbridge-sns-rule</a></p>\n"
        }
      },
      {
        "id": 73686820,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A security engineer has created an AWS Lambda function that checks AWS CloudTrail logs in an Amazon S3 bucket for security related issues. The Lambda function should record results in Amazon CloudWatch Logs. The security engineer has sufficient permissions to execute the function. Upon testing the function the execution fails.</p><p>The Lambda function execution role has the following permissions:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question/2023-01-05_04-00-28-e4e0767ef7f1c4ce3267f194998497f2.jpg\"><p>What is the most likely cause of the issue?</p>",
          "answers": [
            "<p>The Lambda function does not have permissions to CloudWatch Logs.</p>",
            "<p>The \u201cResource\u201d element in the policy does not include the S3 bucket objects.</p>",
            "<p>The Lambda function does not have permissions to access the S3 bucket.</p>",
            "<p>The CloudTrail trail does not allow access in a resource-based policy.</p>"
          ],
          "explanation": "<p>A Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources. You provide this role when you create a function, and Lambda assumes the role when your function is invoked.</p><p>The most likely issue in this scenario is that the policy assigned to the AWS Lambda function execution role does not specify any permissions for Amazon S3. Permissions for CloudTrail do not help here as the logs are stored in S3 and the Lambda function must read the logs from the S3 bucket.</p><p><strong>CORRECT: </strong>\"The Lambda function does not have permissions to access the S3 bucket\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The Lambda function does not have permissions to CloudWatch Logs\" is incorrect.</p><p>Full permissions to CloudWatch are provided in the policy.</p><p><strong>INCORRECT:</strong> \"The \u201cResource\u201d element in the policy does not include the S3 bucket objects\" is incorrect.</p><p>The wildcard for the resource \u201c*\u201d is all inclusive and would include bucket objects. However, S3 permissions are not provided.</p><p><strong>INCORRECT:</strong> \"The CloudTrail trail does not allow access in a resource-based policy\" is incorrect.</p><p>Lambda does not need to be provided with any permissions to CloudTrail in this scenario as it reads the logs from the S3 bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html</a></p>"
        }
      },
      {
        "id": 76398920,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company runs an application behind an Application Load Balancer (ALB). A security engineer has noticed many suspicious HTTP requests hitting the ALB. There is an Amazon CloudFront distribution in front of the ALB. Users are reporting performance problems.</p><p>A security engineer discovers that the website is receiving a high rate of unwanted requests to the CloudFront distribution originating from a series of source IP addresses.</p><p>How should the security engineer address this problem with the LEAST effort?</p>",
          "answers": [
            "<p>Create an Amazon CloudFront distribution to cache content and automatically block access to the suspicious source IP addresses.</p>",
            "<p>Use Amazon GuardDuty to analyze network activity, detect anomalies, and trigger a Lambda function to prevent access.</p>",
            "<p>Use AWS Lambda to analyze a VPC Flow Log, detect the suspicious traffic, and block the IP address in the security groups.</p>",
            "<p>Create an AWS WAF rate-based rule to block this traffic when it exceeds a defined threshold.</p>"
          ],
          "explanation": "<p>AWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting.</p><p>Rate-based Rules are a new type of Rule that can be configured in AWS WAF. This feature allows you to specify the number of web requests that are allowed by a client IP in a trailing, continuously updated, 5-minute period. If an IP address breaches the configured limit, new requests will be blocked until the request rate falls below the configured threshold.</p><p><strong>CORRECT: </strong>\"Create an AWS WAF rate-based rule to block this traffic when it exceeds a defined threshold\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution to cache content and automatically block access to the suspicious source IP addresses\" is incorrect.</p><p>CloudFront cannot automatically detect malicious traffic and block access to the source IP addresses.</p><p><strong>INCORRECT:</strong> \"Use Amazon GuardDuty to analyze network activity, detect anomalies, and trigger a Lambda function to prevent access\" is incorrect.</p><p>GuardDuty can analyze logs and network flows and trigger a function to perform remediation. However, this does not represent the option that requires the least effort as you would need to write the Lambda function.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to analyze a VPC Flow Log, detect the suspicious traffic, and block the IP address in the security groups\" is incorrect.</p><p>This is not the option that requires the least effort (must write a Lambda function), and you cannot block IP addresses in security groups (they do not support deny rules).<strong>References:</strong></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/waf/faq/\">https://aws.amazon.com/waf/faq/</a></p>"
        }
      },
      {
        "id": 73686830,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>Several AWS accounts belonging to different business units are used for development purposes. An additional account is used by the security team. To ensure security best practices are being followed, the security team requires access to review the configuration of the Amazon EC2 instances in the development accounts.</p><p>Which solution will meet these requirements in the MOST secure manner?</p>",
          "answers": [
            "<p>Create an IAM policy in each development account that has read-only access to Amazon EC2 resources. Assign the policy to an IAM user. Share the user credentials with the security team.</p>",
            "<p>Create an IAM policy in each development account that has administrator access to all Amazon EC2 actions. Assign the policy to an IAM user. Share the user credentials with the security team.</p>",
            "<p>Create an IAM policy in each development account that has read-only access to Amazon EC2 resources. Assign the policy to a cross-account IAM role. Ask the security team members to assume the role from their account.</p>",
            "<p>Create an IAM policy in each development account that has administrator access to Amazon EC2 resources. Assign the policy to a cross-account IAM role. Ask the security team members to assume the role from their account.</p>"
          ],
          "explanation": "<p>This question is checking that you know how to configure cross-account access and how to do so securely using the principle of least privilege. This can be achieved through the creation of a policy providing permissions to the resources in each development account, associating the policies to roles, and then assuming those roles from the security admins account.</p><p>To ensure this solution is secure, read-only permissions should be assigned to the permissions policy as per the requirements of the security team.</p><p><strong>CORRECT: </strong>\"Create an IAM policy in each development account that has read-only access to Amazon EC2 resources. Assign the policy to a cross-account IAM role. Ask the security team members to assume the role from their account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM policy in each development account that has administrator access to Amazon EC2 resources. Assign the policy to a cross-account IAM role. Ask the security team members to assume the role from their account\" is incorrect.</p><p>Administrator access provides more permissions than are required by the security team.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy in each development account that has read-only access to Amazon EC2 resources. Assign the policy to an IAM user. Share the user credentials with the security team\" is incorrect.</p><p>Sharing credentials is much less secure than using roles.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy in each development account that has administrator access to all Amazon EC2 actions. Assign the policy to an IAM user. Share the user credentials with the security team\" is incorrect.</p><p>This answer provides too many permissions and an insecure method of authentication.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>"
        }
      },
      {
        "id": 99531673,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Neal Set 3",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company requires that only trusted code can be deployed to AWS Lambda functions. A method of validating the integrity of the code should be implemented and developers should not be able to bypass the solution.</p><p>Which combination of steps should a security engineer take to meet these requirements? (Select TWO.)</p>",
          "answers": [
            "<p>Use AWS Signer to verify code integrity when code packages are deployed to Lambda.</p>",
            "<p>Use IAM policies to enforce that developers can only create functions that have code signing enabled.</p>",
            "<p>Use the AWS Key Management Service (AWS KMS) to encrypt the code and enable automatic key rotation.</p>",
            "<p>Use Amazon S3 to store the code packages and configure default encryption for the S3 bucket.</p>",
            "<p>Use IAM policies to enforce that developers can only deploy code packages with encrypted source code.</p>"
          ],
          "explanation": "<p>To verify code integrity, use AWS Signer to create digitally signed code packages for functions and layers. When a user attempts to deploy a code package, Lambda performs validation checks on the code package before accepting the deployment</p><p>You can use IAM to control who can create code signing configurations. Typically, you allow only specific administrative users to have this ability. Additionally, you can set up IAM policies to enforce that developers only create functions that have code signing enabled.</p><p><strong>CORRECT: </strong>\"Use AWS Signer to verify code integrity when code packages are deployed to Lambda\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use IAM policies to enforce that developers can only create functions that have code signing enabled\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AWS Key Management Service (AWS KMS) to encrypt the code and enable automatic key rotation\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 to store the code packages and configure default encryption for the S3 bucket\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Use IAM policies to enforce that developers can only deploy code packages with encrypted source code\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-codesigning.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-codesigning.html</a></p>"
        }
      },
      {
        "id": 76398938,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A healthcare organization is using Amazon EC2 instances to host an application that stores sensitive patient records. In compliance with healthcare regulations, the organization must restrict access to these records. A system engineer needs to establish a secure connection to the EC2 instances without opening any inbound ports, managing SSH keys, or maintaining bastion hosts.</p><p>The organization also requires that all session activity logs are monitored, stored, and accessible in an encrypted format.</p><p>Which solution would satisfy these requirements?</p>",
          "answers": [
            "<p>Use Amazon Inspector to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed.</p>",
            "<p>Use Amazon GuardDuty to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed.</p>",
            "<p>Create an EC2 Instance Connect Endpoint for private connectivity and then use Amazon EC2 Instance Connect to access the EC2 instances. Configure logging for the instances using Amazon CloudWatch Logs with permanent retention.</p>",
            "<p>Use AWS Systems Manager Session Manager to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and select the option to enforce encryption.</p>"
          ],
          "explanation": "<p>AWS Systems Manager Session Manager provides secure and auditable instance management without the need for bastion hosts, SSH, or open inbound ports. The organization can configure Amazon CloudWatch logging for the session manager sessions and select the option to enforce encryption.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2024-02-06_21-43-52-3868acd6e6da32e2bd8e717298b6b9d5.png\"><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Session Manager to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed\" is incorrect.</p><p>Amazon Inspector is a security assessment service. It can't be used for connecting to EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use Amazon GuardDuty to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed\" is incorrect.</p><p>Amazon GuardDuty is a threat detection service. It can't be used for connecting to EC2 instances.</p><p><strong>INCORRECT:</strong> \"Create an EC2 Instance Connect Endpoint for private connectivity and then use Amazon EC2 Instance Connect to access the EC2 instances. Configure logging for the instances using Amazon CloudWatch Logs with permanent retention\" is incorrect.</p><p>EC2 instance connect uses SSH/RDP protocols for instance management so this option does not meet the requirements. The use of an interface endpoint simply allows managing the instances without the need for public IP addresses but does not affect the protocols used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>"
        }
      },
      {
        "id": 83960218,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The security team at a company needs to follow the security requirements:</p>\n\n<ul>\n<li>Monitor all traffic leaving a particular VPC</li>\n<li>Monitor all traffic whose source is outside of the VPC</li>\n</ul>\n\n<p>The purpose of this traffic monitoring is to put in place a proper content inspection, troubleshooting, and threat monitoring solution.</p>\n\n<p>Which of the following options represents the best solution for the given requirement?</p>\n",
          "answers": [
            "<p>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces in the VPC. Configure a traffic mirror with VPC Flow Logs as the source and the target as the appliance. Create a traffic mirroring filter with a traffic mirroring rule for the TCP inbound traffic</p>",
            "<p>Configure a traffic mirror target for the monitoring appliance. Create a traffic mirror filter with a traffic mirror rule for the TCP inbound traffic. Also, create another traffic mirror filter with a traffic mirror rule for the UDP inbound traffic</p>",
            "<p>Configure a traffic mirror target for the monitoring appliance. Create a traffic mirror filter with a rule for outbound traffic to reject all packets that have a destination IP in the VPC CIDR block and accept all other outbound packets. Also, create another rule for inbound traffic to reject all packets that have a source IP in the VPC CIDR block and accept all other inbound packets</p>",
            "<p>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces in the VPC. Publish the flow log data directly to Amazon CloudWatch for further analysis and alert generation</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a traffic mirror target for the monitoring appliance. Create a traffic mirror filter with a rule for outbound traffic to reject all packets that have a destination IP in the VPC CIDR block and accept all other outbound packets. Also, create another rule for inbound traffic to reject all packets that have a source IP in the VPC CIDR block and accept all other inbound packets</strong></p>\n\n<p>Consider the scenario where you want to monitor traffic leaving your VPC or traffic whose source is outside your VPC. In this case, you will mirror all traffic except traffic passing within your VPC and send it to a single monitoring appliance. You need the following traffic mirror resources:</p>\n\n<ol>\n<li><p>A traffic mirror target for the appliance (Target A)</p></li>\n<li><p>A traffic mirror filter that has two sets of rules for outbound and inbound traffic. For outbound traffic, it will reject all packets that have a destination IP in the VPC CIDR block and accept all other outbound packets. For inbound traffic, it will reject all packets that have a source IP in the VPC CIDR block and accept all other inbound packets.</p></li>\n<li><p>A traffic mirror session that has the following:</p>\n\n<ol>\n<li><p>A traffic mirror source</p></li>\n<li><p>A traffic mirror target for the appliance (Target A)</p></li>\n<li><p>A traffic mirror filter with a traffic mirror rule for the TCP inbound traffic (Filter F)</p></li>\n</ol></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces in the VPC. Publish the flow log data directly to Amazon CloudWatch for further analysis and alert generation</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Whereas, traffic mirroring is a feature used to copy network traffic from a network interface of an Amazon EC2 instance and send it to out-of-band security and monitoring appliances for deep packet inspection. You can detect network and security anomalies, gain operational insights, implement compliance and security controls, and troubleshoot issues.</p>\n\n<p><strong>Enable VPC Flow Logs to capture detailed information about the traffic going to and from network interfaces in the VPC. Configure a traffic mirror with VPC Flow Logs as the source and the target as the appliance. Create a traffic mirroring filter with a traffic mirroring rule for the TCP inbound traffic</strong> - This is not possible and is given only as a distractor.</p>\n\n<p><strong>Configure a traffic mirror target for the monitoring appliance. Create a traffic mirror filter with a traffic mirror rule for the TCP inbound traffic. Also, create another traffic mirror filter with a traffic mirror rule for the UDP inbound traffic</strong> - This configuration is used if you want to mirror inbound TCP and UDP traffic on an instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/monitoring.html\">https://docs.aws.amazon.com/vpc/latest/userguide/monitoring.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/tm-example-non-vpc.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/tm-example-non-vpc.html</a></p>\n"
        }
      },
      {
        "id": 112083285,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A security engineer needs to access log files generated by AWS CloudTrail. The trail stores log files in an Amazon S3 bucket that is encrypted with AWS KMS managed keys (SSE-KMS). The logs should be accessed by assuming an IAM role. When attempting to access the log files the security engineer experienced an access denied error.</p><p>What is the MOST likely cause of this issue?</p>",
          "answers": [
            "<p>The KMS key policy does not grant the IAM role permissions to use the key for decryption.</p>",
            "<p>The KMS key policy does not grant AWS CloudTrail the permissions to write encrypted log files.</p>",
            "<p>The S3 bucket policy does not grant AWS CloudTrail permissions to use the key for decryption.</p>",
            "<p>The log files in the S3 bucket can only be decrypted and viewed using the AWS CloudTrail console.</p>"
          ],
          "explanation": "<p>To use SSE-KMS with CloudTrail, you create and manage a KMS key. You attach a policy to the key that determines which users can use the key for encrypting and decrypting CloudTrail log files. The decryption is seamless through S3. When authorized users of the key read CloudTrail log files, S3 manages the decryption, and the authorized users can read log files in unencrypted form.</p><p>Permissions to use a key can be granted in IAM permissions policies attached to users, groups, or roles, or they can be granted on the S3 bucket policy. In this case the engineer assumes a role and the role may not have the permissions needed to decrypt the data.</p><p><strong>CORRECT: </strong>\"The KMS key policy does not grant the IAM role permissions to use the key for decryption\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The KMS key policy does not grant AWS CloudTrail the permissions to write encrypted log files\" is incorrect.</p><p>The log files exist so they must have been written already.</p><p><strong>INCORRECT:</strong> \"The S3 bucket policy does not grant AWS CloudTrail permissions to use the key for decryption\" is incorrect.</p><p>CloudTrail should be granted decrypt permissions but in this case the log files exist, so CloudTrail has been able to write and encrypt the files. Also, when the user is trying to access the files decryption happens through S3, not CloudTrail.</p><p><strong>INCORRECT:</strong> \"The log files in the S3 bucket can only be decrypted and viewed using the AWS CloudTrail console\" is incorrect.</p><p>This is not true. You can access the files directly through S3 if you have read permissions to S3 and decrypt permissions to use the KMS key.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/encrypting-cloudtrail-log-files-with-aws-kms.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/encrypting-cloudtrail-log-files-with-aws-kms.html</a></p>"
        }
      },
      {
        "id": 73686816,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is building an application that uses Amazon EC2 instances and an Amazon RDS database. The solution must be highly secure, and encryption will be implemented within the application and database using an AWS KMS customer managed KMS key. The security team wants to prevent any other services from using the KMS key.</p><p>Which solution will meet these requirements?</p>",
          "answers": [
            "<p>Create a custom key policy for the KMS key. Use the kms:ViaService condition key to allow the KMS key to be used only when the request comes from Amazon EC2 or Amazon RDS.</p>",
            "<p>Create an instance profile and attach it to the Amazon EC2 instances and Amazon RDS database. Attach an IAM policy to the instance profile that allows use of the KMS key.</p>",
            "<p>Create a custom key policy for the KMS key. Use the kms:GrantOperations condition key to grant access to the KMS key only when the request comes from Amazon EC2 or Amazon RDS.</p>",
            "<p>Create an SCP the explicitly allows permission to the KMS key for Amazon EC2 and Amazon RDS and explicitly denies permission to the KMS key for all other services. Attach the SCP to the AWS account.</p>"
          ],
          "explanation": "<p>The kms:ViaService condition key limits use of an AWS KMS key to requests from specified AWS services. You can specify one or more services in each kms:ViaService condition key.</p><p>For example, the following key policy statement uses the kms:ViaService condition key to allow a customer managed KMS key to be used for the specified actions only when the request comes from Amazon EC2 or Amazon RDS in the US West (Oregon) region on behalf of ExampleUser.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_03-52-34-2f7d96a165ddff57343266d3d2cc7078.jpg\"><p><strong>CORRECT: </strong>\"Create a custom key policy for the KMS key. Use the kms:ViaService condition key to allow the KMS key to be used only when the request comes from Amazon EC2 or Amazon RDS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an instance profile and attach it to the Amazon EC2 instances and Amazon RDS database. Attach an IAM policy to the instance profile that allows use of the KMS key\" is incorrect.</p><p>You cannot attach instance profiles to RDS databases, and this solution does not explicitly deny access to the KMS key from other services.</p><p><strong>INCORRECT:</strong> \"Create a custom key policy for the KMS key. Use the kms:GrantOperations condition key to grant access to the KMS key only when the request comes from Amazon EC2 or Amazon RDS\" is incorrect.</p><p>The GrantOperations condition key is used to control access to the CreateGrant operation and does not restrict or grant access to a KMS key by service.</p><p><strong>INCORRECT:</strong> \"Create an SCP the explicitly allows permission to the KMS key for Amazon EC2 and Amazon RDS and explicitly denies permission to the KMS key for all other services. Attach the SCP to the AWS account\" is incorrect.</p><p>You cannot use conditions with allow statements in an SCP.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/policy-conditions.html#conditions-kms-via-service\">https://docs.aws.amazon.com/kms/latest/developerguide/policy-conditions.html#conditions-kms-via-service</a></p>"
        }
      },
      {
        "id": 73686854,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has deployed an application on Amazon EC2 instances with an Amazon RDS database. A security architect needs a secure solution for storing the database credentials and enabling automatic rotation on a regular basis. The credentials must be encrypted both in transit and at rest.</p><p>Which solution meets these requirements?</p>",
          "answers": [
            "<p>Use AWS Key Management Server (KMS) and rotate the keys using an AWS Lambda function.</p>",
            "<p>Use AWS Systems Manager Parameter Store and configure automatic rotation of the credentials.</p>",
            "<p>Use IAM access keys in and rotate the access keys using an AWS Lambda function.</p>",
            "<p>Use AWS Secrets Manager and configure automatic rotation of the credentials.</p>"
          ],
          "explanation": "<p><em>Rotation</em> is the process of periodically updating a secret. When you rotate a secret, you update the credentials in both the secret and the database or service. In Secrets Manager, you can set up automatic rotation for your secrets. Applications that retrieve the secret from Secrets Manager automatically get the new credentials after rotation.</p><p>Secrets Manager provides complete rotation templates for Amazon RDS, Amazon DocumentDB, and Amazon Redshift secrets. You can also enable encryption in transit and at rest for keys stored in AWS Secrets Manager.</p><p>The slide below provides more information on AWS Secrets Manager:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_04-35-26-a125b1dcc185a051d727086679ca6c06.jpg\"><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager and configure automatic rotation of the credentials\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Parameter Store and configure automatic rotation of the credentials\" is incorrect.</p><p>Systems Manager Parameter Store can be used for storing encrypted secrets, but it does not have a feature for automatic secret rotation.</p><p><strong>INCORRECT:</strong> \"Use AWS Key Management Server (KMS) and rotate the keys using an AWS Lambda function\" is incorrect.</p><p>AWS KMS cannot be used for storing secrets, it is used for storing encryption keys.</p><p><strong>INCORRECT:</strong> \"Use IAM access keys in and rotate the access keys using an AWS Lambda function\" is incorrect.</p><p>IAM access keys cannot be used for authenticating to an Amazon RDS database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_turn-on-for-db.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_turn-on-for-db.html</a></p>"
        }
      },
      {
        "id": 83960224,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>For a threat alert raised by the security team, a company needs content inspection of the traffic passing through an Amazon Route 53 resolver outbound endpoint.</p>\n\n<p>As A Security Specialist, how will you implement a solution for this requirement?</p>\n",
          "answers": [
            "<p>Enable VPC Flow Logs to capture all network traffic information passing through Route 53 resolver endpoints. Use the Athena integration feature in the Amazon VPC Console to create Athena tables for direct querying</p>",
            "<p>To view traffic passing through Route 53 resolver endpoints, configure Amazon Virtual Private Cloud (Amazon VPC) Traffic Mirroring</p>",
            "<p>Turn on Route 53 public query logging in each public-hosted zone. Amazon Route 53 publishes the logs to Amazon CloudWatch Logs for further analysis and troubleshooting</p>",
            "<p>Create a trail in AWS CloudTrail for continuous tracking of all Route 53 events. Deliver the log files to an Amazon S3 bucket and use Athena to query the log data for user patterns and troubleshooting</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p>Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of type <code>interface</code>. You can then send the traffic to out-of-band security and monitoring appliances for content inspection, threat monitoring and troubleshooting.</p>\n\n<p>The security and monitoring appliances can be deployed as individual instances, or as a fleet of instances behind either a Network Load Balancer or a Gateway Load Balancer with a UDP listener. Traffic Mirroring supports filters and packet truncation so that you can extract only the traffic of interest, using the monitoring tools of your choice.</p>\n\n<p>How Traffic Mirroring works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q62-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable VPC Flow Logs to capture all network traffic information passing through Route 53 resolver endpoints. Use the Athena integration feature in the Amazon VPC Console to create Athena tables for direct querying</strong> - VPC Flow Logs capture metadata about the traffic, not the actual traffic itself. VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC, whereas, traffic mirroring streams a copy of the network traffic say to an EC2 or Appliance for packet inspection.</p>\n\n<p><strong>Turn on Route 53 public query logging in each public-hosted zone. Amazon Route 53 publishes the logs to Amazon CloudWatch Logs for further analysis and troubleshooting</strong> - Once you configure Amazon Route 53 to log information about the public DNS queries that Route 53 receives, the following information is captured: Domain or subdomain that was requested, Date and time of the request, DNS record type, Route 53 edge location that responded to the DNS query, DNS response code. This option is irrelevant to the given use case.</p>\n\n<p><strong>Create a trail in AWS CloudTrail for continuous tracking of all Route 53 events. Deliver the log files to an Amazon S3 bucket and use Athena to query the log data for user patterns and troubleshooting</strong> - Route 53 is integrated with AWS CloudTrail that captures all API calls for Route 53 as events, including calls from the Route 53 console and code calls to the Route 53 APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Route 53.</p>\n\n<p>While the logs above are extremely useful in troubleshooting and monitoring the security infrastructure of AWS architecture, content inspection of a packet can only be done through Traffic Mirroring.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html</a></p>\n\n<p><a href=\"https://repost.aws/knowledge-center/route53-view-endpoint-traffic\">https://repost.aws/knowledge-center/route53-view-endpoint-traffic</a></p>\n"
        }
      },
      {
        "id": 83960144,
        "correct_response": [
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A Security Engineer has been asked to create an identity-based policy that allows access to add objects to an Amazon S3 bucket. But, the access should be given from April 1, 2023, through April 30, 2023 (UTC) inclusive.</p>\n\n<p>How will you define this identity-based policy?</p>\n",
          "answers": [
            "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"{aws:logintime}\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"{aws:logintime}\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>",
            "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Deny\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"{aws:logintime}\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"{aws:logintime}\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>",
            "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Principal\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"aws:CurrentTime\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"aws:CurrentTime\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>",
            "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"aws:CurrentTime\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"aws:CurrentTime\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"aws:CurrentTime\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"aws:CurrentTime\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This identity-based policy allows access to actions based on date and time. This policy restricts access to actions that occur between April 1, 2023, and April 30, 2023 (UTC), inclusive. This policy grants the permissions necessary to complete this action programmatically from the AWS API or AWS CLI.</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"{aws:logintime}\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"{aws:logintime}\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy definition is incorrect. You cannot use a policy variable with the Date condition operator. In addition, using the <code>{aws:logintime}</code> policy variable is incorrect for the given use case.</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Deny\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"{aws:logintime}\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"{aws:logintime}\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy definition is incorrect. You cannot use a policy variable with the Date condition operator. In addition, using the <code>{aws:logintime}</code> policy variable is incorrect for the given use case. You should also note that the Deny Effect is logically opposite of what the use case requires.</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Principal\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"aws:CurrentTime\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"aws:CurrentTime\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy definition is incorrect. You cannot use the Principal element in an identity-based policy. Identity-based policies are permissions policies that you attach to IAM identities (users, groups, or roles). In those cases, the principal is implicitly the identity where the policy is attached.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html</a></p>\n"
        }
      },
      {
        "id": 76398908,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has deployed an organization in AWS Organizations with several member accounts. The security team requires that there is at least on AWS CloudTrail trail configured for all existing accounts and any accounts that are created in the future. The logs should be sent to a single centralized Amazon S3 bucket and administrators in member accounts should not be able to modify the configuration.</p><p>Which actions should be taken to accomplish this?</p>",
          "answers": [
            "<p>Create a new trail in each account and use a cross-account role to log to a central S3 bucket.</p>",
            "<p>Enable CloudTrail Insights in the management account and run the \u201caws cloudtrail lookup-events\u201d CLI command.</p>",
            "<p>Create a new trail in each account and use an SCP to deny the \u201ccloudtrail:Delete\u201d API action.</p>",
            "<p>Create an organization trail in the management account and specify a central S3 bucket.</p>"
          ],
          "explanation": "<p>If you have created an organization in AWS Organizations, you can create a trail that will log all events for all AWS accounts in that organization. This is sometimes referred to as an <em>organization trail</em>.</p><p>You can also choose to edit an existing trail in the management account and apply it to an organization, making it an organization trail. Organization trails log events for the management account and all member accounts in the organization.</p><p>When you create an organization trail, a trail with the name that you give it will be created in every AWS account that belongs to your organization. Users with CloudTrail permissions in member accounts will be able to see this trail when they log into the AWS CloudTrail console from their AWS accounts, or when they run AWS CLI commands such as describe-trail.</p><p>However, users in member accounts will not have sufficient permissions to delete the organization trail, turn logging on or off, change what types of events are logged, or otherwise alter the organization trail in any way.</p><p><strong>CORRECT: </strong>\"Create an organization trail in the management account and specify a central S3 bucket\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new trail in each account and use a cross-account role to log to a central S3 bucket\" is incorrect.</p><p>An organization trail is easier to implement and cannot be edited in member accounts which is more secure.</p><p><strong>INCORRECT:</strong> \"Enable CloudTrail Insights in the management account and run the \u201caws cloudtrail lookup-events\u201d CLI command\" is incorrect.</p><p>CloudTrail Insights detects unusual API or error rate activity. This is not used for capturing and logging API events.</p><p><strong>INCORRECT:</strong> \"Create a new trail in each account and use an SCP to deny the \u201ccloudtrail:Delete\u201d API action\" is incorrect.</p><p>An organization trail is easier to implement and cannot be edited in member accounts which is more secure.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>"
        }
      },
      {
        "id": 83960128,
        "correct_response": [
          "c",
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A Security Engineer has been asked to configure an interface VPC endpoint to access an Amazon API Gateway private REST API that is in another AWS account.</p>\n\n<p>What are the key points of consideration while creating an interface endpoint in the Amazon VPC account for the given requirement? (Select two)</p>\n",
          "answers": [
            "<p>For better resilience, it is mandatory to select multiple subnets across multiple Availability Zones when creating an interface endpoint</p>",
            "<p>To connect to public APIs using a VPC endpoint, enable private DNS on your VPC</p>",
            "<p>When you activate private DNS for an interface VPC endpoint, you can no longer access API Gateway public APIs from your Amazon VPC</p>",
            "<p>The security groups that you choose must have a rule that allows TCP Port 443 inbound HTTPS traffic from an IP address range in your Amazon VPC</p>",
            "<p>You cannot access your private API endpoint from an on-premises network using public DNS names</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>When you activate private DNS for an interface VPC endpoint, you can no longer access API Gateway public APIs from your Amazon VPC</strong> - When you activate private DNS for an interface VPC endpoint, you can no longer access API Gateway public APIs from your Amazon VPC.</p>\n\n<p>When a private DNS is enabled on a VPC endpoint, the API's invoke URL is covered by the private DNS name <code>*.execute-api.us-east-1.amazonaws.com</code> where * is a placeholder for the API ID. When a DNS query is resolved for a public API from inside a VPC, the resolved DNS points to the private IP of the associated VPC endpoint instead of the public IP of the public API. The API call is then routed to the public API through the VPC endpoint instead of routing it through the internet. Because VPC endpoints can route traffic only to private APIs, the result is an HTTP 403 error.</p>\n\n<p><strong>The security groups that you choose must have a rule that allows TCP Port 443 inbound HTTPS traffic from an IP address range in your Amazon VPC</strong> - The security groups that you choose must have a rule that allows TCP Port 443 inbound HTTPS traffic from either of the following: An IP address range in your Amazon VPC or another security group in your Amazon VPC.</p>\n\n<p>Key points to remember when creating an interface endpoint:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q14-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-private-cross-account-vpce/\">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-private-cross-account-vpce/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For better resilience, it is mandatory to select multiple subnets across multiple Availability Zones when creating an interface endpoint</strong> - This statement is incorrect. As best practice AWS suggests configuring subnets across multiple Availability Zones to make your interface endpoint resilient to possible Availability Zone failures. However, it is not mandatory. Another best practice is to use a VPC endpoint policy to restrict endpoint access by API ID. It's also a best practice to use the API Gateway resource policy to restrict endpoint access by the principal.</p>\n\n<p><strong>To connect to public APIs using a VPC endpoint, enable private DNS on your VPC</strong> - This statement is incorrect. It is not possible to connect to public APIs using a VPC endpoint.</p>\n\n<p><strong>You cannot access your private API endpoint from an on-premises network using public DNS names</strong> - This is incorrect. You can use AWS Direct Connect to establish a dedicated private connection from an on-premises network to Amazon VPC and access your private API endpoint over that connection by using public DNS names.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-api-test-invoke-url.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-api-test-invoke-url.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-vpc-connections/\">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-vpc-connections/</a></p>\n"
        }
      },
      {
        "id": 83960126,
        "correct_response": [
          "a",
          "d",
          "f"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A Security Engineer has been tasked to evaluate the outcome of different policies, including but not limited to identity-based policies, resource-based policies, IAM permissions boundaries, session policies, and AWS Organizations service control policies (SCPs) of an AWS account.</p>\n\n<p>Which of the following are valid statements regarding the aforementioned policy evaluations? (Select three)</p>\n",
          "answers": [
            "<p>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy does not impact the final decision</p>",
            "<p>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy results in a final <code>Deny</code></p>",
            "<p>If a resource-based policy grants permission to the ARN of the federated IAM user, then requests made by the federated user during the session are not limited by an implicit deny in a permission boundary or session policy</p>",
            "<p>Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy</p>",
            "<p>If there is no explicit <code>Allow</code> in the SCP, the request is implicitly given an <code>Allow</code> after the SCP is evaluated</p>",
            "<p>If no applicable <code>Allow</code> statement is found in the SCPs, the request is explicitly denied, even if the denial is implicit</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy does not impact the final decision</strong></p>\n\n<p>Resource-based policy logic differs from other policy types if the specified principal is an IAM user, an IAM role, or a session principal. If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary or a session policy does not impact the final decision.</p>\n\n<p><strong>Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy</strong></p>\n\n<p>This statement is correct. Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy.</p>\n\n<p>Impact of resource-based policies for different principal types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n\n<p><strong>If no applicable <code>Allow</code> statement is found in the SCPs, the request is explicitly denied, even if the denial is implicit</strong></p>\n\n<p>SCPs apply to principals of the account where the SCPs are attached. If the enforcement code does not find any applicable <code>Allow</code> statements in the SCPs, the request is explicitly denied, even if the denial is implicit. The enforcement code returns a final decision of Deny. If there is no SCP, or if the SCP allows the requested action, the enforcement code evaluation continues.</p>\n\n<p>Determining whether a request is allowed or denied within an account:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q13-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy results in a final <code>Deny</code></strong> - As discussed above, this statement is incorrect.</p>\n\n<p><strong>If there is no explicit <code>Allow</code> in the SCP, the request is implicitly given an <code>Allow</code> after the SCP is evaluated</strong> - As discussed above, this statement is incorrect.</p>\n\n<p><strong>If a resource-based policy grants permission to the ARN of the federated IAM user, then requests made by the federated user during the session are not limited by an implicit deny in a permission boundary or session policy</strong> - This statement is incorrect. If a resource-based policy grants permission to the ARN of the federated IAM user, then requests made by the federated user during the session are limited by an implicit deny in a permission boundary or session policy.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n"
        }
      },
      {
        "id": 76165450,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company has its flagship application hosted on Amazon EC2 instances that are configured in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). The application should only be accessible to users from a specific country. The company also needs the ability to monitor any prohibited requests for further analysis by the security team.</p>\n\n<p>What will you suggest as the most optimal and low-maintenance solution for the given use case?</p>\n",
          "answers": [
            "<p>Set up an AWS Web Application Firewall (WAF) web ACL. Create a rule to deny any requests that do not originate from the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</p>",
            "<p>Set up an AWS WAF web ACL. Create a rule to block the requests that do not originate from the IP range defined in an IP set containing a list of IP ranges that belong to the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</p>",
            "<p>Set up AWS Shield to block any request that does not originate from the specified country. Attach AWS Shield with the ALB</p>",
            "<p>Create a Global Accelerator and attach the WAF to it. Create a rule to block any requests that do not originate from the specified country. Create the Global Accelerator to front the existing ALB</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an AWS Web Application Firewall (WAF) web ACL. Create a rule to deny any requests that do not originate from the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</strong></p>\n\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon CloudFront distribution, and Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API. AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, Amazon CloudFront, Amazon API Gateway, Application Load Balancer, or AWS AppSync responds to requests either with the requested content or with an HTTP 403 status code (Forbidden).</p>\n\n<p>A web access control list (web ACL) gives you fine-grained control over all of the HTTP(S) web requests that your protected resource responds to. You can protect Amazon CloudFront, Amazon API Gateway, Application Load Balancer, and AWS AppSync resources. You can use criteria like the following to allow or block requests:\n1. IP address origin of the request\n2. Country of origin of the request\n3. String match or regular expression (regex) match in a part of the request\n4. Size of a particular part of the request\n5. Detection of malicious SQL code or scripting</p>\n\n<p>More on web request inspection and handling criteria:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q61-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a></p>\n\n<p>Geographic match rule statement - To allow or block web requests based on country of origin, create one or more geographical, or geo, match statements. You can use this to block access to your site from specific countries or to only allow access from specific countries. If you want to allow some web requests and block others based on country of origin, add a geo match statement for the countries that you want to allow and add a second one for the countries that you want to block.</p>\n\n<p>More on Geographic match rule:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q61-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an AWS WAF web ACL. Create a rule to block the requests that do not originate from the IP range defined in an IP set containing a list of IP ranges that belong to the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</strong> - The IP set match statement inspects the IP address of a web request against a set of IP addresses and address ranges. You can use this to allow or block web requests based on the IP addresses that the requests originate from. However, this is not an optimal solution. While defining an IP set, you need to enter one IP address or IP address range per line. Every time an IP address changes, it has to be manually added to this IP set. Hence, this option is not correct for the given use case.</p>\n\n<p><strong>Set up AWS Shield to block any request that does not originate from the specified country. Attach AWS Shield with the ALB</strong> - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield Standard is automatically enabled when you use AWS services like Elastic Load Balancing (ELB), Application Load Balancer, Amazon CloudFront, and Amazon Route 53. You cannot use AWS Shield to block traffic from a specified country.</p>\n\n<p><strong>Create a Global Accelerator and attach the WAF to it. Create a rule to block any requests that do not originate from the specified country. Create the Global Accelerator to front the existing ALB</strong> - AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon CloudFront distribution, and Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API. WAF cannot be configured with AWS Global Accelerator.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html</a></p>\n"
        }
      },
      {
        "id": 99528333,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is deploying an application on Amazon EC2 instances with an Amazon RDS MySQL database. The application will store sensitive data and a security engineer has been tasked with recommending measures to protect the sensitive data against security breaches. The solution must minimize operational overhead and credentials must be regularly and automatically rotated.</p><p>Which measures should the security engineer suggest?</p>",
          "answers": [
            "<p>Enable Amazon RDS encryption using AWS CloudHSM keys for the database and snapshots. Implement TLS connections to the Amazon EBS volumes and RDS instance. Import the credentials in AWS KMS and enable rotation every 365 days.</p>",
            "<p>Implement encryption at rest for the Amazon EBS volumes attached to EC2 instances. Encrypt the RDS database and use a TLS secured connection. Store the database credentials in AWS Secrets Manager and configure automatic rotation.</p>",
            "<p>Enable Amazon RDS encryption using AWS KMS keys for the database and snapshots. Implement encryption at rest for Amazon EBS volumes. Use an ACM certificate for TLS connectivity to the database instance and use Systems Manager Parameter store to automatically rotate credentials.</p>",
            "<p>Implement encryption at rest for the Amazon EBS volumes attached to EC2 instances. Encrypt the RDS database and use a TLS secured connection. Store the database credentials in instance metadata and automatically rotate with AWS Lambda.</p>"
          ],
          "explanation": "<p>The key requirements are to ensure data protection for the application running on EC2 and RDS and to ensure credentials are automatically rotated. For data protection we must ensure data is protected both in transit and at rest.</p><p>At rest encryption can be implemented for Amazon EBS volumes and RDS databases. Whether we use the default AWS managed keys or keys managed in AWS KMS is not important in this scenario if encryption is enabled.</p><p>Encryption in transit is not relevant to Amazon EBS as the volumes are attached at the block level and all data is encrypted in transit by default. For RDS we can use a TLS certificate to protect the data in transit.</p><p>Lastly, rotation of credentials can be implemented by using Secrets Manager which allows RDS database credentials to be rotated automatically without any custom development.</p><p><strong>CORRECT: </strong>\"Implement encryption at rest for the Amazon EBS volumes attached to EC2 instances. Encrypt the RDS database and use a TLS secured connection. Store the database credentials in AWS Secrets Manager and configure automatic rotation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement encryption at rest for the Amazon EBS volumes attached to EC2 instances. Encrypt the RDS database and use a TLS secured connection. Store the database credentials in instance metadata and automatically rotate with AWS Lambda\" is incorrect.</p><p>Credentials should not be stored in instance metadata as this is insecure.</p><p><strong>INCORRECT:</strong> \"Enable Amazon RDS encryption using AWS CloudHSM keys for the database and snapshots. Implement TLS connections to the Amazon EBS volumes and RDS instance. Import the credentials in AWS KMS and enable rotation every 365 days\" is incorrect.</p><p>KMS cannot be used for storing and rotating database credentials, it works only for encryption keys.</p><p><strong>INCORRECT:</strong> \"Enable Amazon RDS encryption using AWS KMS keys for the database and snapshots. Implement encryption at rest for Amazon EBS volumes. Use an ACM certificate for TLS connectivity to the database instance and use Systems Manager Parameter store to automatically rotate credentials\" is incorrect.</p><p>Systems Manager Parameter Store does not automatically rotate credentials, you must write your own Lambda function which increases operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html</a></p>"
        }
      },
      {
        "id": 83960198,
        "correct_response": [
          "a",
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The development team at a company deploys to their AWS production environment through a continuous integration/continuous deployment (CI/CD) pipeline. The pipeline itself has broad access to create AWS resources needed to run the application. The company's security team wants to allow the development team to deploy their own IAM principals and policies for their application. However, the security team also needs a control mechanism that requires all resources created by the pipeline to have minimum privileges that comply with the security guidelines. All teams at the company are only allowed to modify the AWS production environment through their CI/CD pipeline.</p>\n\n<p>Which options will you combine to address this use case? (Select two)</p>\n",
          "answers": [
            "<p>Create an IAM role for the CI/CD pipeline to be used for deploying application resources</p>",
            "<p>Create an IAM role for the CI/CD pipeline to be used for deploying application resources. Also, create resource-based policies for all the AWS resources created by the CI/CD pipeline</p>",
            "<p>The development team should create a permissions boundary policy and attach it to the IAM role used by the CI/CD pipeline</p>",
            "<p>The security team should create a permissions boundary policy and attach it to the IAM role used by the CI/CD pipeline</p>",
            "<p>Create a Service Control Policy (SCP) and attach it to all the member accounts to monitor and control the access privileges given to the IAM roles in the AWS accounts</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an IAM role for the CI/CD pipeline to be used for deploying application resources</strong></p>\n\n<p>The CI/CD pipeline role has broad access to the account to create resources. Access for deployment through the CI/CD pipeline should be tightly controlled and monitored. The CI/CD pipeline is allowed to create new IAM roles for use with the application, but those roles are limited to only the actions allowed by the permissions boundary.</p>\n\n<p><strong>The security team should create a permissions boundary policy and attach it to the IAM role used by the CI/CD pipeline</strong></p>\n\n<p>A permissions boundary is a type of identity-based policy that doesn\u2019t directly grant access. Instead, like an SCP, a permissions boundary acts as a guardrail for your IAM principals that allows you to set coarse-grained access controls. A permissions boundary is typically used to delegate the creation of IAM principals. Delegation enables other individuals in your accounts to create new IAM principals but limits the permissions that can be granted to the new IAM principals.</p>\n\n<p>An example of the permissions boundary policy that the security team should attach to IAM roles created by the CI/CD pipeline is shown below. This same permissions boundary policy can be centrally managed and attached to IAM roles created by other pipelines at Financial Corp. The policy describes the maximum possible permissions that additional roles created by the development team are allowed to have, and it limits those permissions to some Amazon S3 and Amazon SQS data access actions. It\u2019s common for a permissions boundary policy to include data access actions when used to delegate role creation. This is because most applications only need permission to read and write data and only sometimes need permission to modify infrastructure.</p>\n\n<p>The roles, policies, and EC2 instance profiles that the pipeline creates should also be restricted to specific role paths. This enables you to enforce that the pipeline can only modify roles and policies or pass roles that it has created. This helps prevent the pipeline, and roles created by the pipeline, from elevating privileges by modifying or passing a more privileged role.</p>\n\n<p>Example permissions boundary policy attached to IAM roles created by the CI/CD pipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q49-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/\">https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM role for the CI/CD pipeline to be used for deploying application resources Also, create resource-based policies for all the AWS resources created by the CI/CD pipeline</strong> - Access can be granted by either an identity-based policy or a resource-based policy when access is within the same AWS account. Therefore, using resource-based policies for the given use case is unnecessary. You should also note that all AWS resources do not support resource-based policies. This option acts as a distractor.</p>\n\n<p><strong>The development team should create a permissions boundary policy and attach it to all the IAM roles created by the CI/CD pipeline</strong> - Permission boundary policy should be created by the security team for central access and exercising control over the permissions configured by other teams.</p>\n\n<p><strong>Create a Service Control Policy (SCP) and attach it to all the member accounts to monitor and control the access privileges given to the IAM roles in the AWS accounts</strong> - Service control policies (SCPs) are a feature of AWS Organizations. AWS Organizations is a service for grouping and centrally managing the AWS accounts that your business owns. SCPs are policies that specify the maximum permissions for an organization, organizational unit (OU), or individual account. An SCP can limit permissions for principals in member accounts, including the AWS account root user. This option has been added as a distractor since the use case does not mention anything about using AWS Organizations.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/\">https://aws.amazon.com/blogs/security/iam-policy-types-how-and-when-to-use-them/</a></p>\n"
        }
      },
      {
        "id": 112083281,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has two AWS accounts: A production account and a development account. Developers with user accounts in the production account need to be able to access artifacts stored in an Amazon S3 bucket in the development account when deploying resources.</p><p>A cross-account role has been created in the development account with access to the S3 bucket. The security team requires that the users can assume the role only if they are authenticated with multi-factor authentication (MFA).</p><p>Which step should the security engineer take to meet these requirements?</p>",
          "answers": [
            "<p>Add a aws:MultiFactorAuthPresent : true condition to the role\u2019s permissions policy.</p>",
            "<p>Add a aws:MultiFactorAuthPresent : true condition to the session policy.</p>",
            "<p>Add a aws:MultiFactorAuthPresent : true condition to the S3 bucket policy.</p>",
            "<p>Add a aws:MultiFactorAuthPresent : true condition to the role's trust policy.</p>"
          ],
          "explanation": "<p>A cross-account role has been created and must be assumed by the developers to access the S3 bucket. To enforce a restriction that sessions must be authenticated using MFA, there are two options. Either you can configure the bucket policy, or you can configure the trust policy of the role (or both).</p><p>In this case the requirement is to ensure that users can assume the role only if they are authenticated with MFA. Therefore, the best place to add the condition is in the trust policy of the role. If the user is not authenticated with MFA, they will not be able to assume the role.</p><p>An example role trust policy is provided below:</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_03-09-28-e174abd7c53e95bf1feceb9013ca6cd9.jpg\"></p><p><strong>CORRECT: </strong>\"Add an aws:MultiFactorAuthPresent : true condition to the role's trust policy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add an aws:MultiFactorAuthPresent : true condition to the role\u2019s permissions policy\" is incorrect.</p><p>The trust policy defines who is allowed to assume the role. The permissions policy defines the permissions they are granted.</p><p><strong>INCORRECT:</strong> \"Add an aws:MultiFactorAuthPresent : true condition to the session policy\" is incorrect.</p><p>Session policies limit the permissions that the role or user's identity-based policies grant to the session. The role\u2019s trust policy defines who is allowed to assume the role</p><p><strong>INCORRECT:</strong> \"Add an aws:MultiFactorAuthPresent : true condition to the S3 bucket policy\" is incorrect.</p><p>This will restrict access to the bucket if the user is not authenticated with MFA. However, the requirement is to prevent the user from assuming the role if they are not authenticated with MFA.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-do-i-protect-cross-account-access-using-mfa-2/\">https://aws.amazon.com/blogs/security/how-do-i-protect-cross-account-access-using-mfa-2/</a></p>"
        }
      },
      {
        "id": 76398914,
        "correct_response": [
          "a",
          "c",
          "d"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has several AWS accounts that use a combination of the following identity provider:</p><p>\u00b7 Users in AWS Identity and Access Management (IAM)</p><p>\u00b7 Federated sign-in with Active Directory and IAM</p><p>\u00b7 Users in Amazon Cognito user pools</p><p>The company security team requires that password policies are configured for all identity providers to require a minimum password length and password complexity.</p><p>Which configuration settings should the company update? (Select THREE.)</p>",
          "answers": [
            "<p>Configure a password policy in Active Directory for the federation scenario.</p>",
            "<p>Configure an IAM password policy for the federation scenario.</p>",
            "<p>Configure an IAM password policy for the IAM user scenario.</p>",
            "<p>Configure a password policy in the Amazon Cognito user pool.</p>",
            "<p>Configure a password policy in an Amazon Cognito identity pool.</p>",
            "<p>Configure a password policy in AWS Organizations for the IAM user scenario.</p>"
          ],
          "explanation": "<p>This question simply requires an understanding of where the relevant password policies should be configured:</p><p>\u00b7 When using federation with Active Directory and AWS IAM the user account is created and managed in Active Directory so the password policy should be configured there.</p><p>\u00b7 When using AWS IAM to create user accounts you can specify a password policy in IAM.</p><p>\u00b7 When using Amazon Cognito user pools to create user accounts you can specify a password policy within the user pool.</p><p><strong>CORRECT: </strong>\"Configure a password policy in Active Directory for the federation scenario\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure an IAM password policy for the IAM user scenario\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure a password policy in the Amazon Cognito user pool\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure an IAM password policy for the federation scenario\" is incorrect.</p><p>In a federation scenario the user account is not in IAM so the password policy in IAM would not affect the users.</p><p><strong>INCORRECT:</strong> \"Configure a password policy in an Amazon Cognito identity pool\" is incorrect.</p><p>An identity pool is different to a user pool and is used to gain temporary security credentials through IAM roles. A user pool can be configured as an identity provider with user accounts that are created within the pool.</p><p><strong>INCORRECT:</strong> \"Configure a password policy in AWS Organizations for the IAM user scenario\" is incorrect.</p><p>AWS Organizations cannot be used for creating password policies for AWS IAM.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-policies.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-policies.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/\">https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/</a></p>"
        }
      },
      {
        "id": 76165454,
        "correct_response": [
          "d"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The security team at an e-commerce company has noticed that several Amazon Elastic Block Store (Amazon EBS) volumes are not encrypted. These unencrypted EBS volumes are attached to Amazon EC2 instances that are provisioned with an Auto Scaling group and a launch template. You have been hired as an AWS Certified Security Specialist to implement a solution that ensures all EBS volumes are encrypted both now and in the future.</p>\n\n<p>What would you recommend?</p>\n",
          "answers": [
            "<p>Configure a new launch template from the existing launch template, such that the encrypted flag for all EBS volumes is set to true in the new launch template. Update the Auto Scaling group to use the new launch template. In due course of time, let the Auto Scaling group replace all the old instances that have unencrypted EBS volumes</p>",
            "<p>Modify the launch template by setting the encrypted flag for all EBS volumes to true. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</p>",
            "<p>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Propagate this setting to the Auto Scaling group so it will automatically replace existing instances with new instances</p>",
            "<p>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</strong></p>\n\n<p>You can configure your AWS account to enforce the encryption of the new EBS volumes and snapshot copies that you create. For example, Amazon EBS encrypts the EBS volumes created when you launch an instance and the snapshots that you copy from an unencrypted snapshot. You should note that encryption by default does not affect existing EBS volumes or snapshots.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q63-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/</a></p>\n\n<p>For the given use case, you can use the instance refresh feature of an Auto Scaling group to update the instances in your Auto Scaling group instead of manually replacing instances a few at a time. This can be useful when a configuration change requires you to replace instances, and you have a large number of instances in your Auto Scaling group. Amazon EC2 Auto Scaling starts performing a rolling replacement of the instances. It takes a set of instances out of service, terminates them, and launches a set of instances with the new desired configuration. Then, it waits until the instances pass your health checks and complete warmup before it moves on to replacing other instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new launch template from the existing launch template, such that the encrypted flag for all EBS volumes is set to true in the new launch template. Update the Auto Scaling group to use the new launch template. In due course of time, let the Auto Scaling group replace all the existing instances that have unencrypted EBS volumes</strong> - Since the given use case requires all EBS volumes to be encrypted both now and in the future, so you cannot let the Auto Scaling group to replace all the existing instances (with unencrypted EBS volumes) in due course of time. Hence this option is incorrect.</p>\n\n<p><strong>Modify the launch template by setting the encrypted flag for all EBS volumes to true. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</strong> - You cannot modify a launch template, rather you need to create a new version of the launch template and then leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances. This option has been added as a distractor.</p>\n\n<p><strong>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Propagate this setting to the Auto Scaling group so it will automatically replace existing instances with new instances</strong> - This option acts as a distractor, as there is no such setting in the Auto Scaling group that propagates the automatic encryption of new EBS volumes by default.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ebs-automatic-encryption/\">https://aws.amazon.com/premiumsupport/knowledge-center/ebs-automatic-encryption/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-instance-refresh.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-instance-refresh.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/manage-launch-template-versions.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/manage-launch-template-versions.html</a></p>\n"
        }
      },
      {
        "id": 99528331,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). Amazon CloudFront is used as the front-end of the application an AWS WAF is used to protect the front-end with the AWS Managed Rules rule group.</p><p>A security architect is concerned that the infrastructure is vulnerable to layer 7 DDoS attacks. What improvements can be made to the solution to protect against this type of attack?</p>",
          "answers": [
            "<p>Configure a Lambda@Edge function that imposes a rate limit on CloudFront viewer requests and blocks traffic that exceeds the limits.</p>",
            "<p>Configure a rate-based rule on AWS WAF that puts a temporary block on requests from IP addresses that send excessive requests.</p>",
            "<p>Configure an IP set match rule on AWS WAF that blocks web requests based on the IP address of the web request origin.</p>",
            "<p>Configure field-level encryption for the distribution and upload an SSL/TLS certificate from Amazon Certificate Manager (ACM).</p>"
          ],
          "explanation": "<p>A rate-based rule tracks the rate of requests for each originating IP address and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span. You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests. By default, AWS WAF aggregates requests based on the IP address from the web request origin, but you can configure the rule to use an IP address from an HTTP header, like X-Forwarded-For, instead.</p><p>This rule can help prevent layer 7 DDoS attacks as the IP addresses of bots would be automatically blocked once they exceed the rate defined.</p><p><strong>CORRECT: </strong>\"Configure a rate-based rule on AWS WAF that puts a temporary block on requests from IP addresses that send excessive requests\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure a Lambda@Edge function that imposes a rate limit on CloudFront viewer requests and blocks traffic that exceeds the limits\" is incorrect.</p><p>AWS WAF can do this using rate-based rules and is much better suited to the job than writing your own custom code and running it using Lambda.</p><p><strong>INCORRECT:</strong> \"Configure an IP set match rule on AWS WAF that blocks web requests based on the IP address of the web request origin\" is incorrect.</p><p>An IP set match rule uses a list of known IP addresses. With a DDoS attack you don\u2019t know the IP addresses of the bots ahead of time so this would not be effective.</p><p><strong>INCORRECT:</strong> \"Configure field-level encryption for the distribution and upload an SSL/TLS certificate from Amazon Certificate Manager (ACM)\" is incorrect.</p><p>Field level encryption adds protection for certain data in transit and is not useful for protecting against DDoS attacks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html</a></p>"
        }
      },
      {
        "id": 83960142,
        "correct_response": [
          "b",
          "d",
          "f"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has decided to revamp the security for its IT infrastructure and tighten rules for access to AWS resources across the organization. In this context, a Security Engineer has been tasked with creating optimal access credentials/permissions for the company's applications to access the required resources. Some of these applications will run on EC2 instances and need cross-account access privileges for resources present in another AWS account. The company also maintains a few mobile applications that need to access AWS resources.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend as the best practices to configure access credentials/permissions for these applications? (Select three)</p>\n",
          "answers": [
            "<p>Use access keys to provide long-term credentials to AWS for an application running on Amazon EC2 instance. Encrypt the keys to avoid exposure to the internet. Rotate access keys periodically</p>",
            "<p>Use an IAM role to establish trust between accounts, and then grant users in one account limited permissions to access the trusted account</p>",
            "<p>Embed access keys with the mobile application and store them in encrypted storage to avoid exposure. As an added layer of security, you can add envelope encryption to the encrypted access keys</p>",
            "<p>Use Amazon Cognito to manage user identities in your mobile application. You can then use the Amazon Cognito credentials provider to manage credentials that your application uses to make requests to access AWS resources</p>",
            "<p>Create long-term access keys associated with AWS account IAM user and use them to provide access to an application running on EC2 instance. Since the instances run on safe private subnets on AWS Cloud, the long-term credentials are a perfect fit for this scenario with no overhead of creating and maintaining short-term credentials. It is to be noted that long-term access keys should not be associated with the AWS root user account</p>",
            "<p>Define an IAM role that has appropriate permissions for the application and launch the Amazon EC2 instance with this role associated with the instance</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Use an IAM role to establish trust between accounts, and then grant users in one account limited permissions to access the trusted account</strong></p>\n\n<p>Use an IAM role to establish trust between accounts, and then grant users in one account limited permissions to access the trusted account. You share resources in one account with users in a different account. By setting up cross-account access in this way, you don't have to create individual IAM users in each account. In addition, users don't have to sign out of one account and sign into another to access resources in different AWS accounts.</p>\n\n<p><strong>Use Amazon Cognito to manage user identities in your mobile application. You can then use the Amazon Cognito credentials provider to manage credentials that your application uses to make requests to access AWS resources</strong></p>\n\n<p>Don't embed access keys with the app, even in encrypted storage. Instead, use Amazon Cognito to manage user identities in your app. This service lets you authenticate users using Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)\u2013compatible identity provider. You can then use the Amazon Cognito credentials provider to manage the credentials that your app uses to make requests to access AWS resources.</p>\n\n<p><strong>Define an IAM role that has appropriate permissions for the application and launch the Amazon EC2 instance with this role associated with the instance</strong></p>\n\n<p>Don't use access keys directly in your application. Don't pass access keys to the application, embed them in the application, or let the application read access keys from any source. Instead, define an IAM role that has appropriate permissions for your application and launch the Amazon Elastic Compute Cloud (Amazon EC2) instance with this role associated with the instance. This practice also enables the application to get temporary security credentials that it can, in turn, use to make programmatic calls to AWS. The AWS SDKs and the AWS Command Line Interface (AWS CLI) can get temporary credentials from the role automatically.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create long-term access keys associated with the AWS account IAM user and use them to provide access to an application running on an EC2 instance. Since the instances run on safe private subnets on AWS Cloud, the long-term credentials are a perfect fit for this scenario with no overhead of creating and maintaining short-term credentials. It is to be noted that long-term access keys should not be associated with the AWS root user account</strong></p>\n\n<p><strong>Use access keys to provide long-term credentials to AWS for an application running on an Amazon EC2 instance. Encrypt the keys to avoid exposure to the internet. Rotate access keys periodically</strong></p>\n\n<p>These two options are incorrect from a security standpoint. Long-term access keys, such as those associated with IAM users and AWS account root users, remain valid until you manually revoke them. However, temporary security credentials obtained through IAM roles and other features of the AWS Security Token Service expire after a short period of time. AWS suggests using temporary security credentials to help reduce the risk in case credentials are accidentally exposed.</p>\n\n<p><strong>Embed access keys with the mobile application and store them in encrypted storage to avoid exposure. As an added layer of security, you can add envelope encryption to the encrypted access keys</strong> - Don't embed access keys with the app, even in encrypted storage. This is considered a security bad practice.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/accounts/latest/reference/credentials-access-keys-best-practices.html\">https://docs.aws.amazon.com/accounts/latest/reference/credentials-access-keys-best-practices.html</a></p>\n"
        }
      },
      {
        "id": 76398912,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A bespoke application consisting of three tiers is being deployed in a VPC. You need to create three security groups. You have configured the WebSG (web server) security group and now need to configure the AppSG (application tier) and DBSG (database tier). The application runs on port 1030 and the database runs on 3306.</p><p>Which rules should be created according to security best practice? (Select TWO.)</p>",
          "answers": [
            "<p>On the AppSG security group, create a custom TCP rule for TCP 1030 and configure the DBSG security group as the source.</p>",
            "<p>On the AppSG security group, create a custom TCP rule for TCP 1030 and configure the WebSG security group as the source.</p>",
            "<p>On the DBSG security group, create a custom TCP rule for TCP 3306 and configure the AppSG security group as the source.</p>",
            "<p>On the DBSG security group, create a custom TCP rule for TCP 3306 and configure the WebSG security group as the source.</p>",
            "<p>On the WebSG security group, create a custom TCP rule for TCP 1030 and configure the AppSG security group as the source.</p>"
          ],
          "explanation": "<p>With security groups rules are always allow rules. The best practice is to configure the source as another security group which is attached to the EC2 instances that traffic will come from. In this case you need to configure a rule that allows TCP 1030 and configure the source as the web server security group (WebSG).</p><p>This allows traffic from the web servers to reach the application servers. You then need to allow communications on port 3306 (MYSQL/Aurora) from the AppSG security group to enable access to the database from the application servers.</p><p><strong>CORRECT: </strong>\"On the DBSG security group, create a custom TCP rule for TCP 3306 and configure the AppSG security group as the source\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"On the AppSG security group, create a custom TCP rule for TCP 1030 and configure the WebSG security group as the source\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"On the AppSG security group, create a custom TCP rule for TCP 1030 and configure the DBSG security group as the source\" is incorrect.</p><p>The app tier will receive traffic from the web tier.</p><p><strong>INCORRECT:</strong> \"On the DBSG security group, create a custom TCP rule for TCP 3306 and configure the WebSG security group as the source\" is incorrect.</p><p>The databases will be receiving traffic from the app servers.</p><p><strong>INCORRECT:</strong> \"On the WebSG security group, create a custom TCP rule for TCP 1030 and configure the AppSG security group as the source\" is incorrect.</p><p>The web service will be receiving traffic from internet, presumably on standard HTTP/HTTPS ports. This web server security group has already been configured.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>"
        }
      },
      {
        "id": 99528287,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A security engineer created an Amazon S3 bucket and attached the following bucket policy.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question/2023-01-04_02-13-58-26157aa0edcfdbf06a08442548015df5.jpg\"><p>What is the effect of this bucket policy?</p>",
          "answers": [
            "<p>The specified users are granted all S3 permissions to the bucket and objects within the bucket regardless of IAM user policies and ACLs.</p>",
            "<p>The specified users are denied all S3 permissions to the bucket and objects within the bucket regardless of IAM user policies and ACLs.</p>",
            "<p>The specified users are not denied S3 permissions but must be granted permissions through IAM user policies or ACLs.</p>",
            "<p>The specified account will be denied all S3 permissions but specified users from other accounts are excepted.</p>"
          ],
          "explanation": "<p>The first statement in the bucket policy denies all S3 API actions for the AWS account specified with the Effect, Action, and Principal elements. This blocks all access to the account\u2019s IAM principals, regardless of the IAM policy attached to those users or roles.</p><p>This policy prevents unintended access to the secure S3 bucket that could be introduced by IAM policies or ACLs. The policy then uses a Condition element to make exceptions for the allowed principals.</p><p>The Deny statement only applies when the principal whose access is being evaluated is <em>not</em> one of the specified users.</p><p><br></p><p><strong>CORRECT: </strong>\"The specified users are not denied S3 permissions but must be granted permissions through IAM user policies or ACLs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The specified users are granted all S3 permissions to the bucket and objects within the bucket regardless of IAM user policies and ACLs\" is incorrect.</p><p>The policy does not grant any permissions, it simply does not deny any permissions for the specified users. The users will need to have permissions granted elsewhere.</p><p><strong>INCORRECT:</strong> \"The specified users are denied all S3 permissions to the bucket and objects within the bucket regardless of IAM user policies and ACLs\" is incorrect.</p><p>The specified users are not denied permissions based on the Condition element in the policy.</p><p><strong>INCORRECT:</strong> \"The specified account will be denied all S3 permissions but specified users from other accounts are excepted\" is incorrect.</p><p>The policy denies permissions for all users except those specified within the account specified. It does not grant or deny permissions for other AWS accounts.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-create-a-policy-that-whitelists-access-to-sensitive-amazon-s3-buckets/\">https://aws.amazon.com/blogs/security/how-to-create-a-policy-that-whitelists-access-to-sensitive-amazon-s3-buckets/</a></p>"
        }
      },
      {
        "id": 112083275,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A security engineer was reviewing AWS KMS key policies and found this statement in several key policies within the AWS account.</p><p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question/2023-01-05_03-03-35-a7a32145359cd1d49f285e8bbfad3dc3.jpg\"></p><p>What is the purpose of this statement?</p>",
          "answers": [
            "<p>Enables IAM policies in the 554422336677 account to allow access to the key.</p>",
            "<p>Restricts key usage to the root user in account 554422336677.</p>",
            "<p>Allows all principals from account 554422336677 to use the key with Amazon S3.</p>",
            "<p>Enables the root user to grant key usage permissions to principals in account 554422336677.</p>"
          ],
          "explanation": "<p>Key policies are the primary way to control access to AWS KMS keys. Every KMS key must have exactly one key policy. The statements in the key policy document determine who has permission to use the KMS key and how they can use it. You can also use IAM policies and grants to control access to the KMS key, but every KMS key must have a key policy.</p><p>By default, a policy statement like this one in this question is present in the key policy document when you create a new KMS key with the AWS Management Console. It is also present when you create a new KMS key programmatically but do not provide a key policy.</p><p>A key policy document with a statement that allows access to the AWS account (root user) enables IAM policies in the account to allow access to the KMS key. This means that IAM users and roles in the account might have access to the KMS key even if they are not explicitly listed as principals in the key policy document. It is therefore important to check IAM policies for grants.</p><p><strong>CORRECT: </strong>\"Enables IAM policies in the 554422336677 account to allow access to the key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Restricts key usage to the root user in account 554422336677\" is incorrect.</p><p>The root user will be granted access to the key and IAM users <em>may</em> have access if an IAM policy grants it to them.</p><p><strong>INCORRECT:</strong> \"Allows all principals from account 554422336677 to use the key with Amazon S3\" is incorrect.</p><p>Amazon S3 is not mentioned at all and as explained previously all principals will not be granted access based on the key policy.</p><p><strong>INCORRECT:</strong> \"Enables the root user to grant key usage permissions to principals in account 554422336677\" is incorrect.</p><p>This is not the purpose of this key policy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/determining-access-key-policy.html\">https://docs.aws.amazon.com/kms/latest/developerguide/determining-access-key-policy.html</a></p>"
        }
      },
      {
        "id": 76165430,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application hosted on an Amazon EC2 instance writes its request logs, availability logs, and threat logs to a text file. This file is read by a custom program to track and process any security issues inferred from the logs. An increase in log data has resulted in the malfunctioning of the custom program. The company is looking at a scalable solution to collect and analyze log files.</p>\n\n<p>Which design will ensure that the aforementioned criteria are met with the LEAST amount of effort?</p>\n",
          "answers": [
            "<p>Create a scheduled process to copy the application log files to AWS CloudTrail. Configure a Lambda function that processes CloudTrail logs and sends an SNS notification whenever a log file is created</p>",
            "<p>Configure Amazon Inspector to collect all log files from the EC2 instance. Use Amazon EventBridge integration with Amazon Inspector to trigger Lambda function that can read the logs and raise notifications for events on security</p>",
            "<p>Install and configure the unified CloudWatch agent on the application's EC2 instance. Create a CloudWatch metric filter to monitor the application logs. Configure CloudWatch alerts based on these metrics</p>",
            "<p>Create a cron job on the Amazon EC2 instance to copy the logs into the Amazon S3 bucket. Use S3 events to trigger a Lambda function that refreshes Amazon CloudWatch metrics with the log data. Set up CloudWatch alerts based on the metrics</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Install and configure the unified CloudWatch agent on the application's EC2 instance. Create a CloudWatch metric filter to monitor the application logs. Configure CloudWatch alerts based on these metrics</strong></p>\n\n<p>The unified CloudWatch agent enables you to do the following:</p>\n\n<ol>\n<li><p>Collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in the Metrics collected by the CloudWatch agent.</p></li>\n<li><p>Collect system-level metrics from on-premises servers.</p></li>\n<li><p>Retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collected</code> protocols.</p></li>\n<li><p>Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p></li>\n</ol>\n\n<p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs, just like logs collected by the older CloudWatch Logs agent.</p>\n\n<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a scheduled process to copy the application log files to AWS CloudTrail. Configure a Lambda function that processes CloudTrail logs and sends an SNS notification whenever a log file is created</strong> - This option is invalid since the logs for the given use case cannot be imported or written into CloudTrail.</p>\n\n<p><strong>Configure Amazon Inspector to collect all log files from the EC2 instance. Use Amazon EventBridge integration with Amazon Inspector to trigger Lambda function that can read the logs and raise notifications for events on security</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. Amazon Inspector is a very useful tool, but it cannot process log files as described in the given use case.</p>\n\n<p><strong>Create a cron job on the Amazon EC2 instance to copy the logs into the Amazon S3 bucket. Use S3 events to trigger a Lambda function that refreshes Amazon CloudWatch metrics with the log data. Set up CloudWatch alerts based on the metrics</strong> - Although this solution is certainly feasible, however, it is not an optimal solution when compared to directly using the unified CloudWatch agent.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n"
        }
      },
      {
        "id": 99528291,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An attack left several Amazon EC2 Windows instances unresponsive. A security engineer has been asked to collect any memory dumps that may exist on the EC2 instances attached Amazon EBS volumes.</p><p>How should the security collect memory dumps for forensic analysis?</p>",
          "answers": [
            "<p>Install the SSM agent and stream log data to Amazon CloudWatch Logs.</p>",
            "<p>Run the EC2Rescue CLI using the /online mode and specify the instance ID.</p>",
            "<p>Reboot the EC2 Windows Server, enter safe mode, and select memory dump.</p>",
            "<p>Run the EC2Rescue CLI using the /offline mode and specify the device ID.</p>"
          ],
          "explanation": "<p>The EC2Rescue for Windows Server command line interface (CLI) allows you to run an EC2Rescue for Windows Server plugin (referred as an \"action\") programmatically.</p><p>The EC2Rescue for Windows Server tool has two execution modes:</p><ul><li><p><strong>/online</strong>\u2014This allows you to take action on the instance that EC2Rescue for Windows Server is installed on, such as collect log files.</p></li><li><p><strong>/offline:&lt;device_id&gt;</strong>\u2014This allows you to take action on the offline root volume that is attached to a separate Amazon EC2 Windows instance, on which you have installed EC2Rescue for Windows Server.</p></li></ul><p>In this case the instances are unresponsive so the /offline mode must be used and the device ID of the EBS volumes must be specified when running the CLI command.</p><p><strong>CORRECT: </strong>\"Run the EC2Rescue CLI using the /offline mode and specify the device ID\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Run the EC2Rescue CLI using the /online mode and specify the instance ID\" is incorrect.</p><p>Online mode cannot be used on an unresponsive instance as the commands are run on the instance itself. You also do not specify the instance ID as the commands are run locally.</p><p><strong>INCORRECT:</strong> \"Install the SSM agent and stream log data to Amazon CloudWatch Logs\" is incorrect.</p><p>You cannot install the SSM agent on an unresponsive instance and it does not collect memory dumps.</p><p><strong>INCORRECT:</strong> \"Reboot the EC2 Windows Server, enter safe mode, and select memory dump\" is incorrect.</p><p>This is not a valid method of exporting memory dumps from a Windows server.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2rw-cli.html\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2rw-cli.html</a></p>"
        }
      },
      {
        "id": 83960208,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The security team at a retail company utilizes Amazon EventBridge to monitor Amazon S3 objects, aiming to detect public access and any other changes in S3 bucket policies/settings that result in public access. They configure EventBridge to watch specific CloudTrail API calls (s3:PutObjectAcl, s3:DeleteBucketPolicy, and s3:PutBucketPolicy) and use Amazon SNS for immediate email notifications.</p>\n\n<p>However, during development, the team finds that s3:PutObjectAcl doesn't trigger an EventBridge event, while the other two do. CloudTrail for AWS management events is enabled with a basic configuration in the relevant region, and EventBridge pattern verification is correct.</p>\n\n<p>The team needs a solution to ensure s3:PutObjectAcl triggers an EventBridge event without generating false notifications. What is the appropriate solution for this scenario?</p>\n",
          "answers": [
            "<p>Change the EventBridge event pattern by selecting Amazon S3. Select All Events as the event type</p>",
            "<p>Change the EventBridge event pattern by selecting Amazon S3. Select Data Events as the event type</p>",
            "<p>Enable CloudTrail to monitor data events for read and write operations for S3 buckets</p>",
            "<p>Enable CloudTrail to monitor insights events for S3 buckets</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable CloudTrail to monitor data events for read and write operations for S3 buckets</strong></p>\n\n<p>By default, CloudTrail logs S3 bucket-level API calls that were made in the last 90 days, but not log requests made to objects. Bucket-level calls include events such as CreateBucket, DeleteBucket, PutBucketLifecycle, PutBucketPolicy, DeleteBucketPolicy, and so on. You can see bucket-level events on the CloudTrail console. However, you can't view data events (Amazon S3 object-level calls) there\u2014you must parse or query CloudTrail logs for them.</p>\n\n<p>You can also get CloudTrail logs for object-level Amazon S3 actions. To do this, enable data events for your S3 bucket or all buckets in your account. When an object-level action occurs in your account, CloudTrail evaluates your trail settings. If the event matches the object that you specified in a trail, the event is logged. This would take care of the missing PutObjectAcl API call-specific event in EventBridge.</p>\n\n<p>Therefore, for the given use case, you need to enable CloudTrail to monitor data events for read and write operations for S3 buckets.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the EventBridge event pattern by selecting Amazon S3. Select All Events as the event type</strong></p>\n\n<p><strong>Change the EventBridge event pattern by selecting Amazon S3. Select Data Events as the event type</strong></p>\n\n<p>Amazon S3 can send events to Amazon EventBridge whenever certain events happen in your bucket. Unlike other destinations, you don't need to select which event types you want to deliver. You can use EventBridge rules to route events to additional targets.</p>\n\n<p>Using EventBridge with Amazon S3:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q54-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html</a></p>\n\n<p>So, you can see that the relevant bucket-related events (s3:DeleteBucketPolicy, and s3:PutBucketPolicy) are not captured in Eventbridge. So both these options are incorrect.</p>\n\n<p><strong>Enable CloudTrail to monitor insights events for S3 buckets</strong> - CloudTrail Insights events capture unusual API call rate or error rate activity in your AWS account by analyzing CloudTrail management activity. Insights events provide relevant information, such as the associated API, error code, incident time, and statistics, that help you understand and act on unusual activity. This option acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/ev-mapping-troubleshooting.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ev-mapping-troubleshooting.html</a></p>\n"
        }
      },
      {
        "id": 73686870,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has started to deploy resources to the AWS cloud. Initial resources have been deployed in the US West (Oregon) Region and an AWS CloudTrail trail has been created to record API activity in a bucket in the same Region.</p><p>The security team requires that API activity is captured from all Regions and stored in a central Region.</p><p>What is the SIMPLEST way to meet these requirements?</p>",
          "answers": [
            "<p>Create individual trails for each region and capture API activity in a single central Amazon S3 bucket.</p>",
            "<p>Create a new trail that applies to all regions and capture API activity in separate central S3 buckets for each region.</p>",
            "<p>Create individual trails for each region and capture API activity in separate central S3 buckets for each region.</p>",
            "<p>Change the existing single-region trail to log all regions and capture API activity in a single central Amazon S3 bucket.</p>"
          ],
          "explanation": "<p>You can configure CloudTrail to deliver log files from multiple regions to a single S3 bucket for a single account. For example, you have a trail in the US West (Oregon) Region that is configured to deliver log files to an S3 bucket.</p><p>When you change an existing single-region trail to log all regions, CloudTrail logs events from all regions in your account. CloudTrail delivers log files to the same S3 bucket.</p><p>If CloudTrail has permissions to write to an S3 bucket, the bucket for a multi-region trail does not have to be in the trail's home region.</p><p><strong>CORRECT: </strong>\"Change the existing single-region trail to log all regions and capture API activity in a single central Amazon S3 bucket\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create individual trails for each region and capture API activity in a single central Amazon S3 bucket\" is incorrect.</p><p>The simplest solution is to use a single trail configured for all regions.</p><p><strong>INCORRECT:</strong> \"Create a new trail that applies to all regions and capture API activity in separate central S3 buckets for each region\" is incorrect.</p><p>When you create a trail that applies to all regions you specify a single S3 bucket.</p><p><strong>INCORRECT:</strong> \"Create individual trails for each region and capture API activity in separate central S3 buckets for each region\" is incorrect.</p><p>This is the most complex solution, so it does not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html</a></p>"
        }
      },
      {
        "id": 83960108,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A financial services company is running an Amazon RDS for MySQL DB instance in a virtual private cloud (VPC) to store sensitive customer data. Due to strict security policies, the company has implemented a VPC that does not allow any network traffic to or from the internet. A security engineer at the company wants to use AWS Secrets Manager to automatically rotate the DB instance credentials for increased security. However, due to the company's security policy, the engineer is not allowed to use the standard AWS Lambda function provided by Secrets Manager to rotate the credentials.</p>\n\n<p>To address this issue, the security engineer deploys a custom Lambda function within the VPC. This function is responsible for rotating the secret in Secrets Manager. The security engineer also edits the DB instance's security group to allow connections from this custom Lambda function. However, when the function is invoked, it is unable to communicate with Secrets Manager and cannot rotate the secret.</p>\n\n<p>Which of the following options will address the given scenario?</p>\n",
          "answers": [
            "<p>Add a VPC Interface Endpoint for Secrets Manager and configure the Lambda function's subnet to use it</p>",
            "<p>Create a NAT Gateway in the VPC. Configure the Lambda function to use the NAT Gateway for connecting to the Secrets Manager</p>",
            "<p>Create a VPC Peering connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</p>",
            "<p>Create a Direct Connect connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a VPC Interface Endpoint for Secrets Manager and configure the Lambda function's subnet to use it</strong></p>\n\n<p>A VPC interface endpoint is a VPC component that enables the communication between resources in a VPC and services powered by AWS PrivateLink, without the need for an internet gateway, NAT device, VPN connection or AWS Direct Connect link. It allows for communication between the VPC and the service over an Amazon-provided private IP address, eliminating exposure to the public internet.</p>\n\n<p>AWS PrivateLink enables you to access services over an Amazon-provided IP address from within your VPC, without using public IPs or an internet gateway. With VPC interface endpoint, you can create a private connection between your VPC and supported services powered by AWS PrivateLink, using VPC endpoint services powered by AWS PrivateLink.</p>\n\n<p>A service provider creates an endpoint service to make their service available in a Region. A service consumer creates a VPC endpoint to connect their VPC to an endpoint service. A service consumer must specify the service name of the endpoint service when creating a VPC endpoint.</p>\n\n<p>How AWS PrivateLink works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html</a></p>\n\n<p>Secrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q5-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p>\n\n<p>This is the correct option as it allows the custom Lambda function in the VPC to communicate with Secrets Manager without going through the internet. A VPC endpoint for Secrets Manager is a VPC component that enables the communication between the VPC and Secrets Manager without going through the internet or a VPN connection.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a NAT Gateway in the VPC. Configure the Lambda function to use the NAT Gateway for connecting to the Secrets Manager</strong> - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your AWS account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access private resources while the function is running. As mentioned in the explanation above, you can leverage the VPC endpoint to connect to Secrets Manager from within the VPC and thereby avoid the internet.</p>\n\n<p>Internet access from a private subnet requires network address translation (NAT). To give your Lambda function access to the internet, you need to route outbound traffic to a NAT gateway in a public subnet. If you configure the Lambda function to use the NAT Gateway to connect to the Secrets Manager, you will end up using the internet. Therefore, this option serves as a distractor.</p>\n\n<p><strong>Create a Direct Connect connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</strong> - AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. One end of the cable is connected to your router, the other to an AWS Direct Connect router. This option is incorrect because Direct Connect is used to establish a dedicated network connection between an on-premises data center and a VPC, not between a VPC and a service like Secrets Manager.</p>\n\n<p><strong>Create a VPC Peering connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</strong> - This option is incorrect because VPC Peering is used to connect two VPCs together, not a VPC and a service. Secrets Manager is not a VPC, it's a service, therefore it cannot be connected via VPC Peering.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_vpc-endpoint.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_vpc-endpoint.html</a></p>\n"
        }
      },
      {
        "id": 99531691,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 3",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A website runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) which serves as an origin for an Amazon CloudFront distribution. An AWS WAF is being used to protect against SQL injection attacks. A review of security logs revealed an external malicious IP that needs to be blocked from accessing the website.</p><p>What&nbsp;steps should be taken to protect the application?</p>",
          "answers": [
            "<p>Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address.</p>",
            "<p>Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address.</p>",
            "<p>Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.</p>",
            "<p>Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.</p>"
          ],
          "explanation": "<p>The IP match condition / IP set match statement inspects the IP address of a web request's origin against a set of IP addresses and address ranges. Use this to allow or block web requests based on the IP addresses that the requests originate from.</p><p>AWS WAF supports all IPv4 and IPv6 address ranges. An IP set can hold up to 10,000 IP addresses or IP address ranges to check.</p><p><strong>CORRECT: </strong>\"Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address\" is incorrect as CloudFront does not sit within a subnet so network ACLs do not apply to it.</p><p><strong>INCORRECT:</strong> \"Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address\" is incorrect as the source IP addresses of the data in the EC2 instances\u2019 subnets will be the ELB IP addresses.</p><p><strong>INCORRECT:</strong> \"Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.\" is incorrect as you cannot create deny rules with security groups.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>"
        }
      },
      {
        "id": 112083293,
        "correct_response": [
          "d",
          "e"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An operations engineer plans to launch a collection of Amazon EC2 instances. The instances will run a custom application which will be managed by operations users who are members of a group. The operations users should be granted access only to the custom application instances.</p><p>Which actions should a security engineer take to control access? (Select TWO.)</p>",
          "answers": [
            "<p>Attach an IAM role to the Amazon EC2 instances.</p>",
            "<p>Attach an instance profile to the Amazon EC2 instances.</p>",
            "<p>Create an IAM policy that grants access to the instances based on the Principal element.</p>",
            "<p>Attach an IAM policy to the operations users that grants access to the instances with the specific tag using the Condition element.</p>",
            "<p>Add specific tags to the Amazon EC2 instances.</p>"
          ],
          "explanation": "<p>The condition element of an IAM policy can be used to identify the EC2 instances to which access should be granted. The IAM policy can be attached to the users or groups that require access. The best practice is to add the users to a group and attach the policy to the group. In this case the Condition element can be used to identify the instances by tag.</p><p><strong>CORRECT: </strong>\"Add specific tags to the Amazon EC2 instances\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM policy to the operations users that grants access to the instances with the specific tag using the Condition element\" is a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an IAM role to the Amazon EC2 instances\" is incorrect.</p><p>An IAM role would give the instance permissions to other AWS services. In this case the question asks how the operations users can be granted access to the instances.</p><p><strong>INCORRECT:</strong> \"Attach an instance profile to the Amazon EC2 instances\" is incorrect.</p><p>When you attach an IAM role to an instance you do so using an instance profile. As per the previous explanation this is assigning permissions to EC2 rather than the users.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy that grants access to the instances based on the Principal element\" is incorrect.</p><p>The condition element should be used to identify the tags. The principal element is not relevant when attaching a policy to a user, role, or group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonec2.html#amazonec2-policy-keys\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonec2.html#amazonec2-policy-keys</a></p>"
        }
      },
      {
        "id": 76165412,
        "correct_response": [
          "a",
          "c"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company's security policy mandates enforcing VPC Flow Logs for all the VPCs defined on AWS. A Security Engineer has been tasked to automate this compliance check and subsequently inform the governance teams if any VPC is found to be non-compliant.</p>\n\n<p>Which steps will you combine for automating the process to meet the compliance guidelines? (Select two)</p>\n",
          "answers": [
            "<p>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create a custom Config rule that uses this Lambda function as its source</p>",
            "<p>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create an Amazon CloudWatch Event rule that triggers when the state of the earlier declared Lambda function changes to non-compliant</p>",
            "<p>Create a Lambda function that polls Config to detect non-compliant resources daily and send notifications via Amazon SNS</p>",
            "<p>Publish VPC Flow Logs to Amazon S3 bucket and query the data with AWS Athena for determining the non-compliant resources</p>",
            "<p>Create a Lambda function that checks the AWS Athena query status on a daily basis for detecting any non-compliant resources daily and sending notifications via Amazon SNS</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create a custom Config rule that uses this Lambda function as its source</strong></p>\n\n<p><strong>Create a Lambda function that polls Config to detect non-compliant resources daily and send notifications via Amazon SNS</strong></p>\n\n<p>To create a custom rule that audits AWS resources for security compliance by enabling VPC Flow Logs for an Amazon Virtual Private Cloud (VPC), the following steps have to be followed:</p>\n\n<ol>\n<li>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant.</li>\n<li>Create a custom Config rule that uses the Lambda function created in Step 1 as the source.</li>\n<li>Create a Lambda function that polls Config to detect non-compliant resources daily and send notifications via Amazon SNS.</li>\n</ol>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q19-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/\">https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create an Amazon CloudWatch Event rule that triggers when the state of the earlier declared Lambda function changes to non-compliant</strong> - AWS Lambda has predefined states - such as active, inactive, pending, failed - that can be used while implementing business logic. Lambda does not propagate a <code>non-compliant</code> state. To understand the full list of states that AWS Lambda supports, please check the link in the references.</p>\n\n<p><strong>Publish VPC Flow Logs to Amazon S3 bucket and query the data with AWS Athena for determining the non-compliant resources</strong> - This option has been added as a distractor. Just querying the VPC Flow Logs using Athena is not sufficient as this option does not provide any information about the total numbers of VPCs set up in the given AWS account and how many of those VPCs have Flow Logs enabled. So it cannot detect non-compliant status for the given requirement.</p>\n\n<p><strong>Create a Lambda function that checks the AWS Athena query status on a daily basis for detecting any non-compliant resources daily and sending notifications via Amazon SNS</strong> - This option has been added as a distractor. This option relies on Athena queries for detecting any non-compliant resources. As mentioned earlier, any Athena query running only on the VPC Flow Logs data would have insufficient information to detect non-compliant status for the given requirement.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/\">https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/coming-soon-expansion-of-aws-lambda-states-to-all-functions/\">https://aws.amazon.com/blogs/compute/coming-soon-expansion-of-aws-lambda-states-to-all-functions/</a></p>\n"
        }
      },
      {
        "id": 76165452,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A security specialist with administrator permissions is using the AWS management console to access the CloudWatch logs for a Lambda function named \"myFunc\". However, upon choosing the option to view the logs in the AWS Lambda console, the specialist encountered an error message reading \"error loading Log Streams\". The specialist was unable to retrieve the logs as desired and must now find a solution to this issue.</p>\n\n<p>Following is an example IAM policy for the Lambda function's execution role:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:CreateLogGroup\",\n            \"Resource\": \"arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:log-group:/aws/lambda/myFunc:*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Which of the following solutions would you suggest to the specialist for addressing the issue?</p>\n",
          "answers": [
            "<p>Add the logs:GetLogEvents action to the second Allow statement</p>",
            "<p>Add the logs:CreateLogStream action to the second Allow statement</p>",
            "<p>Add the logs:DescribeLogStreams action to the second Allow statement</p>",
            "<p>Move the logs:CreateLogGroup action to the second Allow statement</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Add the logs:CreateLogStream action to the second Allow statement</strong></p>\n\n<p>A log stream is a sequence of log events that share the same source. Each separate source of logs in CloudWatch Logs makes up a separate log stream.</p>\n\n<p>A log group is a group of log streams that share the same retention, monitoring, and access control settings. You can define log groups and specify which streams to put into each group. There is no limit on the number of log streams that can belong to one log group.</p>\n\n<p>CreateLogStream creates a log stream for the specified log group.</p>\n\n<p>For the given use case, you must ensure that the write actions CreateLogGroup and CreateLogStream are allowed.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q62-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/</a></p>\n\n<p>Incorrect options:</p>\n\n<p>Since the security specialist already has administrator privileges as an IAM user, so there is no lack of permissions that's causing the error while the specialist is trying to \"view\" the logs.</p>\n\n<p>The root cause of the issue is that the Lambda function itself needs the CreateLogStream permission to be able to create the log stream and thereby successfully write the logs into CloudWatch Logs.</p>\n\n<p><strong>Add the logs:GetLogEvents action to the second Allow statement</strong></p>\n\n<p><strong>Add the logs:DescribeLogStreams action to the second Allow statement</strong></p>\n\n<p>The GetLogEvents and DescribeLogStreams are both \"read\" type of permissions which are not needed for the Lambda to successfully write the logs. Hence, both these options are incorrect.</p>\n\n<p><strong>Move the logs:CreateLogGroup action to the second Allow statement</strong> - This option is a distractor. The CreateLogGroup action needs to be in the first Allow statement only.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/permissions-reference-cwl.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/permissions-reference-cwl.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/</a></p>\n"
        }
      },
      {
        "id": 76165372,
        "correct_response": [
          "d",
          "e",
          "f"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The development team at an e-commerce company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Security Specialist to advise on CloudFront capabilities on routing and security.</p>\n\n<p>Which of the following would you identify as correct regarding CloudFront? (Select three)</p>\n",
          "answers": [
            "<p>Use KMS encryption in CloudFront to protect sensitive data for specific content</p>",
            "<p>Use geo-restriction to configure CloudFront for high-availability and failover</p>",
            "<p>CloudFront can route to multiple origins based on the price class</p>",
            "<p>CloudFront can route to multiple origins based on the content type</p>",
            "<p>Use an origin group with primary and secondary origins to configure CloudFront for high-availability and failover</p>",
            "<p>Use field-level encryption in CloudFront to protect sensitive data for specific content</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>CloudFront can route to multiple origins based on the content type</strong></p>\n\n<p>You can configure a single CloudFront web distribution to serve different types of requests from multiple origins. For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a CloudFront web distribution.</p>\n\n<p><strong>Use an origin group with primary and secondary origins to configure CloudFront for high availability and failover</strong></p>\n\n<p>You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.</p>\n\n<p>To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/origingroups-overview.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p><strong>Use field-level encryption in CloudFront to protect sensitive data for specific content</strong></p>\n\n<p>Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the data\u2014and have the credentials to decrypt it\u2014can do so.</p>\n\n<p>To use field-level encryption, when you configure your CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request. (You can\u2019t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt.)</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/fleoverview.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use KMS encryption in CloudFront to protect sensitive data for specific content</strong> - This option has been added as a distractor. You can use field level encryption in CloudFront to protect sensitive data for specific content.</p>\n\n<p><strong>Use geo-restriction to configure CloudFront for high-availability and failover</strong> - You can use geo-restriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront distribution. Geo restriction is not used to configure CloudFront for high availability and failover.</p>\n\n<p><strong>CloudFront can route to multiple origins based on the price class</strong> - CloudFront edge locations are grouped into geographic regions, and AWS has grouped regions into price classes. The default price class includes all regions. Another price class includes most regions (the United States; Canada; Europe; Hong Kong, Philippines, South Korea, Taiwan, and Singapore; Japan; India; South Africa; and Middle East regions) but excludes the most expensive regions. A third price class includes only the least expensive regions (the United States, Canada, and Europe regions). CloudFront can only route to multiple origins based on content type and not based on the price class.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html</a></p>\n"
        }
      },
      {
        "id": 76165356,
        "correct_response": [
          "b",
          "d",
          "e"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A Systems Administrator is no longer able to access the Windows Amazon EC2 instance because the Windows administrator password is lost. As a Security Engineer, you have been tasked with the job of resetting the password of the instance.</p>\n\n<p>Which of the following steps would you suggest to reset the password using EC2Launch v2? (Select three)</p>\n",
          "answers": [
            "<p>To reset the administrator password, Download the EC2Rescue for Windows Server zip file, extract the contents, and run <code>EC2Rescue.exe</code></p>",
            "<p>Verify that the EC2Launch v2 service is running. Detach the EBS root volume from the instance</p>",
            "<p>Select Offline Instance Option -&gt; Diagnose and Rescue -&gt; Reset Administrator Password. Reattach the volume to the original instance, then restart the instance</p>",
            "<p>Reattach the volume to the original instance as the root volume and connect to the instance using its key pair to retrieve the administrator password. Connect to the instance using its current public DNS name</p>",
            "<p>Launch a temporary instance and attach the volume to it as a secondary volume. Delete the <code>.run-once</code> file from the instance, located at <code>%ProgramData%/Amazon/EC2Launch/state/.run-once</code></p>",
            "<p>When you launch the temporary instance, to avoid disk signature collisions, you must select an AMI for the same version of Windows</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Verify that the EC2Launch v2 service is running. Detach the EBS root volume from the instance</strong></p>\n\n<p><strong>Launch a temporary instance and attach the volume to it as a secondary volume. Delete the <code>.run-once</code> file from the instance, located at <code>%ProgramData%/Amazon/EC2Launch/state/.run-once</code></strong></p>\n\n<p><strong>Reattach the volume to the original instance as the root volume and connect to the instance using its key pair to retrieve the administrator password. Connect to the instance using its current public DNS name</strong></p>\n\n<p>If you have lost your Windows administrator password and are using a supported Windows AMI that includes the EC2Launch v2 agent, you can use EC2Launch v2 to generate a new password.</p>\n\n<p>The steps to be followed are as follows:\n1. Verify that the EC2Launch v2 service is running</p>\n\n<ol>\n<li><p>Detach the root volume from the instance - You can't use EC2Launch v2 to reset an administrator password if the volume on which the password is stored is attached to an instance as the root volume. You must detach the volume from the original instance before you can attach it to a temporary instance as a secondary volume.</p></li>\n<li><p>Attach the volume to a temporary instance - Launch a temporary instance and attach the volume to it as a secondary volume. This is the instance you use to modify the configuration file. The temporary instance must be in the same Availability Zone as the original instance.</p></li>\n<li><p>Delete the <code>.run-once</code> file - After you have attached the volume to the temporary instance as a secondary volume, delete the <code>.run-once</code> file from the instance, located at <code>%ProgramData%/Amazon/EC2Launch/state/.run-once</code>. This directs EC2Launch v2 to run all tasks with a frequency of once, which includes setting the administrator password.</p></li>\n<li><p>Restart the original instance - After you have deleted the .run-once file, reattach the volume to the original instance as the root volume and connect to the instance using its key pair to retrieve the administrator password.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>To reset the administrator password, Download the EC2Rescue for Windows Server zip file, extract the contents, and run <code>EC2Rescue.exe</code></strong></p>\n\n<p><strong>Select Offline Instance Option -&gt; Diagnose and Rescue -&gt; Reset Administrator Password. Reattach the volume to the original instance, then restart the instance</strong></p>\n\n<p>These two steps are part of resetting the Windows administrator password using EC2Launch when EC2Launch v2 is not installed on the instance.</p>\n\n<p><strong>When you launch the temporary instance, to avoid disk signature collisions, you must select an AMI for the same version of Windows</strong> - To avoid disk signature collisions, you must select an AMI for a different version of Windows. For example, if the original instance runs Windows Server 2019, launch the temporary instance using the base AMI for Windows Server 2016. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ResettingAdminPassword_EC2Launchv2.html\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ResettingAdminPassword_EC2Launchv2.html</a></p>\n"
        }
      },
      {
        "id": 99531703,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 3",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A security architect is designing a highly secure application and must determine the best solution for storage of encryption keys. The encryption keys must be accessible only from within a VPC on single-tenant hardware security modules (HSMs). The solution must also include access logging and high availability.</p><p>Which of the following services meets these requirements?</p>",
          "answers": [
            "<p>Amazon Certificate Manager (ACM).</p>",
            "<p>AWS CloudHSM.</p>",
            "<p>AWS Key Management Service (KMS).</p>",
            "<p>AWS Secrets Manager.</p>"
          ],
          "explanation": "<p>AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. CloudHSM runs on single-tenant hardware security modules (HSMs) in your Amazon VPC. In addition to the logging features built into the Client SDK, you can also use AWS CloudTrail, Amazon CloudWatch Logs, and Amazon CloudWatch to monitor AWS CloudHSM.</p><p>AWS CloudHSM automatically load balances requests and securely duplicates keys stored in any HSM to all the other HSMs in the cluster. This provides additional cryptographic capacity and improves the durability of the keys. By storing multiple copies of your keys across HSMs located in different Availability Zones (AZs), your keys will be available and protected in the event that a single HSM becomes unavailable. Using at least two HSMs across multiple AZs is Amazon\u2019s recommended configuration for availability and durability.</p><p><strong>CORRECT: </strong>\"AWS CloudHSM\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Key Management Service (KMS)\" is incorrect.</p><p>AWS KMS is a service you can use for creating and managing encryption keys, but it is not single-tenant and does not run within your VPC.</p><p><strong>INCORRECT:</strong> \"Amazon Certificate Manager (ACM)\" is incorrect.</p><p>ACM is used for creating and managing SSL/TLS certificates only and does not run on single-tenant HSMs within your VPC.</p><p><strong>INCORRECT:</strong> \"AWS Secrets Manager \" is incorrect.</p><p>AWS Secrets Manager is not used for creating and managing encryption keys, it is used for storing secrets such as passwords and database connection strings.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudhsm/features/\">https://aws.amazon.com/cloudhsm/features/</a></p><p><a href=\"https://docs.aws.amazon.com/cloudhsm/latest/userguide/get-logs.html\">https://docs.aws.amazon.com/cloudhsm/latest/userguide/get-logs.html</a></p>"
        }
      },
      {
        "id": 83960204,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The development team at a company accesses resources across all AWS Regions. The management wants only the security team to have access to resources from all AWS Regions. Any access to members of the development team needs to be restricted to the resources in a single AWS Region (<code>us-west-2</code>) except for the global AWS services. The development team is sized at 40 members, with all members being part of the <code>developers</code> IAM group. The company needs to implement this access restriction immediately.</p>\n\n<p>What is the optimal way to meet this requirement?</p>\n",
          "answers": [
            "<p>Create an identity-based policy with the IAM <code>aws:RequestedRegion</code> condition key that denies access to all actions outside the specified Region, except for actions related to the global AWS services specified using <code>NotAction</code>. Attach the policy to the <code>developers</code> IAM group</p>",
            "<p>Create an identity-based policy with the IAM <code>aws:SourceIp</code> that denies access to all actions outside the specified IP address range of the given AWS Region, except for actions related to the global AWS services. Attach the policy to the <code>developers</code> IAM group</p>",
            "<p>Create a Service control policy (SCP) that denies access to any operations outside of the specified AWS Regions. Apart from the Condition and Resource elements, configure the NotAction element to allow access to the needed AWS services only. Attach the policy to the <code>developers</code> IAM group</p>",
            "<p>Create an identity-based policy that allows adding and removing the IAM tag with the tag key Region from IAM entities. Restrict the access using the defined tags. Attach the policy to the <code>developers</code> IAM group</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an identity-based policy with the IAM <code>aws:RequestedRegion</code> condition key that denies access to all actions outside the specified Region, except for actions related to the global AWS services specified using <code>NotAction</code>. Attach the policy to the <code>developers</code> IAM group</strong></p>\n\n<p>You can create an identity-based policy that denies access to any actions outside the Regions specified using the <code>aws:RequestedRegion</code> condition key, except for actions in the services specified using <code>NotAction</code>. The policy uses the <code>NotAction</code> element with the Deny effect, which explicitly denies access to all of the actions not listed in the statement. Actions in the CloudFront, IAM, Route 53, and AWS Support services should not be denied because these are popular AWS global services with a single endpoint that is physically located in the us-east-1 Region. Because all requests to these services are made to the us-east-1 Region, the requests would be denied without the NotAction element.</p>\n\n<p>Example identity-based policy with <code>aws:RequestedRegion</code> condition key:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q52-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an identity-based policy with the IAM <code>aws:SourceIp</code> that denies access to all actions outside the specified IP address range of the given AWS Region, except for actions related to the global AWS services. Attach the policy to the <code>developers</code> IAM group</strong> - You can use <code>aws:SourceIp</code> to create an identity-based policy that denies access to all AWS actions in the account when the request comes from principals outside the specified IP range. Using sourceIp represents an unnecessarily complicated solution when you could directly reference the <code>aws:RequestedRegion</code> condition.</p>\n\n<p><strong>Create an identity-based policy that allows adding and removing the IAM tag with the tag key Region from IAM entities. Restrict the access using the defined tags. Attach the policy to the <code>developers</code> IAM group</strong> - You can create identity-based policies that allow adding and removing the IAM tag, restricting tag values, and restricting access using tags. However, for the given use case, leveraging an existing <code>aws:RequestedRegion</code> condition is the optimal way of implementing the requirement.</p>\n\n<p><strong>Create a Service control policy (SCP) that denies access to any operations outside of the specified AWS Regions. Apart from the Condition and Resource elements, configure the NotAction element to allow access to the needed AWS services only. Attach the policy to the <code>developers</code> IAM group</strong> - When AWS Organizations is defined for any organization, using SCP is an optimal way to restrict access based on AWS regions. Since the use case does not reference AWS Organizations, SCP is not the right choice.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/iam-restrict-access-policy\">https://repost.aws/knowledge-center/iam-restrict-access-policy</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html</a></p>\n"
        }
      },
      {
        "id": 83960190,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A social media company runs all its workloads on AWS and it uses AWS Organizations to implement a multi-account strategy. The company currently has multiple AWS member accounts for its departments. The company anticipates that it will not have more than a total of 15 AWS accounts at any time in the future.</p>\n\n<p>The company wants to enforce a new security policy with the following requirements:</p>\n\n<p><strong>The company should use a centrally managed VPC that all departmental AWS accounts can access to launch workloads in subnets</strong>\n<strong>The centrally managed VPC should reside in an existing AWS account (Account X) within the organization</strong>\n<strong>No departmental AWS account should use a VPC within its own account for workloads</strong>\n<strong>No departmental AWS account should be able to modify another department's AWS account-specific application resources within the centrally managed VPC</strong></p>\n\n<p>Which solution will facilitate the security setup to address these requirements?</p>\n",
          "answers": [
            "<p>Use AWS Resource Access Manager (AWS RAM) to share the subnets in Account X's centrally managed VPC with the other member accounts. Configure the member accounts to use the shared subnets to launch workloads</p>",
            "<p>Use AWS Systems Manager to share the subnets in Account X's centrally managed VPC with the other member accounts. Configure the member accounts to use the shared subnets to launch workloads</p>",
            "<p>Set up VPC Peering among Account X's centrally managed VPC and the VPC's in all other member accounts. Configure the member accounts to use the shared subnets in Account X to launch workloads</p>",
            "<p>Configure a transit gateway in Account X's centrally managed VPC. Configure the member accounts to leverage the transit gateway to access the shared subnets in Account X to launch workloads</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Resource Access Manager (AWS RAM) to share the subnets in Account X's centrally managed VPC with the other member accounts. Configure the member accounts to use the shared subnets to launch workloads</strong></p>\n\n<p>AWS Resource Access Manager (AWS RAM) helps you securely share your resources across AWS accounts, within your organization or organizational units (OUs), and with IAM roles and users for supported resource types.</p>\n\n<p>Overview of AWS Resource Access Manager (AWS RAM):\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q45-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/ram/\">https://aws.amazon.com/ram/</a></p>\n\n<p>VPC sharing allows multiple AWS accounts to create their application resources, such as Amazon EC2 instances, Amazon Relational Database Service (RDS) databases, Amazon Redshift clusters, and AWS Lambda functions, into shared, centrally-managed virtual private clouds (VPCs). In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner.</p>\n\n<p>For the given use case, you can set up AWS Resource Access Manager (AWS RAM) to share the subnets in Account X's centrally managed VPC with the other member accounts. You can share non-default subnets with other accounts within your organization. To share subnets, you must first create a Resource Share with the subnets to be shared and the AWS accounts, organizational units, or an entire organization that you want to share the subnets with. Then, you can configure the member accounts to use the shared subnets to launch workloads.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Systems Manager to share the subnets in Account X's centrally managed VPC with the other member accounts. Configure the member accounts to use the shared subnets to launch workloads</strong> - You cannot use AWS Systems Manager to share the subnets in Account X's centrally managed VPC with the other member accounts.</p>\n\n<p><strong>Set up VPC Peering among Account X's centrally managed VPC and the VPC's in all other member accounts. Configure the member accounts to use the shared subnets in Account X to launch workloads</strong> - You cannot use VPC Peering to share the subnets in Account X's centrally managed VPC with the other member accounts to launch resources in the shared subnets.</p>\n\n<p><strong>Configure a transit gateway in Account X's centrally managed VPC. Configure the member accounts to leverage the transit gateway to access the shared subnets in Account X to launch workloads</strong> - You cannot use a transit gateway to access the shared subnets to launch workloads.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ram/\">https://aws.amazon.com/ram/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/configure-fine-grained-access-to-your-resources-shared-using-aws-resource-access-manager/\">https://aws.amazon.com/blogs/security/configure-fine-grained-access-to-your-resources-shared-using-aws-resource-access-manager/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/\">https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/</a></p>\n"
        }
      },
      {
        "id": 112083279,
        "correct_response": [
          "a",
          "e"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A security engineer was asked to configure an automated alert that notifies the security team when configuration changes occur on security groups. The engineer has created an AWS CloudTrail trail, specified a log group, and assigned appropriate IAM permissions to CloudTrail. The solution must be simple and cost-effective.</p><p>Which additional actions should the security engineer take? (Select TWO.)</p>",
          "answers": [
            "<p>Create an alarm that sends an Amazon SNS notification if security group changes are identified.</p>",
            "<p>Stream the CloudWatch Logs to Amazon Kinesis Data Streams and use Kinesis Data Analytics to identify security group changes in near real-time.</p>",
            "<p>Create a query in Amazon CloudWatch Logs Insights and write an AWS Lambda function that runs the query on a schedule.</p>",
            "<p>Create a subscription to an AWS Lambda function that analyses the logs and a subscription filter to filter the log events that are forwarded.</p>",
            "<p>Create a metric filter and define a metric pattern that matches security group changes.</p>"
          ],
          "explanation": "<p>You can create a solution that sends automatic notifications when security group changes occur. The solution in this scenario uses AWS CloudTrail to send information about the API actions that occur in the account to an Amazon CloudWatch Logs log group.</p><p>CloudTrail must be granted sufficient IAM permissions to be able to create a CloudWatch Logs log stream in the log group that you specify and to deliver CloudTrail events to that log stream.</p><p>When the logs are being correctly sent to the specified log group a metric filter should be created that filters out the log events which the security engineer is looking for. An alarm can then be created that is based on the filter. The alarm should send a notification to the security team using an Amazon SNS topic.</p><p><strong>CORRECT: </strong>\"Create a metric filter and define a metric pattern that matches security group changes\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an alarm that sends an Amazon SNS notification if security group changes are identified\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Stream the CloudWatch Logs to Amazon Kinesis Data Streams and use Kinesis Data Analytics to identify security group changes in near real-time\" is incorrect.</p><p>This solution is more expensive, and complex compared to using metric filters with CloudWatch Logs. It also does not specify a method of sending a notification.</p><p><strong>INCORRECT:</strong> \"Create a query in Amazon CloudWatch Logs Insights and write an AWS Lambda function that runs the query on a schedule\" is incorrect.</p><p>CloudWatch Logs Insights can be used to interactively search and analyze data in CloudWatch Logs. However, metric filters are automatic and free which is a simpler and cheaper solution.</p><p><strong>INCORRECT:</strong> \"Create a subscription to an AWS Lambda function that analyses the logs and a subscription filter to filter the log events that are forwarded\" is incorrect.</p><p>This is a workable solution but is more complex and costly compared to using metric filters. It also does not specify a method of sending a notification.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html</a></p>"
        }
      },
      {
        "id": 76165398,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A web application is hosted on Amazon EC2 instances that are fronted by Application Load Balancer (ALB) configured with an Auto Scaling group (ASG). Enhanced security is provided to the ALB by AWS WAF web ACLs. As per the company's security policy, AWS CloudTrail is activated and logs are configured to be stored on Amazon S3 and CloudWatch Logs.</p>\n\n<p>A discount sales offer was run on the application for a week. The support team has noticed that a few of the instances have rebooted taking down the log files and all temporary data with them. Initial analysis has confirmed that the incident took place during off-peak hours. Even though the incident did not cause any sales or revenue loss, the CTO has asked the security team to fix the security error that has allowed the incident to go unnoticed and eventually untraceable.</p>\n\n<p>As Security Engineer, which series of steps will you implement to permanently record all traffic coming into the application?</p>\n",
          "answers": [
            "<p>Configure the WAF web ACL to deliver logs to Amazon CloudTrail and create a trail that applies to all Regions. This delivers log files from all Regions to an S3 bucket. Use Athena to query the logs for errors and tracking</p>",
            "<p>Configure the WAF web ACL to deliver logs to Amazon Kinesis Data Firehose, which should be configured to eventually store the logs in an Amazon S3 bucket. Use Athena to query the logs for errors and tracking</p>",
            "<p>To capture information about the IP traffic going to and from network interfaces, configure VPC Flow Logs to be directly streamed to Kinesis Data Streams and create alarms for automatic monitoring</p>",
            "<p>Configure Elastic Load Balancing to write access logs to Amazon Kinesis Data Firehose. The logs can be further directed from Firehose into an Amazon S3 bucket for further analysis and reporting</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the WAF web ACL to deliver logs to Amazon Kinesis Data Firehose, which should be configured to eventually store the logs in an Amazon S3 bucket. Use Athena to query the logs for errors and tracking</strong></p>\n\n<p>The logging destinations that you can choose from for your AWS WAF logs are:</p>\n\n<ol>\n<li>Amazon CloudWatch Logs</li>\n<li>Amazon Simple Storage Service</li>\n<li>Amazon Kinesis Data Firehose</li>\n</ol>\n\n<p>To send logs to Amazon Kinesis Data Firehose, you send logs from your web ACL to an Amazon Kinesis Data Firehose with a configured storage destination. After you enable logging, AWS WAF delivers logs to your storage destination through the HTTPS endpoint of Kinesis Data Firehose.</p>\n\n<p>One AWS WAF log is equivalent to one Kinesis Data Firehose record. If you typically receive 10,000 requests per second and you enable full logs, you should have 10,000 records per second setting in Kinesis Data Firehose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the WAF web ACL to deliver logs to Amazon CloudTrail and create a trail that applies to all Regions. This delivers log files from all Regions to an S3 bucket. Use Athena to query the logs for errors and tracking</strong> - As discussed above, the logging destinations that you can choose from for your AWS WAF logs are Amazon CloudWatch Logs, Amazon Simple Storage Service, and Amazon Kinesis Data Firehose. Amazon CloudTrail is not a valid destination for WAF ACL logs.</p>\n\n<p><strong>To capture information about the IP traffic going to and from network interfaces, configure VPC Flow Logs to be directly streamed to Kinesis Data Streams and create alarms for automatic monitoring</strong> - VPC Flow You should also note that VPC Flow Logs cannot capture all traffic coming into the application as these can only capture information about the IP traffic going to and from network interfaces in your VPC. In addition, VPC Flow Logs can be directly published only to the following destinations: Amazon CloudWatch Logs, Amazon S3, or Amazon Kinesis Data Firehose. So this option is incorrect.</p>\n\n<p><strong>Configure Elastic Load Balancing to write access logs to Amazon Kinesis Data Firehose. The logs can be further directed from Firehose into an Amazon S3 bucket for further analysis and reporting</strong> - Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Elastic Load Balancing access logs are stored in Amazon S3 buckets and it is not possible to directly write the logs to Kinesis Data Firehose.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2021/12/awf-waf-cloudwatch-log-s3-bucket/\">https://aws.amazon.com/about-aws/whats-new/2021/12/awf-waf-cloudwatch-log-s3-bucket/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n"
        }
      },
      {
        "id": 76165444,
        "correct_response": [
          "d"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The security team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you identify as an INVALID option for setting up such a configuration?</p>\n",
          "answers": [
            "<p>You can use a security group as the custom source for the inbound rule</p>",
            "<p>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</p>",
            "<p>You can use an IP address as the custom source for the inbound rule</p>",
            "<p>You can use an Internet Gateway ID as the custom source for the inbound rule</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>You can use an Internet Gateway ID as the custom source for the inbound rule</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, you can use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.</p>\n\n<p>Please see this list of allowed source or destination for security group rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n\n<p>Therefore, you cannot use an Internet Gateway ID as the custom source for the inbound rule.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can use a security group as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use an IP address as the custom source for the inbound rule</strong></p>\n\n<p>As described in the list of allowed sources or destinations for security group rules, the above options are supported.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</p>\n"
        }
      },
      {
        "id": 83960230,
        "correct_response": [
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A corporation X is looking for a solution that provides automatic scanning of operating system and programming language package vulnerabilities for all its container images stored on Amazon Elastic Container Registry (Amazon ECR). The images should only be scanned once when they are pushed onto the repository.</p>\n\n<p>Which of the following options is the right fit for the given requirements?</p>\n",
          "answers": [
            "<p>Opt for basic scanning on push filter</p>",
            "<p>Opt for enhanced scanning and set to manual scan frequency</p>",
            "<p>Opt for enhanced scanning and specify a filter for continuous scanning</p>",
            "<p>Opt for enhanced scanning and specify a filter for a scan on push</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Opt for enhanced scanning and specify a filter for a scan on push</strong></p>\n\n<p>With enhanced scanning, Amazon ECR integrates with Amazon Inspector to provide automated, continuous scanning of your repositories. Your container images are scanned for both operating systems and programming language package vulnerabilities. As new vulnerabilities appear, the scan results are updated and Amazon Inspector emits an event to EventBridge to notify you.</p>\n\n<p>Since both the OS and programming language vulnerabilities have to be scanned, enhanced scanning has to be selected. Let's look at the filter options available for enhanced scanning.</p>\n\n<p>When enhanced scanning is used, you may specify separate filters for scan on push and continuous scanning. Any repositories that do not match an enhanced scanning filter will have scanning disabled. If you are using enhanced scanning and specify separate filters for scan on push and continuous scanning where multiple filters match the same repository, then Amazon ECR enforces the continuous scanning filter over the scan on push filter for that repository.</p>\n\n<p>Key considerations while enabling Amazon ECR enhanced scanning:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q65-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for enhanced scanning and specify a filter for continuous scanning</strong> - Continuous scanning will continue to scan the images event after the scan at the initial push. This does not meet the given requirements.</p>\n\n<p><strong>Opt for enhanced scanning and set to manual scan frequency</strong> - This option is incorrect. Manual scans that use enhanced scanning aren't supported.</p>\n\n<p><strong>Opt for basic scanning on push filter</strong> - With basic scanning, Amazon ECR uses the Common Vulnerabilities and Exposures (CVEs) database from the open-source Clair project. Programming language package vulnerabilities are not covered in this scan plan.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html\">https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html</a></p>\n"
        }
      },
      {
        "id": 99531681,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 3",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A developer who recently left a company was found to have published many access keys IDs to a public source code repository. A list of the exposed access key IDs has been created. A security engineer needs to quickly identify which users the access key IDs belong to so the credentials can be immediately rotated. The company uses multiple accounts in an AWS Organization.</p><p>Which approach should the security engineer take?</p>",
          "answers": [
            "<p>Generate a credential report in the root account in the Organization. Identify the users the access key IDs belong to. Rotate the access key IDs.</p>",
            "<p>Generate an IAM Access Analyzer report in the root account in the Organization. Identify the users the access key IDs belong to. Rotate the access key IDs.</p>",
            "<p>Generate an IAM Access Analyzer report in each account in the Organization. Consolidate the reports and identify the users the access key IDs belong to. Rotate the access key IDs.</p>",
            "<p>Generate a credential report in each account in the Organization. Consolidate the reports and identify the users the access key IDs belong to. Rotate the access key IDs.</p>"
          ],
          "explanation": "<p>You can generate and download a <em>credential report</em> that lists all users in your account and the status of their various credentials, including passwords, access keys, and MFA devices.</p><p>You can use credential reports to assist in your auditing and compliance efforts. You can use the report to audit the effects of credential lifecycle requirements, such as password and access key rotation.</p><p>In this case the credential report must be created in each account within the AWS Organization. Then, the reports can be consolidated, and the compromised access key IDs can be identified. Finally, the access key IDs can be rotated.</p><p><strong>CORRECT: </strong>\"Generate a credential report in each account in the Organization. Consolidate the reports and identify the users the access key IDs belong to. Rotate the access key IDs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Generate a credential report in the root account in the Organization. Identify the users the access key IDs belong to. Rotate the access key IDs\" is incorrect.</p><p>You cannot create a single credential report for an organization.</p><p><strong>INCORRECT:</strong> \"Generate an IAM Access Analyzer report in the root account in the Organization. Identify the users the access key IDs belong to. Rotate the access key IDs\" is incorrect.</p><p>IAM access analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, shared with an external entity. It does not identify access key IDs that may have been compromised.</p><p><strong>INCORRECT:</strong> \"Generate an IAM Access Analyzer report in each account in the Organization. Consolidate the reports and identify the users the access key IDs belong to. Rotate the access key IDs\" is incorrect.</p><p>As above, the IAM access analyzer is the wrong tool for this purpose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html</a></p>"
        }
      },
      {
        "id": 76398932,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A DevOps engineer has deployed several custom-built images provided by the development team using Amazon Elastic Container Service (ECS) with the Fargate launch type. The engineer now needs to aggregate the logs from all the containers into a pre-existing CloudWatch log group.</p><p>Which solution will satisfy these requirements?</p>",
          "answers": [
            "<p>Enable the awslogs log driver by including awslogs-group and awslogs-region parameters in the LogConfiguration property.</p>",
            "<p>Install and configure the CloudWatch agent on the running container instances.</p>",
            "<p>Implement Fluent Bit and FluentD in a DaemonSet configuration to direct logs to Amazon CloudWatch Logs.</p>",
            "<p>Define an IAM policy that encompasses the logs:CreateLogGroup action and assign this policy to the running container instances.</p>"
          ],
          "explanation": "<p>The Fargate launch type supports the awslogs log driver. You need to specify the awslogs-group (CloudWatch log group name) and awslogs-region (AWS Region of the log group) parameters in the LogConfiguration property in the task definition for Amazon ECS.</p><p><strong>CORRECT: </strong>\"Enable the awslogs log driver by including awslogs-group and awslogs-region parameters in the LogConfiguration property\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install and configure the CloudWatch agent on the running container instances\" is incorrect.</p><p>The CloudWatch agent cannot be installed directly on the containers running on AWS Fargate. Instead, the awslogs log driver needs to be used.</p><p><strong>INCORRECT:</strong> \"Implement Fluent Bit and FluentD in a DaemonSet configuration to direct logs to Amazon CloudWatch Logs\" is incorrect.</p><p>While Fluent Bit and FluentD are often used for log aggregation in Kubernetes environments, Amazon ECS with Fargate doesn't natively support this type of configuration. Instead, you would use the awslogs driver in your task definition.</p><p><strong>INCORRECT:</strong> \"Define an IAM policy that encompasses the logs:CreateLogGroup action and assign this policy to the running container instances\" is incorrect.</p><p>Just providing an IAM policy with the logs:CreateLogGroup action to the running container instances would not be enough. The task definition needs to be configured to use the awslogs log driver to send logs to CloudWatch Logs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html</a></p>"
        }
      },
      {
        "id": 73686902,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company stores highly confidential information in an Amazon S3 bucket. The security team requires that any changes to the bucket policy are automatically remediated, and alerts of these changes are sent to their team members.</p><p>Which actions should a security engineer take to meet these requirements with the LEAST effort?</p>",
          "answers": [
            "<p>Use an Amazon CloudWatch alarm with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS.</p>",
            "<p>Use AWS Lambda with Macie to automatically remediate S3 bucket policy changes. Use Macie automatic alerting capabilities for alerts.</p>",
            "<p>Use Amazon EventBridge rules with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS.</p>",
            "<p>Use AWS Config with Auto Remediation to remediate any changes to S3 bucket policies. Configure alerting with AWS Config and Amazon SNS.</p>"
          ],
          "explanation": "<p>The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention.</p><p>An AWS Config rule can be applied to identify and remediate any unauthorized changes to the policy associated with the S3 bucket. Amazon SNS can be integrated as a destination for alerts.</p><p><strong>CORRECT: </strong>\"Use AWS Config with Auto Remediation to remediate any changes to S3 bucket policies. Configure alerting with AWS Config and Amazon SNS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda with Macie to automatically remediate S3 bucket policy changes. Use Macie automatic alerting capabilities for alerts\" is incorrect.</p><p>Macie is not used for identifying changes to S3 bucket policies or for alerting. Macie is used for identifying personally identifiable information in data sets.</p><p><strong>INCORRECT:</strong> \"Use Amazon EventBridge rules with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS\" is incorrect.</p><p>EventBridge can alert on API events relating to bucket changes. However, this would require creating a custom function and is therefore more effort compared to the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an Amazon CloudWatch alarm with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS\" is incorrect.</p><p>CloudWatch alarms cannot be configured to trigger based on changes to S3 buckets</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>"
        }
      },
      {
        "id": 99528305,
        "correct_response": [
          "c"
        ],
        "source": "Neal Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An application running in a private subnet needs outbound connectivity to an internet service using the IPv6 protocol. A security engineer has created a separate route table for the private subnet.</p><p>The security engineer needs to enable outbound connectivity to the internet service. The solution should ensure inbound connections from the internet cannot be initiated.</p><p>Which actions should the network engineer take to meet this requirement?</p>",
          "answers": [
            "<p>Create a NAT gateway in a public subnet and update the route table in the private subnet.</p>",
            "<p>Create an internet gateway in a private subnet and update the route table in the private subnet.</p>",
            "<p>Create an egress-only internet gateway and update the route table in the private subnet.</p>",
            "<p>Create an internet gateway in a public subnet and update the route table in the private subnet.</p>"
          ],
          "explanation": "<p>An egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet and prevents the internet from initiating an IPv6 connection with your instances.</p><p><strong>CORRECT: </strong>\"Create an egress-only internet gateway and update the route table in the private subnet\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a NAT gateway in a public subnet and update the route table in the private subnet\" is incorrect.</p><p>NAT gateways are used for IPv4 not IPv6.</p><p><strong>INCORRECT:</strong> \"Create an internet gateway in a private subnet and update the route table in the private subnet\" is incorrect.</p><p>Internet gateways are used for routing traffic out of the VPC and are attached at the VPC level. To enable outbound IPv6 an egress-only internet gateway is also needed.</p><p><strong>INCORRECT:</strong> \"Create an internet gateway in a public subnet and update the route table in the private subnet\" is incorrect.</p><p>Internet gateways are used for routing traffic out of the VPC and are attached at the VPC level. To enable outbound IPv6 an egress-only internet gateway is also needed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p>"
        }
      },
      {
        "id": 73686832,
        "correct_response": [
          "b",
          "c",
          "d"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company's security engineer receives an abuse notification from AWS. The notification indicates that malware is being hosted in the AWS account. The security engineer investigated the issue and found an unauthorized Amazon S3 bucket.</p><p>Which combination of steps should the security engineer take to MINIMIZE the consequences of this compromise? (Select THREE.)</p>",
          "answers": [
            "<p>Login as root and delete all IAM users.</p>",
            "<p>Rotate and delete all root and IAM access keys.</p>",
            "<p>Delete any unauthorized IAM users.</p>",
            "<p>Delete any unauthorized resources.</p>",
            "<p>Enable the AWS Shield Advanced service.</p>",
            "<p>Take a snapshot of all Amazon EBS volumes.</p>"
          ],
          "explanation": "<p>The AWS Trust &amp; Safety Team sends abuse reports to the security contact on your account. If there is no security contact listed, the AWS Trust &amp; Safety Team contacts you using the email address listed on your account.</p><p>If you observe unauthorized activity within your AWS account, or you believe that an unauthorized party accessed your account, then do the following:</p><ul><li><p>Rotate and delete all root and AWS Identity and Access Management (IAM) access keys.</p></li><li><p>Delete any potentially unauthorized IAM users, and then change the password for all other IAM users.</p></li><li><p>Check your bill. Your bill can help you identify resources that you didn't create.</p></li><li><p>Delete any resources on your account that you didn't create.</p></li><li><p>Enable MFS on the root user and any IAM users with console access.</p></li><li><p>Verify that your account information is correct.</p></li><li><p>Respond to the notifications that you received from AWS Support through the AWS Support Center.</p></li></ul><p><strong>CORRECT: </strong>\"Rotate and delete all root and IAM access keys\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Delete any unauthorized IAM users\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Delete any unauthorized resources\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Login as root and delete all IAM users\" is incorrect.</p><p>This would be highly disruptive and is not recommended as a response measure.</p><p><strong>INCORRECT:</strong> \"Enable the AWS Shield Advanced service\" is incorrect.</p><p>This service protects against DDoS attacks. This account has already been compromised with malware so Shield will not assist in this case.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of all Amazon EBS volumes\" is incorrect.</p><p>This is not one of the recommended response measures for this circumstance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/\">https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/</a></p>"
        }
      },
      {
        "id": 83960182,
        "correct_response": [
          "d"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company maintains independent AWS accounts for its departments. For a specific requirement, a user in the Finance account needs full access to an Amazon S3 bucket in the Audit account. The security administrator has attached the necessary IAM permissions to the user of the Finance account. But, the user still has no access to the S3 bucket.</p>\n\n<p>Which additional configuration is needed for the given requirement?</p>\n",
          "answers": [
            "<p>Create an S3 bucket policy in the Finance account that allows access to the S3 bucket for the user from the Finance account</p>",
            "<p>Enable the <code>bucket owner enforced</code> setting in the Audit account. Use Access Control Lists (ACLs) to grant cross-account access</p>",
            "<p>Configure S3 bucket ARN as Principal for the IAM trust policy for the user</p>",
            "<p>Create an S3 bucket policy in the Audit account that allows access to the S3 bucket for the user from the Finance account</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an S3 bucket policy in the Audit account that allows access to the S3 bucket for the user from the Finance account</strong></p>\n\n<p>Depending on the type of access that you want to provide, use one of the following solutions to grant cross-account access to objects:</p>\n\n<ol>\n<li><p>AWS Identity and Access Management (IAM) policies and resource-based bucket policies for programmatic-only access to S3 bucket objects.</p></li>\n<li><p>IAM policies and resource-based Access Control Lists (ACLs) for programmatic-only access to S3 bucket objects.</p></li>\n<li><p>Cross-account IAM roles for programmatic and console access to S3 bucket objects.</p></li>\n</ol>\n\n<p>While necessary permissions are needed for an IAM user or an IAM role to connect to the Amazon S3 bucket, it is not sufficient. The bucket policy of the Amazon S3 bucket should also allow access to the user or role for successful access to the data present in the S3 buckets.</p>\n\n<p>Following is an example bucket policy for Account A to grant permissions to the IAM role or user that you created in Account B. Use this bucket policy to grant a user the permissions to GetObject and PutObject for objects in a bucket owned by Account A.</p>\n\n<p>Example bucket policy for cross-account access:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://repost.aws/knowledge-center/cross-account-access-s3\">https://repost.aws/knowledge-center/cross-account-access-s3</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the <code>bucket owner enforced</code> setting in the Audit account. Use Access Control Lists (ACLs) to grant cross-account access</strong> - This statement is incorrect. When the <code>bucket owner enforced</code> setting is enabled, all bucket and object ACLs are disabled. Therefore, you can't use ACLs to grant cross-account access. By default, all newly created buckets have the <code>bucket owner enforced</code> setting enabled.</p>\n\n<p><strong>Create an S3 bucket policy in the Finance account that allows access to the S3 bucket for the user from the Finance account</strong> - The S3 bucket is in the Audit account and hence creating the bucket policy in the Finance account does not make sense. So this option is incorrect.</p>\n\n<p><strong>Configure S3 bucket ARN as Principal for the IAM trust policy for the user</strong> - You cannot add S3 bucket ARN as a Principal in an IAM trust policy. You can specify any of the following principals in a policy: AWS account and root user, IAM roles, Role sessions, IAM users, Federated user sessions, AWS services, or All principals.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html</a></p>\n\n<p><a href=\"https://repost.aws/knowledge-center/cross-account-access-s3\">https://repost.aws/knowledge-center/cross-account-access-s3</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html</a></p>\n"
        }
      },
      {
        "id": 83960118,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A mid-sized company stores sensitive data on an Amazon Elastic Block Store (EBS) volume attached to an Amazon Elastic Compute Cloud (EC2) instance. To ensure data durability, the company also replicates this sensitive data to an Amazon Simple Storage Service (S3) bucket. Both the EBS volume and S3 bucket are encrypted using the same AWS Key Management Service (KMS) Customer Master Key (CMK). The security team at the company has noticed that the CMK has been deleted as a former employee had set the key for deletion before leaving the company.</p>\n\n<p>As a Security Specialist, what do you suggest to access the data?</p>\n",
          "answers": [
            "<p>Login as the AWS account root user to restore the deleted key and then use it to recover the data from the EBS volume</p>",
            "<p>Copy the data directly from the EBS encrypted volume before the volume is detached from the EC2 instance</p>",
            "<p>Take a snapshot of the EBS encrypted volume before the volume is detached from the EC2 instance</p>",
            "<p>Raise a ticket with AWS Support to restore the deleted key and then use it to recover the data from the EBS volume</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Copy the data directly from the EBS encrypted volume before the volume is detached from the EC2 instance</strong></p>\n\n<p>You can use AWS Key Management Service (KMS) to create and control the cryptographic keys that are used to protect your data on AWS. An AWS KMS key is a logical representation of a cryptographic key. A KMS key contains metadata, such as the key ID, key spec, key usage, creation date, description, and key state. Most importantly, it contains a reference to the key material that is used when you perform cryptographic operations with the KMS key.</p>\n\n<p>By default, AWS KMS creates the key material for a KMS key. However, you can import your own key material into a KMS key, or use a custom key store to create KMS keys that use key material in your AWS CloudHSM cluster, or key material in an external key manager that you own and manage outside of AWS.</p>\n\n<p>Deleting a KMS key deletes the key material and all metadata associated with the KMS key and is irreversible. After a KMS key is deleted, you can no longer decrypt the data that was encrypted under that KMS key, which means that data becomes unrecoverable. Because it is destructive and potentially dangerous to delete a KMS key, AWS KMS requires you to set a waiting period of 7 \u2013 30 days. The default waiting period is 30 days.</p>\n\n<p>For the given use case, the EBS volume has been encrypted with a KMS key. When you attach the EBS volume to an EC2 instance, Amazon EC2 uses your KMS key to decrypt the EBS volume's encrypted data key. Amazon EC2 stores the plaintext data key in hypervisor memory and uses it to encrypt disk I/O to the EBS volume. The data key persists in memory as long as the EBS volume is attached to the EC2 instance. Even if someone has scheduled the key for deletion and the key is past the waiting period for deletion, this has no immediate effect on the EC2 instance or the EBS volume. Amazon EC2 is using the plaintext data key\u2014not the KMS key\u2014to encrypt all disk I/O while the volume is attached to the instance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Login as the AWS account root user to restore the deleted key and then use it to recover the data from the EBS volume</strong> - You cannot use the AWS account root user privileges to restore the deleted key and then use it to recover the data from the EBS volume. Once deleted, a KMS key can never be accessed again.</p>\n\n<p><strong>Raise a ticket with AWS Support to restore the deleted key and then use it to recover the data from the EBS volume</strong> - AWS Support cannot restore a deleted KMS key. Once deleted, a KMS key can never be accessed again.</p>\n\n<p><strong>Take a snapshot of the EBS encrypted volume before the volume is detached from the EC2 instance</strong> - You cannot take a snapshot of an EBS encrypted volume for which the KMS key has been deleted as the snapshot itself needs to be encrypted using the same KMS key.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q9-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p>\n"
        }
      },
      {
        "id": 73686806,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A new application runs on Amazon EC2 instances behind an Application Load Balancer. Some of the company\u2019s other applications have recently seen attacks with high rates of requests from single IP addresses. A security engineer wants to ensure the new application is protected from such attacks.</p><p>How can the security engineer add protection to the application without permanently blocking the IP address?</p>",
          "answers": [
            "<p>Generate a custom error page in Amazon CloudFront.</p>",
            "<p>Use AWS WAF to create a rate-based rule.</p>",
            "<p>Add AWS Shield protection to the Application Load Balancer.</p>",
            "<p>Enable geo restriction in Amazon CloudFront.</p>"
          ],
          "explanation": "<p>A rate-based rule tracks the rate of requests for each originating IP address and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span. You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests.</p><p><strong>CORRECT: </strong>\"Use AWS WAF to create a rate-based rule\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Generate a custom error page in Amazon CloudFront\" is incorrect.</p><p>Custom error pages cannot be generated based on the rate of requests from a specific IP address.</p><p><strong>INCORRECT:</strong> \"Add AWS Shield protection to the Application Load Balancer\" is incorrect.</p><p>AWS Shield Advanced can be used to protect ALBs but it will still leverage AWS WAF for rate-based rules.</p><p><strong>INCORRECT:</strong> \"Enable geo restriction in Amazon CloudFront\" is incorrect.</p><p>Geo restriction does not restrict based on the rate of requests from a specific IP address, it restricts based on the geographic location of the originating user.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html</a></p>"
        }
      },
      {
        "id": 76398906,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company requires that all traffic to a specific application is captured and inspected for network and security anomalies. The application runs on several Amazon EC2 instances. The detection software has been installed on an intrusion detection instance running on EC2.</p><p>What should a security engineer do next to route traffic to the intrusion detection instance?</p>",
          "answers": [
            "<p>Disable source/destination checks on the Amazon EC2 instances and enable VPC Flow Logs on the ENIs.</p>",
            "<p>Configure VPC traffic mirroring to send traffic to the intrusion detection EC2 instance using a Network Load Balancer.</p>",
            "<p>Use Amazon Inspector to capture and inspect traffic and trigger an AWS Lambda function to send route anomalous traffic to the EC2 instance.</p>",
            "<p>Configure VPC Flow Logs at the VPC level and write logs to Amazon S3. Use event notifications to trigger an AWS Lambda function to inspect the logs.</p>"
          ],
          "explanation": "<p>Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of Amazon EC2 instances. You can then send the traffic to out-of-band security and monitoring appliances for:</p><ul><li><p>Content inspection</p></li><li><p>Threat monitoring</p></li><li><p>Troubleshooting</p></li></ul><p>The security and monitoring appliances can be deployed as individual instances, or as a fleet of instances behind a Network Load Balancer with a UDP listener. Traffic Mirroring supports filters and packet truncation, so that you only extract the traffic of interest to monitor by using monitoring tools of your choice.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-04_01-45-41-29d5cf5b4c0baf12b0baf99559c4beb6.jpg\"><p><strong>CORRECT: </strong>\"Configure VPC traffic mirroring to send traffic to the intrusion detection EC2 instance using a Network Load Balancer\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Disable source/destination checks on the Amazon EC2 instances and enable VPC Flow Logs on the ENIs\" is incorrect.</p><p>Disabling source/destination checks is required for NAT instances but is not a step required to setup traffic mirroring. VPC Flow Logs can capture log information relating to traffic flows but not the entire packet.</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector to capture and inspect traffic and trigger an AWS Lambda function to send route anomalous traffic to the EC2 instance\" is incorrect.</p><p>Amazon Inspector does not perform traffic capturing.</p><p><strong>INCORRECT:</strong> \"Configure VPC Flow Logs at the VPC level and write logs to Amazon S3. Use event notifications to trigger an AWS Lambda function to inspect the logs\" is incorrect.</p><p>VPC Flow Logs can capture log information relating to traffic flows but not the entire packet so will not be sufficient for intrusion detection. There is also not solution for sending the traffic to the intrusion detection instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html</a></p>"
        }
      },
      {
        "id": 83960210,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company maintains a robust security posture by use of AWS services like AWS Config, AWS Firewall Manager, Amazon GuardDuty, Amazon Inspector, Amazon Detective, and AWS Trusted Advisor. Earlier, the company was using a custom dashboard to aggregate information about its security footprint, the company has now decided to use AWS Security Hub to help assess its AWS environment vis-a-vis the security best practices.</p>\n\n<p>Which of the following statements are correct about Security Hub integration with other AWS services? (Select two)</p>\n",
          "answers": [
            "<p>AWS Trusted Advisor inspects your AWS environment and sends recommendations as findings to AWS Security Hub</p>",
            "<p>AWS Firewall Manager sends findings to Security Hub when AWS Shield Advanced is not protecting the resources</p>",
            "<p>AWS Security Hub sends the Amazon GuardDuty findings to Amazon Detective to visualize and investigate the findings</p>",
            "<p>Amazon Detective automatically collects log data from the integrated AWS resources and uses machine learning and graph theory to conduct faster and more efficient security investigations. These investigations are sent to AWS Security Hub as findings in JSON format</p>",
            "<p>AWS Security Hub doesn't retroactively detect and consolidate security findings that were generated before you enabled AWS Security Hub. However, as a global service, AWS Security Hub can consolidate findings from all AWS regions to a single S3 bucket of your choice</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>AWS Firewall Manager sends findings to AWS Security Hub when AWS Shield Advanced is not protecting the resources</strong></p>\n\n<p>This statement is true. AWS Firewall Manager sends findings to AWS Security Hub when AWS Shield Advanced is not protecting resources, or when an attack is identified. AWS Firewall Manager also sends findings when a web application firewall (WAF) policy for resources or a web access control list (web ACL) rule is not in compliance.</p>\n\n<p>After you enable Amazon Security Hub, this integration is automatically activated. AWS Firewall Manager immediately begins to send findings to AWS Security Hub.</p>\n\n<p><strong>AWS Security Hub sends the Amazon GuardDuty findings to Amazon Detective to visualize and investigate the findings</strong></p>\n\n<p>Amazon Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to help you visualize and conduct faster and more efficient security investigations.</p>\n\n<p>AWS Security Hub integration with Amazon Detective allows you to pivot from Amazon GuardDuty findings in Security Hub into Amazon Detective. You can then use the Detective tools and visualizations to investigate them. The integration does not require any additional configuration in AWS Security Hub or Amazon Detective.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Trusted Advisor inspects your AWS environment and sends recommendations as findings to AWS Security Hub</strong> - Trusted Advisor draws upon best practices learned from serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment and then makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps. Security Hub sends the results of its AWS Foundational Security Best Practices checks to Trusted Advisor. In short, Trusted Advisor receives findings from Security Hub.</p>\n\n<p><strong>Amazon Detective automatically collects log data from the integrated AWS resources and uses machine learning and graph theory to conduct faster and more efficient security investigations. These investigations are sent to AWS Security Hub as findings in JSON format</strong> - Amazon Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to help you visualize and conduct faster and more efficient security investigations. Amazon Detective receives findings from AWS Security Hub, rather than sending the findings to Security Hub.</p>\n\n<p><strong>AWS Security Hub doesn't retroactively detect and consolidate security findings that were generated before you enabled Security Hub. However, as a global service, Security Hub can consolidate findings from all AWS regions to a single S3 bucket of your choice</strong> - Indeed, Security Hub doesn't retroactively detect and consolidate security findings that were generated before you enabled Security Hub. However, Security Hub is a regional service and not a global service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-internal-providers.html\">https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-internal-providers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html\">https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html</a></p>\n"
        }
      },
      {
        "id": 76165386,
        "correct_response": [
          "a"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The IT Security team at a financial services firm has informed that a user's AWS access key has been found on the internet. As a security engineer, you must ensure that the access key is immediately disabled and the user's activities must be assessed for a potential breach.</p>\n\n<p>Which steps must be taken to meet the above needs?</p>\n",
          "answers": [
            "<p>Delete or rotate the user\u2019s key. Review the AWS CloudTrail logs in all AWS regions and delete any unauthorized resources created or updated</p>",
            "<p>Delete the IAM user and all the resources created by the user. Create fresh user credentials and relaunch the resources from this user</p>",
            "<p>Call on the user to remove the access credentials from the internet. Rotate the user's key and re-deploy all the resources with the new credentials</p>",
            "<p>Call on the user to remove the access credentials from the internet. Report abuse to AWS Trust &amp; Safety team</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Delete or rotate the user\u2019s key. Review the AWS CloudTrail logs in all AWS regions and delete any unauthorized resources created or updated</strong></p>\n\n<p>Deleting or rotating the user\u2019s access key ensures that it is not further used for any unauthorized activities. AWS CloudTrail logs will log the user access key usage which will help in tracking the AWS resources for which the key has been used. This information is valuable in narrowing down on any unauthorized resources created by using the access key or any existing resources modified by using the access key.</p>\n\n<p>Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK). As a best practice, use temporary security credentials (IAM roles) instead of access keys and disable any AWS account root user access keys.</p>\n\n<p>If you still need to use long-term access keys, you can create, modify, view, or rotate your access keys (access key IDs and secret access keys). You can have a maximum of two access keys. This allows you to rotate the active keys according to best practices.</p>\n\n<p>Rotating IAM user access keys from the console:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_RotateAccessKey\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_RotateAccessKey</a></p>\n\n<p>As AWS CloudTrail logs API activity for supported services, it provides an audit trail of your AWS account that you can use to track the history of an adversary. The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights, and does not require any tools to be installed. Refer to the complete list of steps below:</p>\n\n<p>Steps to investigate AWS CloudTrail:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q6-i2.jpg\">\nvia - <a href=\"https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/\">https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Delete the IAM user and all the resources created by the user. Create fresh user credentials and relaunch the resources from this user</strong> - Deleting the IAM user will also remove all the bonafide resources that are created by the user account (resources, policies, tags, S3 buckets, etc). Hence, this option is not a workable solution.</p>\n\n<p><strong>Call on the user to remove the access credentials from the internet. Rotate the user's key and re-deploy all the resources with the new credentials</strong> - This option does not mention the steps needed to access the historic usage of the access key, which is crucial to identify any potential breach.</p>\n\n<p><strong>Call on the user to remove the access credentials from the internet. Report abuse to AWS Trust &amp; Safety team</strong> - This option does not address any of the identified issues mentioned in the use case, such as, disabling the current key and assessing any potential breach.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/\">https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/security-logging-and-monitoring.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/security-logging-and-monitoring.html</a></p>\n"
        }
      },
      {
        "id": 83960168,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A business maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the business has moved from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all data to the Amazon S3 bucket, the business is looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data.</p>\n\n<p>How will you implement this requirement without adding the overhead of splitting the data into logical groups?</p>\n",
          "answers": [
            "<p>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>",
            "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>",
            "<p>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</p>",
            "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong></p>\n\n<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates.</p>\n\n<p>Note: Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong> - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement.</p>\n\n<p><strong>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect.</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</strong> - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n"
        }
      },
      {
        "id": 76165402,
        "correct_response": [
          "c",
          "d"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A Security Engineer has followed the best practices to set up a trusted IP address list for Amazon GuardDuty. However, GuardDuty is generating alert findings for the configured trusted IP addresses.</p>\n\n<p>Which of the following checks will you perform to ensure GuardDuty works as expected? (Select two)</p>\n",
          "answers": [
            "<p>Ensure that multiple trusted IP lists per AWS account per Region have been configured</p>",
            "<p>Ensure that in multi-account environments, GuardDuty generates findings for member accounts based on activity that involves IP addresses from the administrator's trusted IP lists</p>",
            "<p>Ensure that IP addresses added in the trusted IP list are publicly routable IPv4 addresses</p>",
            "<p>Ensure that the trusted IP lists are uploaded in the same AWS Region as your GuardDuty findings</p>",
            "<p>Ensure that the same IP is not enlisted on both a trusted IP list as well as a threat list, as it will be processed by the threat list on priority, thereby resulting in a finding</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Ensure that IP addresses added in the trusted IP list are publicly routable IPv4 addresses</strong></p>\n\n<p>Trusted IP lists and threat lists apply only to traffic destined for publicly routable IP addresses. The effects of a list apply to all VPC Flow Log and CloudTrail findings, but do not apply to DNS findings.</p>\n\n<p><strong>Ensure that the trusted IP lists are uploaded in the same AWS Region as your GuardDuty findings</strong></p>\n\n<p>Trusted IP lists and threat lists are account and Region-specific. At any given time, you can have only one uploaded trusted IP list per AWS account per Region. Whereas, you can have up to six uploaded threat lists per AWS account per Region.</p>\n\n<p>AWS suggested best practices to verify the trusted IP list settings:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q14-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/\">https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ensure that multiple trusted IP lists per AWS account per Region have been configured</strong> - This statement is incorrect. At any given time, you can have only one uploaded trusted IP list per AWS account per Region.</p>\n\n<p><strong>Ensure that in multi-account environments, GuardDuty generates findings for member accounts based on activity that involves IP addresses from the administrator's trusted IP lists</strong> - This statement is incorrect. In multi-account environments, GuardDuty generates findings for member accounts based on activity that involves known malicious IP addresses from the administrator's threat lists. It does not generate findings based on activity that involves IP addresses from the administrator's trusted IP lists.</p>\n\n<p><strong>Ensure that the same IP is not enlisted on both a trusted IP list as well as a threat list, as it will be processed by the threat list on priority, thereby resulting in a finding</strong> - If you include the same IP on both a trusted IP list and threat list it will be processed by the trusted IP list first, and will not generate a finding.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/\">https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/</a></p>\n"
        }
      },
      {
        "id": 73686840,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An international media company has recently migrated their operations to AWS, operating across multiple accounts within AWS Organizations. They have a critical need to log all user actions across these accounts for audit purposes. For certain key actions, they want to be immediately notified through an email list.</p><p>Which solution best fits their needs?</p>",
          "answers": [
            "<p>Configure an organizational trail with AWS CloudTrail, forwarding logs to CloudWatch Logs. Set a metric filter within CloudWatch Logs to catch specific actions and create a CloudWatch alarm to send messages to an SNS topic upon these actions.</p>",
            "<p>Implement CloudTrail and set it to direct logs to CloudWatch Logs. Create a metric filter in CloudWatch Logs to catch specific actions and create a CloudWatch alarm to send messages to an SQS queue.</p>",
            "<p>Establish an organizational trail with CloudTrail, storing logs in an S3 bucket. Set up an EC2 instance to scan the logs for specific actions and set it to publish messages to an SNS topic.</p>",
            "<p>Deploy CloudTrail and set it to store logs in an S3 bucket. Every hour, use Glue to create a Data Catalog that references the S3 bucket. Configure Athena to initiate queries against the Data Catalog to identify specific actions.</p>"
          ],
          "explanation": "<p>An organizational trail with CloudTrail logs all user actions across accounts. Forwarding these logs to CloudWatch Logs and setting a metric filter for specific actions allows an alarm to be set, which sends notifications in real-time through an SNS topic when these actions occur.</p><p><strong>CORRECT: </strong>\"Configure an organizational trail with AWS CloudTrail, forwarding logs to CloudWatch Logs. Set a metric filter within CloudWatch Logs to catch specific actions and create a CloudWatch alarm to send messages to an SNS topic upon these actions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement CloudTrail and set it to direct logs to CloudWatch Logs. Create a metric filter in CloudWatch Logs to catch specific actions and create a CloudWatch alarm to send messages to an SQS queue\" is incorrect.</p><p>This answer isn't correct as it doesn't consider multiple accounts and the SQS queue does not meet the requirement of notifying an email list.</p><p><strong>INCORRECT:</strong> \"Establish an organizational trail with CloudTrail, storing logs in an S3 bucket. Set up an EC2 instance to scan the logs for specific actions and set it to publish messages to an SNS topic\" is incorrect.</p><p>This answer isn't correct as it isn't as efficient or real-time as using CloudWatch Logs and metric filters, although it does use an organizational trail.</p><p><strong>INCORRECT:</strong> \"Deploy CloudTrail and set it to store logs in an S3 bucket. Every hour, use Glue to create a Data Catalog that references the S3 bucket. Configure Athena to initiate queries against the Data Catalog to identify specific actions\" is incorrect.</p><p>This answer isn't correct as it does not use organizational trails and therefore does not accommodate the multiple accounts within the organization.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>"
        }
      },
      {
        "id": 76165456,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>The security team at a company has set up an IAM user with full permissions for the EC2 service, yet the user is unable to start an Amazon EC2 instance after it was stopped for maintenance purposes. The instance would change its state to \"Pending\" but would eventually switch back to \"Stopped\" with the error \"client error on launch\". Upon investigating the issue, it was discovered that the EC2 instance had attached Amazon EBS volumes that were encrypted using a Customer Master Key (CMK). Detaching the encrypted volumes from the EC2 instance resolved the issue and allowed the user to start the instance successfully.</p>\n\n<p>Following is a snippet of the existing IAM user policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                    &lt;action&gt;\n            ],\n            \"Resource\": \"arn:aws:kms:&lt;region&gt;:&lt;accountId&gt;:key/kms-encryption-key-for-ebs\",\n            \"Condition\": &lt;condition&gt;\n        }\n    ]\n}\n</code></pre>\n\n<p>You have been tasked to build a solution to fix this issue. What do you recommend? (Select two)</p>\n",
          "answers": [
            "<p>Add the &lt;action&gt; as <code>kms:CreateGrant</code> to the IAM user policy</p>",
            "<p>Add the condition as <code>{ \"Bool\": { \"kms:GrantIsForAWSResource\": true }</code> to the IAM user policy</p>",
            "<p>Add the &lt;action&gt; as <code>kms:DescribeKey</code> to the IAM user policy</p>",
            "<p>Add the &lt;action&gt; as <code>kms:CreateKey</code> to the IAM user policy</p>",
            "<p>Add the condition as <code>{ \"Bool\": { \"kms:ViaService\": \"ec2.&lt;region&gt;.amazonaws.com\"}</code> to the IAM user policy</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Add the &lt;action&gt; as <code>kms:CreateGrant</code> to the IAM user policy</strong></p>\n\n<p>This issue occurs with EC2 instances with encrypted volumes attached if:</p>\n\n<p>The AWS Key Management Service (AWS KMS) or an AWS Identity and Access Management (IAM) user launching the instances doesn't have the required permissions.\nThe KMS key usage is restricted by the SourceIp condition key.\nThe IAM user must have permission from AWS KMS to decrypt the AWS KMS key.</p>\n\n<p>You should note that the default KMS key policy gives the AWS account that owns the KMS key permission to use IAM policies to allow access to all AWS KMS operations on the KMS key. To allow access to decrypt a KMS key, you must use the key policy with IAM policies or grants. IAM policies alone aren't sufficient to allow access to a KMS key, but you can use them in combination with a KMS key's policy (which is the default key policy in this case).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q64-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html\">https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html</a></p>\n\n<p><strong>Add the condition as <code>{ \"Bool\": { \"kms:GrantIsForAWSResource\": true }</code> to the IAM user policy</strong></p>\n\n<p>The condition <code>kms:GrantIsForAWSResource</code> allows or denies permission for the CreateGrant, ListGrants, or RevokeGrant operations only when an AWS service integrated with AWS KMS calls the operation on the user's behalf. This policy condition doesn't allow the user to call these grant operations directly. This policy condition can be applied to Key policies as well as IAM policies.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q64-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource\">https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add the &lt;action&gt; as <code>kms:DescribeKey</code> to the IAM user policy</strong> - <code>kms:EnableKey</code> controls permission to view detailed information about an AWS KMS key. This option is not relevant to the given use case.</p>\n\n<p><strong>Add the &lt;action&gt; as <code>kms:CreateKey</code> to the IAM user policy</strong> - <code>kms:CreateKey</code> controls permission to create an AWS KMS key that can be used to protect data keys and other sensitive information. Since the key is already created, this option is not relevant.</p>\n\n<p><strong>Add the condition as <code>{ \"Bool\": { \"kms:ViaService\": \"ec2.&lt;region&gt;.amazonaws.com\"}</code> to the IAM user policy</strong> - The <code>kms:ViaService</code> condition key limits the use of a KMS key to requests from specified AWS services. You can specify one or more services in each kms:ViaService condition key. The operation must be a KMS key resource operation, that is, an operation that is authorized for a particular KMS key. This option has been added as a distractor since the issue is with the EBS volumes (and not the EC2 service) that were encrypted using a CMK and the lack of sufficient key-specific grants thereof.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/encrypted-volumes-stops-immediately/\">https://aws.amazon.com/premiumsupport/knowledge-center/encrypted-volumes-stops-immediately/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource\">https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html\">https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html</a></p>\n"
        }
      },
      {
        "id": 112083289,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 4",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has as an AWS Organization for developers. The organization includes several accounts and SCPs are used to control access to AWS services. A single SCP exists at the root of the organization and has the following policy statements:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question/2023-01-05_03-20-17-63fdc92057a0a8cb88e757cd2193068f.jpg\"><p>A group of developers are working on a project that requires an Amazon RDS database. These developers use an account that is in a child OU with an SCP attached that allows all Amazon RDS API actions. The developers have full IAM permissions for RDS but are unable to launch RDS database instances.</p><p>Which change must a security engineer implement so that the developers can access Amazon RDS?</p>",
          "answers": [
            "<p>Add a statement to the SCP of the child OU that allows all actions on all resources.</p>",
            "<p>Remove the deny statement for Amazon RDS from the root SCP.</p>",
            "<p>Attach a resource-based policy to Amazon RDS granting launch permissions.</p>",
            "<p>Remove the root SCP from the root of the organization.</p>"
          ],
          "explanation": "<p>Inheritance for service control policies behaves like a filter through which permissions flow to all parts of the tree below. If an action is blocked by a Deny statement, then all OUs and accounts affected by that SCP are denied access to that action. An SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level. SCPs can <strong><em>only</em></strong> filter; they never add permissions.</p><p>In this case the root SCP denies all actions for Amazon RDS. This deny statement will flow down to all OUs and accounts in the hierarchy. Therefore it is not possible to allow access to RDS anywhere in the organization if this policy statement exists.</p><p>The simplest solution to this issue is to simply remove the deny statement for RDS at the root level.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_03-20-17-59111c1ef9f0c147d83cdfe31c3f7dfb.jpg\"><p><strong>CORRECT: </strong>\"Remove the deny statement for Amazon RDS from the root SCP\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Remove the root SCP from the root of the organization\" is incorrect.</p><p>You must always have a policy attached at the root and any OU level. You can remove the root SCP but only after you have attached another SCP.</p><p><strong>INCORRECT:</strong> \"Add a statement to the SCP of the child OU that allows all actions on all resources\" is incorrect.</p><p>The deny statement at the root SCP level overrides any allows anywhere else in the organization.</p><p><strong>INCORRECT:</strong> \"Attach a resource-based policy to Amazon RDS granting launch permissions\" is incorrect.</p><p>You cannot attach resource-based policies to Amazon RDS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html</a></p>"
        }
      },
      {
        "id": 83960184,
        "correct_response": [
          "b",
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has recently set up AWS Organizations to get all its AWS accounts under one organization to standardize the monitoring and compliance needs of the company. The company has the following requirements:</p>\n\n<p>a) All user actions have to be logged.\nb) Based on the company's security needs, define alarms that respond to specific user actions.\nc) Send real-time alerts for the alarms raised.</p>\n\n<p>Which of the following options can be combined to create an optimal solution for the given requirements? (Select two)</p>\n",
          "answers": [
            "<p>Define an AWS Lambda function to process the logs and send messages to an Amazon Simple Queue Service (Amazon SQS) queue</p>",
            "<p>Implement an AWS CloudTrail trail as an organizational trail. Configure the trail to forward the trail data to an Amazon CloudWatch Logs log group</p>",
            "<p>In CloudWatch Logs, set a metric filter for any user action event the company needs to track. Create an Amazon CloudWatch alarm against the metric. When triggered, the alarm sends notifications to the subscribed users through an Amazon Simple Notification Service (Amazon SNS) topic</p>",
            "<p>Implement an AWS CloudTrail trail as an organizational trail. Configure the trail to store logs in an Amazon S3 bucket</p>",
            "<p>Use Amazon Athena to analyze the logs and trigger notification to an Amazon Simple Notification Service (Amazon SNS) topic</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Implement an AWS CloudTrail trail as an organizational trail. Configure the trail to forward the trail data to an Amazon CloudWatch Logs log group</strong></p>\n\n<p><strong>In CloudWatch Logs, set a metric filter for any user action event the company needs to track. Create an Amazon CloudWatch alarm against the metric. When triggered, the alarm sends notifications to the subscribed users through an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>You can configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when specific activity occurs.</p>\n\n<ol>\n<li><p>Configure your trail to send log events to CloudWatch Logs.</p></li>\n<li><p>Define CloudWatch Logs metric filters to evaluate log events for matches in terms, phrases, or values. For example, you can monitor for ConsoleLogin events.</p></li>\n<li><p>Assign CloudWatch metrics to the metric filters.</p></li>\n<li><p>Create CloudWatch alarms that are triggered according to thresholds and time periods that you specify. You can configure alarms to send notifications when alarms are triggered so that you can take action.</p></li>\n</ol>\n\n<p>Note: Only the management account for an AWS Organizations organization can configure a CloudWatch Logs log group for an organization trail.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define an AWS Lambda function to process the logs and send messages to an Amazon Simple Queue Service (Amazon SQS) queue</strong> - Amazon SQS is ruled out since we need a near real-time notification solution and not a queue-based solution.</p>\n\n<p><strong>Implement an AWS CloudTrail trail as an organizational trail. Configure the trail to store logs in an Amazon S3 bucket</strong></p>\n\n<p><strong>Use Amazon Athena to analyze the logs and trigger a notification to an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>While it is possible to configure AWS CloudTrail to send trail logs to an Amazon S3 bucket and then use Amazon Athena to analyze the logs and trigger SNS notification, this is not an optimal solution for the given use case. As mentioned earlier, CloudWatch metrics and alarms can get the job done more optimally.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/monitor-cloudtrail-log-files-with-cloudwatch-logs.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/monitor-cloudtrail-log-files-with-cloudwatch-logs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html#cloudtrail-aws-service-specific-topics-integrations\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html#cloudtrail-aws-service-specific-topics-integrations</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-event-sources.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-event-sources.html</a></p>\n"
        }
      },
      {
        "id": 76165408,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A Security Engineer received a GuardDuty security alert pertaining to one of the Amazon EC2 instances that is attempting to communicate with the IP address of a remote host known to hold credentials and stolen data captured by malware. The Security Engineer immediately tried to isolate the instance by activating the isolation security group on the instance. However, within a few minutes, the engineer received a similar alert again.</p>\n\n<p>Which of the following represents the underlying reason for this behavior and what is the solution to remediate the issue?</p>\n",
          "answers": [
            "<p>When you associate multiple security groups with an instance, rules with deny access need to be mutually exclusive. Delete all the security groups and create only the isolation security group to isolate the compromised instance</p>",
            "<p>When you change a security group rule, its tracked connections are not immediately interrupted. The tracked connections need to be configured to change to untracked connections and then apply the isolation security group to isolate the compromised instance</p>",
            "<p>When the isolation security group is unable to isolate an instance, the immediate fix is to shut down the compromised instance to cut off further damage to your AWS resources</p>",
            "<p>If you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Hence, to isolate the instance, cut off Internet Gateway from the instance</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>When you change a security group rule, its tracked connections are not immediately interrupted. The tracked connections need to be configured to change to untracked connections and then apply the isolation security group to isolate the compromised instance</strong></p>\n\n<p>When you change a security group rule, its tracked connections are not immediately interrupted. The security group continues to allow packets until existing connections time out. To ensure that traffic is immediately interrupted, the tracked connections need to be configured to change to untracked connections and then apply the isolation security group to isolate the instance completely.</p>\n\n<p>An untracked flow of traffic is immediately interrupted if the rule that enables the flow is removed or modified. For example, if you have an open (0.0.0.0/0) outbound rule, and you remove a rule that allows all (0.0.0.0/0) inbound SSH (TCP port 22) traffic to the instance (or modify it such that the connection would no longer be permitted), your existing SSH connections to the instance are immediately dropped. The connection was not previously being tracked, so the change will break the connection. On the other hand, if you have a narrower inbound rule that initially allows an SSH connection (meaning that the connection was tracked), but change that rule to no longer allow new connections from the address of the current SSH client, the existing SSH connection is not interrupted because it is tracked.</p>\n\n<p>There are multiple ways to change tracked connections to being untracked. You can implement the isolation with an existing security group using the following steps:</p>\n\n<ol>\n<li>Identify the security group of the instance</li>\n<li>Delete all existing rules</li>\n<li>Create a single rule of 0.0.0.0/0 (0-65535) for all traffic in both inbound and outbound rules. This converts all existing and new traffic to being untracked.</li>\n<li>Remove the 0.0.0.0/0 (0-65535) inbound and outbound rules to terminate all connections and isolate the instance.</li>\n</ol>\n\n<p>Security Group level containment:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q17-i1.jpg\">\nvia - <a href=\"https://www.youtube.com/watch?v=pPCuCYrhIyI\">https://www.youtube.com/watch?v=pPCuCYrhIyI</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When you associate multiple security groups with an instance, rules with deny access need to be mutually exclusive. Delete all the security groups and create only the isolation security group to isolate the compromised instance</strong> - Security group rules can only allow traffic; you can't create rules that deny access. Hence, this option is incorrect.</p>\n\n<p><strong>When the isolation security group is unable to isolate an instance, the immediate fix is to shut down the compromised instance to cut off further damage to your AWS resources</strong> - Shutting down an instance is the last resort when trying to isolate the compromised instance. The reason is when an instance is shut down all the cache data is lost which is crucial in understanding the security breach that has taken place on the instance and the extent to which AWS resources have been compromised.</p>\n\n<p><strong>If you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Hence, to isolate the instance, cut off Internet Gateway from the instance</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway enables resources (like EC2 instances) in your public subnets to connect to the internet if the resource has a public IPv4 address or an IPv6 address. Internet Gateway operates at the VPC level and not at the instance level. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-connection-tracking.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-connection-tracking.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/automate-amazon-ec2-instance-isolation-by-using-tags/\">https://aws.amazon.com/blogs/security/automate-amazon-ec2-instance-isolation-by-using-tags/</a></p>\n"
        }
      },
      {
        "id": 73686822,
        "correct_response": [
          "d"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has on on-premises corporate identity provider (IdP) with thousands of corporate users. The company needs to allow the users to access a set of AWS services from the corporate network. The security engineer has been instructed that the company would prefer to avoid having multiple sets of identities and credentials to manage for each user.</p><p>Which actions will meet the requirements?</p>",
          "answers": [
            "<p>Create an Amazon Cognito identity pool and attach the corporate IdP. Use IAM user accounts to provide access to AWS resources.</p>",
            "<p>Create an AWS Managed Microsoft AD and create a two-way trust relationship. Run ADSync and assign IAM permissions to the synchronized user accounts in AWS.</p>",
            "<p>Establish an AWS Managed VPN connection to AWS. Assign permissions to AWS resources in the corporate IdP through resource-based policies.</p>",
            "<p>Enable federated access between the corporate IdP and the AWS account using IAM. Use IAM roles to provide access to AWS resources.</p>"
          ],
          "explanation": "<p>The best solution for this scenario is to enable federated access between the corporate IdP and the AWS account using IAM. In this configuration the users can assume roles in the AWS account and can access resources they are granted access to. This solution ensures that there is no duplication of identities or credentials as per the requirements.</p><img src=\"https://img-c.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_04-01-42-b979b7b6853a7bfe0bb8533e40c93a3c.jpg\"><p><strong>CORRECT: </strong>\"Enable federated access between the corporate IdP and the AWS account using IAM. Use IAM roles to provide access to AWS resources\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon Cognito identity pool and attach the corporate IdP. Use IAM user accounts to provide access to AWS resources\" is incorrect.</p><p>With Cognito identity pools users can access AWS resources by assuming IAM roles and gaining temporary security credentials. They do not get an IAM user account.</p><p><strong>INCORRECT:</strong> \"Create an AWS Managed Microsoft AD and create a two-way trust relationship. Run ADSync and assign IAM permissions to the synchronized user accounts in AWS\" is incorrect.</p><p>This would create a duplicate copy of the user accounts in the Microsoft AD (though ADSync is used for Azure, not an on-premises IdP).</p><p><strong>INCORRECT:</strong> \"Establish an AWS Managed VPN connection to AWS. Assign permissions to AWS resources in the corporate IdP through resource-based policies\" is incorrect.</p><p>You cannot assign resource-based policies to users in an external IdP.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/identity/federation/\">https://aws.amazon.com/identity/federation/</a></p>"
        }
      },
      {
        "id": 83960180,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company has migrated most of its legacy applications to AWS Cloud. Compliance guidelines mandate that the company must keep its data center on-premises and it must implement IPsec encryption for all network communications outside the premises. The workloads are sensitive to network latency vis-a-vis the data center.</p>\n\n<p>Which of the following would you suggest to implement a solution for AWS applications to connect to the data center?</p>\n",
          "answers": [
            "<p>Use AWS Direct Connect, as it encrypts in-transit traffic, by default</p>",
            "<p>Use AWS PrivateLink to establish a private connection between the AWS cloud and the on-premises data center</p>",
            "<p>Combine AWS Direct Connect with AWS Site-to-Site VPN</p>",
            "<p>Use VPC peering that supports MACsec to encrypt data from the on-premises data center to the AWS cloud</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Combine AWS Direct Connect with AWS Site-to-Site VPN</strong></p>\n\n<p>AWS Direct Connect does not encrypt your traffic that is in transit by default. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service.</p>\n\n<p>With AWS Direct Connect and AWS Site-to-Site VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.</p>\n\n<p>AWS Direct Connect + AWS Site-to-Site VPN:\n<img src=\"https://docs.aws.amazon.com/images/whitepapers/latest/aws-vpc-connectivity-options/images/aws-direct-connect-and-aws-site-to-site-vpn.png\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Direct Connect, as it encrypts in-transit traffic, by default</strong> - AWS Direct Connect does not encrypt your traffic that is in transit by default. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service.</p>\n\n<p><strong>Use AWS PrivateLink to establish a private connection between the AWS cloud and the on-premises data center</strong> - AWS PrivateLink provides private connectivity between virtual private clouds (VPCs), supported AWS services, and your on-premises networks without exposing your traffic to the public internet. Interface VPC endpoints, powered by PrivateLink, connect you to services hosted by AWS Partners and supported solutions available in AWS Marketplace.</p>\n\n<p><strong>Use VPC peering that supports MACsec to encrypt data from the on-premises data center to the AWS cloud</strong> - VPC peering cannot be used to connect the AWS cloud to the on-premises network.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html</a></p>\n"
        }
      },
      {
        "id": 83960160,
        "correct_response": [
          "a",
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A retail company has a three-tier web application with separate subnets for Web, Application, and Database tiers. The CTO at the company wants to monitor any malicious activity targeting the web application running on EC2 instances. You have been tasked with developing a solution to notify the security team in case the network exposure of EC2 instances on specific ports violates the security policies of the company.</p>\n\n<p>Which AWS Services would you use to build an automated notification system to meet these requirements with the least development effort? (Select two)</p>\n",
          "answers": [
            "<p>Amazon SNS</p>",
            "<p>Amazon Inspector</p>",
            "<p>AWS Shield</p>",
            "<p>Amazon CloudWatch</p>",
            "<p>VPC Flow Logs</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Amazon SNS</strong></p>\n\n<p><strong>Amazon Inspector</strong></p>\n\n<p>Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.</p>\n\n<p>You can perform network security assessments via your own custom solutions, however, that entails significant time and effort. You might need to run network port-scanning tools to test routing and firewall configurations, then validate what processes are listening on your instance network ports, before finally mapping the IPs identified in the port scan back to the host\u2019s owner.</p>\n\n<p>To make this process simpler for its customers, AWS offers the Network Reachability rules package in Amazon Inspector, which is an automated security assessment service that enables you to understand and improve the security and compliance of applications deployed on AWS. The existing Amazon Inspector host assessment rules packages check the software and configurations on your Amazon Elastic Compute Cloud (Amazon EC2) instances for vulnerabilities and deviations from best practices.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/\">https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/</a></p>\n\n<p>You can use these rules packages to analyze the accessibility of critical ports, as well as all other network ports. For critical ports, Amazon Inspector will show the exposure of each and will offer findings per port. When critical, well-known ports (based on Amazon\u2019s standard guidance) are reachable, findings will be created with higher severities.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q30-i2.jpg\">\nvia <a href=\"https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/\">https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/</a></p>\n\n<p>The findings also have recommendations that include information about exactly which Security Group you can edit to remove the access. And like all Amazon Inspector findings, these can be published to an SNS topic for additional processing or you could use a Lambda to automatically remove ingress rules in the Security Group to address a network reachability finding. For the given use case, the network engineer can use the SNS topic to send notifications to the team.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Shield</strong> - AWS Shield is a managed service that protects against Distributed Denial of Service (DDoS) attacks for applications running on AWS. AWS Shield Standard is automatically enabled for all AWS customers at no additional cost. AWS Shield Advanced is an optional paid service. AWS Shield Advanced provides additional protections against more sophisticated and larger attacks for your applications running on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53. AWS Shield cannot be used to assess network exposure of EC2 instances on specific ports.</p>\n\n<p><strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. CloudWatch cannot be used to assess network exposure of EC2 instances on specific ports.</p>\n\n<p><strong>VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. You can use VPC Flow Logs to assess network exposure of EC2 instances on specific ports but the solution would entail significant development effort to parse through the logs and identify the exposed ports. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/\">https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/</a></p>\n"
        }
      },
      {
        "id": 83960120,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>During regular maintenance tasks, an application support team noticed an abnormal activity on an Amazon EC2 instance that is configured with an EBS volume. The team immediately informed a Security Engineer of the anomaly. The instance is part of an Auto Scaling Group fronted by an Elastic Load Balancer.</p>\n\n<p>What immediate steps should the Security Engineer take for preventing any further attacks to secure the connecting systems and understand the root cause?</p>\n",
          "answers": [
            "<p>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group. Launch a new EC2 instance with a forensic toolkit, and allow the forensic toolkit image to connect to the suspicious instance to perform the investigation</p>",
            "<p>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group and snapshot the Amazon EBS data volumes that are attached to the EC2 instance. Launch an EC2 instance with a forensic toolkit and attach an EBS volume created from the snapshot of the suspicious EBS volume</p>",
            "<p>Remove the instance from the Auto Scaling group. Place the instance within an isolation security group. Launch an EC2 instance with a forensic toolkit, and use the forensic toolkit image to deploy another ENI to inspect all traffic coming from the suspicious instance</p>",
            "<p>Detach the instance from the Auto Scaling group and place it within an isolation security group. Detach the suspicious EBS volume. Launch an EC2 instance with a forensic toolkit and attach the detached EBS volume to investigate</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group and snapshot the Amazon EBS data volumes that are attached to the EC2 instance. Launch an EC2 instance with a forensic toolkit and attach an EBS volume created from the snapshot of the suspicious EBS volume</strong></p>\n\n<p>AWS recommends the following actions when a potential security anomaly is detected on your Amazon EC2 instance:</p>\n\n<ol>\n<li><p>Capture the metadata from the Amazon EC2 instance, before you make any changes to your environment.</p></li>\n<li><p>Protect the Amazon EC2 instance from accidental termination by enabling termination protection for the instance.</p></li>\n<li><p>Isolate the Amazon EC2 instance by switching the VPC Security Group. However, be aware of VPC connection tracking and other containment techniques.</p></li>\n<li><p>Detach the Amazon EC2 instance from any AWS Auto Scaling groups.</p></li>\n<li><p>Deregister the Amazon EC2 instance from any related Elastic Load Balancing service.</p></li>\n<li><p>Snapshot the Amazon EBS data volumes that are attached to the EC2 instance for preservation and follow-up investigations.</p></li>\n<li><p>Tag the Amazon EC2 instance as quarantined for investigation, and add any pertinent metadata, such as the trouble ticket associated with the investigation.</p></li>\n</ol>\n\n<p>You can perform all of the preceding steps using the AWS APIs, AWS SDKs, AWS CLI, and AWS Management Console. To interact with AWS using these methods, the IAM service helps you securely control access to AWS resources. You use IAM to control who is authenticated and authorized to use resources at the Account Level. The IAM service provides the authentication and authorization for you to perform these actions and interact with the service domain.</p>\n\n<p>A snapshot of an Amazon EBS volume is a point-in-time, block-level copy of an EBS data volume, which occurs asynchronously and might take time to complete, but it is a delta of that data going forward. You can create new EBS volumes from these copies and mount them to the forensic EC2 instance for deep analysis offline by forensic investigators.</p>\n\n<p>EC2 Instance Isolation and Snapshots:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q10-i1.jpg\">\nvia - <a href=\"https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf\">https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group. Launch a new EC2 instance with a forensic toolkit, and allow the forensic toolkit image to connect to the suspicious instance to perform the investigation</strong> - You cannot launch a new EC2 instance with a forensic toolkit and then connect it to the suspicious instance for investigation as this goes against the best practice of quarantining and isolating the suspicious instance. You must also note that placing the suspicious instance within an isolation security group would not allow any new connections from the forensic EC2 instance. Therefore, this option is incorrect.</p>\n\n<p><strong>Remove the instance from the Auto Scaling group. Place the instance within an isolation security group. Launch an EC2 instance with a forensic toolkit, and use the forensic toolkit image to deploy another ENI to inspect all traffic coming from the suspicious instance</strong> - You must quarantine and isolate the suspicious instance immediately. You must not inspect live traffic coming from the suspicious instance. You must also note that placing the suspicious instance within an isolation security group would not allow any new connections from the forensic EC2 instance. Hence this option is incorrect.</p>\n\n<p><strong>Detach the instance from the Auto Scaling group and place it within an isolation security group. Detach the suspicious EBS volume. Launch an EC2 instance with a forensic toolkit and attach the detached EBS volume to investigate</strong> - Connecting the suspicious EBS volume directly to another EC2 instance does not help contain the malicious activity. Also, the EBS volume connected to the EC2 instance is of great importance in investigating the root cause of the attack. A snapshot of EBS volume is necessary for data preservation and follow-up investigations. Hence, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf\">https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf</a></p>\n"
        }
      },
      {
        "id": 73686814,
        "correct_response": [
          "b"
        ],
        "source": "Neal Set 5",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company is deploying a web application that runs in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB will be configured to terminate a TLS connection from clients. Security requirements mandate that all TLS traffic to the ALB must remain secure even if the certificate private key is compromised.</p><p>How can a security engineer meet this requirement?</p>",
          "answers": [
            "<p>Create an HTTPS listener that uses a certificate that was imported into AWS Certificate Manager (ACM).</p>",
            "<p>Create an HTTPS listener that uses a predefined security policy that supports forward secrecy (FS).</p>",
            "<p>Create an HTTPS listener that uses the Server Order Preference security feature.</p>",
            "<p>Create a HTTPS listener that uses a custom security policy supports forward secrecy (FS).</p>"
          ],
          "explanation": "<p>Elastic Load Balancing uses a Secure Socket Layer (SSL) negotiation configuration, known as a security policy, to negotiate SSL connections between a client and the load balancer. A security policy is a combination of protocols and ciphers.</p><p>The protocol establishes a secure connection between a client and a server and ensures that all data passed between the client and your load balancer is private. A cipher is an encryption algorithm that uses encryption keys to create a coded message. Protocols use several ciphers to encrypt data over the internet.</p><p>During the connection negotiation process, the client and the load balancer present a list of ciphers and protocols that they each support, in order of preference. By default, the first cipher on the server's list that matches any one of the client's ciphers is selected for the secure connection.</p><p>Forward Secrecy (FS) uses a derived session key to provide additional safeguards against the eavesdropping of encrypted data. This prevents the decoding of captured data, even if the secret long-term key is compromised.</p><p>In this case the security engineer must select a predefined security policy that supports FS to meet the requirements of the scenario.</p><p><strong>CORRECT: </strong>\"Create an HTTPS listener that uses a predefined security policy that supports forward secrecy (FS)\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a HTTPS listener that uses a custom security policy supports forward secrecy (FS)\" is incorrect.</p><p>The ALB does not support custom security policies.</p><p><strong>INCORRECT:</strong> \"Create an HTTPS listener that uses a certificate that was imported into AWS Certificate Manager (ACM)\" is incorrect.</p><p>It doesn\u2019t make any difference whether the certificate was added manually, through ACM, or whether it was imported or generated by ACM.</p><p><strong>INCORRECT:</strong> \"Create an HTTPS listener that uses the Server Order Preference security feature\" is incorrect.</p><p>This is not relevant to the ALB. You must select a security policy and the ALB will perform the negotiation for protocols and ciphers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p>"
        }
      },
      {
        "id": 83960106,
        "correct_response": [
          "b"
        ],
        "source": "Stephan Set 1",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A rapidly growing e-commerce company stores all of its sensitive customer data in an Amazon S3 bucket. To ensure the safety and security of this data, the company has chosen to encrypt it using an AWS Key Management Service (AWS KMS) customer managed key. The company also uses AWS Lambda functions to perform various tasks within the same account as the S3 bucket. The Lambda functions need to access the data in the S3 bucket but the company must ensure that each Lambda function has its own programmatic access control permissions to use the KMS key.</p>\n\n<p>Which of the following options would you recommend?</p>\n",
          "answers": [
            "<p>Assign an IAM policy to each Lambda function that grants access to the KMS key</p>",
            "<p>Establish a Lambda execution role that provides access to the KMS key for each Lambda function</p>",
            "<p>Create a key grant for the Lambda service principal, and adjust the permissions as needed</p>",
            "<p>Set up each Lambda function to assume an IAM role that grants access to the AWS-managed KMS key for Amazon S3</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Establish a Lambda execution role that provides access to the KMS key for each Lambda function</strong></p>\n\n<p>A Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources. You provide an execution role when you create a function. When you invoke your function, Lambda automatically provides your function with temporary credentials by assuming this role. You don't have to call sts:AssumeRole in your function code. For Lambda to properly assume your execution role, the role's trust policy must specify the Lambda service principal (lambda.amazonaws.com) as a trusted service.</p>\n\n<p>For the given use case, you need to create a Lambda execution role that provides specific access permissions to use the KMS key for each Lambda function. This is a more efficient solution as it allows for easier management of access permissions for multiple functions. This allows the company to define the permissions for each Lambda function, ensuring that each function only has the necessary access to the KMS key to perform its intended task.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Assign an IAM policy to each Lambda function that grants access to the KMS key</strong> - You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines its permissions. You cannot assign an IAM policy to a Lambda function, so this option is incorrect.</p>\n\n<p><strong>Create a key grant for the Lambda service principal, and adjust the permissions as needed</strong> - A grant is a policy instrument that allows AWS principals to use KMS keys in cryptographic operations. It also can let them view a KMS key (DescribeKey) and create and manage grants. When authorizing access to a KMS key, grants are considered along with key policies and IAM policies. Grants are often used for temporary permissions because you can create one, use its permissions, and delete it without changing your key policies or IAM policies.</p>\n\n<p>Grants are a very flexible and useful access control mechanism. When you create a grant for a KMS key, the grant allows the grantee principal to call the specified grant operations on the KMS key provided that all conditions specified in the grant are met.</p>\n\n<p>The grantee principal can be any AWS principal, including an AWS account (root), an IAM user, an IAM role, a federated role or user, or an assumed role user. The grantee principal can be in the same account as the KMS key or a different account. However, the grantee principal cannot be a service principal, an IAM group, or an AWS organization. Therefore, this option is incorrect.</p>\n\n<p><strong>Set up each Lambda function to assume an IAM role that grants access to the AWS-managed KMS key for Amazon S3</strong> - This option is a distractor as it refers to an AWS-managed KMS key whereas the use case refers to the customer managed key. In addition, you should note that typically you would assume an IAM role within a Lambda function to access resources in another AWS account.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/grants.html\">https://docs.aws.amazon.com/kms/latest/developerguide/grants.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a></p>\n"
        }
      },
      {
        "id": 73686894,
        "correct_response": [
          "a"
        ],
        "source": "Neal Set 6",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A company manages an application that runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The NLB has access logs enabled which are being stored in an Amazon S3 bucket. A security engineer requires a solution to run ad hoc queries against the access logs to identify application access patterns.</p><p>How should the security engineer accomplish this task with the least amount of administrative overhead?</p>",
          "answers": [
            "<p>Create an Amazon Athena table that uses the S3 bucket containing the access logs. Run SQL queries using Athena.</p>",
            "<p>Write an AWS Lambda function to query the access logs. Use event notifications to trigger the Lambda functions when log entries are added.</p>",
            "<p>Import the access logs into Amazon CloudWatch Logs. Use CloudWatch Logs Insights to analyze the log data.</p>",
            "<p>Use the S3 copy command to copy logs to a separate bucket. Enable S3 analytics to analyze access patterns.</p>"
          ],
          "explanation": "<p>Amazon Athena is a serverless service you can use to run SQL queries against data in Amazon S3. You just need to point Athena to your data in Amazon S3, define the schema, and start querying using the built-in query editor. This is ideal for running ad-hoc queries on access logs stored in an S3 bucket.</p><p><strong>CORRECT: </strong>\"Create an Amazon Athena table that uses the S3 bucket containing the access logs. Run SQL queries using Athena\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the S3 copy command to copy logs to a separate bucket. Enable S3 analytics to analyze access patterns\" is incorrect.</p><p>There\u2019s no need to copy the data and S3 analytics is used to identify object access patterns for requests to S3 objects. It is used for storage class analytics. It does not help with identifying access patterns for your application by reading the file and looking at source IP addresses (for example).</p><p><strong>INCORRECT:</strong> \"Write an AWS Lambda function to query the access logs. Use event notifications to trigger the Lambda functions when log entries are added\" is incorrect.</p><p>This will be more complex and is less useful for running ad hoc queries as it is something that will run every time a file is added.</p><p><strong>INCORRECT:</strong> \"Import the access logs into Amazon CloudWatch Logs. Use CloudWatch Logs Insights to analyze the log data\" is incorrect.</p><p>You cannot natively import logs into CloudWatch Logs from Amazon S3. You may be able to achieve this with a custom Lambda function, but it will be more work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p><p><a href=\"https://aws.amazon.com/athena/features/\">https://aws.amazon.com/athena/features/</a></p>"
        }
      },
      {
        "id": 76165360,
        "correct_response": [
          "c"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>An AWS organization manages its security and compliance units through two different AWS accounts. Both the accounts need AWS Config configuration and compliance data from multiple AWS accounts and Regions to get a centralized view of the resource inventory. Currently, the teams use shared access to the management account to fetch the required data.</p>\n\n<p>To enforce enhanced security measures, the company is looking at eliminating the need to share management account credentials with the team. As a Security Engineer, how will you implement this requirement with the least time and effort?</p>\n",
          "answers": [
            "<p>Use OpsCenter capability of AWS Systems Manager with AWS Config to detect a resource that is out of compliance and automate remediation</p>",
            "<p>Configure AWS Systems Manager Explorer with a customizable operations dashboard that displays information from AWS Config</p>",
            "<p>Use the aggregator feature of AWS Config to provide access to AWS Config data to both accounts without the need to share the management account details</p>",
            "<p>Use Configuration snapshots feature of AWS Config to create point-in-time capture of all your resources and their configurations. Save these snapshots in an Amazon S3 bucket and analyze the data using AWS Athena for trends and security aberrations</p>"
          ],
          "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the aggregator feature of AWS Config to provide access to AWS Config data to both accounts without the need to share the management account details</strong> - An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from multiple AWS accounts and Regions into a single account and Region to get a centralized view of your resource inventory and compliance.</p>\n\n<p>You can also use an aggregator to collect configuration and compliance data from an organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled. Previously, organization-wide data aggregation was available only from the organization management account, but AWS Config recently now supports organization-wide resource data aggregation in a delegated administrator account.</p>\n\n<p>A delegated administrator account is an account in an AWS organization that is granted additional administrative permissions for a specified AWS service. This means that in addition to the management account, you can also use a delegated admin account to aggregate data from all the member accounts in AWS Organizations without any additional authorization. With this capability, different teams in an organization (auditing, security, or compliance) can use separate accounts and aggregate organization-wide data in their respective administration accounts for centralized governance. This capability also eliminates the need for those teams to gain access to the management account to fetch the aggregated data.</p>\n\n<p>How AWS Config Aggregator works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use OpsCenter capability of AWS Systems Manager with AWS Config to detect a resource that is out of compliance and automate remediation</strong> - OpsCenter is a Systems Manager capability that provides a central location where operations engineers, IT professionals, and others can view, investigate, and resolve operational issues related to their environment. You can use a central account to view operational issues in another account (e.g. impaired instances, degraded storage volumes, or non-compliant resources), view pertinent diagnostic information for each issue, and use pre-defined automation runbooks to remediate the issues.</p>\n\n<p>OpsCenter is designed to reduce mean time to resolution (MTTR) for impacted AWS and hybrid cloud resources. For the AWS resource, OpsCenter aggregates information from AWS Config, AWS CloudTrail logs, and Amazon CloudWatch Events, so you don't have to navigate across multiple console pages during your investigation.</p>\n\n<p><strong>Configure AWS Systems Manager Explorer with a customizable operations dashboard that displays information from AWS Config</strong> - AWS Systems Manager Explorer is a customizable operations dashboard that reports information about your AWS resources. Explorer displays an aggregated view of operations data (OpsData) for your AWS accounts and across AWS Regions. Explorer displays information from supporting AWS services like AWS Config, AWS Trusted Advisor, AWS Compute Optimizer, and AWS Support (support cases).</p>\n\n<p>For Multiple-account/multiple-Region support, Explorer aggregates all account data into a management account. This is exactly what the use case wants to avoid, hence this is not the right option.</p>\n\n<p><strong>Use the Configuration snapshots feature of AWS Config to create point-in-time capture of all your resources and their configurations. Save these snapshots in an Amazon S3 bucket and analyze the data using AWS Athena for trends and security aberrations</strong> - AWS Config provides you with a configuration snapshot, which is a point-in-time capture of all your resources and their configurations. Configuration snapshots are generated on demand by using the AWS CLI or API and delivered to the Amazon S3 bucket that you specify. But saving to S3 buckets and analyzing from buckets is not an optimal way of implementing that asked requirement when the aggregator feature of AWS Config is tailor-made for such requirements.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/\">https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2022/11/aws-systems-manager-opscenter-supports-managing-opsitems-across-accounts/\">https://aws.amazon.com/about-aws/whats-new/2022/11/aws-systems-manager-opscenter-supports-managing-opsitems-across-accounts/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/Explorer.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/Explorer.html</a></p>\n"
        }
      },
      {
        "id": 76165332,
        "correct_response": [
          "d",
          "e"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>A project manager has connected with you for the resolution of an issue. Although an AWS Identity and Access Management (IAM) entity has admin permissions, it has received an access denied error.</p>\n\n<p>As an AWS Certified Security Specialist, how will you troubleshoot and resolve this issue? (Select two)</p>\n",
          "answers": [
            "<p>A resource-based policy defines the maximum permissions that an identity-based policy can grant to an entity. Check for any restrictive resource-based policies</p>",
            "<p>An Organization NACL can restrict access to member account IAM entities. Check for restrictions coming from an NACL using the management account of the Organization</p>",
            "<p>If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the concerned resource-based policy. Check for any restrictive permissions boundary</p>",
            "<p>If the requests are routed through a VPC endpoint, then check for any restrictions coming from the associated VPC endpoint policy</p>",
            "<p>A session policy is in place and is causing an authorization issue</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>If the requests are routed through a VPC endpoint, then check for any restrictions coming from the associated VPC endpoint policy</strong></p>\n\n<p>A VPC endpoint policy is a resource-based policy that you can attach to a VPC endpoint. It can restrict access to IAM entities. If you route your requests through a VPC endpoint, then check for any restrictions coming from the associated VPC endpoint policy.</p>\n\n<p><strong>A session policy is in place and is causing an authorization issue</strong></p>\n\n<p>Session policies can be passed programmatically when you create a temporary session for your IAM role for a federated user. The permissions for a session are at the intersection of the identity-based policies assigned to the IAM entity that the session is created for and the session policy itself. Check if a session policy is passed for your IAM role session using the AWS CloudTrail logs for <code>AssumeRole/AssumeRoleWithSAML/AssumeRoleWithWebIdentity</code> API calls. To check for session policies passed for a federated user session, check CloudTrail logs for GetFederationToken API calls.</p>\n\n<p>Resolving authorization issues for IAM entities with admin permissions\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q34-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A resource-based policy defines the maximum permissions that an identity-based policy can grant to an entity. Check for any restrictive resource-based policies</strong> - A permission boundary defines the maximum permissions that an identity-based policy can grant to an entity, not a resource-based policy. If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the permissions boundary. So the definition of the resource-based policy is incorrect in this statement.</p>\n\n<p><strong>An Organization NACL can restrict access to member account IAM entities. Check for restrictions coming from an NACL using the management account of the Organization</strong> - An Organization has a Service Control Policy (SCP) for restricting access to a service. A Network Access Control List (NACL) is an optional layer of security for a VPC that acts as a firewall for controlling traffic in and out of one or more subnets.</p>\n\n<p><strong>If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the concerned resource-based policy. Check for any restrictive permissions boundary</strong> - A permissions boundary limits the actions your entity can perform. If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the permissions boundary. So the reference to the resource-based policy is incorrect in this statement.</p>\n\n<p>Evaluating effective permissions with boundaries:\n<img src=\"https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/EffectivePermissions-rbp-boundary-id.png\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n"
        }
      },
      {
        "id": 76165394,
        "correct_response": [
          "a",
          "d"
        ],
        "source": "Stephan Set 2",
        "isNew": false,
        "isAlwaysIncorrect": false,
        "prompt": {
          "question": "<p>After a recent DDoS assault, the IT security team of a media company has asked the Security Engineer to revamp the security of the application to prevent future attacks. The website is hosted on an Amazon EC2 instance and data is maintained on Amazon RDS. A large part of the application data is static and this data is in the form of images.</p>\n\n<p>Which of the following steps can be combined to constitute the revamped security model? (Select two)</p>\n",
          "answers": [
            "<p>Use Amazon Route 53 to distribute traffic</p>",
            "<p>Configure the Amazon EC2 instance with an Auto Scaling Group (ASG) to scale in case of a DDoS assault. Front the ASG with AWS Web Application Firewall (AWS WAF) for another layer of security</p>",
            "<p>Use Global Accelerator to distribute traffic</p>",
            "<p>Move the static content to Amazon S3, and front this with an Amazon CloudFront distribution. Configure another layer of protection by adding AWS Web Application Firewall (AWS WAF) to the CloudFront distribution</p>",
            "<p>Configure Amazon Inspector with AWS Security Hub to mitigate DDoS attacks by continual scanning that delivers near real-time vulnerability findings</p>"
          ],
          "explanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon Route 53 to distribute traffic</strong></p>\n\n<p><strong>Move the static content to Amazon S3, and front this with an Amazon CloudFront distribution. Configure another layer of protection by adding AWS Web Application Firewall (AWS WAF) to the CloudFront distribution</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting.</p>\n\n<p>AWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync \u2013 services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end users. Blocked requests are stopped before they reach your web servers.</p>\n\n<p>Route 53 DNS requests and subsequent application traffic routed through CloudFront are inspected inline. Always-on monitoring, anomaly detection, and mitigation against common infrastructure DDoS attacks such as SYN/ACK floods, UDP floods, and reflection attacks are built into both Route 53 and CloudFront.</p>\n\n<p>Route 53 is also designed to withstand DNS query floods, which are real DNS requests that can continue for hours and attempt to exhaust DNS server resources. Route 53 uses shuffle sharding and anycast striping to spread DNS traffic across edge locations and help protect the availability of the service.</p>\n\n<p>When used with Amazon CloudFront distribution, AWS Shield adds security against DDoS attacks.</p>\n\n<p>AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection.</p>\n\n<p>All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the Amazon EC2 instance with an Auto Scaling Group (ASG) to scale in case of a DDoS assault. Front the ASG with AWS Web Application Firewall (AWS WAF) for another layer of security</strong> - AWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync \u2013 services that AWS customers commonly use to deliver content for their websites and applications. WAF cannot be directly configured in front of an ASG, so this option is incorrect.</p>\n\n<p><strong>Use Global Accelerator to distribute traffic</strong> - Global Accelerator is effective in traffic distribution across AWS Regions. However, the given use case needs services that can help mitigate DDoS attacks.</p>\n\n<p><strong>Configure Amazon Inspector with AWS Security Hub to mitigate DDoS attacks by continual scanning that delivers near real-time vulnerability findings</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. It cannot be used to mitigate DDoS attacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos-attacks-by-using-amazon-cloudfront-and-amazon-route-53/\">https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos-attacks-by-using-amazon-cloudfront-and-amazon-route-53/</a></p>\n\n<p><a href=\"https://aws.amazon.com/waf/faqs/b\">https://aws.amazon.com/waf/faqs/</a></p>\n"
        }
      }
    ],
    "answers": {}
  }
]